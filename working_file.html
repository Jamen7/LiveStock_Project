<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>working_file</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="working_file_files/libs/clipboard/clipboard.min.js"></script>
<script src="working_file_files/libs/quarto-html/quarto.js"></script>
<script src="working_file_files/libs/quarto-html/popper.min.js"></script>
<script src="working_file_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="working_file_files/libs/quarto-html/anchor.min.js"></script>
<link href="working_file_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="working_file_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="working_file_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="working_file_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="working_file_files/libs/bootstrap/bootstrap-8a79a254b8e706d3c925cde0a310d4f0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>



</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Loading the data</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="d48067de" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> local_settings <span class="im">import</span> OPENAI_KEY <span class="co"># Assumes you have a local_settings.py file in your folder with your OpenAI key  </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the OpenAI client</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> OpenAI(api_key<span class="op">=</span>OPENAI_KEY) <span class="co"># I need a new API_key</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="582d81aa" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itables <span class="im">import</span> show</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>rw_data <span class="op">=</span> pd.read_excel(<span class="st">"livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx"</span>, sheet_name<span class="op">=</span><span class="st">"Sheet1"</span>, header<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are 100 articles in the dataset with 24 columns, for the analysis: columns doi, first_page, last_page, etc can be remove since they do not contain necessary information at this stage</p>
<div id="90580598" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>clean_data <span class="op">=</span> rw_data.drop(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"doi"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"first_page"</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"last_page"</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"conference_series"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"volume"</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"number"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"journal"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"url"</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>yearVcount <span class="op">=</span> clean_data.groupby([<span class="st">"year"</span>, <span class="st">"publication_type"</span>]).count()[<span class="st">"abstract"</span>]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>fig_line <span class="op">=</span> px.line(</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    clean_data.groupby(<span class="st">"year"</span>).count()[<span class="st">"abstract"</span>],</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Abstracts trends from 1994 on Livestock Detection"</span>,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>{<span class="st">"value"</span>: <span class="st">"Abstract Count"</span>, <span class="st">"year"</span>: <span class="st">"Year"</span>},</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>fig_line.show()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>yearVcount <span class="op">=</span> yearVcount.to_frame().reset_index()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>fig_bar <span class="op">=</span> px.bar(</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    yearVcount,</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"year"</span>,</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"abstract"</span>,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"publication_type"</span>,</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Abstracts trends from 1994 on Livestock Detection"</span>,</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>{<span class="st">"abstract"</span>: <span class="st">"Abstract Count"</span>, <span class="st">"year"</span>: <span class="st">"Year"</span>, <span class="st">"publication_type"</span>: <span class="st">"Type"</span>},</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># nbins=40,</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>fig_bar.show()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># clean_data["year"].sort_values(ascending=False).head(20)</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="co"># def llm_chat(message):</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="co">#     response = client.chat.completions.create(</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co">#         model="gpt-4o-mini", messages=[{"role": "user", "content": message}]</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="co">#     )</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co">#     return response.choices[0].message.content</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="co"># # Test the function here</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="co"># llm_chat("What is Python (the language) named after?")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="63ede3b8-44dc-4939-9b2f-bf990a29a8c3" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("63ede3b8-44dc-4939-9b2f-bf990a29a8c3")) {                    Plotly.newPlot(                        "63ede3b8-44dc-4939-9b2f-bf990a29a8c3",                        [{"hovertemplate":"variable=abstract\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"abstract","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"abstract","orientation":"v","showlegend":true,"x":[1994.0,1997.0,2001.0,2007.0,2009.0,2011.0,2013.0,2014.0,2015.0,2016.0,2017.0,2018.0,2019.0,2020.0,2021.0,2022.0,2023.0,2024.0],"xaxis":"x","y":[1,2,1,1,1,4,1,2,4,4,4,2,9,13,2,16,13,16],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Abstract Count"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Abstracts trends from 1994 on Livestock Detection"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('63ede3b8-44dc-4939-9b2f-bf990a29a8c3');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="c3da8b40-129a-4d60-bb47-3350d83ccf1a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c3da8b40-129a-4d60-bb47-3350d83ccf1a")) {                    Plotly.newPlot(                        "c3da8b40-129a-4d60-bb47-3350d83ccf1a",                        [{"alignmentgroup":"True","hovertemplate":"Type=Patent\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Patent","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"Patent","offsetgroup":"Patent","orientation":"v","showlegend":true,"textposition":"auto","x":[1994.0,1997.0,2001.0,2007.0,2009.0,2011.0,2013.0,2015.0,2016.0,2017.0,2019.0,2020.0],"xaxis":"x","y":[1,2,1,1,1,1,1,1,3,2,5,2],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Type=Journal Article\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Journal Article","marker":{"color":"#EF553B","pattern":{"shape":""}},"name":"Journal Article","offsetgroup":"Journal Article","orientation":"v","showlegend":true,"textposition":"auto","x":[2011.0,2015.0,2016.0,2019.0,2020.0,2021.0,2022.0,2023.0,2024.0],"xaxis":"x","y":[1,2,1,2,8,2,5,7,14],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Type=Book Chapter\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Book Chapter","marker":{"color":"#00cc96","pattern":{"shape":""}},"name":"Book Chapter","offsetgroup":"Book Chapter","orientation":"v","showlegend":true,"textposition":"auto","x":[2014.0,2018.0,2020.0,2022.0,2024.0],"xaxis":"x","y":[1,1,1,1,2],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Type=Proceedings Article\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Proceedings Article","marker":{"color":"#ab63fa","pattern":{"shape":""}},"name":"Proceedings Article","offsetgroup":"Proceedings Article","orientation":"v","showlegend":true,"textposition":"auto","x":[2014.0,2015.0,2017.0,2018.0,2020.0,2022.0,2023.0],"xaxis":"x","y":[1,1,1,1,1,9,5],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Type=Posted Content\u003cbr\u003eYear=%{x}\u003cbr\u003eAbstract Count=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"Posted Content","marker":{"color":"#FFA15A","pattern":{"shape":""}},"name":"Posted Content","offsetgroup":"Posted Content","orientation":"v","showlegend":true,"textposition":"auto","x":[2023.0],"xaxis":"x","y":[1],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Abstract Count"}},"legend":{"title":{"text":"Type"},"tracegroupgap":0},"title":{"text":"Abstracts trends from 1994 on Livestock Detection"},"barmode":"relative"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('c3da8b40-129a-4d60-bb47-3350d83ccf1a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div id="1ca21fbe" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import pandas as pd</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> (</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx"</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_excel(file_path, sheet_name<span class="op">=</span><span class="st">"Sheet1"</span>).drop(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"doi"</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"first_page"</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"last_page"</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"conference_series"</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"volume"</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"number"</span>,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"journal"</span>,</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"url"</span>,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>).drop_duplicates(subset<span class="op">=</span><span class="st">'abstract'</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert 'year' column to numeric</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"year"</span> <span class="kw">in</span> df.columns:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"year"</span>] <span class="op">=</span> pd.to_numeric(df[<span class="st">"year"</span>], errors<span class="op">=</span><span class="st">"coerce"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Abstract Counts vs. Year using matplotlib</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>abstract_counts <span class="op">=</span> df.groupby(<span class="st">"year"</span>).size()</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>plt.bar(abstract_counts.index, abstract_counts.values, color<span class="op">=</span><span class="st">"skyblue"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Year"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Number of Abstracts"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Abstract Counts per Year"</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Slice Data for Repeated Authors</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"authors"</span> <span class="kw">in</span> df.columns:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    authors_series <span class="op">=</span> (</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">"authors"</span>].dropna().<span class="bu">str</span>.split(<span class="st">","</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Assuming authors are separated by ','</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    authors_flat <span class="op">=</span> [author.strip() <span class="cf">for</span> authors <span class="kw">in</span> authors_series <span class="cf">for</span> author <span class="kw">in</span> authors]</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    author_counts <span class="op">=</span> pd.Series(authors_flat).value_counts()</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    repeated_authors <span class="op">=</span> author_counts[author_counts <span class="op">&gt;</span> <span class="dv">1</span>].index.tolist()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    repeated_authors_df <span class="op">=</span> df[</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">"authors"</span>].<span class="bu">str</span>.contains(<span class="st">"|"</span>.join(repeated_authors), na<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Repeated Authors Data:"</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(repeated_authors_df[[<span class="st">"authors"</span>, <span class="st">"year"</span>, <span class="st">"Methods Used"</span>]])</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Slice Data for Latest Patents</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"publication_type"</span> <span class="kw">in</span> df.columns <span class="kw">and</span> <span class="st">"year"</span> <span class="kw">in</span> df.columns:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    patents_df <span class="op">=</span> df[df[<span class="st">"publication_type"</span>].<span class="bu">str</span>.contains(<span class="st">"patent"</span>, case<span class="op">=</span><span class="va">False</span>, na<span class="op">=</span><span class="va">False</span>)]</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    latest_patents_df <span class="op">=</span> patents_df.sort_values(by<span class="op">=</span><span class="st">"year"</span>, ascending<span class="op">=</span><span class="va">False</span>).drop(columns<span class="op">=</span>[<span class="st">'Literature Survey'</span>, <span class="st">'Limitations'</span>, <span class="st">'Future Research'</span>, <span class="st">'Research Gap'</span>])</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Latest Patents Data:"</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(latest_patents_df[[<span class="st">"title"</span>, <span class="st">"year"</span>, <span class="st">"authors"</span>]].head())</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Oldest Patents Data:"</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(latest_patents_df[[<span class="st">"title"</span>, <span class="st">"year"</span>, <span class="st">"authors"</span>]].tail())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="working_file_files/figure-html/cell-5-output-1.png" width="808" height="470" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Repeated Authors Data:
                                              authors    year  \
4        Chao Zuo, Liang Han, Pin Tao, Xiang lei Meng  2020.0   
6   Wangli Hao, Li Zhang, S H Xu, Meng Han, Fuzhon...  2024.0   
9                 Liang Han, Pin Tao, Ralph R. Martin  2019.0   
11                                        Zishun Zhou  2022.0   
18                                             Su Rui  2020.0   
22  Xingyu Chen, Xiaodong Ye, Miao Li, Zhixian Son...  2023.0   
23  Xingyu Chen, Xiaodong Ye, Miao Li, Hualong Li,...  2022.0   
32                                        Zishun Zhou     NaN   
36  Wangli Hao, Li Zhang, Meng Han, Kai Zhang, Fuz...  2023.0   
65                                             Su Rui  2019.0   
81  Jasper A. J. Eikelboom, Johan Wind, Eline van ...  2019.0   
94                                         Li Minghua  2016.0   
96                                         Li Minghua  2016.0   

                                         Methods Used  
4    - The paper proposes an improved livestock de...  
6    - The paper introduces a novel model called Y...  
9    - The paper presents a livestock detection al...  
11   - The paper proposes an advanced improved YOL...  
18   - The method involves acquiring an original i...  
22   - The paper proposes an object detection meth...  
23   - The paper proposes a Light Attention YOLO m...  
32   - The paper proposes an advanced improved YOL...  
36   - The paper proposes a novel model called YOL...  
65   - The method involves acquiring an original i...  
81   - The study evaluated the performance of a mu...  
94   - The automatic counting and weighing equipme...  
96   - The livestock automatic counting weighing a...  
Latest Patents Data:
                                                title    year         authors
18  Livestock quantity identification method and a...  2020.0          Su Rui
98  Animal count identification method, device, me...  2020.0   Wang Huaiqing
48  Smart farm livestock management system based o...  2019.0  Choi Seung Kyu
75  A video surveillance apparatus for detecting a...  2019.0  Joo Young Hoon
65  livestock number identification method and device  2019.0          Su Rui
Oldest Patents Data:
                                                title    year  \
97                            Animal detection system  2007.0   
78            Automated system for counting livestock  2001.0   
84  Animal detection sensor and animal number mana...  1997.0   
89                 Livestock identification apparatus  1997.0   
95  Automatic system for separating, counting and ...  1994.0   

                                              authors  
97  Sudo Tomonori, Fujii Hiroyuki, Oda Toshinori, ...  
78                                        Jerry Starr  
84              Hashimoto Kazuhiko, Yoshiike Nobuyuki  
89  Lars H. Andersson, Joseph S. Sheen, William E....  
95                                  Jean-Louis Hubert  </code></pre>
</div>
</div>
<div id="882e227a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop NaN values in the "Methods Used" column</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>methods_text <span class="op">=</span> df[<span class="st">"Methods Used"</span>].dropna().<span class="bu">str</span>.lower()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize and count common method-related words/phrases</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>method_words <span class="op">=</span> []</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> methods_text:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> text.split()  <span class="co"># Simple split, can be improved with NLP techniques</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    method_words.extend(words)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the most common method words (filtering out generic terms manually may be needed)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>common_methods <span class="op">=</span> Counter(method_words).most_common(<span class="dv">50</span>)  <span class="co"># Checking top words first</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract meaningful method names (refining this further using domain knowledge)</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>common_method_names <span class="op">=</span> [method <span class="cf">for</span> method, count <span class="kw">in</span> common_methods <span class="cf">if</span> <span class="bu">len</span>(method) <span class="op">&gt;</span> <span class="dv">3</span>][</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    :<span class="dv">10</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the dataset for rows that contain these methods</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>filtered_df <span class="op">=</span> df[</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"Methods Used"</span>].<span class="bu">str</span>.contains(<span class="st">"|"</span>.join(common_method_names), case<span class="op">=</span><span class="va">False</span>, na<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Select relevant columns</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> filtered_df[[<span class="st">"Methods Used"</span>, <span class="st">"Results"</span>]].head(<span class="dv">10</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the extracted DataFrame</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>show(result_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table id="itables_d87b9b1d_7aa9_44b5_bc99_d3683e1ef4f8" class="display nowrap" data-quarto-disable-processing="true" style="table-layout:auto;width:auto;margin:auto;caption-side:bottom">
<thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Methods Used</th>
      <th>Results</th>
    </tr>
  </thead><tbody><tr>
<td style="vertical-align:middle; text-align:left">
<div style="float:left; margin-right: 10px;">
<a href="https://mwouts.github.io/itables/"><svg class="main-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" width="64" viewbox="0 0 500 400" style="font-family: 'Droid Sans', sans-serif;">
    <g style="fill:#d9d7fc">
        <path d="M100,400H500V357H100Z"></path>
        <path d="M100,300H400V257H100Z"></path>
        <path d="M0,200H400V157H0Z"></path>
        <path d="M100,100H500V57H100Z"></path>
        <path d="M100,350H500V307H100Z"></path>
        <path d="M100,250H400V207H100Z"></path>
        <path d="M0,150H400V107H0Z"></path>
        <path d="M100,50H500V7H100Z"></path>
    </g>
    <g style="fill:#1a1366;stroke:#1a1366;">
   <rect x="100" y="7" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="5s" repeatcount="indefinite"></animate>
      <animate attributename="x" values="100;100;500" dur="5s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="0" y="107" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="3.5s" repeatcount="indefinite"></animate>
    <animate attributename="x" values="0;0;400" dur="3.5s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="100" y="207" width="300" height="43">
    <animate attributename="width" values="0;300;0" dur="3s" repeatcount="indefinite"></animate>
    <animate attributename="x" values="100;100;400" dur="3s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="100" y="307" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="4s" repeatcount="indefinite"></animate>
      <animate attributename="x" values="100;100;500" dur="4s" repeatcount="indefinite"></animate>
  </rect>
        <g style="fill:transparent;stroke-width:8; stroke-linejoin:round" rx="5">
            <g transform="translate(45 50) rotate(-45)">
                <circle r="33" cx="0" cy="0"></circle>
                <rect x="-8" y="32" width="16" height="30"></rect>
            </g>

            <g transform="translate(450 152)">
                <polyline points="-15,-20 -35,-20 -35,40 25,40 25,20"></polyline>
                <rect x="-15" y="-40" width="60" height="60"></rect>
            </g>

            <g transform="translate(50 352)">
                <polygon points="-35,-5 0,-40 35,-5"></polygon>
                <polygon points="-35,10 0,45 35,10"></polygon>
            </g>

            <g transform="translate(75 250)">
                <polyline points="-30,30 -60,0 -30,-30"></polyline>
                <polyline points="0,30 -30,0 0,-30"></polyline>
            </g>

            <g transform="translate(425 250) rotate(180)">
                <polyline points="-30,30 -60,0 -30,-30"></polyline>
                <polyline points="0,30 -30,0 0,-30"></polyline>
            </g>
        </g>
    </g>
</svg>
</a>
</div>
<div>
Loading ITables v2.2.2 from the internet...
(need <a href="https://mwouts.github.io/itables/troubleshooting.html">help</a>?)</div></td>

</tr></tbody>

</table>
<link href="https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css" rel="stylesheet">
<script type="module">
    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';

    document.querySelectorAll("#itables_d87b9b1d_7aa9_44b5_bc99_d3683e1ef4f8:not(.dataTable)").forEach(table => {
        if (!(table instanceof HTMLTableElement))
            return;

        // Define the table data
        const data = [[0, " - The paper proposes a livestock management and monitoring system that utilizes the end-to-end deep learning model You Only Look Once version 5 (YOLOv5) for feature extraction and detection of livestock of different sizes, enabling accurate counting and tracking of individual animals in various conditions.\\n\\n- It also incorporates data augmentation techniques using a real-time Kaggle dataset and employs the Roboflow application for scaling, resizing, and manipulating the dataset, while comparing the performance of YOLOv5 with Faster Regional convolutional neural networks (R-CNN) to demonstrate improved accuracy in livestock monitoring.", " - The proposed monitoring system using the YOLOv5 deep learning model achieved an accuracy of 93% on mAP@_0.5%, demonstrating its effectiveness in livestock classification and counting under various conditions, including challenges like occlusion and animal overlapping.\\n\\n- The system outperformed other Faster Regional convolutional neural networks (R-CNN) models, indicating that the YOLOv5 backbone network is a promising option for intelligent farm monitoring and management."], [1, " - The paper discusses traditional surveying methods for manual onsite counting of animals, highlighting their drawbacks such as being hard to accomplish, time-consuming, expensive, and potentially dangerous.\\n- The paper reviews potential methods for automated remote detection and counting of animals, including simple computer vision operations like spatial filtering, edge detection, binarization, and image subtraction, as well as more complex machine learning and AI approaches for object detection and recognition.", " - The paper reviews potential methods for automated remote detection and counting of animals, including computer vision operations like spatial filtering, edge detection, binarization, and image subtraction.\\n- Scientists worldwide are experimenting with computer vision and artificial intelligence approaches for automating farming and livestock management, demonstrating the significant potential of these technologies in achieving efficient animal population monitoring."], [2, " - The paper proposes a Dual-scale image Decomposition based Fusion technique (DDF) for fusing visible and thermal images, which enhances the detection and counting of livestock in the automated monitoring system.\\n\\n- A novel Restricted Supervised Learning (RSL) technique is introduced, which allows for efficient and accurate results with minimal training data, significantly improving average precision from 4.05% with fully supervised learning to 80.58% with restricted supervised learning using only five training images and five seed labels.", " - The proposed Restricted Supervised Learning (RSL) technique significantly improved the average precision on test data, achieving 80.58% with only five training images and five seed labels, compared to 4.05% using fully supervised learning. With 50 seed labels, the average precision further increased to 91.56%.\\n\\n- The model demonstrated high accuracy, achieving an average accuracy of 98.3% when extensively tested on benchmark animal datasets."], [3, " - The approach utilizes a fully convolutional neural network to transform input images, converting original RGB images into gray-scale images that highlight animal positions as gradient circles.\\n- A locator is employed to detect the positions of the highlighted circles in the transformed images, enabling accurate positioning and counting of the animals.", " - The proposed approach for livestock positioning and counting achieved a precision rate of 0.9842, indicating a high level of accuracy in identifying the positions of animals in the images processed.\\n- The recall rate of 0.9911 demonstrates the method's effectiveness in detecting nearly all actual animal positions, suggesting that the system is reliable for real-time applications in large animal farms."], [4, " - The paper proposes an improved livestock detection algorithm that utilizes MultiResUNet for image segmentation, followed by morphological operations on the segmented image patches to identify candidate regions for livestock detection.\\n\\n- The classification of the candidate regions is performed using an Inception V4 network combined with SENet, which enhances object features and reduces noise, leading to improved detection accuracy compared to other algorithms.", " - The proposed livestock detection algorithm based on a full convolutional neural network demonstrates an average accuracy (AP) that is superior to other livestock detection algorithms by approximately 4.2%.\\n- The method utilizes MultiResUNet for image segmentation and combines the Inception V4 network with SENet to enhance object features and reduce noise in the classification of candidate regions."], [5, " - The paper employs the Mask R-CNN algorithm, which is an object detection framework that includes instance segmentation capabilities. This method allows for the detection and classification of livestock (specifically sheep and cattle) in aerial images captured by quadcopters, enabling the system to associate specific image pixels with detected objects.\\n\\n- The proposed system utilizes a continuous data acquisition approach through quadcopters, which capture videos of livestock in pastures. The system then extracts video frames, performs segmentation, and evaluates the performance of the model using various training dataset sizes (200, 400, 600, 800, and 1000 images) to optimize classification and counting accuracy.", " - The proposed livestock classification and counting system achieved an accuracy of 96.0% for livestock counting and 92.0% for classification, with sheep performing better than cattle in both metrics (97.3% and 93.5% for sheep compared to 94.7% and 90.4% for cattle).\\n\\n- The performance evaluation indicated that the detection of sheep was superior to that of cattle, as evidenced by higher precision and recall rates, with the best precision rates being 0.955 for livestock, 0.960 for sheep, and 0.950 for cattle at an IoU threshold of 0.4."], [6, " - The paper introduces a novel model called YOLOv5-MHSA-DS, which integrates the YOLOv5 framework with Multi-Head Self-Attention (MHSA) and DySample modules. The Multi-Head Self-Attention component enhances the model's ability to capture diverse features, thereby improving the accuracy of pig detection and counting.\\n\\n- The DySample module dynamically adjusts sampling strategies based on the input data, allowing the model to focus on the most critical parts of the image. This adaptability significantly enhances the performance of pig detection and counting compared to traditional methods.", " - The YOLOv5-MHSA-DS model achieved a mean Average Precision (mAP) of 93.8%, indicating a high level of accuracy in detecting pigs compared to other models.\\n- The model demonstrated a counting accuracy of 95.0%, which surpasses the performance of other models by significant margins of 12.2% and 19.0%, respectively."], [7, " - A multiscene dataset was created by selecting images from various pig farms to enhance the generalization performance of the improved YOLOv5n model, allowing it to better handle different conditions such as occlusion, illumination differences, and varying numbers of pigs.\\n\\n- The Backbone of YOLOv5n was replaced with the FasterNet model to reduce the number of parameters and calculations, while the Neck was optimized using the E-GFPN structure to improve feature fusion capability, and the Focal EIoU loss function was implemented to enhance identification accuracy compared to the original CIoU loss function.", " - The improved YOLOv5n model achieved an Average Precision (AP) of 97.72%, demonstrating high identification accuracy for pig counting in complex environments, while also reducing the number of parameters by 50.57%, the amount of calculation by 32.20%, and the model size by 47.21% compared to the original YOLOv5n model.\\n\\n- The detection speed of the improved algorithm reached 75.87 frames per second (f/s), indicating that the model not only maintained accuracy and robustness in multiscene conditions but also provided efficient performance suitable for practical applications, such as a pig counting application developed for the Android system."], [8, " - The paper utilizes a model based on Region-based Convolutional Neural Networks (R-CNN) for detecting and counting sheep in a paddock from UAV video footage. This approach leverages deep learning algorithms to enhance the accuracy of livestock counting.\\n\\n- The results obtained from the R-CNN model are compared with other techniques to evaluate their performance, indicating a comprehensive analysis of different methodologies for sheep counting in various climatic conditions.", " - The paper presents a model based on Region-based Convolutional Neural Networks (R-CNN) for detecting and counting sheep in a paddock from UAV video, which aims to provide more accurate and timely stock information for farmers.\\n- The results of the sheep counting model are compared with other techniques to evaluate their performance, although specific performance metrics or outcomes are not detailed in the provided information."], [9, " - The paper presents a livestock detection algorithm that utilizes modified versions of U-net and Google Inception-v4 net. U-net is employed for segmenting aerial images to obtain regions of interest (ROIs), which helps retain feature information for small targets due to its feature map having the same resolution as the original image.\\n\\n- The modified Inception-net is used to classify each ROI, accurately identifying the targets within the segmented regions. The performance of this method is evaluated against Faster RCNN and Yolo-v3 algorithms, demonstrating better average precision than Yolo-v3 and comparable results to Faster RCNN.", " - The livestock detection algorithm presented in the paper, which utilizes modified versions of U-net and Google Inception-v4 net, achieved better average precision than the Yolo-v3 algorithm and comparable results to the Faster RCNN algorithm when evaluated on the aerial livestock dataset.\\n\\n- The method demonstrated improved detection capabilities for dense and touching instances of livestock, although it faced challenges with light-colored instances due to their limited representation in the training dataset, resulting in lower precision for those specific cases."]];

        // Define the dt_args
        let dt_args = {"layout": {"topStart": null, "topEnd": null, "bottomStart": null, "bottomEnd": null}, "order": [], "warn_on_selected_rows_not_rendered": true};
        dt_args["data"] = data;

        
        new DataTable(table, dt_args);
    });
</script>
</div>
</div>
<div id="426a78b4" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df_type_filter <span class="op">=</span> df.query(<span class="st">'publication_type == ["Proceedings Article", "Journal Article"]'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define phrases to search for</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>keywords <span class="op">=</span> [<span class="st">"deep learning"</span>, <span class="st">"yolo"</span>, <span class="st">"cnn"</span>, <span class="st">'convolutional neural network'</span>, <span class="st">'mrvifnet'</span>, <span class="st">'ldr'</span>, <span class="st">'rfid'</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_keywords(text, keywords):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Finds all matching phrases or keywords in a given text and returns them as a comma-separated string."""</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pd.isna(text):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    text_lower <span class="op">=</span> text.lower()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    found <span class="op">=</span> [kw <span class="cf">for</span> kw <span class="kw">in</span> keywords <span class="cf">if</span> kw <span class="kw">in</span> text_lower]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">", "</span>.join(found) <span class="cf">if</span> found <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply function to the "Methods Used" column</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>df_type_filter[<span class="st">"Found Methods"</span>] <span class="op">=</span> df_type_filter[<span class="st">"Methods Used"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: find_keywords(x, keywords))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows to verify the results</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>show(df_type_filter[[<span class="st">"Methods Used"</span>, <span class="st">"Found Methods"</span>]])</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>px.histogram(df_type_filter, y<span class="op">=</span> <span class="st">'Found Methods'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\James\AppData\Local\Temp\ipykernel_5948\3146927497.py:16: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
</code></pre>
</div>
<div class="cell-output cell-output-display">
<table id="itables_a99708f8_7ca9_448a_a28d_96ca68f1bcad" class="display nowrap" data-quarto-disable-processing="true" style="table-layout:auto;width:auto;margin:auto;caption-side:bottom">
<thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Methods Used</th>
      <th>Found Methods</th>
    </tr>
  </thead><tbody><tr>
<td style="vertical-align:middle; text-align:left">
<div style="float:left; margin-right: 10px;">
<a href="https://mwouts.github.io/itables/"><svg class="main-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" width="64" viewbox="0 0 500 400" style="font-family: 'Droid Sans', sans-serif;">
    <g style="fill:#d9d7fc">
        <path d="M100,400H500V357H100Z"></path>
        <path d="M100,300H400V257H100Z"></path>
        <path d="M0,200H400V157H0Z"></path>
        <path d="M100,100H500V57H100Z"></path>
        <path d="M100,350H500V307H100Z"></path>
        <path d="M100,250H400V207H100Z"></path>
        <path d="M0,150H400V107H0Z"></path>
        <path d="M100,50H500V7H100Z"></path>
    </g>
    <g style="fill:#1a1366;stroke:#1a1366;">
   <rect x="100" y="7" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="5s" repeatcount="indefinite"></animate>
      <animate attributename="x" values="100;100;500" dur="5s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="0" y="107" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="3.5s" repeatcount="indefinite"></animate>
    <animate attributename="x" values="0;0;400" dur="3.5s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="100" y="207" width="300" height="43">
    <animate attributename="width" values="0;300;0" dur="3s" repeatcount="indefinite"></animate>
    <animate attributename="x" values="100;100;400" dur="3s" repeatcount="indefinite"></animate>
  </rect>
        <rect x="100" y="307" width="400" height="43">
    <animate attributename="width" values="0;400;0" dur="4s" repeatcount="indefinite"></animate>
      <animate attributename="x" values="100;100;500" dur="4s" repeatcount="indefinite"></animate>
  </rect>
        <g style="fill:transparent;stroke-width:8; stroke-linejoin:round" rx="5">
            <g transform="translate(45 50) rotate(-45)">
                <circle r="33" cx="0" cy="0"></circle>
                <rect x="-8" y="32" width="16" height="30"></rect>
            </g>

            <g transform="translate(450 152)">
                <polyline points="-15,-20 -35,-20 -35,40 25,40 25,20"></polyline>
                <rect x="-15" y="-40" width="60" height="60"></rect>
            </g>

            <g transform="translate(50 352)">
                <polygon points="-35,-5 0,-40 35,-5"></polygon>
                <polygon points="-35,10 0,45 35,10"></polygon>
            </g>

            <g transform="translate(75 250)">
                <polyline points="-30,30 -60,0 -30,-30"></polyline>
                <polyline points="0,30 -30,0 0,-30"></polyline>
            </g>

            <g transform="translate(425 250) rotate(180)">
                <polyline points="-30,30 -60,0 -30,-30"></polyline>
                <polyline points="0,30 -30,0 0,-30"></polyline>
            </g>
        </g>
    </g>
</svg>
</a>
</div>
<div>
Loading ITables v2.2.2 from the internet...
(need <a href="https://mwouts.github.io/itables/troubleshooting.html">help</a>?)</div></td>

</tr></tbody>

</table>
<link href="https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.css" rel="stylesheet">
<script type="module">
    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.13/dt_bundle.js';

    document.querySelectorAll("#itables_a99708f8_7ca9_448a_a28d_96ca68f1bcad:not(.dataTable)").forEach(table => {
        if (!(table instanceof HTMLTableElement))
            return;

        // Define the table data
        const data = [[0, " - The paper proposes a livestock management and monitoring system that utilizes the end-to-end deep learning model You Only Look Once version 5 (YOLOv5) for feature extraction and detection of livestock of different sizes, enabling accurate counting and tracking of individual animals in various conditions.\\n\\n- It also incorporates data augmentation techniques using a real-time Kaggle dataset and employs the Roboflow application for scaling, resizing, and manipulating the dataset, while comparing the performance of YOLOv5 with Faster Regional convolutional neural networks (R-CNN) to demonstrate improved accuracy in livestock monitoring.", "deep learning, yolo, cnn, convolutional neural network"], [1, " - The paper discusses traditional surveying methods for manual onsite counting of animals, highlighting their drawbacks such as being hard to accomplish, time-consuming, expensive, and potentially dangerous.\\n- The paper reviews potential methods for automated remote detection and counting of animals, including simple computer vision operations like spatial filtering, edge detection, binarization, and image subtraction, as well as more complex machine learning and AI approaches for object detection and recognition.", "None"], [2, " - The paper proposes a Dual-scale image Decomposition based Fusion technique (DDF) for fusing visible and thermal images, which enhances the detection and counting of livestock in the automated monitoring system.\\n\\n- A novel Restricted Supervised Learning (RSL) technique is introduced, which allows for efficient and accurate results with minimal training data, significantly improving average precision from 4.05% with fully supervised learning to 80.58% with restricted supervised learning using only five training images and five seed labels.", "None"], [4, " - The paper proposes an improved livestock detection algorithm that utilizes MultiResUNet for image segmentation, followed by morphological operations on the segmented image patches to identify candidate regions for livestock detection.\\n\\n- The classification of the candidate regions is performed using an Inception V4 network combined with SENet, which enhances object features and reduces noise, leading to improved detection accuracy compared to other algorithms.", "None"], [5, " - The paper employs the Mask R-CNN algorithm, which is an object detection framework that includes instance segmentation capabilities. This method allows for the detection and classification of livestock (specifically sheep and cattle) in aerial images captured by quadcopters, enabling the system to associate specific image pixels with detected objects.\\n\\n- The proposed system utilizes a continuous data acquisition approach through quadcopters, which capture videos of livestock in pastures. The system then extracts video frames, performs segmentation, and evaluates the performance of the model using various training dataset sizes (200, 400, 600, 800, and 1000 images) to optimize classification and counting accuracy.", "cnn"], [6, " - The paper introduces a novel model called YOLOv5-MHSA-DS, which integrates the YOLOv5 framework with Multi-Head Self-Attention (MHSA) and DySample modules. The Multi-Head Self-Attention component enhances the model's ability to capture diverse features, thereby improving the accuracy of pig detection and counting.\\n\\n- The DySample module dynamically adjusts sampling strategies based on the input data, allowing the model to focus on the most critical parts of the image. This adaptability significantly enhances the performance of pig detection and counting compared to traditional methods.", "yolo"], [7, " - A multiscene dataset was created by selecting images from various pig farms to enhance the generalization performance of the improved YOLOv5n model, allowing it to better handle different conditions such as occlusion, illumination differences, and varying numbers of pigs.\\n\\n- The Backbone of YOLOv5n was replaced with the FasterNet model to reduce the number of parameters and calculations, while the Neck was optimized using the E-GFPN structure to improve feature fusion capability, and the Focal EIoU loss function was implemented to enhance identification accuracy compared to the original CIoU loss function.", "yolo"], [8, " - The paper utilizes a model based on Region-based Convolutional Neural Networks (R-CNN) for detecting and counting sheep in a paddock from UAV video footage. This approach leverages deep learning algorithms to enhance the accuracy of livestock counting.\\n\\n- The results obtained from the R-CNN model are compared with other techniques to evaluate their performance, indicating a comprehensive analysis of different methodologies for sheep counting in various climatic conditions.", "deep learning, cnn, convolutional neural network"], [9, " - The paper presents a livestock detection algorithm that utilizes modified versions of U-net and Google Inception-v4 net. U-net is employed for segmenting aerial images to obtain regions of interest (ROIs), which helps retain feature information for small targets due to its feature map having the same resolution as the original image.\\n\\n- The modified Inception-net is used to classify each ROI, accurately identifying the targets within the segmented regions. The performance of this method is evaluated against Faster RCNN and Yolo-v3 algorithms, demonstrating better average precision than Yolo-v3 and comparable results to Faster RCNN.", "yolo, cnn"], [10, " - The paper employs a cattle detection and counting system based on Convolutional Neural Networks (CNNs), utilizing aerial images captured by an Unmanned Aerial Vehicle (UAV). The approach involves detecting cattle in images and then estimating the count from the merged results, which is categorized as count-by-detection.\\n\\n- To evaluate the detection results, the paper uses Intersection over Union (IOU) as a metric, with a threshold of 0.2 set for true positives (TP). The evaluation process also includes penalizing multiple detections of the same animal, ensuring that only the detection with the highest IOU is counted as a TP, while others are classified as false positives (FP).", "cnn, convolutional neural network"], [11, " - The paper proposes an advanced improved YOLO_v5 method for pig detection and counting, named YOLOV5_Plus, which incorporates an attention mechanism to enhance the model's ability to handle overlapping and misidentification of pigs during detection.\\n\\n- A series of data augmentation methods are utilized, including translation, color augmentation, rescaling, and mosaic, to improve the speed and accuracy of pig detection and counting in the YOLOV5_Plus model.", "yolo"], [13, " - The research utilizes a combination of small unmanned aerial vehicles (sUAV) and machine vision technology to automate the detection and counting of cattle in large pasture and rangeland areas, which are typically difficult to access by traditional means.\\n\\n- A fully automatic technique was developed to detect animals in UAV imagery based on their spatial and spectral characteristics, allowing for the identification of even small animals that may be partially obscured by vegetation.", "None"], [15, " - The study proposed using the Faster Region-based Convolutional Neural Network (Faster R-CNN) method for counting broiler livestock in a poultry house, which is an object detection model suitable for counting the number of objects in video monitoring.\\n  \\n- The experiment utilized different architectures within the Faster R-CNN framework, specifically comparing ResNet-101, ResNet-50, and ResNeXt-101, with ResNet-101 achieving the best accuracy due to its deeper convolution layers, which enhance feature extraction capabilities.", "cnn, convolutional neural network"], [16, " - The paper utilizes the Mask YOLOv7 object detection model, which incorporates a mask mechanism into the YOLOv7 algorithm for instance segmentation of individual cattle objects. This allows for precise detection and counting of cattle in both controlled (feedlot) and uncontrolled (open-range) environments.\\n\\n- The performance of the Mask YOLOv7 model is evaluated using metrics such as Intersection over Union (IoU) with a threshold of 0.5, average precision (AP), and mean average precision (mAP), demonstrating an accuracy of 93% in controlled environments and 95% in uncontrolled environments.", "yolo"], [17, " - The study proposes a novel graph-based method that utilizes multiple attributes such as velocity, direction, state (lying down or standing), color, and distance to improve the accuracy of duplicate cattle removal and counting in large pasture areas. This method integrates these attributes through automated hyper-parameter learning.\\n\\n- The Ford\u2013Fulkerson graph algorithm is employed to detect and remove duplicated cattle based on the identified multiple attributes, enhancing the overall counting accuracy and efficiency compared to traditional and existing automated approaches.", "None"], [19, " - The SCS-YOLOv5 model incorporates a CSP structured SPPFCSPC in place of the original SPPF module in the YOLOv5 backbone network, enhancing feature extraction and fusion capabilities for improved cattle detection in complex breeding environments.\\n\\n- The model integrates a Coordinate Attention (CA) mechanism in the neck network, replaces the Standard Convolution (SC) with a light convolution GSConv, and introduces a Slim Neck, along with employing multi-scale training strategies to balance localization accuracy and detection speed.", "yolo"], [21, " - The paper utilized the tiling function as part of the preprocessing methodology, which involved dividing large images into smaller, non-overlapping tiles to improve the detection of small objects against a large background. This approach allowed the models to focus on specific regions of the image, enhancing the accuracy and efficiency of the analysis.\\n\\n- Several state-of-the-art network architectures were implemented for object detection, including three versions of the You Only Look Once (YOLO) architecture and a Single Shot Multibox Detector (SSD). These models were trained on a manually annotated set of images, and their performance was evaluated based on metrics such as mean average precision (mAP), precision, recall, and F1 score, as well as their real-time performance capabilities.", "yolo"], [22, " - The paper proposes an object detection method that utilizes a fusion allocation strategy, which selects the pre-selection box based on a cross-grid strategy and calculates the minimum allocation loss function to effectively distinguish between positive and negative samples.\\n\\n- It also employs a multi-objective loss function that modifies the confidence loss to VarifocalLoss, enhancing the model's detection performance in multi-objective situations.", "None"], [23, " - The paper proposes a Light Attention YOLO model that integrates an attention depth separable module with CSPDarknet and PAFPN, enhancing the model's ability to detect sheep in overlapping and occluded scenarios.\\n- The model incorporates a CBAM attention module added to the 4th and 5th layers of the backbone, which improves the detection performance in various complex environments.", "yolo"], [24, " - The intelligent classifier was built on the Roboflow platform using the YOLOv8 model, which was trained with a dataset of 35,204 images. Initially, a Convolutional Neural Network (CNN) model was developed, but it did not perform optimally, leading to the adaptation of a pre-trained VGG16 model with additional fine-tuning through data augmentation techniques.\\n\\n- The dataset was divided into training (88%), validation (8%), and test (4%) sets, and the performance of the classifier was evaluated using precision, recall, and F1-Score metrics. The YOLOv8 classifier achieved 95.8% accuracy in distinguishing between goat and sheep images, demonstrating superior performance compared to the CNN and VGG16 models in terms of accuracy and computational efficiency.", "yolo, cnn, convolutional neural network"], [25, " - The algorithm utilizes Partial Convolution (PConv) to replace traditional 3 \u00d7 3 convolutions in the Neck segment of the YOLOv7 network, which enhances computational speed and reduces complexity, thereby improving cattle detection in challenging conditions such as being obscured by fences and varying image brightness.\\n\\n- It introduces a novel bounding box similarity metric called Minimum Point DioU (MPDIoU), which is based on minimum point distance and incorporates factors from existing loss functions while simplifying computations, contributing to the overall efficiency and accuracy of the cattle counting process.", "yolo"], [26, " - The proposed framework, FSSCaps-DetCountNet, utilizes fuzzy soft sets (FSS) for similarity measures to effectively discriminate target animals from nontargets and the background in aerial images. This approach enhances the detection capabilities in complex environments.\\n\\n- The model incorporates a capsule network (CapsNet), which is advantageous due to its requirement for very few training data and its robustness to rotation and affine transformations, making it suitable for challenging image conditions such as dense backgrounds and overlapping animals.", "None"], [27, " - The method involves remote sensing techniques, specifically utilizing aerial surveying with an RGB camera mounted on an unmanned aerial vehicle (UAV) to collect images of livestock in extensive areas.\\n- The collected images are processed and stored in space-time databases, followed by the construction of a rural property orthomosaic and the application of deep learning techniques, particularly convolutional neural networks, for pattern discovery and animal identification.", "deep learning, convolutional neural network"], [30, " - The paper utilizes the YOLOv5 algorithm as the core method for sheep counting, which is optimized for real-time detection and management of sheep in pasture environments. This method allows for accurate recording of sheep numbers and operates stably and reliably.\\n\\n- A bidirectional collision counting method is implemented, where two lines are set to control the counting of sheep exiting and returning through a constructed channel. This method ensures that sheep are accurately tracked and counted without duplication, enhancing the overall accuracy of the counting process.", "yolo"], [31, " - The system utilizes a DarkNet deep learning model to classify and detect various wild animals, enabling efficient monitoring in their natural habitats. This approach helps in automating the identification process, which can be challenging due to the diversity of animal species.\\n\\n- To enhance the detection and alerting process, the system incorporates GSM and GPS devices integrated with Arduino embedded systems. This technology allows for real-time alerts regarding the presence of animals, which can help mitigate risks such as animal-vehicle accidents and agricultural damage.", "deep learning"], [33, " - The study constructed a new dataset of video images of goats for the object tracking task and utilized YOLOv5 as the baseline object detector. The model was improved through various advanced methods, including RandAugment for data augmentation, AF-FPN to enhance multi-scale object representation, and the Dynamic Head framework to unify the attention mechanism with the detector\u2019s heads, resulting in a significant increase in mean Average Precision (mAP) from 84.26% to 92.19%.\\n\\n- For goat tracking and counting, the information obtained from the improved detector was input into DeepSORT. The proposed method achieved an average overlap rate of 89.69%, which is higher than the original combination of YOLOv5 and DeepSORT at 82.78%. Additionally, a single-line counting method was employed based on goat head tracking results to minimize the risk of double counting.", "yolo"], [34, " - The paper proposes a method for counting cattle that combines a deep learning model for rough animal location, which helps in identifying where the animals are located in the images captured by UAVs.\\n\\n- It utilizes color space manipulation to enhance the contrast between the animals and the background, along with mathematical morphology techniques to isolate the animals and infer the number of individuals in clustered groups, and image matching to account for any overlap in the images.", "deep learning"], [35, " - The paper proposes an image recognition method that preprocesses the morphological characteristics of livestock using a combination of the Kennard Stone algorithm and the K-means II algorithm, along with deep learning models tailored for different scenes and behaviors of live pigs.\\n\\n- The research focuses on optimizing model training and parameters for livestock recognition in various captivity scenarios, which contributes to advancements in smart animal husbandry and the development of high accuracy livestock auto-weighing systems.", "deep learning"], [36, " - The paper proposes a novel model called YOLOv5-SA-FC, which enhances the traditional YOLOv5 model by incorporating a shuffle attention (SA) module. This SA module facilitates multi-channel information fusion with minimal additional parameters, thereby improving the richness and robustness of feature extraction for pig detection and counting.\\n\\n- Additionally, the model utilizes Focal-CIoU (FC) as a localization loss function, which helps mitigate the effects of sample imbalance on detection results. This approach contributes to the overall performance improvement of the model, leading to higher accuracy in pig detection and counting.", "yolo"], [39, " - The paper employs advanced image and video processing methods, specifically utilizing YoloV5, which is an efficient algorithm for animal detection and classification in agricultural fields.\\n- It incorporates IoT technology for real-time information transfer, enabling timely responses to prevent crop destruction by stray animals.", "yolo"], [41, " - The paper proposes a pig counting method that utilizes the YOLOv5s detector, enhanced by replacing the ordinary Non-Maximum Suppression (NMS) with DIOU_NMS, which optimizes the output and improves the stability of detection boxes, thereby reducing missed detections of multiple pigs in parallel.\\n\\n- An improved DeepSORT algorithm is employed, where the CA model is used instead of the CV model to enhance the prediction results of the Kalman filter for tracking trajectories, along with the establishment of a virtual detection area to count pigs based on the accumulated downside probability from captured key frames.", "yolo"], [42, " - The paper proposes a pig counting method that utilizes the YOLOv5s detector, enhanced by replacing the ordinary Non-Maximum Suppression (NMS) with DIOU_NMS, which optimizes the output and improves the stability of detection boxes, thereby reducing missed detections of multiple pigs in parallel.\\n\\n- An improved DeepSORT algorithm is employed, where the CA model is used instead of the CV model to enhance the prediction results of the Kalman filter for tracking trajectories, along with the establishment of a virtual detection area to count pigs based on frame accumulation and downside probability.", "yolo"], [43, " - The research proposes a video-based dynamic counting method that combines the YOLOv7 network structure with DeepSORT, optimizing the second and third 3 \u00d7 3 convolution operations in the head network ELAN-W with PConv to reduce computational demand and improve inference speed without sacrificing accuracy.\\n\\n- To enhance the model's robustness in complex scenarios, the study introduces the coordinate attention (CA) mechanism before the three re-referentialization paths (REPConv) in the head network, ensuring accurate position perception at oblique angles and rich semantic information extraction.", "yolo"], [44, " - The paper proposes an improved pig counting algorithm (MPC-YD) that utilizes the YOLOv5 model enhanced with two different sizes of SPP networks and replaces MaxPool operations with SoftPool to improve the detection rate of pig body parts.\\n\\n- The algorithm incorporates a pig reidentification network, a spatial state correction-based tracking method, and a counting method based on frame number judgment within the DeepSORT algorithm to enhance the accuracy of pig tracking and counting.", "yolo"], [45, "NaN", "None"], [46, " - The paper utilizes key point detection methods such as SIFT (Scale-invariant feature transform) and SURF (Speeded Up Robust Features) to identify key points in images captured by the watching devices. These methods are chosen for their effectiveness in detecting features that are invariant to scale and rotation.\\n\\n- For classification, the system employs Support Vector Machine (SVM) with a radial basis function (RBF) kernel to classify the visual descriptors derived from the detected key points. This method is effective in finding an optimal hyperplane that separates different classes of data, facilitating the identification of various animal species based on their visual characteristics.", "None"], [47, " - The paper proposes a method for sheep number estimation based on a multi-scale residual visual information fusion Network (MRVIFNet), which utilizes multiple parallel hole convolutions with different hole rates to extract multi-scale features of sheep targets. This approach aims to reduce the grid effect caused by hole convolution and better adapt to the multi-scale changes of sheep in the pasture.\\n\\n- Additionally, the study explores a convolutional neural network model based on view branch sharing, which is designed to address the challenges of pedestrian scale change and chaotic distribution in complex scenes. This model demonstrates better performance compared to five popular methods in the context of sheep counting.", "convolutional neural network, mrvifnet"], [49, " - The paper employs an automatic thresholding method based on the gray scale histogram proposed by Otsu to filter depth images and detect objects. This method allows for the extraction of regions of interest by setting minimum and maximum threshold values for each pixel in the depth image.\\n\\n- Several shape-based constraints are applied to refine the detected object regions, including circularity, rectangularity, main radius, and anisometry. These constraints help in accurately estimating the size and weight of the detected animals, facilitating further processing for livestock monitoring and assessment.", "None"], [50, " - The paper proposes a method for sheep number estimation based on a multi-scale residual visual information fusion Network (MRVIFNet), which utilizes multiple parallel hole convolutions with different hole rates to extract multi-scale features of sheep targets, thereby reducing the grid effect and adapting to multi-scale changes in sheep distribution.\\n\\n- Additionally, the study explores a convolutional neural network model based on view branch sharing, which is designed to address issues related to pedestrian scale change and chaotic distribution in complex scenes, demonstrating better performance compared to five popular methods in the field.", "convolutional neural network, mrvifnet"], [51, " - The study employs various learning approaches, including machine learning and deep learning techniques, to classify animals based on datasets and live motion sensor data. These methods are crucial for detecting wildlife and categorizing incidents related to animal-human conflict and animal-vehicle collisions.\\n\\n- The identification and categorization of animals are primarily based on image processing techniques, utilizing camera trap images and video sequences to monitor wildlife effectively and track their movement patterns and interactions with human environments.", "deep learning"], [52, " - The study employs various learning approaches, including machine learning and deep learning techniques, to classify animals based on datasets and live motion sensor data. These methods are crucial for detecting wildlife and categorizing incidents related to animal-human conflict and animal-vehicle collisions.\\n\\n- The identification and categorization of animals are primarily based on image processing methods, utilizing camera trap images and video sequences to monitor wildlife effectively and track their movement patterns and interactions with human environments.", "deep learning"], [53, " - **FilterDetector**: This method utilizes various filters combined with bounding box ensemble algorithms to enhance the accuracy of bounding box detection in camera trap images.\\n\\n- **DLEDetector**: This is an ensemble method that employs two base deep learning models to improve and refine the detection results produced by Microsoft MegaDetector V 4.", "deep learning"], [54, " - **FilterDetector**: This method utilizes various filters combined with bounding box ensemble algorithms to enhance the accuracy of bounding box detection in camera trap images.\\n\\n- **DLEDetector**: This is an ensemble method that employs two base deep learning models to improve and refine the detection results produced by Microsoft MegaDetector V 4.", "deep learning"], [55, " - The paper discusses the application of computer vision systems to generate real-time, non-invasive, and accurate animal-level information, which includes monitoring animal growth and behavior, automated feed bunk management, individual animal recognition, and assessing particle size distribution in total mixed rations.\\n\\n- It highlights the need for sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as the development of a computer vision system involves handling massive datasets to create predictive modeling for precise management decisions.", "None"], [56, " - The paper employs image processing techniques to develop a Convolutional Neural Network (CNN)-based system for the early diagnosis of lumpy illness in cattle populations, utilizing a dataset of 700 images from the Lumpy Skin Image dataset to quickly identify and isolate sick animals, thereby reducing disease spread within the herd.\\n\\n- An RFID technology is implemented for a productive tracking and identification system, which assigns a unique ID to each cow for effective tracking, facilitating the identification and management of every animal in the herd, while also monitoring and controlling real-time resource usage and nutritional intake.", "cnn, convolutional neural network, rfid"], [57, " - The paper proposes a framework based on Transfer Learning (TL) in a Convolutional Neural Network (CNN) for the construction of an automated animal identification system, which is utilized to analyze and identify focal species in images obtained from camera traps.\\n\\n- The model achieves superior performance in species classification by employing Deep Convolutional Neural Networks (DCNN), achieving an accuracy of 96% on the test dataset after 18 epochs with a batch size of 32.", "cnn, convolutional neural network"], [60, " - The chapter discusses the integration of Artificial Intelligence (AI) and the Internet of Things (IoT) in livestock management, focusing on how these technologies can optimize feeding, track animal activity, monitor health, and predict potential issues before they escalate.\\n\\n- It provides a comprehensive review of the current technological state in livestock management, highlighting recent developments, challenges, and future applications of AI and IoT to enhance productivity, sustainability, and efficiency in the industry.", "None"], [61, " - The paper utilizes deep learning techniques, specifically a UNet model with various backbones, for automating animal counting tasks in aerial remote sensing images.\\n- Gaussian density maps are used for training the model, eliminating the need for training a detector. This regression-based method avoids predicting object coordinates and directly focuses on the counting task, making it less costly in terms of annotations and faster compared to detection-based counting methods.", "deep learning"], [63, " - The methods involve an extensive review of the current literature and case studies on intelligent monitoring, data analytics, automation in feeding and climate control, and renewable energy integration.\\n- The study focuses on the application of intelligent technologies such as real-time monitoring, machine learning, and the Internet of Things to address challenges in the livestock industry.", "None"], [66, " - The proposed system employs preprocessing techniques such as histogram equalisation to enhance image contrast and mathematical morphology filtering to remove noise from the muzzle images, ensuring better quality input for classification.\\n  \\n- For feature extraction, the box-counting algorithm is utilized to detect features of each muzzle image, which are then classified using multiclass support vector machines (MSVMs) to achieve high classification accuracy.", "None"], [67, " - The paper introduces a new point-based CNN architecture called HerdNet, inspired by crowd counting, to detect and count large mammals at close range in aerial images.\\n- The study compares HerdNet with two other architectures, Faster-RCNN and a density-based adapted version of DLA-34, in terms of localization, counting accuracy, and processing speed, with HerdNet outperforming the baselines in terms of precision and speed while maintaining equivalent recall.", "cnn"], [68, " - The paper presents a method for resolving the detectable instance disappearance problem by estimating probabilities of an object's placement in the image, which addresses issues caused by occlusion and camera movement.\\n- The proposed method involves fusing results from multiple frames to enhance cattle counting accuracy and significantly reduce the occurrence of double-counted individuals in the field.", "None"], [72, " - The paper employs a semi-supervised approach that consists of three main parts: first, an autoencoder and classifier are jointly trained to create fixed-dimensional embeddings from GPS trajectories; second, these embeddings are clustered to generate cluster labels; and third, the cluster labels are combined with human-engineered features to train a final classifier.\\n\\n- The classification models are trained using both supervised and semi-supervised methods, with the semi-supervised approach demonstrating the best performance, achieving an overall classification accuracy of 69% and an F1 score of 56% for theft events, which is only 4% lower than human performance.", "None"], [73, " - The first step of the algorithm involves using the magnitude of each axis of the accelerometer signal to classify cattle behaviors into two groups: walking-grazing and standing, and lying.\\n- The second step utilizes the variance of the Y-axis to differentiate between walking-grazing and standing behaviors, allowing for more precise classification of cattle activities.", "None"], [81, " - The study evaluated the performance of a multi-class convolutional neural network called RetinaNet, which was used to detect elephants, giraffes, and zebras in aerial images from two Kenyan animal counts. This algorithm demonstrated high detection rates, identifying 95% of elephants, 91% of giraffes, and 90% of zebras, while also detecting additional animals that were missed by human observers.\\n\\n- The proposed method involves using cost-efficient Unmanned Aerial Vehicles or microlight aircraft equipped with cameras and an automated animal detection algorithm, allowing for a semi-automatic approach to aerial animal counts. This method aims to replace manual counting by utilizing the algorithm's detections and applying a correction factor to account for undercounting bias, ultimately improving the accuracy and precision of population estimates.", "convolutional neural network"], [83, " - The paper examines three methods to identify and count individual animals in aerial imagery: \\n  1. **Manual Photo Interpretation**: This technique involves human interpreters analyzing aerial photographs to identify and circle suspected animals, resulting in a high probability of detecting animals (81% \u00b1 24%) with low rates of over-counting (8% \u00b1 16%) and under-counting (19% \u00b1 24%).\\n  \\n  2. **Unsupervised Classification (ISODATA)**: This method utilizes an unsupervised classification technique with background image subtraction to identify animals. It achieved a high detection probability (82% \u00b1 10%) but had a high probability of over-counting (69% \u00b1 27%) and a low probability of under-counting (18% \u00b1 18%).\\n\\n- The third method evaluated is the **Multi-Image, Multi-Step Technique (MIMS)**: This semi-automated approach incorporates multiple steps and additional information to identify animals but resulted in the lowest probability of detecting animals (50% \u00b1 26%), the highest probability of over-counting (72% \u00b1 26%), and the highest probability of under-counting (50% \u00b1 26%).", "None"], [86, " - The PDC-YOLO network is built upon the YOLOv7 architecture and incorporates the SPD-Conv structure into the YOLOv7 backbone, which enhances detection capabilities under varying lighting conditions and improves the detection of small-scale pigs.\\n\\n- The model replaces the neck of YOLOv7 with an Adaptive Feature Pyramid Network (AFPN) to efficiently fuse features of different scales, and it utilizes rotated bounding boxes to achieve improved accuracy in pig detection.", "yolo"], [87, " - The paper proposes the use of Convolutional Neural Networks (CNN) for detecting and classifying animals in digital images, focusing on extracting features from the input images to aid in decision making for classification.\\n- The experimental results indicate that the proposed CNN algorithm has a positive impact on animal classification performance, achieving an accuracy of approximately 64% with images sized at 50 \u00d7 50 pixels.", "cnn, convolutional neural network"], [88, " - The study employed a 'clean state' moving window behaviour state classification algorithm, which analyzed 3, 5, and 10 second duration segments of data collected from collar, leg, and ear-mounted accelerometers on Merino ewes. This method allowed for the categorization of animal behaviours such as grazing, standing, walking, and lying, with varying sensitivity across different sensor placements.\\n\\n- Feature importance ranking was conducted using a random forest model, which identified the three most significant features for each deployment type based on the raw acceleration measurements. These features were then utilized in a quadratic discriminant analysis (QDA) classifier to predict behaviours, ensuring that the model could effectively differentiate between various animal activities based on the specific signals captured by each type of sensor.", "None"], [90, " - The system utilizes WiFi signals to determine the number of people in a predefined area by analyzing the effect of humans on the WiFi signal strength, which helps in accurately counting individuals entering or exiting the area.\\n  \\n- It employs entrance-exit control based on distance and LDR (Light Dependent Resistor) sensors, allowing for effective monitoring of the number of people or animals, such as cattle, in a closed space.", "ldr"], [91, " - The paper utilizes eight machine learning methods for analyzing a dataset of livestock animal health issues, including logistic regression, XGBoost, Catboost, light gradient boost, Random Forests, and Support Vector Machines, integrated with feature selection algorithms to enhance disease prediction accuracy.\\n\\n- Among the classifiers analyzed, the Catboost and Random Forest classifiers demonstrated the highest and most reliable accuracy rates of 83.7% and 83.5%, respectively, indicating their effectiveness in early diagnosis and prediction of diseases in livestock animals.", "None"], [92, " - The paper reviews current technological advancements in remote sensing and detection algorithms specifically for livestock censuses, highlighting the development of Unmanned Aerial Vehicles (UAVs) that provide high-resolution images and flexibility for effective detection of livestock.\\n\\n- It discusses the use of automated detection methods that are characterized as fast, efficient, and accurate, while also addressing the challenges posed by factors such as the surrounding background of different livestock species, herd size, and the spatial resolution of the datasets, which can affect detection accuracy.", "None"]];

        // Define the dt_args
        let dt_args = {"layout": {"topStart": "pageLength", "topEnd": "search", "bottomStart": "info", "bottomEnd": "paging"}, "order": [], "warn_on_selected_rows_not_rendered": true};
        dt_args["data"] = data;

        
        new DataTable(table, dt_args);
    });
</script>
</div>
<div class="cell-output cell-output-display">
<div>                            <div id="ed7bdfa4-0908-4264-b488-02d69f2b5797" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ed7bdfa4-0908-4264-b488-02d69f2b5797")) {                    Plotly.newPlot(                        "ed7bdfa4-0908-4264-b488-02d69f2b5797",                        [{"alignmentgroup":"True","bingroup":"y","hovertemplate":"count=%{x}\u003cbr\u003eFound Methods=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"","offsetgroup":"","orientation":"h","showlegend":false,"xaxis":"x","y":["deep learning, yolo, cnn, convolutional neural network",null,null,null,"cnn","yolo","yolo","deep learning, cnn, convolutional neural network","yolo, cnn","cnn, convolutional neural network","yolo",null,"cnn, convolutional neural network","yolo",null,"yolo","yolo",null,"yolo","yolo, cnn, convolutional neural network","yolo",null,"deep learning, convolutional neural network","yolo","deep learning","yolo","deep learning","deep learning","yolo","yolo","yolo","yolo","yolo","yolo",null,null,"convolutional neural network, mrvifnet",null,"convolutional neural network, mrvifnet","deep learning","deep learning","deep learning","deep learning",null,"cnn, convolutional neural network, rfid","cnn, convolutional neural network",null,"deep learning",null,null,"cnn",null,null,null,"convolutional neural network",null,"yolo","cnn, convolutional neural network",null,"ldr",null,null],"yaxis":"y","type":"histogram"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"count"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Found Methods"}},"legend":{"tracegroupgap":0},"barmode":"relative"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('ed7bdfa4-0908-4264-b488-02d69f2b5797');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>