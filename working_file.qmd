
---
title: "Livestock Detection and Counting Methods: Review"
format: html
author: "James Matosse"
execute: 
  echo: false
---

# Introduction

This report aims to review the current literature on livestock detection, identification and counting in a farm that uses computer vision, deep learning, machine learning. Livestocks are considered for mammals like goats, cattles, pigs, sheep, etc in exclusion to poultry, vegetation, plants and/or animal welfare or behaviours. Two datasets were obtained, namely 100 articles that were parsed to an AI model to summarise and/or extract from each article some specific information for example; methods used, future research, results and etc. The other dataset contained 708 articles from scopus which will be filtered following a defined criterion.

# Loading the data

```{python}
from openai import OpenAI
from local_settings import OPENAI_KEY # Assumes you have a local_settings.py file in your folder with your OpenAI key  
# Initialize the OpenAI client
client = OpenAI(api_key=OPENAI_KEY) # I need a new API_key
```

```{python}
import pandas as pd
import plotly.express as px
from itables import show
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

rw_data = pd.read_excel(
    "livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx",
    sheet_name="Sheet1",
)

rw_data
```

## Preliminary analysis

There are 100 articles in this dataset with 24 columns, for the analysis: columns first_page, last_page, etc can be remove since they do not contain necessary information at this stage.

The publication type variable can be used to see trends of the articles contained against years.

```{python}
clean_data = rw_data.drop(
    columns=[
        "first_page",
        "last_page",
        "conference_series",
        "volume",
        "number",
        "journal",
        "url",
    ]
)

# Convert 'year' column to numeric
if "year" in clean_data.columns:
    clean_data["year"] = pd.to_numeric(clean_data["year"], errors="coerce")

yearVcount = clean_data.groupby(["year", "publication_type"]).count()["abstract"]

fig_line = px.line(
    clean_data.groupby("year").count()["abstract"],
    title="Abstracts trends from 1994 on Livestock Detection",
    labels={"value": "Abstract Count", "year": "Year"},
)

# fig_line.show()

yearVcount = yearVcount.to_frame().reset_index()

fig_bar = px.bar(
    yearVcount,
    x="year",
    y="abstract",
    color="publication_type",
    title="Abstracts trends from 1994 on Livestock Detection",
    labels={"abstract": "Abstract Count", "year": "Year", "publication_type": "Type"},
    # nbins=40,
)
fig_bar.show()

# clean_data["year"].sort_values(ascending=False).head(20)


def llm_chat(message):
    response = client.chat.completions.create(
        model="gpt-4o-mini", messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content


# Test the function here
# llm_chat("What is Python (the language) named after?")
```

The column chart displays the number of abstracts per year, categorized by publication type. Research on remote livestock counting became a recurring topic from 2014 onward, whereas earlier publications were exclusively patentsâ€”except for a single journal article in 2011. The impact of the COVID-19 pandemic is evident in 2021, with the number of abstracts dropping to just two, compared to 12 in 2020. By 2022, academic activity had returned to normal, reflected in an increased abstract count of 15.

```{python}


# Load the data
# file_path = (
#     "livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx"
# )
# df = pd.read_excel(file_path, sheet_name="Sheet1").drop(
#     columns=[
#         "doi",
#         "first_page",
#         "last_page",
#         "conference_series",
#         "volume",
#         "number",
#         "journal",
#         "url",
#     ]
# ).drop_duplicates(subset='abstract')

# Plot Abstract Counts vs. Year using matplotlib
abstract_counts = clean_data.groupby("year").size()
# plt.figure(figsize=(10, 5))
# plt.bar(abstract_counts.index, abstract_counts.values, color="skyblue")
# plt.xlabel("Year")
# plt.ylabel("Number of Abstracts")
# plt.title("Abstract Counts per Year")
# plt.xticks(rotation=45)
# plt.show()

# Slice Data for Repeated Authors
if "authors" in clean_data.columns:
    authors_series = (
        clean_data["authors"].dropna().str.split(",")
    )  # Assuming authors are separated by ','
    authors_flat = [author.strip() for authors in authors_series for author in authors]
    author_counts = pd.Series(authors_flat).value_counts()
    repeated_authors = author_counts[author_counts > 1].index.tolist()
    repeated_authors_df = clean_data[
        clean_data["authors"].str.contains("|".join(repeated_authors), na=False)
    ]
    print("Repeated Authors Data:")
    # print(repeated_authors_df[["authors", "year", "Methods Used"]])

# Truncate function
def truncate_label(label, max_length=10):
    return label[:max_length] + "..." if len(label) > max_length else label

# Apply truncation to the `authors` column
repeated_authors_df['authors_truncated'] = repeated_authors_df['authors'].apply(lambda x: truncate_label(x, max_length=35))


show(repeated_authors_df[["authors_truncated", "year", "title"]])

# Slice Data for Latest Patents
if "publication_type" in clean_data.columns and "year" in clean_data.columns:
    patents_df = clean_data[clean_data["publication_type"].str.contains("patent", case=False, na=False)]
    latest_patents_df = patents_df.sort_values(by="year", ascending=False).drop(columns=['Literature Survey', 'Limitations', 'Future Research', 'Research Gap'])

    # print("Latest Patents Data:")
    # print(latest_patents_df[["title", "year", "authors"]].head())
    
    # print("Oldest Patents Data:")
    # print(latest_patents_df[["title", "year", "authors"]].tail())
```

# Methods used reviewed

Filtering the data for articles only and only the last five years of articles to be considered. Manually searching for known methods used with a defined function that can be re-utilised.

```{python}
# | echo: true

df_type_filter = (
    clean_data.query(
        'publication_type == ["Proceedings Article", "Journal Article"] & year >= 2021'
    )
    .dropna(subset=["Methods Used"])
    .drop_duplicates(subset=["Methods Used"])
)

# Define phrases to search for
keywords = [
    "deep learning",
    "yolo",
    "cnn",
    "mrvifnet",
    "ldr",
    "rfid",
    "machine learning",
    "ddf",
]


def find_keywords(text, keywords):
    """Finds all matching phrases or keywords in a given text and returns them as a comma-separated string."""
    if pd.isna(text):
        return None
    text_lower = text.lower()
    found = [kw for kw in keywords if kw in text_lower]
    return ", ".join(found) if found else None


# Apply function to the "Methods Used" column
df_type_filter["Found Methods"] = df_type_filter["Methods Used"].apply(
    lambda x: find_keywords(x, keywords)
)
clean_data["Found Methods"] = clean_data["Methods Used"].apply(
    lambda x: find_keywords(x, keywords)
)


# filter to yolo and deep learning
df_future = df_type_filter[
    df_type_filter["Found Methods"].isin(["yolo", "deep learning"])
]

# Save the updated DataFrame
df_future[
    ["Methods Used", "Found Methods", "year", "Results", "Future Research"]
].to_excel("updated_methods.xlsx", index=False)

# show(df_future[["Methods Used", "Found Methods", "Results", "Future Research"]])

px.histogram(df_type_filter, y="Found Methods")

```

Finding the common future research for the articles with yolo and deep learning methods is done as follows:

```{python}
# | echo: true

# Drop NaN values in the "Future Research" column
research_text = df_future["Future Research"].dropna().str.lower()

# Tokenize and count common research-related words/phrases

research_words = []
for text in research_text:
    words = text.split()  # Simple split, can be improved with NLP techniques
    research_words.extend(words)

# Get the most common research words (filtering out generic terms manually may be needed)
common_research = Counter(research_words).most_common(50)  # Checking top words first

# Extract meaningful research names (refining this further using domain knowledge)
common_research_names = [
    method for method, count in common_research if len(method) > 3
][:10]

# Filter the dataset for rows that contain these research
filtered_df = df_future[
    df_future["Future Research"].str.contains(
        "|".join(common_research_names), case=False, na=False
    )
]

# Select relevant columns
result_df = filtered_df[["Future Research", "year", "Found Methods"]]

result_df.to_excel("future_research.xlsx", index=False)

# Display the extracted DataFrame
show(result_df)

```

Now using the large language model to summarize the data.



```{python}
def extract_info(df_type_filter):
    prompt = f"You are an AI assistant reviewing research articles, extract the method used (such as yolo, cnn, deep learning) on this article {df_type_filter} into a category for my report. No explainations. "

    return llm_chat(prompt)

extract_info_vec = np.vectorize(extract_info)
```

```{python}
# Function to review an article
def review_article(article_text):
    prompt = f"You are an AI assistant reviewing research articles. Review this article:\n\n{article_text} "

    return llm_chat(prompt)

```

```{python}
df_test = df_type_filter[["title", "abstract", "Problem Statement", "Methods Used", "Research Gap", "Results", "Limitations", "Objectives"]].head()
# df_test["full_dict"] = df_test.to_dict(orient="records")
df_type_filter["llm_method"] = extract_info_vec(df_type_filter["Methods Used"])
# df_test[["Methods Used", "Results"]]

```

```{python}
df_type_filter.to_csv("papers_review.csv", index=False)
```


# Scopus data

The scopus data filtered through year, language, document type, keywords, and abstract which reduced the entries from 708 to 79. The keywords were manually scanned especially those that needed to be negated.

```{python}
# | echo: true

scopus_rawdata = pd.read_csv("scopus - livestock and deep learning.csv")

# keep articles from 2021 onward
scopus = scopus_rawdata[
    [
        "Authors",
        "Title",
        "Year",
        "Source title",
        "Abstract",
        "Author Keywords",
        "Index Keywords",
        "Language of Original Document",
        "Document Type",
        "DOI",
    ]
].query("Year >= 2021")

# Restrict to English language
scopus = scopus[scopus["Language of Original Document"].isin(["English"])]

# Keep some source title
# scopus = scopus[(scopus["Source title"].str.lower().str.contains("sensors|agriculture|computer|ieee|animals|", na=False))]

# Choose from articles and conference papers
scopus = scopus[
    scopus["Document Type"].str.contains("Article|Conference paper", na=False)
]

# Find certain author keywords to keep
scopus = scopus[
    (
        scopus["Author Keywords"].str.lower().str.contains(
            "deep learning|computer vision|cnn|livestock", na=False
        )
    )
]

# Find certain author keywords to negate
scopus = scopus[
    ~(
        scopus["Author Keywords"].str.lower().str.contains(
            "chicken|poultry|crop|behaviour|behavior|animal welfare|disease|health|climate change|plants|carbon emission|genomic|food|meat quality|dairy|sow|beef", na=False
        )
    )
]

# Find certain index keywords to negate
scopus = scopus[
    ~(
        scopus["Index Keywords"].str.lower().str.contains(
            "crop|disease|health|chicken|poultry|behaviour|behavior|welfare|feces|vegetation|genomic|genome|genetic|food|diagnosis|physiological|reproduction|pollution|plant|dairy|beef|parasite", na=False
        )
    )
]

# Find certain abstract to negate
scopus = scopus[
    ~(
        scopus["Abstract"].str.lower().str.contains(
            "crop|disease|health|chicken|poultry|behaviour|behavior|welfare|feces|vegetation|genomic|genome|genetic|food|diagnosis|physiological|reproduction|pollution|plant|dairy|beef|parasite", na=False
        )
    )
]

scopus
```

Assign a new variable for the found methods using the find_keywords function defined before. Then save the sliced data to an excel file for yolo and deep learning methods which leads to 39 entries.


```{python}
scopus["Found Methods"] = scopus["Abstract"].apply(lambda x: find_keywords(x, keywords))


# filter to yolo and deep learning
scopus_method = scopus[
    scopus["Found Methods"].isin(["yolo", "deep learning", "deep learning, yolo"])
]

# Save the updated DataFrame
scopus_method[
    ["Abstract", "Found Methods", "Year", "Title", "Author Keywords", "DOI"]
].to_excel("scopus_methods.xlsx", index=False)

# show(df_future[["Methods Used", "Found Methods", "Results", "Future Research"]])

px.histogram(scopus, y="Found Methods")
```

```{python}
scopus1 = scopus.head()
# scopus1["full_dict"] = scopus1.to_dict(orient="records")
scopus["llm_method"] = extract_info_vec(scopus["Abstract"])
```

```{python}
# Apply the function to each article (assuming "Abstract" is the column name)
scopus["Review"] = scopus["Abstract"].apply(lambda text: review_article(text) if pd.notna(text) else "No abstract provided")

# Save the reviewed articles
scopus.to_excel("reviewed_articles.xlsx", index=False)
```