
# Loading the data

```{python}
from openai import OpenAI
from local_settings import OPENAI_KEY # Assumes you have a local_settings.py file in your folder with your OpenAI key  
# Initialize the OpenAI client
client = OpenAI(api_key=OPENAI_KEY) # I need a new API_key
```

```{python}
import pandas as pd
import plotly.express as px
from itables import show
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

rw_data = pd.read_excel("livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx", sheet_name="Sheet1", header=0)
```

There are 100 articles in the dataset with 24 columns, for the analysis: columns doi, first_page, last_page, etc can be remove since they do not contain necessary information at this stage

```{python}
clean_data = rw_data.drop(
    columns=[
        "doi",
        "first_page",
        "last_page",
        "conference_series",
        "volume",
        "number",
        "journal",
        "url",
    ]
)

yearVcount = clean_data.groupby(["year", "publication_type"]).count()["abstract"]

fig_line = px.line(
    clean_data.groupby("year").count()["abstract"],
    title="Abstracts trends from 1994 on Livestock Detection",
    labels={"value": "Abstract Count", "year": "Year"},
)

# fig_line.show()

yearVcount = yearVcount.to_frame().reset_index()

fig_bar = px.bar(
    yearVcount,
    x="year",
    y="abstract",
    color="publication_type",
    title="Abstracts trends from 1994 on Livestock Detection",
    labels={"abstract": "Abstract Count", "year": "Year", "publication_type": "Type"},
    # nbins=40,
)
fig_bar.show()

# clean_data["year"].sort_values(ascending=False).head(20)


# def llm_chat(message):
#     response = client.chat.completions.create(
#         model="gpt-4o-mini", messages=[{"role": "user", "content": message}]
#     )
#     return response.choices[0].message.content


# # Test the function here
# llm_chat("What is Python (the language) named after?")
```

```{python}


# Load the data
file_path = (
    "livestock-detection-counting-and-classification_2025-02-06_10_54_12_export.xlsx"
)
df = pd.read_excel(file_path, sheet_name="Sheet1").drop(
    columns=[
        "doi",
        "first_page",
        "last_page",
        "conference_series",
        "volume",
        "number",
        "journal",
        "url",
    ]
).drop_duplicates(subset='abstract')

# Convert 'year' column to numeric
if "year" in df.columns:
    df["year"] = pd.to_numeric(df["year"], errors="coerce")

# Plot Abstract Counts vs. Year using matplotlib
abstract_counts = df.groupby("year").size()
plt.figure(figsize=(10, 5))
plt.bar(abstract_counts.index, abstract_counts.values, color="skyblue")
plt.xlabel("Year")
plt.ylabel("Number of Abstracts")
plt.title("Abstract Counts per Year")
plt.xticks(rotation=45)
plt.show()

# Slice Data for Repeated Authors
if "authors" in df.columns:
    authors_series = (
        df["authors"].dropna().str.split(",")
    )  # Assuming authors are separated by ','
    authors_flat = [author.strip() for authors in authors_series for author in authors]
    author_counts = pd.Series(authors_flat).value_counts()
    repeated_authors = author_counts[author_counts > 1].index.tolist()
    repeated_authors_df = df[
        df["authors"].str.contains("|".join(repeated_authors), na=False)
    ]
    print("Repeated Authors Data:")
    print(repeated_authors_df[["authors", "year", "Methods Used"]])


# Slice Data for Latest Patents
if "publication_type" in df.columns and "year" in df.columns:
    patents_df = df[df["publication_type"].str.contains("patent", case=False, na=False)]
    latest_patents_df = patents_df.sort_values(by="year", ascending=False).drop(columns=['Literature Survey', 'Limitations', 'Future Research', 'Research Gap'])

    print("Latest Patents Data:")
    print(latest_patents_df[["title", "year", "authors"]].head())
    
    print("Oldest Patents Data:")
    print(latest_patents_df[["title", "year", "authors"]].tail())
```


Filtering the data for articles only and only the last five years of articles to be considered. Manually searching for known methods used with a defined function that can be re-utilised.

```{python}
df_type_filter = (
    df.query(
        'publication_type == ["Proceedings Article", "Journal Article"] & year >= 2021'
    )
    .dropna(subset=["Methods Used"])
    .drop_duplicates(subset=["Methods Used"])
)

# Define phrases to search for
keywords = [
    "deep learning",
    "yolo",
    "cnn",
    "mrvifnet",
    "ldr",
    "rfid",
    "machine learning",
    "ddf",
]


def find_keywords(text, keywords):
    """Finds all matching phrases or keywords in a given text and returns them as a comma-separated string."""
    if pd.isna(text):
        return None
    text_lower = text.lower()
    found = [kw for kw in keywords if kw in text_lower]
    return ", ".join(found) if found else None


# Apply function to the "Methods Used" column
df_type_filter["Found Methods"] = df_type_filter["Methods Used"].apply(
    lambda x: find_keywords(x, keywords)
)
df["Found Methods"] = df["Methods Used"].apply(lambda x: find_keywords(x, keywords))


# filter to yolo and deep learning
df_future = df_type_filter[df_type_filter["Found Methods"].isin(["yolo", "deep learning"])]

# Save the updated DataFrame
df_future[["Methods Used", "Found Methods", 'year', "Results", "Future Research"]].to_excel("updated_methods.xlsx", index=False)

# show(df_future[["Methods Used", "Found Methods", "Results", "Future Research"]])

px.histogram(df_type_filter, y="Found Methods")

```

Finding the common future research for the articles with yolo and deep learning methods are done as follows:

```{python}
# Drop NaN values in the "Future Research" column
research_text = df_future["Future Research"].dropna().str.lower()

# Tokenize and count common research-related words/phrases

research_words = []
for text in research_text:
    words = text.split()  # Simple split, can be improved with NLP techniques
    research_words.extend(words)

# Get the most common research words (filtering out generic terms manually may be needed)
common_research = Counter(research_words).most_common(50)  # Checking top words first

# Extract meaningful research names (refining this further using domain knowledge)
common_research_names = [
    method for method, count in common_research if len(method) > 3
][:10]

# Filter the dataset for rows that contain these research
filtered_df = df_future[
    df_future["Future Research"].str.contains(
        "|".join(common_research_names), case=False, na=False
    )
]

# Select relevant columns
result_df = filtered_df[["Future Research", "year", "Found Methods"]]

result_df.to_excel("future_research.xlsx", index=False)

# Display the extracted DataFrame
show(result_df)

```

The scopus data filtered through year, language, document type, keywords, and abstract which reduced the entries from 708 to 79. The keywords were manually scanned especially those that needed to be negated.

```{python}
scopus_rawdata = pd.read_csv("scopus - livestock and deep learning.csv")

# keep articles from 2021 onward
scopus = scopus_rawdata[
    [
        "Authors",
        "Title",
        "Year",
        "Source title",
        "Abstract",
        "Author Keywords",
        "Index Keywords",
        "Language of Original Document",
        "Document Type",
    ]
].query("Year >= 2021")

# Restrict to English language
scopus = scopus[scopus["Language of Original Document"].isin(["English"])]

# Keep some source title
# scopus = scopus[(scopus["Source title"].str.lower().str.contains("sensors|agriculture|computer|ieee|animals|", na=False))]

# Choose from articles and conference papers
scopus = scopus[
    scopus["Document Type"].str.contains("Article|Conference paper", na=False)
]

# Find certain author keywords to keep
scopus = scopus[
    (
        scopus["Author Keywords"].str.lower().str.contains(
            "deep learning|computer vision|cnn|livestock", na=False
        )
    )
]

# Find certain author keywords to negate
scopus = scopus[
    ~(
        scopus["Author Keywords"].str.lower().str.contains(
            "chicken|poultry|crop|behaviour|behavior|animal welfare|disease|health|climate change|plants|carbon emission|genomic|food|meat quality|dairy|sow|beef", na=False
        )
    )
]

# Find certain index keywords to negate
scopus = scopus[
    ~(
        scopus["Index Keywords"].str.lower().str.contains(
            "crop|disease|health|chicken|poultry|behaviour|behavior|welfare|feces|vegetation|genomic|genome|genetic|food|diagnosis|physiological|reproduction|pollution|plant|dairy|beef|parasite", na=False
        )
    )
]

# Find certain abstract to negate
scopus = scopus[
    ~(
        scopus["Abstract"].str.lower().str.contains(
            "crop|disease|health|chicken|poultry|behaviour|behavior|welfare|feces|vegetation|genomic|genome|genetic|food|diagnosis|physiological|reproduction|pollution|plant|dairy|beef|parasite", na=False
        )
    )
]

scopus
```

Assign a new variable for the found methods using the find_keywords func defined before. Then save the sliced data to an excel file

```{python}
scopus["Found Methods"] = scopus["Abstract"].apply(lambda x: find_keywords(x, keywords))


# filter to yolo and deep learning
scopus_method = scopus[
    scopus["Found Methods"].isin(["yolo", "deep learning", "deep learning, yolo"])
]

# Save the updated DataFrame
scopus_method[
    ["Abstract", "Found Methods", "Year", "Title", "Author Keywords"]
].to_excel("scopus_methods.xlsx", index=False)

# show(df_future[["Methods Used", "Found Methods", "Results", "Future Research"]])

px.histogram(scopus, y="Found Methods")
```