"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Funding Details","Funding Texts","Correspondence Address","Editors","Publisher","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Abbreviated Source Title","Document Type","Publication Stage","Open Access","Source","EID"
"Cao Y.; Yin Z.; Duan Y.; Cao R.; Hu G.; Liu Z.","Cao, Yue (59340309100); Yin, Zhe (58844263400); Duan, Yongpeng (59340356100); Cao, Riliang (59340295000); Hu, Guangying (59526968300); Liu, Zhenyu (55714953500)","59340309100; 58844263400; 59340356100; 59340295000; 59526968300; 55714953500","Research on improved sound recognition model for oestrus detection in sows","2025","Computers and Electronics in Agriculture","231","","109975","","","","0","10.1016/j.compag.2025.109975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216023324&doi=10.1016%2fj.compag.2025.109975&partnerID=40&md5=3b6f391d40c3b6d06d3022518638bab4","School of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; College of Software, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; College of Animal Science, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; College of Agricultural Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Dryland Farm Machinery Key Technology and Equipment Key Laboratory of Shanxi Province, Taigu, 030801, China","Cao Y., School of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Yin Z., College of Software, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Duan Y., School of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Cao R., College of Animal Science, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Hu G., College of Animal Science, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Liu Z., College of Agricultural Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China, Dryland Farm Machinery Key Technology and Equipment Key Laboratory of Shanxi Province, Taigu, 030801, China","In modern pig farming, precise monitoring of the sow's oestrus status can significantly improve reproductive efficiency and litter size, reduce labor costs, decrease non-productive days, safeguard sow health and welfare, and promote the development of intelligent, unmanned real-time monitoring systems. To address this need, this study proposes an improved model based on MobileViT, named UM-ASPP-MobileViT, for the automated recognition of sow vocalisations across different oestrus stages. During dataset construction, vocal samples from sows were collected and annotated across various oestrus phases, non-oestrus periods, and pseudo-oestrus periods. This dataset provides crucial support for accurately determining whether a sow is in oestrus and identifying specific oestrus stages. As some of the collected audio samples contained environmental noise, discrete wavelet transform (DWT) was used for denoising, effectively removing noise while preserving key features. Mel-frequency cepstral coefficients (MFCC) were employed for feature extraction, and Mel spectrograms reflecting three-dimensional information were generated as model inputs to improve the accuracy of feature representation. The experimental results show that the improved MobileViT model, while reducing the computational complexity and speeding up the inference speed, achieves a classification precision of 96.52 %, an F1 value of 96.53 %, and a computational amount of only 1.44 GFLOPs, which is a significant progress. This model provides an efficient and accurate automated solution for sow oestrus monitoring, applicable to unmanned, non-contact real-time monitoring systems for precise oestrus identification and timely warning, showcasing significant application prospects and economic benefits. © 2025 Elsevier B.V.","Classification model; Deep learning spectrogram; MobileViT; Oestrus sound recognition; Sound feature extraction","Macroinvertebrates; Wages; Wavelet transforms; Classification models; Deep learning spectrogram; Estrus sound recognition; Mobilevit; Pig farming; Real time monitoring system; Recognition models; Sound feature extractions; Sound recognition; Spectrograms; classification; identification method; instrumentation; livestock farming; machine learning; pattern recognition; pig; reproductive strategy; research work; vocalization; Spectrographs","","","Z. Liu; College of Agricultural Engineering, Shanxi Agricultural University, Taigu, Shanxi, 030801, China; email: lzysyb@126.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85216023324"
"Ma C.; Deng M.; Yin Y.","Ma, Chuang (58297591900); Deng, Minghui (35745662400); Yin, Yanling (55274316800)","58297591900; 35745662400; 55274316800","Pig face recognition based on improved YOLOv4 lightweight neural network","2024","Information Processing in Agriculture","11","3","","356","371","15","5","10.1016/j.inpa.2023.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160690515&doi=10.1016%2fj.inpa.2023.03.004&partnerID=40&md5=ae4dc5eeb77ee9e7cc539ff9b97a5624","School of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China","Ma C., School of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; Deng M., School of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; Yin Y., School of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China","With the vigorous development of intelligence agriculture, the progress of automated large-scale and intensive pig farming has accelerated significantly. As a biological feature, the pig face has important research significance for precise breeding of pigs and traceability of health. In the management of live pigs, many managers adopt traditional methods, including color marking and RFID identification, but there will be problems such as off-label, mixed-label and waste of manpower. This work proposes a non-invasive way to study the identification of multiple individuals in pigs. The model was to first replace the original backbone network of YOLOv4 with MobileNet-v3, a popular lightweight network. Then depth-wise separable convolution was adopted in YOLOv4′s feature extraction network SPP and PANet to further reduce network parameters. Moreover, CBAM attention mechanism formed by the concatenation of CAM and SAM was added to PANet to ensure the network accuracy while reducing the model weight. The introduction of multi-attention mechanism selectively strengthened key areas of pig face and filtered out weak correlation features, so as to improve the overall model effect. Finally, an improved MobileNetv3-YOLOv4-PACNet (M-YOLOv4-C) network model was proposed to identify individual sows. The mAP were 98.15 %, the detection speed FPS were 106.3frames/s, and the model parameter size was only 44.74 MB, which can be well implanted into the small-volume pig house management sensors and applied to the pig management system in a lightweight, fast and accurate manner. This model will provide model support for subsequent pig behavior recognition and posture analysis. © 2023 China Agricultural University","Attention mechanism; Convolutional neural network; Deep learning; Pig face recognition","Behavioral research; Convolution; Deep neural networks; Farms; Mammals; Radio frequency identification (RFID); Waste management; Attention mechanisms; Biological features; Color markings; Convolutional neural network; Deep learning; Large-scales; Neural-networks; Pig face recognition; Pig farming; Research significances; artificial neural network; livestock farming; machine learning; pattern recognition; pig; sensor; Face recognition","Abnormal State and Acoustic Sensing Mechanism of Pigs; General Project of National Natural Science Foundation of China, (32172784)","Funding text 1: The authors would like to thank the editor in chief and the anonymous referees for their valuable suggestions and useful comments that improved the paper content substantially. This study was supported by General Project of National Natural Science Foundation of China, Research on Model Construction and Early Warning Method of Abnormal State and Acoustic Sensing Mechanism of Pigs (32172784).; Funding text 2: The authors would like to thank the editor in chief and the anonymous referees for their valuable suggestions and useful comments that improved the paper content substantially. This study was supported by General Project of National Natural Science Foundation of China, Research on Abnormal State and Acoustic Perception Model Construction and Early Warning Method of Pigs (32172784).","M. Deng; School of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; email: markdmh@163.com","","China Agricultural University","20970153","","","","English","Inf. Process. Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160690515"
"Wutke M.; Lensches C.; Hartmann U.; Traulsen I.","Wutke, Martin (57217124907); Lensches, Clara (57216583979); Hartmann, Ulrich (59343858000); Traulsen, Imke (35410826800)","57217124907; 57216583979; 59343858000; 35410826800","Towards automatic farrowing monitoring—A Noisy Student approach for improving detection performance of newborn piglets","2024","PLoS ONE","19","10","e0310818","","","","0","10.1371/journal.pone.0310818","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205528558&doi=10.1371%2fjournal.pone.0310818&partnerID=40&md5=0bbc2500a5717865b5ac5fe39d5fadbe","Institute of Animal Breeding and Husbandry, Faculty of Agricultural and Nutritional Sciences, University of Kiel, Kiel, Germany; Faculty of Agriculture, South Westphalia University of Applied Sciences, Soest, Germany; Chamber of Agriculture Lower Saxony, Division Agriculture, Oldenburg, Germany; Department of Animal Sciences, Georg-August University, Göttingen, Germany","Wutke M., Institute of Animal Breeding and Husbandry, Faculty of Agricultural and Nutritional Sciences, University of Kiel, Kiel, Germany, Faculty of Agriculture, South Westphalia University of Applied Sciences, Soest, Germany; Lensches C., Department of Animal Sciences, Georg-August University, Göttingen, Germany; Hartmann U., Chamber of Agriculture Lower Saxony, Division Agriculture, Oldenburg, Germany; Traulsen I., Institute of Animal Breeding and Husbandry, Faculty of Agricultural and Nutritional Sciences, University of Kiel, Kiel, Germany, Department of Animal Sciences, Georg-August University, Göttingen, Germany","Nowadays, video monitoring of farrowing and automatic video evaluation using Deep Learning have become increasingly important in farm animal science research and open up new possibilities for addressing specific research questions like the determination of husbandry relevant indicators. A robust detection performance of newborn piglets is essential for reliably monitoring the farrowing process and to access important information about the welfare status of the sow and piglets. Although object detection algorithms are increasingly being used in various scenarios in the field of livestock farming, their usability for detecting newborn piglets has so far been limited. Challenges such as frequent animal occlusions, high overlapping rates or strong heterogeneous animal postures increase the complexity and place new demands on the detection model. Typically, new data is manually annotated to improve model performance, but the annotation effort is expensive and time-consuming. To address this problem, we propose a Noisy Student approach to automatically generate annotation information and train an improved piglet detection model. By using a teacher-student model relationship we transform the image structure and generate pseudo-labels for the object classes piglet and tail. As a result, we improve the initial detection performance of the teacher model from 0.561, 0.838, 0.672 to 0.901, 0.944, 0.922 for the performance metrics Recall, Precision and F1-score, respectively. The results of this study can be used in two ways. Firstly, the results contribute directly to the improvement of piglet detection in the context of birth monitoring systems and the evaluation of the farrowing progress. Secondly, the approach presented can be transferred to other research questions and species, thereby reducing the problem of cost-intensive annotation processes and increase training efficiency. In addition, we provide a unique dataset for the detection and evaluation of newborn piglets and sow body parts to support researchers in the task of monitoring the farrowing process. © 2024 Wutke et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","","Algorithms; Animal Husbandry; Animal Welfare; Animals; Animals, Newborn; Behavior, Animal; Deep Learning; Female; Swine; Video Recording; algorithm; animal experiment; animal welfare; Article; automatic farrowing monitoring; body mass; deep learning; detection algorithm; environmental noise; farm animal; lactation; learning algorithm; livestock; machine learning; molecular genetics; motivation; newborn; nonhuman; piglet; training; animal; animal behavior; animal husbandry; female; newborn; pig; procedures; videorecording","German Federal Office for Agriculture and Food; Bundesanstalt für Landwirtschaft und Ernährung, BLE, (28DE109G18)","The project (DigiSchwein) was supported by funds of the German Federal Office for Agriculture and Food (BLE) (grant numbers: 28DE109G18). The following authors have been funded: M.W., C.L., U.H. https://www.ble.de/EN/ Home/home_node.html The funder did not play any role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. The authors wish to thank the project partners and the farm staff of the experimental research farm of the Chamber of Agriculture Lower Saxony in Bad Zwischenahn\u2014Wehnen, Germany for their support and providing animals and housing for the project.","M. Wutke; Institute of Animal Breeding and Husbandry, Faculty of Agricultural and Nutritional Sciences, University of Kiel, Kiel, Germany; email: mwutke@tierzucht.uni-kiel.de","","Public Library of Science","19326203","","POLNC","39356687","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85205528558"
"Kumar H.; Qin X.; Bhushan B.; Dutt T.; Panigrahi M.","Kumar, Harshit (57201643475); Qin, Xinghu (57190873088); Bhushan, Bharat (55553734478); Dutt, Triveni (7004644381); Panigrahi, Manjit (35198580600)","57201643475; 57190873088; 55553734478; 7004644381; 35198580600","DeepGenomeScan of 15 Worldwide Bovine Populations Detects Spatially Varying Positive Selection Signals","2024","OMICS A Journal of Integrative Biology","28","10","","504","513","9","0","10.1089/omi.2024.0154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205921821&doi=10.1089%2fomi.2024.0154&partnerID=40&md5=635fa842b162bba3d697817b3e3a244f","Division of Animal Genetics, Indian Veterinary Research Institute, Izatnagar, India; ICAR—National Research Centre on Mithun, Medziphema, India; School of Ecology and Nature Conservation, Beijing Forestry University, Beijing, China; Indian Veterinary Research Institute, Izatnagar, India","Kumar H., Division of Animal Genetics, Indian Veterinary Research Institute, Izatnagar, India, ICAR—National Research Centre on Mithun, Medziphema, India; Qin X., School of Ecology and Nature Conservation, Beijing Forestry University, Beijing, China; Bhushan B., Division of Animal Genetics, Indian Veterinary Research Institute, Izatnagar, India; Dutt T., Indian Veterinary Research Institute, Izatnagar, India; Panigrahi M., Division of Animal Genetics, Indian Veterinary Research Institute, Izatnagar, India","Identifying genomic regions under selection is essential for understanding the genetic mechanisms driving species evolution and adaptation. Traditional methods often fall short in detecting complex, spatially varying selection signals. Recent advances in deep learning, however, present promising new approaches for uncovering subtle selection signals that traditional methods might miss. In this study, we utilized the deep learning framework DeepGenomeScan to detect spatially varying selection signatures across 15 bovine populations worldwide. Our analysis uncovered novel insights into selective sweep hotspots within the bovine genome, revealing key genes associated with physiological and adaptive traits that were previously undetected. We identified significant quantitative trait loci linked to milk protein and fat percentages. By comparing the selection signatures identified in this study with those reported in the Bovine Genome Variation Database, we discovered 38 novel genes under selection that were not identified through traditional methods. These genes are primarily associated with milk and meat yield and quality. Our findings enhance our understanding of spatially varying selection’s impact on bovine genomic diversity, laying a foundation for future research in genetic improvement and conservation. This is the first deep learning-based study of selection signatures in cattle, offering new insights for evolutionary and livestock genomics research. ª Mary Ann Liebert, Inc.","bovine genomics; deep learning; ecology; genome scan; natural selection; signatures of selection","Animals; Cattle; Deep Learning; Genetics, Population; Genome; Genomics; Quantitative Trait Loci; Selection, Genetic; fat; Ki 67 antigen; milk protein; organic anion transporter D; potassium channel KCNQ1; sodium proton exchange protein 1; Angus cattle; animal experiment; armc9 gene; Article; atpaf2 gene; bovine; brown swiss cattle; ccdc171 gene; controlled study; cyfip2 gene; dach1 gene; data base; deep learning; gene; gene structure; genome; geographic origin; gid4 gene; Gir cattle; Guernsey; guernsey cattle; hariana cattle; Hereford cattle; hissar cattle; Holstein cattle; ift88 gene; Jersey cattle; kankraj cattle; msrb3 gene; Nellore cattle; nonhuman; Ongole cattle; population; principal component analysis; quantitative trait locus; red sindhi cattle; Sahiwal cattle; selective sweep; single nucleotide polymorphism; taurine cattle; tharparkar cattle; zebu; animal; bovine; deep learning; genetic selection; genetics; genome; genomics; population genetics; procedures; quantitative trait locus","ICAR - National Agricultural Science Fund, NASF; Research and Innovation Program of Beijing Forestry University; Indian Council of Agricultural Research, ICAR; Department of Agricultural Research and Education, Government of India; Chinese Academy of Sciences, CAS; China Postdoc Council; ICAR-Indian Veterinary Research Institute, ICAR-IVRI, (YJ20210310); ICAR-Indian Veterinary Research Institute, ICAR-IVRI; China Postdoctoral Science Foundation, (2022M723075); China Postdoctoral Science Foundation","This study was conducted at the Indian Veterinary Research Institute (IVRI), Izatnagar, an institute under the Indian Council of Agricultural Research (ICAR), Department of Agricultural Research and Education, Government of India, and funded by ICAR-National Agricultural Science Foundation (NASF). The authors would like to express their gratitude to the Director, ICAR-IVRI, for providing all the requisite facilities for the study. X.Q. would like to thank the International Postdoctoral Exchange Fellowship Program (Talent-Introduction Program, YJ20210310) from the China Postdoc Council and CAS Special Research Associate Funding from the Chinese Academy of Sciences, as well as The General Program of China Postdoctoral Science Foundation (2022M723075) and the Research and Innovation Program of Beijing Forestry University.","M. Panigrahi; Division of Animal Genetics ICAR-Indian Veterinary Research Institute, Bareilly, Izatnagar, UP, 243122, India; email: manjit707@gmail.com; X. Qin; School of Ecology and Nature Conservation, Beijing Forestry University, Beijing, 100083, China; email: qin.xinghu@163.com","","Mary Ann Liebert Inc.","15362310","","OMICA","39315920","English","OMICS J. Integr. Biol.","Article","Final","","Scopus","2-s2.0-85205921821"
"Li L.; Awada T.; Shi Y.; Jin V.L.; Kaiser M.","Li, Lidong (57209245381); Awada, Tala (6506777405); Shi, Yeyin (57020187200); Jin, Virginia L. (6602385633); Kaiser, Michael (55437114300)","57209245381; 6506777405; 57020187200; 6602385633; 55437114300","Global Greenhouse Gas Emissions From Agriculture: Pathways to Sustainable Reductions","2025","Global Change Biology","31","1","e70015","","","","0","10.1111/gcb.70015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213696981&doi=10.1111%2fgcb.70015&partnerID=40&md5=f83399ddcbee4f12b305f22c10b49db6","University of Nebraska-Lincoln, Lincoln, NE, United States; Agroecosystem Management Research Unit, USDA-Agricultural Research Service (ARS), Lincoln, NE, United States","Li L., University of Nebraska-Lincoln, Lincoln, NE, United States; Awada T., University of Nebraska-Lincoln, Lincoln, NE, United States; Shi Y., University of Nebraska-Lincoln, Lincoln, NE, United States; Jin V.L., Agroecosystem Management Research Unit, USDA-Agricultural Research Service (ARS), Lincoln, NE, United States; Kaiser M., University of Nebraska-Lincoln, Lincoln, NE, United States","Agriculture serves as both a source and a sink of global greenhouse gases (GHGs), with agricultural intensification continuing to contribute to GHG emissions. Climate-smart agriculture, encompassing both nature- and technology-based actions, offers promising solutions to mitigate GHG emissions. We synthesized global data, between 1990 and 2021, from the Food and Agriculture Organization (FAO) of the United Nations to analyze the impacts of agricultural activities on global GHG emissions from agricultural land, using structural equation modeling. We then obtained predictive estimates of agricultural GHG emissions for the future period of 2022–2050 using deep-learning models. The FAO data show that, from 1990 to 2021, global livestock numbers, inorganic nitrogen (N) fertilizer use, crop residue, and irrigation area increased by 27%, 47%, 49%, and 37%, respectively. The increased livestock numbers contributed to the increases in CH4 and N2O emissions, while inorganic N fertilizer, crop residue, and irrigation mainly contributed to the increases in N2O emissions. Emissions of CO2 decreased because of a 29% reduction in net forest loss. As a result of the reduced deforestation emissions, the overall agricultural GHG emissions declined from 11.50 to 10.89 GtCO2eq from 1990 to 2021 despite the increases in livestock numbers, inorganic N fertilizer, crop residue, and irrigation. Looking ahead, our model predicts that if current agricultural trends persist, GHG emissions will rise to 11.82 ± 0.07 GtCO2eq in 2050. However, maintaining agricultural GHG emissions at the 2021 level through 2050 is possible if the rate of reduction in net forest loss is doubled. Furthermore, if the rate is tripled, agricultural GHG emissions can be limited to 9.85 ± 0.07 GtCO2eq in 2050. Our findings suggest that reductions in agricultural GHG emissions, alongside sustainable agricultural intensification and climate-smart agricultural practices, can be achieved through parallel efforts emphasizing accelerated forest conservation. © 2024 The Author(s). Global Change Biology published by John Wiley & Sons Ltd.","carbon dioxide; cropland; fertilizer; forest loss; livestock; methane; nitrous oxide; pasture","Agriculture; Animals; Carbon Dioxide; Conservation of Natural Resources; Fertilizers; Greenhouse Effect; Greenhouse Gases; Livestock; Models, Theoretical; Nitrous Oxide; carbon dioxide; fertilizer; nitrous oxide; agricultural practice; agriculture; alternative agriculture; crop residue; emission control; fertilizer; greenhouse gas; inorganic nitrogen; livestock; methane; nitrogen dioxide; nitrous oxide; pasture; sustainable development; agriculture; animal; environmental protection; greenhouse effect; greenhouse gas; livestock; procedures; theoretical model","Long‐Term Agroecosystem Research; University of Nebraska-Lincoln, UNL; Agricultural Research Division; Nebraska Research Initiative; United States Department of Agriculture and Long-Term Agroecosystem Research; Long-Term Agroecosystem Research; U.S. Department of Agriculture, USDA; Office of Research and Economic Development, University of Nebraska-Lincoln; Foundation for Appalachian Ohio, FAO, (FAO 2024); Foundation for Appalachian Ohio, FAO","Funding text 1: We thank Christopher Misar for providing insights on agricultural management. We thank Francesco Tubiello for providing comments and edits. We thank FAO for the publicly available database (FAO 2024 ). This work was completed utilizing the Holland Computing Center of the University of Nebraska, which receives support from the UNL Office of Research and Economic Development, the Nebraska Research Initiative and the Agricultural Research Division. This study was supported by the Long\u2010Term Agroecosystem Research (LTAR) network of the United States Department of Agriculture (USDA), and Partnership on Data Innovation (PDI), USDA. ; Funding text 2: Funding: This work was supported by the Partnership on Data Innovation of the United States Department of Agriculture and Long-Term Agroecosystem Research (LTAR) network of the United States Department of Agriculture. We thank Christopher Misar for providing insights on agricultural management. We thank Francesco Tubiello for providing comments and edits. We thank FAO for the publicly available database (FAO\u00A02024). This work was completed utilizing the Holland Computing Center of the University of Nebraska, which receives support from the UNL Office of Research and Economic Development, the Nebraska Research Initiative and the Agricultural Research Division. This study was supported by the Long-Term Agroecosystem Research (LTAR) network of the United States Department of Agriculture (USDA), and Partnership on Data Innovation (PDI), USDA.","L. Li; University of Nebraska-Lincoln, Lincoln, United States; email: lli32@unl.edu","","John Wiley and Sons Inc","13541013","","","39740017","English","Global Change Biol.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85213696981"
"Rivas H.; Touchais H.; Thierion V.; Millet J.; Curtet L.; Fauvel M.","Rivas, Henry (57384150800); Touchais, Hélène (57221253073); Thierion, Vincent (36783564600); Millet, Jerome (59383588400); Curtet, Laurence (36767023900); Fauvel, Mathieu (11839212900)","57384150800; 57221253073; 36783564600; 59383588400; 36767023900; 11839212900","Nationwide operational mapping of grassland first mowing dates combining machine learning and Sentinel-2 time series","2024","Remote Sensing of Environment","315","","114476","","","","0","10.1016/j.rse.2024.114476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207336345&doi=10.1016%2fj.rse.2024.114476&partnerID=40&md5=700e5ca34370a3ea25a1a8d06e50cb4d","Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France; Office Français de la Biodiversité (OFB), Direction de la Recherche et de l'Appui Scientifique, Villiers-en-Bois, 79360, France; Office Français de la Biodiversité (OFB), Direction de la Recherche et de l'Appui Scientifique, Birieux, Montfort, 01330, France","Rivas H., Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France; Touchais H., Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France; Thierion V., Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France; Millet J., Office Français de la Biodiversité (OFB), Direction de la Recherche et de l'Appui Scientifique, Villiers-en-Bois, 79360, France; Curtet L., Office Français de la Biodiversité (OFB), Direction de la Recherche et de l'Appui Scientifique, Birieux, Montfort, 01330, France; Fauvel M., Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France","Grassland dynamics are modulated by management intensity and impact overall ecosystem functioning. In mowed grasslands, the first mowing date is a key indicator of management intensification. The aim of this work was to assess several supervised regression models for mapping grassland first mowing date at national-level using Sentinel-2 time series. Three deep-learning architectures, two conventional machine learning models and two threshold-based methods (fixed and relative) were compared. Algorithms were trained/calibrated and tested from field observations, using a spatial cross-validation approach. Our findings showed that time aware deep-learning models – Lightweight Temporal Attention Encoder (LTAE) and 1D Convolutional Neural Network (1D-CNN) – yielded higher performances compared to Multilayer Perceptron, Random Forest and Ridge Regression models. Threshold-based methods under-performed compared to all other models. Best model (LTAE) mean absolute error was within six days with a coefficient of determination of 0.52. Additionally, errors were accentuated at extreme (late/early) mowing dates, which were underrepresented in the data set. Oversampling techniques did not improve predicting extreme mowing dates. Finally, the best prediction accuracy was obtained when the number of clear dates surrounding the mowing event was greater than 2. Our outputs evidenced time aware deep-learning models’ potential for large-scale grassland first mowing event monitoring. A national-level map was produced to support bird-life monitoring or public policies for biodiversity and agro-ecological transition in France. © 2024 The Authors","Deep-learning models; Grassland management intensification; Mowing dates mapping; Regression; Satellite image time series","Abiotic; Contrastive Learning; Convolutional neural networks; Deep learning; Federated learning; Livestock; Multilayer neural networks; Vegetation mapping; Deep-learning model; Grassland management intensification; Image time-series; Learning models; Mowing date mapping; Regression; Regression modelling; Satellite image time series; Satellite images; Times series; accuracy assessment; algorithm; ecosystem function; grassland; machine learning; mowing; regression analysis; satellite imagery; Sentinel; time series analysis; Adversarial machine learning","Centre National d’Etudes Spatiales, CNES; Agence française pour la biodiversité, AFB; Office Français de la Biodiversité, OFB","The authors would like to thank OFB's fieldworker (departmental services 03, 14, 15, 25, 37, 39, 43, 50, 71) and Permanent Center for Environmental Initiatives of Val de Vienne for the field campaign, the INRAe-Herbip\u00F4le Team for sharing their data set, the Theia Center for making available the Sentinel-2 satellite image time series, the CNES for funding H. Touchais and H. Rivas work and for providing the HPC facility. M. Fauvel wants to thank the CESBIO AI Team for fruitful discussions and the IOTA2 core team for their helps, and the French Biodiversity Agency (OFB) for co-funding this project. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.","M. Fauvel; Centre d'Etudes Spatiales de la Biosphère (CESBIO), Université de Toulouse, CNES/CNRS/INRAE/IRD/UT3-Paul Sabatier, Toulouse, 31401, France; email: mathieu.fauvel@inrae.fr","","Elsevier Inc.","00344257","","RSEEA","","English","Remote Sens. Environ.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85207336345"
"AlZubi A.A.","AlZubi, Ahmad Ali (57318589300)","57318589300","Detection of Disease in Calves using Artificial Intelligence","2024","Indian Journal of Animal Research","58","12","","2138","2145","7","0","10.18805/IJAR.BF-1760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213485159&doi=10.18805%2fIJAR.BF-1760&partnerID=40&md5=707a6fef383419ec1e77cd80501d4563","Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia","AlZubi A.A., Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia","Background: Livestock farming is experiencing a digital transformation and is becoming more information-driven. However, this type of data is often kept in separate storage towers, making it incapable of practicing its potential to boost animal welfare. Lumpy Skin Disease (LSD) is a serious threat to the health of cattle worldwide and has caused financial problems for many cattle farming enterprises. It has been shown that combining machine learning (ML) and artificial intelligence (AI) with biosensor data and conventional visual inspections can improve the identification and diagnosis of this serious condition. Methods: This study presents an extremely precise livestock farming framework that combines data streams from a wide array of disciplines of dairy cattle to see if ever wider/vast data sources enhance the overall projections for diseases and if using the more complicated prediction models can reimburse for less diverse data to some extent. Using images from the farming landscape, this study highlights the utility of convolutional neural networks (CNNs) in the identification of the Lumpy Skin Disease Virus (LSDV) in animals. Result: An analysis of the relative weight given to individual factors in predicting accuracy shows that disease in dairy cattle results from the intricate interactions between many life domains/parameters, such as housing, nutrition and climate; that prediction performance is enhanced by incorporating a wider range of data sources and that current information can be repurposed to produce useful information for vaccine development. The study highlights the potential of data-driven dairy interventions, focusing on artificial intelligence for disease prediction in cattle, to improve animal welfare and reduce the risk of disease. A Convolutional Neural Network (CNN) based model effectively classified skin conditions with an overall accuracy of 73.89% after 27 training epochs. This study demonstrates CNN’s useful applications in the field of veterinary medicine by highlighting its potential for early detection of Lumpy Skin Disease (LSD). © 2024 Agricultural Research Communication Centre. All rights reserved.","Animal health; Artificial intelligence; Bovine respiratory disease (BRD); Cattle farming; Deep learning; Machine learning; Neonatal calf diarrhoea (NCD)","accuracy; African swine fever; animal experiment; animal lameness; animal model; Article; artificial intelligence; augmentation index; calf (bovine); cattle farming; coccidiosis; convolutional neural network; dairy cattle; data processing; electric conductivity; facial recognition; feature extraction; female; gradient boosted tree; lactose yield; leg movement; livestock; lumpy skin disease; Lumpy skin disease virus; machine learning; mastitis; measurement precision; milk yield; musculoskeletal function; neck movement; nonhuman; nutrition; physical phenomena; prediction; puerperal disorder; random forest; recall bias; somatic cell count; training; validation process; velocity","King Saud University, KSU","This work was supported by the Researchers Supporting Project number (RSP2024R395), King Saud University, Riyadh, Saudi Arabia The author thank King Saud University for funding this work through the Researchers Supporting Project number (RSP2024R395), King Saud University, Riyadh, Saudi Arabia.","A.A. AlZubi; Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia; email: aalzubi@ksu.edu.sa","","Agricultural Research Communication Centre","03676722","","","","English","Ind. J. Ani. Res","Article","Final","","Scopus","2-s2.0-85213485159"
"Liu Y.; Li W.; Liu X.; Li Z.; Yue J.","Liu, Yeqiang (58938458400); Li, Weiran (57310991800); Liu, Xue (57361745500); Li, Zhenbo (55613118900); Yue, Jun (7101875757)","58938458400; 57310991800; 57361745500; 55613118900; 7101875757","Deep learning in multiple animal tracking: A survey","2024","Computers and Electronics in Agriculture","224","","109161","","","","6","10.1016/j.compag.2024.109161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196782688&doi=10.1016%2fj.compag.2024.109161&partnerID=40&md5=865abe962e19b6aac8d28f50f0e89993","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; National Innovation Center for Digital Fishery, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China; Key Laboratory of Smart farming for aquatic animal and livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; College of Information and Electrical Engineering, LuDong University, Yantai, 264025, China","Liu Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China, Key Laboratory of Smart farming for aquatic animal and livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Li W., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China, Key Laboratory of Smart farming for aquatic animal and livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Liu X., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Li Z., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China, Key Laboratory of Smart farming for aquatic animal and livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Yue J., College of Information and Electrical Engineering, LuDong University, Yantai, 264025, China","Multiple animal tracking based on deep learning is a crucial and demanding task with extensive applications in agriculture, livestock farming, and ecological research. The primary objectives comprise monitoring livestock health, preserving endangered wildlife, and automatically detecting animal behaviors. The task involves a two-step procedure of localizing and tracking individual animals across sequential video frames. However, tracking animals in complex scenarios is still prone to challenges like occlusion and identity switches due to intensive and similar motion patterns. Conventional animal tracking methods fail to meet the precision and real-time speed requirements of real-world applications. In recent years, deep learning has attained remarkable achievements in multiple animal tracking by leveraging deep neural networks for feature extraction. This paper presents a comprehensive review of the development and applications of various multiple animal tracking approaches over the past five years. These methods are categorized according to tracking paradigms, covering diverse animal species comprising both livestock and wildlife. To begin with, we analyze various animal tracking algorithms constructed upon diverse tracking paradigms and summarize their strengths and weaknesses concerning animal subjects. Subsequently, we present open-source benchmark datasets and evaluation metrics, while also discussing and analyzing their applicability in animal tracking. In addition, we review practical applications of animal tracking in agricultural, wildlife, and laboratory environments. Finally, we present an in-depth discussion of datasets, tracking approaches, and promising future perspectives. We hope that our work can make some contributions to the field of multiple animal tracking. © 2024 Elsevier B.V.","Behavior recognition; Computer vision; Deep learning; Multiple animal tracking","Agriculture; Behavioral research; Computer vision; Deep neural networks; Animal behaviour; Animal tracking; Behaviour recognition; Deep learning; Endangered wildlife; Livestock farming; Multiple animal tracking; Primary objective; Tracking approaches; Two-step procedure; benchmarking; biomonitoring; computer vision; livestock; machine learning; precision; recognition; survey method; tracking; Animals","National Major Science and Technology Projects of China, (2021ZD0113805); National Major Science and Technology Projects of China","The authors acknowledge the financial support from the National Science and Technology Major Project (2021ZD0113805).","Z. Li; P.O. Box121, China Agricultural University, Beijing, 17 Tsinghua East Road, 100083, China; email: zhenboli@126.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","","Scopus","2-s2.0-85196782688"
"Zhai J.; Duan S.; Luo B.; Jin X.; Dong H.; Wang X.","Zhai, Jiawei (57829690200); Duan, Shuhao (59415635100); Luo, Bin (57203865682); Jin, Xiaotong (57829457600); Dong, Hongtu (57205494897); Wang, Xiaodong (57213151389)","57829690200; 59415635100; 57203865682; 57829457600; 57205494897; 57213151389","Classification techniques of ion selective electrode arrays in agriculture: a review","2024","Analytical Methods","16","47","","8068","8079","11","0","10.1039/d4ay01346h","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209552995&doi=10.1039%2fd4ay01346h&partnerID=40&md5=ecd524a9f03d5a1367007ecf4c858a7c","Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Department of Artificial Intelligence and Automation, Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China; Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China","Zhai J., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Department of Artificial Intelligence and Automation, Faculty of Information Technology, Beijing University of Technology, Beijing, 100124, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China; Duan S., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China; Luo B., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China; Jin X., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China; Dong H., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China; Wang X., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, Key Laboratory of Agricultural Sensors, Ministry of Agriculture and Rural Affairs, Beijing, 100097, China","Agriculture has a substantial demand for classification, and each agricultural product exhibits a unique ion signal. This paper summarizes the classification techniques of ion-selective electrode arrays in agriculture. Initially, data sample collection methods based on ion-selective electrode arrays are summarized. The paper then discusses the current state of classification algorithms from the perspectives of machine learning, artificial neural networks, extreme learning machines, and deep learning, along with their existing research in ion-selective electrodes and related fields. Then, the potential applications in crop and livestock growth status classification, soil classification, agricultural product quality classification, and agricultural product type classification are discussed. Ultimately, the future challenges of ion-selective electrode research are discussed from the perspectives of the sensor itself and algorithms combined with sensor arrays, which also positively impact the promotion of their application in agriculture. This work will advance the application of classification techniques combined with ion-selective electrode arrays in agriculture. © 2024 The Royal Society of Chemistry.","","Deep neural networks; ion; 'current; Classification algorithm; Classification technique; Collection methods; Data sample; Ion signals; Ion-selective electrode; Ion-selective electrodes array; Machine-learning; Sample collection; agriculture; algorithm; artificial neural network; classification; classification algorithm; deep learning; ion selective electrode; livestock; machine learning; product quality; review; sensor; Ion selective electrodes","Construction Project of Key Laboratory of Agricultural Sensors; Key Research and Development Program of Jiangxi Province, (BE2021379); Key Research and Development Program of Jiangxi Province; National Key Research and Development Program of China, NKRDPC, (2022YFD2002300); National Key Research and Development Program of China, NKRDPC; Ministry of Agriculture and Rural Affairs of the People's Republic of China, MARA, (PT2024-46); Ministry of Agriculture and Rural Affairs of the People's Republic of China, MARA","This study was supported by grants from the Key-Area Research and Development Program of Jiangsu Province (BE2021379), the National Key R&D Program of China (2022YFD2002300), and the Construction Project of Key Laboratory of Agricultural Sensors of the Ministry of Agriculture and Rural Affairs (PT2024-46). We also greatly appreciate the help provided by the reviewers, which has been very helpful in improving this paper.","X. Wang; Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; email: wangxd@nercita.org.cn","","Royal Society of Chemistry","17599660","","","39543972","English","Anal. Methods","Review","Final","","Scopus","2-s2.0-85209552995"
"Sharma A.; Randewich L.; Andrew W.; Hannuna S.; Campbell N.; Mullan S.; Dowsey A.W.; Smith M.; Hansen M.; Burghardt T.","Sharma, Asheesh (57472692800); Randewich, Lucy (59067488000); Andrew, William (57192573531); Hannuna, Sion (9239823000); Campbell, Neill (7201796804); Mullan, Siobhan (14058552800); Dowsey, Andrew W. (12799011000); Smith, Melvyn (55495905800); Hansen, Mark (57192115769); Burghardt, Tilo (24461140200)","57472692800; 59067488000; 57192573531; 9239823000; 7201796804; 14058552800; 12799011000; 55495905800; 57192115769; 24461140200","Universal bovine identification via depth data and deep metric learning","2025","Computers and Electronics in Agriculture","229","","109657","","","","0","10.1016/j.compag.2024.109657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210737559&doi=10.1016%2fj.compag.2024.109657&partnerID=40&md5=ccd768a65f53bb915051c062bdbce9ac","Bristol Veterinary School, University of Bristol, Langford House, Bristol, BS40 5DU, United Kingdom; Department of Population Health Sciences, University of Bristol, Oakfield House, Oakfield Grove, Bristol, BS8 2BN, United Kingdom; Centre for Machine Vision, University of the West of England Bristol, Coldharbour Lane, Bristol, BS16 1QY, United Kingdom; School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom","Sharma A., Bristol Veterinary School, University of Bristol, Langford House, Bristol, BS40 5DU, United Kingdom, Centre for Machine Vision, University of the West of England Bristol, Coldharbour Lane, Bristol, BS16 1QY, United Kingdom, School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom; Randewich L., School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom; Andrew W., Bristol Veterinary School, University of Bristol, Langford House, Bristol, BS40 5DU, United Kingdom, School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom; Hannuna S., School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom; Campbell N., School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom; Mullan S., Bristol Veterinary School, University of Bristol, Langford House, Bristol, BS40 5DU, United Kingdom; Dowsey A.W., Bristol Veterinary School, University of Bristol, Langford House, Bristol, BS40 5DU, United Kingdom, Department of Population Health Sciences, University of Bristol, Oakfield House, Oakfield Grove, Bristol, BS8 2BN, United Kingdom; Smith M., Centre for Machine Vision, University of the West of England Bristol, Coldharbour Lane, Bristol, BS16 1QY, United Kingdom; Hansen M., Centre for Machine Vision, University of the West of England Bristol, Coldharbour Lane, Bristol, BS16 1QY, United Kingdom; Burghardt T., School of Computer Science, University of Bristol, Merchant Venturers Building, Woodland Road, Bristol, BS8 1UB, United Kingdom","This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data as a novel biometric measure acquired with an off-the-shelf 3D camera. In contrast to our previous work, which was limited to breeds with distinct coat patterns, this study introduces a breed-agnostic pipeline for universal cattle identification. The results show that depth, as a biometric, can potentially broaden the real-world applicability of our method to the rest 68% of UK cattle breeds that lack distinctive coat patterns. The method relies on Convolutional Neural Network (CNN) and Multi-Layered Perceptron (MLP) backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals — requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simple algorithm such as k-Nearest Neighbours (k-NN) for highly accurate identification, thus eliminating the need to retrain the network for enrolling new individuals. We evaluate two backbone architectures, Residual Neural Network (ResNet), as previously used to identify Holstein Friesians using RGB images, and PointNet, which is specialised to operate on 3D point clouds. We also present CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet architectures, which consume depth maps and point clouds, respectively, led to high accuracy that is on par with the coat pattern-based backbone. This new universal methodology also addresses the case of all-black and all-white breeds, where the previous coat pattern-based approach fell short. The ResNet colour backbone resulted in 99.97% k-NN identification accuracy, while the PointNet accuracy was 99.36%. Furthermore, we also show that the PointNet architecture is robust to noise and missing data by significantly reducing the number of 3D points and observing the drop in accuracy. Our research indicates that these techniques can identify animals using dorsal-view depth maps alone. Regardless of the substantial inter-class variety in the body shape, we show that the models spatially rely on similar body surfaces using Gradient-weighted Class Activation Mapping (Grad-CAM) and Point Cloud Saliency Mapping (PC-SM). © 2024 The Authors","Bovine identification; Deep metric learning; Depth maps; Precision livestock farming","Deep learning; Fertilizers; Flow visualization; Multilayer neural networks; Nearest neighbor search; Network embeddings; Body shapes; Bovine identification; Cattles; Deep metric learning; Depthmap; Metric learning; Nearest-neighbour; Neural-networks; Point-clouds; Precision livestock farming; algorithm; body shape; cattle; data set; livestock farming; machine learning; Convolutional neural networks","Centre for Machine Vision; John Oldacre Centre at the Bristol Veterinary School; John Oldacre Centre; Biotechnology and Biological Sciences Research Council, BBSRC, (2593504, BB/T008741/1); Biotechnology and Biological Sciences Research Council, BBSRC","Funding text 1: This study is supported by a studentship from the Biotechnology and Biological Sciences Research Council (BBSRC; UK) South West Biosciences Doctoral Training Programme (Project ref. 2593504, BB/T008741/1). We acknowledge the support provided by the John Oldacre Centre at the Bristol Veterinary School and the Centre for Machine Vision at the University of West of England Bristol to carry out this research project.; Funding text 2: This study is supported by a studentship from the Biotechnology and Biological Sciences Research Council (BBSRC; UK) South West Biosciences Doctoral Training Programme (Project ref. 2593504 , BB/T008741/1 ). We acknowledge the support provided by the John Oldacre Centre at the Bristol Vet School to carry out this research project. ","A.W. Dowsey; Bristol Veterinary School, University of Bristol, Bristol, Langford House, BS40 5DU, United Kingdom; email: andrew.dowsey@bristol.ac.uk; M. Smith; Centre for Machine Vision, University of the West of England Bristol, Bristol, Coldharbour Lane, BS16 1QY, United Kingdom; email: melvyn.smith@uwe.ac.uk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85210737559"
"Barrasso C.; Krüger R.; Eltner A.; Cord A.F.","Barrasso, Caterina (57224592954); Krüger, Robert (57203863768); Eltner, Anette (56548964400); Cord, Anna F. (34881399900)","57224592954; 57203863768; 56548964400; 34881399900","Mapping indicator species of segetal flora for result-based payments in arable land using UAV imagery and deep learning","2024","Ecological Indicators","169","","112780","","","","0","10.1016/j.ecolind.2024.112780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208283271&doi=10.1016%2fj.ecolind.2024.112780&partnerID=40&md5=ff7d1d791e3f5704e7367930ce41b913","Chair of Computational Landscape Ecology, TUD Dresden University of Technology, Germany; Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, Germany; Institute of Photogrammetry and Remote Sensing, TUD Dresden University of Technology, Germany; Agro-Ecological Modeling Group, Institute of Crop Science and Resource Conservation, University of Bonn, Germany","Barrasso C., Chair of Computational Landscape Ecology, TUD Dresden University of Technology, Germany, Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, Germany; Krüger R., Institute of Photogrammetry and Remote Sensing, TUD Dresden University of Technology, Germany; Eltner A., Institute of Photogrammetry and Remote Sensing, TUD Dresden University of Technology, Germany; Cord A.F., Chair of Computational Landscape Ecology, TUD Dresden University of Technology, Germany, Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, Germany, Agro-Ecological Modeling Group, Institute of Crop Science and Resource Conservation, University of Bonn, Germany","The decline of segetal flora species across Europe, driven by intensified agricultural practices, is impacting other taxa and ecosystem functions. Result-based payments to farmers offer an effective solution to conserve these species, but the high cost of biodiversity monitoring remains a challenge. In this study, we conducted UAV flights with an RGB camera and used the deep learning model YOLO to detect these species in four winter barley fields under different management intensities in Germany. Field measurements of plant traits were used to evaluate their impact on species detectability. Additionally, we investigated the potential of spatial co-occurrence and canopy height heterogeneity to predict the presence of species difficult to detect by UAVs. We found that half of the species observed could be remotely detected, with a minimum ground sampling distance (GSD) of 1.22 mm required for accurate annotation. The same detection ratio was estimated for key indicator species not present in our study area based on trait information. Plant height was crucial for species detection, with accuracy ranging between 49–100 %. YOLO models effectively predicted species from images taken at 40 m, reducing the monitoring time to eight minutes per hectare. Co-occurrence with UAV-detectable species and canopy height heterogeneity proved promising for identifying areas where undetectable species are likely to occur, although further research is needed for landscape-level applications. Our study highlights the potential for large-scale, cost-effective monitoring of segetal flora species in agricultural landscapes, and provides valuable insights for developing robust ‘smart indicators’ for future biodiversity monitoring. © 2024 The Author(s)","Agricultural landscapes; Deep learning; Ground sampling distance; Indicator species; Point cloud; Result-based payments; Segetal flora; UAV-based RGB imagery","Abiotic; Invertebrates; Macroinvertebrates; Plant diseases; Agricultural landscapes; Biodiversity monitoring; Co-occurrence; Deep learning; Ground sampling distances; Indicator species; Point-clouds; Result-based payment; Segetal flora; UAV-based RGB imagery; agricultural land; algorithm; arable land; mapping method; remote sensing; satellite imagery; species occurrence; unmanned vehicle; Livestock","Saxon State Ministry of Science, Culture and Tourism; United Nations Educational, Scientific and Cultural Organization, UNESCO; Center for Scalable Data Analytics and Artificial Intelligence Dresden; Bundesministerium für Bildung und Forschung, BMBF; Sächsisches Staatsministerium für Wissenschaft und Kunst, SMWK; Center for Information Services; Technische Universität Dresden, TUD; Leipzig, (SCADS24B); Horizon Europe project Earth Bridge, (101079310); Deutsche Forschungsgemeinschaft, DFG, (EXC 2070 – 390732324); Deutsche Forschungsgemeinschaft, DFG; HORIZON EUROPE Framework Programme, (101079310); HORIZON EUROPE Framework Programme","Funding text 1: CB received financial support from the German Federal Ministry of Education and Research (BMBF) and the Saxon State Ministry of Science, Culture and Tourism ( SMWK ) by funding the \u201CCenter for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig\u201D, project identification number: SCADS24B . RK was funded by the Horizon Europe project Earth Bridge (grant agreement no. 101079310 ). AFC was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy \u2013 EXC 2070 \u2013 390732324. Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union . Neither the European Union nor the granting authority can be held responsible for them. The authors gratefully acknowledge the GWK support for funding this project by providing computing time through the Center for Information Services and HPC ( ZIH ) at TU Dresden . We would like to thank the UNESCO biosphere reserve \u201CUpper Lusatian Health and Pond Landscape\u201D and the agricultural cooperative \u201CHeidefarm Sdier eG\u201D for allowing us to collect the data for this study and for logistic support. We further thank others who helped with data collection and labeling of the images, in particular Sophia Lewitz, Bela Rehnen, Stephanie Roilo and Anja Steingrobe. ; Funding text 2: CB received financial support from the German Federal Ministry of Education and Research (BMBF) and the Saxon State Ministry of Science, Culture and Tourism (SMWK) by funding the \u201CCenter for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig\u201D, project identification number: SCADS24B. RK was funded by the Horizon Europe project EarthBridge (grant agreement no. 101079310). AFC was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy \u2013 EXC 2070 \u2013 390732324. Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. The authors gratefully acknowledge the GWK support for funding this project by providing computing time through the Center for Information Services and HPC (ZIH) at TU Dresden. We would like to thank the UNESCO biosphere reserve \u201CUpper Lusatian Health and Pond Landscape\u201D and the agricultural cooperative \u201CHeidefarm Sdier eG\u201D for allowing us to collect the data for this study and for logistic support. We further thank others who helped with data collection and labeling of the images, in particular Sophia Lewitz, Bela Rehnen, Stephanie Roilo and Anja Steingrobe.","C. Barrasso; Chair of Computational Landscape Ecology, TUD Dresden University of Technology, Germany; email: caterina.barrasso@tu-dresden.de","","Elsevier B.V.","1470160X","","","","English","Ecol. Indic.","Article","Final","","Scopus","2-s2.0-85208283271"
"García-Vázquez F.A.","García-Vázquez, Francisco A. (8917671800)","8917671800","Artificial intelligence and porcine breeding","2024","Animal Reproduction Science","269","","107538","","","","2","10.1016/j.anireprosci.2024.107538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197912453&doi=10.1016%2fj.anireprosci.2024.107538&partnerID=40&md5=5cd46f72507750bc0f8460026827a8d1","Departamento de Fisiología, Facultad de Veterinaria, Campus de Excelencia Mare Nostrum, Universidad de Murcia, Murcia, 30100, Spain; Instituto Murciano de Investigación Biosanitaria (IMIB-Arrixaca), Murcia, Spain","García-Vázquez F.A., Departamento de Fisiología, Facultad de Veterinaria, Campus de Excelencia Mare Nostrum, Universidad de Murcia, Murcia, 30100, Spain, Instituto Murciano de Investigación Biosanitaria (IMIB-Arrixaca), Murcia, Spain","Livestock management is evolving into a new era, characterized by the analysis of vast quantities of data (Big Data) collected from both traditional breeding methods and new technologies such as sensors, automated monitoring system, and advanced analytics. Artificial intelligence (A-In), which refers to the capability of machines to mimic human intelligence, including subfields like machine learning and deep learning, is playing a pivotal role in this transformation. A wide array of A-In techniques, successfully employed in various industrial and scientific contexts, are now being integrated into mainstream livestock management practices. In the case of swine breeding, while traditional methods have yielded considerable success, the increasing amount of information requires the adoption of new technologies such as A-In to drive productivity, enhance animal welfare, and reduce environmental impact. Current findings suggest that these techniques have the potential to match or exceed the performance of traditional methods, often being more scalable in terms of efficiency and sustainability within the breeding industry. This review provides insights into the application of A-In in porcine breeding, from the perspectives of both sows (including welfare and reproductive management) and boars (including semen quality and health), and explores new approaches which are already being applied in other species. © 2024 The Authors","Agriculture; AI, Algorithm; ARTs; Data science; Porcine; Sperm; Swine reproduction","Animal Husbandry; Animals; Artificial Intelligence; Breeding; Female; Male; Swine; animal; animal husbandry; artificial intelligence; breeding; female; male; physiology; pig; procedures","Ministerio de Ciencia e Innovación, MCIN; Fundación Seneca-Agencia de Ciencia y Tecnología de la Región de Murcia-Prueba de concepto, (22263/PDC/23, PID2019-106380RB-I00/AEI/ 10.13039/501100011033)","Funding text 1: Funding was provided by \u201CFundaci\u00F3n Seneca-Agencia de Ciencia y Tecnolog\u00EDa de la Regi\u00F3n de Murcia-Prueba de concepto\u201D (22263/PDC/23) and Project PID2019-106380RB-I00/AEI/ 10.13039/501100011033 from Spanish Ministry of Science and Innovation.; Funding text 2: Funding was provided by \u201CFundaci\u00F3n Seneca-Agencia de Ciencia y Tecnolog\u00EDa de la Regi\u00F3n de Murcia-Prueba de concepto\u201D (22263/PDC/23) and Project PID2019-106380RB-I00/AEI/ 10.13039/501100011033. ","","","Elsevier B.V.","03784320","","ANRSD","38926001","English","Anim. Reprod. Sci.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85197912453"
"Mehdizadeh S.A.; Siriani A.L.R.; Pereira D.F.","Mehdizadeh, Saman Abdanan (58756294400); Siriani, Allan Lincoln Rodrigues (57192177832); Pereira, Danilo Florentino (56187746200)","58756294400; 57192177832; 56187746200","Optimizing Deep Learning Algorithms for Effective Chicken Tracking through Image Processing","2024","AgriEngineering","6","3","","2749","2767","18","0","10.3390/agriengineering6030160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205133394&doi=10.3390%2fagriengineering6030160&partnerID=40&md5=645f0ad6eb6e173c342d0e2ea686e1a9","Department of Mechanics of Biosystems Engineering, Faculty of Agricultural Engineering and Rural Development, Agricultural Sciences and Natural Resources University of Khuzestan, Ahvaz, 63417-73637, Iran; Graduate Program in Agribusiness and Development, School of Sciences and Engineering, São Paulo State University, SP, Tupã, 17602-496, Brazil; Department of Management, Development, and Technology, School of Sciences and Engineering, São Paulo State University, SP, Tupã, 17602-496, Brazil","Mehdizadeh S.A., Department of Mechanics of Biosystems Engineering, Faculty of Agricultural Engineering and Rural Development, Agricultural Sciences and Natural Resources University of Khuzestan, Ahvaz, 63417-73637, Iran; Siriani A.L.R., Graduate Program in Agribusiness and Development, School of Sciences and Engineering, São Paulo State University, SP, Tupã, 17602-496, Brazil; Pereira D.F., Department of Management, Development, and Technology, School of Sciences and Engineering, São Paulo State University, SP, Tupã, 17602-496, Brazil","Identifying bird numbers in hostile environments, such as poultry facilities, presents significant challenges. The complexity of these environments demands robust and adaptive algorithmic approaches for the accurate detection and tracking of birds over time, ensuring reliable data analysis. This study aims to enhance methodologies for automated chicken identification in videos, addressing the dynamic and non-standardized nature of poultry farming environments. The YOLOv8n model was chosen for chicken detection due to its high portability. The developed algorithm promptly identifies and labels chickens as they appear in the image. The process is illustrated in two parallel flowcharts, emphasizing different aspects of image processing and behavioral analysis. False regions such as the chickens’ heads and tails are excluded to calculate the body area more accurately. The following three scenarios were tested with the newly modified deep-learning algorithm: (1) reappearing chicken with temporary invisibility; (2) multiple missing chickens with object occlusion; and (3) multiple missing chickens with coalescing chickens. This results in a precise measure of the chickens’ size and shape, with the YOLO model achieving an accuracy above 0.98 and a loss of less than 0.1. In all scenarios, the modified algorithm improved accuracy in maintaining chicken identification, enabling the simultaneous tracking of several chickens with respective error rates of 0, 0.007, and 0.017. Morphological identification, based on features extracted from each chicken, proved to be an effective strategy for enhancing tracking accuracy. © 2024 by the authors.","animal welfare; image analysis; laying hens; precision livestock farming; YOLO","","","","D.F. Pereira; Department of Management, Development, and Technology, School of Sciences and Engineering, São Paulo State University, Tupã, SP, 17602-496, Brazil; email: danilo.florentino@unesp.br","","Multidisciplinary Digital Publishing Institute (MDPI)","26247402","","","","English","AgriEng.","Article","Final","","Scopus","2-s2.0-85205133394"
"Meng H.; Zhang L.; Yang F.; Hai L.; Wei Y.; Zhu L.; Zhang J.","Meng, Hua (59506676800); Zhang, Lina (57044322600); Yang, Fan (58704934700); Hai, Lan (59505815100); Wei, Yuxing (59507102400); Zhu, Lin (59506239900); Zhang, Jue (57201479427)","59506676800; 57044322600; 58704934700; 59505815100; 59507102400; 59506239900; 57201479427","Livestock Biometrics Identification Using Computer Vision Approaches: A Review","2025","Agriculture (Switzerland)","15","1","102","","","","0","10.3390/agriculture15010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214445086&doi=10.3390%2fagriculture15010102&partnerID=40&md5=c450b692b02cd53801e984ccc0f4a17d","College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China","Meng H., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Zhang L., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Yang F., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Hai L., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Wei Y., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Zhu L., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; Zhang J., College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China","In the domain of animal management, the technology for individual livestock identification is in a state of continuous evolution, encompassing objectives such as precise tracking of animal activities, optimization of vaccination procedures, effective disease control, accurate recording of individual growth, and prevention of theft and fraud. These advancements are pivotal to the efficient and sustainable development of the livestock industry. Recently, visual livestock biometrics have emerged as a highly promising research focus due to their non-invasive nature. This paper aims to comprehensively survey the techniques for individual livestock identification based on computer vision methods. It begins by elucidating the uniqueness of the primary biometric features of livestock, such as facial features, and their critical role in the recognition process. This review systematically overviews the data collection environments and devices used in related research, providing an analysis of the impact of different scenarios on recognition accuracy. Then, the review delves into the analysis and explication of livestock identification methods, based on extant research outcomes, with a focus on the application and trends of advanced technologies such as deep learning. We also highlight the challenges faced in this field, such as data quality and algorithmic efficiency, and introduce the baseline models and innovative solutions developed to address these issues. Finally, potential future research directions are explored, including the investigation of multimodal data fusion techniques, the construction and evaluation of large-scale benchmark datasets, and the application of multi-target tracking and identification technologies in livestock scenarios. © 2025 by the authors.","deep learning; device; environment; identification methods; livestock individual identification; visual biometrics","","","","L. Zhang; College of Physics and Electronic Information, Inner Mongolia Normal University, Hohhot, 010022, China; email: 20061113@imnu.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20770472","","","","English","Agric.","Review","Final","","Scopus","2-s2.0-85214445086"
"Wu Z.; Chen Z.; Tian X.; Yang J.; Yin L.; Zhang S.","Wu, Zhenbang (59254006100); Chen, Zekai (59254605500); Tian, Xuhong (11641329500); Yang, Jie (57102242100); Yin, Ling (36024801300); Zhang, Sumin (55437438600)","59254006100; 59254605500; 11641329500; 57102242100; 36024801300; 55437438600","A method for pig gait scoring based on 3D convolution video analysis; [基于 3D 卷积视频分析的猪步态评分方法]","2024","Journal of South China Agricultural University","45","5","","743","753","10","0","10.7671/j.issn.1001-411X.202311019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200892792&doi=10.7671%2fj.issn.1001-411X.202311019&partnerID=40&md5=11eaac6ff8bdb21d752565054d9b9e64","College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China; College of Animal Science, South China Agricultural University, Guangzhou, 510642, China; State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China","Wu Z., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; Chen Z., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; Tian X., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; Yang J., National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China, College of Animal Science, South China Agricultural University, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China; Yin L., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China, National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China; Zhang S., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China","【Objective】Swine limb and hoof disease is one of the significant reasons for culling breeding swine, resulting in substantial economic losses for livestock farms. The diagnosis of swine limb and hoof disease typically relies on manual observation of pig gaits, which consumes high labor costs and has low efficiency. The aim of this study is to achieve automated pig gait scoring, and efficiently determine the health status of swine limb and hoof.【Method】This study proposed an “end-to-end” pig gait scoring method. Videos of individual breeding swine passing through designated channels were collected and a four-point gait dataset was created. Deep learning techniques were employed for video analysis. A time attention module (TAM) based on a 3D convolutional neural network was designed to effectively extract feature information between video frame images. By combining TAM with residual structures, the pig gait scoring model TA3D was constructed for feature extraction and gait classification scoring in the gait videos. To further improve model performance and achieve automation, the gait focus module (GFM) was designed. GFM could autonomously extract effective information from real-time video streams to synthesize high-quality gait videos, improving model performance while reducing computational costs. 【Result】The experimental results demonstrated that GFM could operate in real-time and reduced the size of gait videos by over 90%, significantly reducing storage cost, and the gait scoring accuracy of the TA3D model was 96.43%. Moreover, the comparison test results with other classic video analysis models showed that TA3D achieved optimal levels of accuracy and inference speed. 【Conclusion】This paper proposes a solution that can be applied to the automatic scoring of pig gait, providing a reference for the automatic detection of swine limb and hoof disease. © 2024 Editorial Department, Journal of South Agricultural University. All rights reserved.","Attention mechanism; Deep learning; Gait scoring; Image processing; Limb and hoof disease; Pig; Video analysis","","","","L. Yin; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; email: yin_ling@scau.edu.cn","","Editorial  Department, Journal of South Agricultural University","1001411X","","","","Chinese","J. South China Agri.  Univ.","Article","Final","","Scopus","2-s2.0-85200892792"
"Zhang H.; Zhang Y.; Niu K.; He Z.","Zhang, Haoyu (57219634925); Zhang, Yuqi (59478016300); Niu, Kai (7007085232); He, Zhiqiang (7403885147)","57219634925; 59478016300; 7007085232; 7403885147","Neural network-based method for contactless estimation of carcass weight from live beef images","2025","Computers and Electronics in Agriculture","229","","109830","","","","0","10.1016/j.compag.2024.109830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212435510&doi=10.1016%2fj.compag.2024.109830&partnerID=40&md5=c3591001ea08c2f73a27cffda709be3c","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Zhang H., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Zhang Y., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Niu K., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; He Z., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Accurately estimating the carcass weight of the live beef cattle is crucial in the breeding industry as it is essential for evaluating the quality and production capacity of beef cattle and directly impacts the economic benefits of breeding farms. Although current animal husbandry research predominantly focuses on estimating live body weight, few studies explore the relationship between live images and carcass weight. Additionally, existing methods for estimating carcass weight rely on manually measured body dimensions, a process that is time-consuming, laborious, and compromises animal welfare. In this study, we propose a contactless method utilizing dual-input deep neural networks to estimate the carcass weight of live beef cattle, and explore the impact of both top and side views images on the estimation results while performing experimental analyses of specific scenarios encountered in practical applications to highlight the model's robustness. The feature extraction network employs two SE-ResNeXt-50 models to extract back features from top view images and abdominal features from side view images, respectively. By merging the extracted information from both views, the combined features are processed through a network to obtain the estimated carcass weight. The proposed model has been trained and tested on a dataset collected by our team, demonstrating superior performance compared to other typical deep learning models across four indicators: MAE, RMSE, MAPE, and R2, particularly achieving a notable RMSE of 17.713 kg. Ablation experiments are conducted to validate the contributions of the group convolution structure and the Squeeze and Excitation (SE) block. Overall, the method presented in this study bears significant implications for animal quality and production capacity evaluation in the breeding industry. © 2024 Elsevier B.V.","Beef cattle; Carcass weight; Computer vision; Deep learning; Precision livestock farming","Beef cattle; Carcass weight; Contact less; Deep learning; Network-based; Neural-networks; Precision livestock farming; Production capacity; Side view; Top views; animal husbandry; artificial neural network; cattle; computer vision; data set; livestock farming; machine learning; precision agriculture; Livestock","National Natural Science Foundation of China, NSFC, (62071058); National Natural Science Foundation of China, NSFC","This work was supported by the National Natural Science Foundation of China under Grant 62071058.","K. Niu; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; email: niukai@bupt.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85212435510"
"Vázquez I.X.; García-Vico A.M.; Seker H.; Sedano J.","Vázquez, I.X. (57213409709); García-Vico, A.M. (57191692832); Seker, H. (6701451767); Sedano, J. (23475274300)","57213409709; 57191692832; 6701451767; 23475274300","Low Consumption Models for Disease Diagnosis in Isolated Farms","2025","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ","15346 LNCS","","","233","243","10","0","10.1007/978-3-031-77731-8_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210482202&doi=10.1007%2f978-3-031-77731-8_22&partnerID=40&md5=fac73f49494cf74766de4b508e5fe932","ITCL Technology Center, Burgos, 09001, Spain; Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Jaén, Jaén, 23071, Spain; Department of Information Systems, College of Computing and Informatics, The University of Sharjah, Sharjah, United Arab Emirates","Vázquez I.X., ITCL Technology Center, Burgos, 09001, Spain; García-Vico A.M., Andalusian Research Institute in Data Science and Computational Intelligence (DaSCI), University of Jaén, Jaén, 23071, Spain; Seker H., Department of Information Systems, College of Computing and Informatics, The University of Sharjah, Sharjah, United Arab Emirates; Sedano J., ITCL Technology Center, Burgos, 09001, Spain","The detection of bacterial and viral microbes is pivotal for both human and animal well-being in public health services and veterinary care, but it traditionally requires time-consuming procedures and expert technicians. However, the rise of Machine Learning and Deep Learning has led to a surge in the application of new techniques that can perform bacterial and viral detection faster and at a lower cost. Yet, despite that success, Deep Learning approaches tend to have high energy demands, which can in some contexts limit their application, increasing both costs and environmental concerns. In this study, a new hybrid methodology, in which an Artificial Neural Network was combined with a more energy efficient Spiking Neural Network, was employed to develop a model able to classify 18 species of Eimeria parasites, affecting both rabbits and chickens, from microscope images. We show how significant energy savings can be obtained from SNN layers, while their use in the model can improve its performance. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Eimeria; Green AI; Microorganisms Detection; Spiking Neural Network","Invertebrates; Livestock; Macroinvertebrates; Plant diseases; Consumption modeling; Disease diagnosis; Eimeria; Green AI; Low consumption; Microorganisms detections; Neural-networks; Public health services; Spiking neural network; Well being","CICERO; Centro para el Desarrollo Tecnológico Industrial, CDTI, (CER-20231019); Centro para el Desarrollo Tecnológico Industrial, CDTI","This research has been founded from the Recovery, Resilience and Transformation Plan under the project CICERO: CDTI Project CER-20231019 (CICERO).","I.X. Vázquez; ITCL Technology Center, Burgos, 09001, Spain; email: iago.vazquez@itcl.es","Julian V.; Camacho D.; Yin H.; Alberola J.M.; Nogueira V.B.; Novais P.; Tallón-Ballesteros A.","Springer Science and Business Media Deutschland GmbH","03029743","978-303177730-1","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85210482202"
"Li R.; Dai B.; Hu Y.; Dai X.; Fang J.; Yin Y.; Liu H.; Shen W.","Li, Ran (57738507700); Dai, Baisheng (57225754325); Hu, Yuhang (57347720500); Dai, Xin (58913616200); Fang, Junlong (23969987100); Yin, Yanling (55274316800); Liu, Honggui (55650669700); Shen, Weizheng (23390072800)","57738507700; 57225754325; 57347720500; 58913616200; 23969987100; 55274316800; 55650669700; 23390072800","Multi-behavior detection of group-housed pigs based on YOLOX and SCTS-SlowFast","2024","Computers and Electronics in Agriculture","225","","109286","","","","0","10.1016/j.compag.2024.109286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199939445&doi=10.1016%2fj.compag.2024.109286&partnerID=40&md5=4e930d57455b062b1c10118f89668dfc","College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Key Laboratory of Pig-breeding Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China","Li R., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Dai B., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Pig-breeding Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; Hu Y., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Dai X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Fang J., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Pig-breeding Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; Yin Y., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Pig-breeding Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; Liu H., Key Laboratory of Pig-breeding Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China, College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China; Shen W., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China","Accurately and rapidly recognizing the behaviors of group-housed pigs plays a very important role in pig farm production management. Recognizing multiple behaviors in one scene remains a challenge. This study proposed a multi-behavior detection method of group-housed pigs based on YOLOX and SCTS-SlowFast (SC-Conv-TS-SlowFast) to recognize four behaviors (Eating or Interaction with feeding though, Drinking or Interaction with drinker, Standing and Walking) and locate corresponding pig locations. Firstly, YOLOX object detection module is used to locate the locations of group-housed pigs. Secondly, SCTS-SlowFast behavior recognition module is proposed to classify the behaviors category of pigs in the located regions, in which Self-Calibrated Convolution (SC-Conv) and Temporal-Spatial (TS) attention mechanism are specially introduced to improve behavior feature extraction capability of the model. Finally, the results of two modules are combined to realize the task of multi-behavior detection of group-housed pigs. To evaluate the proposed method, a multi-behavior video dataset of group-housed pigs with 420 video segments is established. This study achieved a mAP value of 80.05% for four behaviors of group-housed pigs, and counted the duration of these behaviors throughout one day from 8:00 to 16:00 as well as their corresponding changing trends. It verifies the potential and feasibility of proposed method in automatically and simultaneously monitoring and analyzing multiple typical behaviors of group-housed pigs. We shared our behavior detection dataset at https://github.com/IPCLab-NEAU/Group-housed-pigs-Multi-Behavior-Detection for precision livestock farming research community. © 2024 Elsevier B.V.","Deep learning; Group-housed pigs; Multi-behavior detection; SCTS-SlowFast","Agriculture; Behavioral research; Deep learning; Mammals; Behavior detection; Deep learning; Detection methods; Farm production; Group-housed pig; Multi-behavior detection; Objects detection; Pig farms; Production management; SC-conv-TS-slowfast; data set; detection method; livestock farming; machine learning; pig; Object detection","National Natural Science Foundation of China, NSFC, (31902210, 32172784); National Natural Science Foundation of China, NSFC; Natural Science Foundation of Heilongjiang Province, (QC2018074, YQ2023C012); Natural Science Foundation of Heilongjiang Province","This work was supported in part by the National Natural Science Foundation of China under Grant 31902210 and 32172784 , in part by the Natural Science Foundation of Heilongjiang Province under Grant QC2018074 and YQ2023C012 .","B. Dai; College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; email: bsdai@neau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85199939445"
"Zhang Z.; Chu Y.; Wang Y.; Wang L.; Shen Y.; Li X.","Zhang, Zhirong (59476783200); Chu, Yanhua (59477305800); Wang, Yueming (57203164342); Wang, Liying (59476657300); Shen, Yuhao (59477174000); Li, Xin (59476915200)","59476783200; 59477305800; 57203164342; 59476657300; 59477174000; 59476915200","Detecting cow lameness using the key points of head, neck, and back; [基于头颈背部关键点的奶牛跛行检测]","2024","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","40","21","","157","164","7","0","10.11975/j.issn.1002-6819.202406078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212329990&doi=10.11975%2fj.issn.1002-6819.202406078&partnerID=40&md5=0476ef1c30a1e9b77a517c8061e7e6a0","School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China; Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China; School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China","Zhang Z., School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China, Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China; Chu Y., School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China, Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China; Wang Y., Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China, School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China; Wang L., School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China, Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China; Shen Y., School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China, Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China; Li X., School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China, Inner Mongolia Autonomous Region Engineering Research Center for Artificial Intelligence in Grassland Animal Husbandry, Baotou, 014010, China","Cow lameness has represented a significant challenge on the economic viability of dairy operations. The overall performance can increase the risk of health issues for affected cows, leading to reduced milk production. Consequently, cow lameness is crucial to maintain both the welfare of the herd and the profitability of dairy farms. Lame cows typically show the observable indicators during walking, such as a lowered head position, pronounced head movement, and an arched back, whereas, healthy cows demonstrate the minimal head movement, straight back, normal gait and body equilibrium. In this study, a deep learning-based algorithm was proposed to automatic detect the lameness in cows, according to these outstanding movement features. A systematic investigation was implemented to detect the cow lameness, thus tracking the movement patterns of six key anatomical points: the head, neck, shoulder, center of the back, loin, and tail. Firstly, two mobile devices were positioned adjacent to the passage, leading to the milking area. The video data was collected for 160 walking sequences from 83 cows. YOLOv8n-seg instance segmentation was employed to accurately identify the cows in the images, and then extract their coordinates and pixel regions. The computational efficiency and accuracy were improved to reduce the effects of light variations in the channel, background barbed wire fence boundaries, and foreground fence occlusion. Secondly, the six types of keypoint detection datasets were constructed after instance segmentation, including RGB images, binary mask images, segmentation images along with their cropped versions, according to the target detection frame. Four backbone networks, MobileNet-V2, ResNet-50, ResNet-101, and ResNet-152, were used to train and test these datasets. Segmented images that cropped by the detection frame were selected as the optimal input format, with ResNet-152 chosen as the best-performing backbone network. Then, the DeepLabCut algorithm was used to automatically extract the coordinates of six key points from the video sequences: the head, neck, shoulder, center of the back, waist, and tail, resulting in the creation of a lameness detection dataset. Lastly, a comparative analysis was performed to evaluate the performance of Temporal Convolutional Network (TCN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), Bidirectional LSTM, BiLSTM, and FN-BiLSTM models in the claudication detection. (Bidirectional LSTM, BiLSTM), and FN-BiLSTM models in the lameness detection. Ablation experiments were conducted on the FN-BiLSTM model, in order to verify the effects of the Filter and Noise layers on the lameness detection in cows. The results demonstrated that the FN-BiLSTM model was achieved in the optimal performance with 97.16% accuracy, 95.71% precision, and 99.04% recall for the lameness recognition on a test set of 16 videos from 16 cows. Moreover, the instance segmentation model exhibited the high efficacy to capture the image sequences of cows and their whole-body semantic information, even under variable illumination conditions and different bovine-to-camera distances. The precision, recall, and mAP of the test set reached 99.97%, 100%, and 99.50%, respectively. During the keypoint detection phase, the optimal performance was achieved, when utilizing the cropped segmentation maps as input, with ResNet-152 as the backbone network, resulting in the root mean square errors of 2.04 pixels and 4.28 pixels for the training and test sets, respectively. These findings can offer a valuable technical approach for the automated detection of cow lameness in the livestock industry. This finding has the potential to enhance the efficiency and animal welfare of dairy operations, thereby promoting the sustainable development of the livestock. © 2024 Chinese Society of Agricultural Engineering. All rights reserved.","cow lameness; deep learning; instance segmentation; key point detection","Fences; Fertilizers; Hemp; Image segmentation; Medical imaging; Optical flows; Pixels; Risk assessment; Video recording; Back-bone network; Cow lameness; Deep learning; Head/neck; Instance segmentation; Key point detection; Keypoints; Lameness detection; Point detection; Short term memory; Health risks","","","Y. Chu; School of Digital and Intelligent Industry, School of Cyber Science and Technology, Inner Mongolia University of Science and Technology, Baotou, 014010, China; email: chuyanhua@imust.edu.cn","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85212329990"
"Ying X.; Zhao J.; Yang L.; Zhou X.; Wang L.; Gao Y.; Zan L.; Yang W.; Liu H.; Song H.","Ying, Xiaoyi (59225219600); Zhao, Jizheng (56144394200); Yang, Lingling (58631301000); Zhou, Xinyi (59224625000); Wang, Lei (59140157500); Gao, Yannian (59224477000); Zan, Linsen (35216915700); Yang, Wucai (35789452400); Liu, Han (59224923800); Song, Huaibo (17342958900)","59225219600; 56144394200; 58631301000; 59224625000; 59140157500; 59224477000; 35216915700; 35789452400; 59224923800; 17342958900","Re-identifying beef cattle using improved AlignedReID++; [基于改进 AlignedReID++的肉牛个体重识别方法]","2024","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","40","18","","132","146","14","0","10.11975/j.issn.1002-6819.202404229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207060435&doi=10.11975%2fj.issn.1002-6819.202404229&partnerID=40&md5=4511a21f7091c143953955e706cb33df","College of Information Engineering, Northwest A & F University, Yangling, 712100, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710000, China; College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; College of Animal Science and Technology, Northwest A & F University, Yangling, 712100, China","Ying X., College of Information Engineering, Northwest A & F University, Yangling, 712100, China; Zhao J., School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, 710000, China; Yang L., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; Zhou X., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; Wang L., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; Gao Y., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; Zan L., College of Animal Science and Technology, Northwest A & F University, Yangling, 712100, China; Yang W., College of Animal Science and Technology, Northwest A & F University, Yangling, 712100, China; Liu H., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; Song H., College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China","Accurate and continuous identification of individual cattle is crucial to precision farming in recent years. It is also the prerequisite to monitor the individual feed intake and feeding time of beef cattle at medium to long distances over different cameras. However, beef cattle can tend to frequently move and change their feeding position during feeding. Furthermore, the great variations in their head direction and complex environments (light, occlusion, and background) can also lead to some difficulties in the recognition, particularly for the bio-similarities among individual cattle. Among them, AlignedReID++ model is characterized by both global and local information for image matching. In particular, the dynamically matching local information (DMLI) algorithm has been introduced into the local branch to automatically align the horizontal local information. In this research, the AlignedReID++ model was utilized and improved to achieve the better performance in cattle re-identification (ReID). Initially, triplet attention (TA) modules were integrated into the BottleNecks of ResNet50 Backbone. The feature extraction was then enhanced through cross-dimensional interactions with the minimal computational overhead. Since the TA modules in AlignedReID++ baseline model increased the model size and floating point operations (FLOPs) by 0.005 M and 0.05 G, the rank-1 accuracy and mean average precision (mAP) were improved by 1.0 percentage points and 2.94 percentage points, respectively. Specifically, the rank-1 accuracies were outperformed by 0.86 percentage points and 0.12 percentage points, respectively, compared with the convolution block attention module (CBAM) and efficient channel attention (ECA) modules, although 0.94 percentage points were lower than that of squeeze-and-excitation (SE) modules. The mAP metric values were exceeded by 0.22, 0.86 and 0.12 percentage points, respectively, compared with the SE, CBAM, and ECA modules. Additionally, the Cross-Entropy Loss function was replaced with the CosFace Loss function in the global branch of baseline model. CosFace Loss and Hard Triplet Loss were jointly employed to train the baseline model for the better identification on the similar individuals. AlignedReID++ with CosFace Loss was outperformed the baseline model by 0.24 and 0.92 percentage points in the rank-1 accuracy and mAP, respectively, whereas, AlignedReID++ with ArcFace Loss was exceeded by 0.36 and 0.56 percentage points, respectively. The improved model with the TA modules and CosFace Loss was achieved in a rank-1 accuracy of 94.42%, rank-5 accuracy of 98.78%, rank-10 accuracy of 99.34%, mAP of 63.90%, FLOPs of 5.45 G, frames per second (FPS) of 5.64, and model size of 23.78 M. The rank-1 accuracies were exceeded by 1.84, 4.72, 0.76 and 5.36 percentage points, respectively, compared with the baseline model, part-based convolutional baseline (PCB), multiple granularity network (MGN), and relation-aware global attention (RGA), while the mAP metrics were surpassed 6.42, 5.86, 4.30 and 7.38 percentage points, respectively. Meanwhile, the rank-1 accuracy was 0.98 percentage points lower than TransReID, but the mAP metric was exceeded by 3.90 percentage points. Moreover, the FLOPs of improved model were only 0.05 G larger than that of baseline model, while smaller than those of PCB, MGN, RGA, and TransReID by 0.68, 6.51, 25.4, and 16.55 G, respectively. The model size of improved model was 23.78 M, which was smaller than those of the baseline model, PCB, MGN, RGA, and TransReID by 0.03, 2.33, 45.06, 14.53 and 62.85 M, respectively. The inference speed of improved model on a CPU was lower than those of PCB, MGN, and baseline model, but higher than TransReID and RGA. The t-SNE feature embedding visualization demonstrated that the global and local features were achieve in the better intra-class compactness and inter-class variability. Therefore, the improved model can be expected to effectively re-identify the beef cattle in natural environments of breeding farm, in order to monitor the individual feed intake and feeding time. © 2024 Chinese Society of Agricultural Engineering. All rights reserved.","AlignedReID++; beef cattle; deep learning; identify; method; precision livestock; re-identification","Fertilizers; Alignedreid++; Baseline models; Beef cattle; Deep learning; Identify; Method; Part based; Percentage points; Precision livestock; Re identifications; Image matching","","","H. Song; College of Mechanical and Electronic Engineering, Northwest A & F University, Yangling, 712100, China; email: songhuaibo@nwafu.edu.cn","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","English","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85207060435"
"Omotara G.; Tousi S.M.A.; Decker J.; Brake D.; DeSouza G.N.","Omotara, Gbenga (57195594066); Tousi, Seyed Mohamad Ali (57220201411); Decker, Jared (35263658500); Brake, Derek (36611389000); DeSouza, G.N. (6701775683)","57195594066; 57220201411; 35263658500; 36611389000; 6701775683","High-Throughput and Accurate 3D Scanning of Cattle Using Time-of-Flight Sensors and Deep Learning","2024","Sensors","24","16","5275","","","","0","10.3390/s24165275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202444246&doi=10.3390%2fs24165275&partnerID=40&md5=c7b0ef21b5774da79a047c5f9ecd5c8c","Vision-Guided and Intelligent Robotics Laboratory, Electrical Engineering and Computer Science Department, University of Missouri, Columbia, 65201, MO, United States; Division of Animal Sciences, University of Missouri, Columbia, 65201, MO, United States","Omotara G., Vision-Guided and Intelligent Robotics Laboratory, Electrical Engineering and Computer Science Department, University of Missouri, Columbia, 65201, MO, United States; Tousi S.M.A., Vision-Guided and Intelligent Robotics Laboratory, Electrical Engineering and Computer Science Department, University of Missouri, Columbia, 65201, MO, United States; Decker J., Division of Animal Sciences, University of Missouri, Columbia, 65201, MO, United States; Brake D., Division of Animal Sciences, University of Missouri, Columbia, 65201, MO, United States; DeSouza G.N., Vision-Guided and Intelligent Robotics Laboratory, Electrical Engineering and Computer Science Department, University of Missouri, Columbia, 65201, MO, United States","We introduce a high-throughput 3D scanning system designed to accurately measure cattle phenotypes. This scanner employs an array of depth sensors, i.e., time-of-flight (ToF) sensors, each controlled by dedicated embedded devices. The sensors generate high-fidelity 3D point clouds, which are automatically stitched using a point could segmentation approach through deep learning. The deep learner combines raw RGB and depth data to identify correspondences between the multiple 3D point clouds, thus creating a single and accurate mesh that reconstructs the cattle geometry on the fly. In order to evaluate the performance of our system, we implemented a two-fold validation process. Initially, we quantitatively tested the scanner for its ability to determine accurate volume and surface area measurements in a controlled environment featuring known objects. Next, we explored the impact and need for multi-device synchronization when scanning moving targets (cattle). Finally, we performed qualitative and quantitative measurements on cattle. The experimental results demonstrate that the proposed system is capable of producing high-quality meshes of untamed cattle with accurate volume and surface area measurements for livestock studies. © 2024 by the authors.","3D surface reconstruction; cattle scanner; deep learning; segmentation","Animals; Cattle; Deep Learning; Imaging, Three-Dimensional; Image segmentation; Surface measurement; Velocity measurement; 3D point cloud; 3D surface reconstruction; 3D-scanning; Cattle scanner; Cattles; Deep learning; High-throughput; Segmentation; Surface area measurement; Time-of-flight sensor; animal; bovine; deep learning; procedures; three-dimensional imaging; Mesh generation","National Institute of Food and Agriculture, NIFA","This research was funded by the Agriculture and Food Research Initiative Competitive Grant no. 2021-67021-33448 from the USDA National Institute of Food and Agriculture.","G.N. DeSouza; Vision-Guided and Intelligent Robotics Laboratory, Electrical Engineering and Computer Science Department, University of Missouri, Columbia, 65201, United States; email: desouzag@missouri.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","39204969","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85202444246"
"Zhang M.; Zhu Y.; Wu J.; Zhao Q.; Zhang X.; Luo H.","Zhang, Mengjie (57203716957); Zhu, Yanfei (59406478200); Wu, Jiabao (59406200700); Zhao, Qinan (58287051800); Zhang, Xiaoshuan (8448836000); Luo, Hailing (57204320300)","57203716957; 59406478200; 59406200700; 58287051800; 8448836000; 57204320300","Improved composite deep learning and multi-scale signal features fusion enable intelligent and precise behaviors recognition of fattening Hu sheep","2024","Computers and Electronics in Agriculture","227","","109635","","","","1","10.1016/j.compag.2024.109635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208759208&doi=10.1016%2fj.compag.2024.109635&partnerID=40&md5=747b4b01d57624389f23238dbd49e42d","China Agricultural University, Beijing, 100083, China; Inner Mongolia Academy of Agricultural & Animal Husbandry Sciences, Hohhot, 010000, China","Zhang M., China Agricultural University, Beijing, 100083, China; Zhu Y., China Agricultural University, Beijing, 100083, China; Wu J., China Agricultural University, Beijing, 100083, China; Zhao Q., Inner Mongolia Academy of Agricultural & Animal Husbandry Sciences, Hohhot, 010000, China; Zhang X., China Agricultural University, Beijing, 100083, China; Luo H., China Agricultural University, Beijing, 100083, China","The integration of artificial intelligence and advanced sensing technologies can improve the intelligence and precision level of livestock management. This study focuses on fattening Hu sheep as the object of study, and aims to assess the effectiveness of integrating multi-scale biological signals with improved composite deep learning model in identifying and classifying behaviors of fattening Hu sheep. The multi-scale biological signals were collected using the respiratory sensor and the multi-dimensional posture sensor (composed of an accelerometers, gyroscope, and magnetometer), and then, after data processing, extracted signal features and used the dimensionality reduction method of principal component analysis (PCA). Attention-based particle swarm optimized convolution and long short-term memory (APSO-CALM) model was developed using the feature fused dataset, and its performance was compared with other models. The results showed that: (1) The multi-scale biological signals were analyzed and categorized into five distinct behaviors based on experimental records: feeding, rumination, mating, free movement and running. Each of these behaviors exhibits unique characteristics in their signal images. (2) PCA was utilized to reduce the dimensionality of the feature fused dataset of the multi-scale biological signals, preserving principal components with a cumulative contribution rate of 98 %. Among all components of the first and second contribution rates, except for a few individuals, there are significant differences (P < 0.05) between the data of different behaviors of the same component. (3) The improved composite deep learning model, APSO-CALM, demonstrates significant advantages over single models in behavior recognition. Its accuracy, precision, recall, and F1 score are 95.0 %, 94.8 %, 94.5 %, and 94.6 %, respectively. By utilizing the APSO-CALM model, the drawbacks of individual models are mitigated, enhancing overall performance and overcoming the limitations of single model applications. This study effectively identified five behaviors of fattening Hu sheep, providing theoretical and practical basis for intelligent and precise management of fattening Hu sheep. © 2024 Elsevier B.V.","Flattening Hu sheep; Improved composite deep learning; Intelligent and precise behaviors recognition; Multi-scale biological signal","Adversarial machine learning; Dimensionality reduction; Federated learning; Behaviour recognition; Biological signals; Flattening hu sheep; Improved composite deep learning; Intelligent and precise behavior recognition; Multi-scale biological signal; Multi-scales; Particle swarm; Short term memory; Signal features; artificial intelligence; data set; livestock; machine learning; posture; principal component analysis; Contrastive Learning","National Key Research and Development Program of China, NKRDPC, (2022YFD1300205); National Key Research and Development Program of China, NKRDPC; National Postdoctoral Program for Innovative Talents, (BX20220343); National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation, (2022M723409); China Postdoctoral Science Foundation; Pinduoduo-China Agricultural University, (PC2023B02020)","This research was supported by the National Key R&D Program of China (Grant No. 2022YFD1300205), the National Postdoctoral Program for Innovative Talents (Grant No. BX20220343), the fellowship of China Postdoctoral Science Foundation (Grant No. 2022M723409) and the Pinduoduo-China Agricultural University Research Fund (Grant No. PC2023B02020).","X. Zhang; China Agricultural University, Beijing, 100083, China; email: zhxshuan@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85208759208"
"Singh N.; Mahore V.; Das M.; Kaur S.; Basumatary S.; Shadap N.R.","Singh, Naseeb (57225962597); Mahore, Vijay (58497937800); Das, Meena (55389272100); Kaur, Simardeep (57222130981); Basumatary, Surabhi (59468455000); Shadap, Naphi Roi (59468292000)","57225962597; 58497937800; 55389272100; 57222130981; 59468455000; 59468292000","Development of deep learning-based mobile application for the identification of Coccidia species in pigs using microscopic images","2025","Veterinary Parasitology","334","","110376","","","","0","10.1016/j.vetpar.2024.110376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211742098&doi=10.1016%2fj.vetpar.2024.110376&partnerID=40&md5=83f40a9b7b4465bbc72a9eab661c1cc1","ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721 302, India","Singh N., ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; Mahore V., Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721 302, India; Das M., ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; Kaur S., ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; Basumatary S., ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; Shadap N.R., ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India","Coccidiosis is a gastrointestinal parasitic disease caused by different species of Eimeria and Isospora, poses a significant threat to pig farming, leading to substantial economic losses attributed to reduced growth rates, poor feed conversion, increased mortality rates, and the expense of treatment. Traditional methods for identifying Coccidia species in pigs rely on fecal examination and microscopic analysis, necessitating expert personnel for accurate species identification. To address this need, a deep learning-based mobile application capable of automatically identifying different species of Eimeria and Isospora was developed. The present study focused on six species, namely, E. debliecki, E. perminuta, E. porci, E. spinosa, E. suis, and Isospora suis, commonly found in pigs of the North Eastern Hill (NEH) region of India. Utilizing a two-stage approach, segmentation of coccidia oocysts in microscopic images using convolutional neural networks (CNNs), followed by species identification by same network was carried out in this work. Resource-efficient models, including EfficientNetB0, EfficientNetB1, MobileNet, and MobileNetV2, within an encoder-decoder architecture were utilized to extract features. Transfer learning was applied to enhance model accuracy during training. Additionally, a marker-controlled watershed algorithm was implemented to separate touching cells, thus reducing misclassification. The results demonstrate that all the developed models effectively segmented/classified Coccidia species, achieving mean Intersection-over-Union (m-IoU) values exceeding 0.92, with individual class IoU scores above 0.90. MobileNetV2 exhibited the highest m-IoU of 0.95, followed by EfficientNetB1 with an m-IoU of 0.94. For classification, MobileNetV2 demonstrated the highest performance, with accuracy, precision, and recall values of 0.93, 0.96, and 0.96, respectively. EfficientNetB1 yielded an accuracy of 0.91. The developed mobile application, tested on new data, achieved an identification accuracy of 91.0 %. These findings highlight the potential of deep learning-based mobile applications in effectively identifying Coccidia species in pigs, thus, providing a promising solution to mitigate reliance on expert personnel and laborious time-consuming experiments in this domain. © 2024 Elsevier B.V.","Coccidiosis monitoring; Deep learning; Eimeria spp.; Isospora-suis; Mobile application; Smart livestock farming","accuracy; animal husbandry; Article; Coccidia; coccidiosis; deep learning; Eimeria; Eimeria debliecki; Eimeria perminuta; Eimeria porci; Eimeria spinosa; Eimeria suis; feature extraction; feature extraction algorithm; identification accuracy; image segmentation; India; Isospora suis; laboratory automation; livestock; microscope image; nonhuman; oocyst; parasite identification; pig; product development","Indian Council of Agricultural Research, ICAR; Department of Biotechnology, Ministry of Science and Technology, India, DBT, (OXX5395); Department of Biotechnology, Ministry of Science and Technology, India, DBT","We are thankful to the Director, ICAR Research Complex for NEH Region, Umiam, Meghalaya, for their support and providing necessary facilities for this work. We also acknowledge the financially support provided by the Department of Biotechnology (DBT), Government of India, New Delhi, under project number OXX5395 (Project Title: DBT Societal Development Project on \u2018Establishment of Rural Bioresource Complex in the aspirational district of Meghalaya for improving livelihood of hill farmers through sustainable livestock based integrated cluster farming\u2019).","N. Singh; ICAR-Research Complex for North Eastern Hill (NEH) Region, Umiam, Meghalaya, 793 103, India; email: naseeb501@gmail.com","","Elsevier B.V.","03044017","","VPARD","","English","Vet. Parasitol.","Article","Final","","Scopus","2-s2.0-85211742098"
"Torrusio S.; Gonzalez J.; Errázquin M.; Cereceda A.; Cassano M.J.; Morales N.; Saucedo R.","Torrusio, Sandra (26656226600); Gonzalez, Juan (59479186700); Errázquin, Martin (59479635800); Cereceda, Abril (56768109800); Cassano, M. Julia (57192070693); Morales, Natalia (57195223038); Saucedo, Ricardo (59479362700)","26656226600; 59479186700; 59479635800; 56768109800; 57192070693; 57195223038; 59479362700","Counting Cattle in Argentina with AI and Satellite Images","2024","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","48","4","","459","464","5","0","10.5194/isprs-archives-XLVIII-4-2024-459-2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212524063&doi=10.5194%2fisprs-archives-XLVIII-4-2024-459-2024&partnerID=40&md5=c4ed337378ae9cb1c77d027bb5282b27","Faculty of Natural Sciences and Museum, National University of La Plata (UNLP), Av.122 & 60 street, La Plata (1900), Buenos Aires, Argentina; National Service of Agri-Food Health and Quality (SENASA), Av. Paseo Colon 367, Ciudad de Buenos Aires, 1063, Argentina; Faculty of Engineering, National University of Buenos Aires (UBA), Av. Paseo Colon 870, Ciudad de Buenos Aires, 1063, Argentina","Torrusio S., Faculty of Natural Sciences and Museum, National University of La Plata (UNLP), Av.122 & 60 street, La Plata (1900), Buenos Aires, Argentina; Gonzalez J., National Service of Agri-Food Health and Quality (SENASA), Av. Paseo Colon 367, Ciudad de Buenos Aires, 1063, Argentina, Faculty of Engineering, National University of Buenos Aires (UBA), Av. Paseo Colon 870, Ciudad de Buenos Aires, 1063, Argentina; Errázquin M., Faculty of Engineering, National University of Buenos Aires (UBA), Av. Paseo Colon 870, Ciudad de Buenos Aires, 1063, Argentina; Cereceda A., Faculty of Natural Sciences and Museum, National University of La Plata (UNLP), Av.122 & 60 street, La Plata (1900), Buenos Aires, Argentina; Cassano M.J., Faculty of Natural Sciences and Museum, National University of La Plata (UNLP), Av.122 & 60 street, La Plata (1900), Buenos Aires, Argentina; Morales N., Faculty of Natural Sciences and Museum, National University of La Plata (UNLP), Av.122 & 60 street, La Plata (1900), Buenos Aires, Argentina; Saucedo R., National Service of Agri-Food Health and Quality (SENASA), Av. Paseo Colon 367, Ciudad de Buenos Aires, 1063, Argentina","From a specific need of the national institution responsible for plant and animal health in Argentina, SENASA, this collaborative work between the public sectors together with academia arises. The objective was the development of a Deep Learning algorithm for the detection of livestock, cows in particular, in very high-resolution satellite images (provided by the Argentine Space Agency (CONAE)) and its subsequent counting. The basic elements involved in the development of artificial intelligence are detailed, such as the selection and acquisition of satellite images, their very thorough preprocessing and labelling, details of the training stage and some forms of error quantification. The image database is made up of about 320 scenes (based on very high resolution (VHR) satellite data from the Pleiades and Pleiades NEO sensors, ©Airbus 2022, distributed by CONAE) of the Pampas and Patagonian regions in Argentina. Around 8,000 labels of different types of animals were generated, the most common were cows and sheep. Also labels that did not represent animals to contribute to training. Finally, some very promising preliminary results are presented, such as the average error in the count that was achieved was +/-four cows. The usefulness of these tools is reflected in better management of renewable natural resources linked to animal health issues, fiscal issues and within the framework of the 2030 Agenda and Sustainable Development Goals #12, #15 and #17. © Author(s) 2024.","AI; Counting Cattle; Natural Resources; VHR Satellite Images","Fertilizers; Livestock; Satellite imagery; Veterinary medicine; Animal health; Argentina; Cattles; Collaborative Work; Counting cattle; National institutions; Plant healths; PLEIADES; Satellite images; Very high resolution satellite images","Comisión Nacional de Actividades Espaciales, CONAE; Facultad de Ciencias Naturales y Museo, Universidad Nacional de La Plata, FCNyM - UNLP; Universidad de Buenos Aires, UBA; National University of Buenos Aires; Universidad Nacional de La Plata, UNLP; SENASA","The authors thank the support received by SENASA and its staff (Direcci\u00F3n de Tecnolog\u00EDas de la Informaci\u00F3n), by the National University of La Plata (UNLP) (Dr. Javier Diaz), by Faculty of Natural Sciences and Museum (Dr. Eduardo Kruse) and by the National University of Buenos Aires (UBA). Another special thanks to CONAE (National Commission of Space Activities) (Nathalie Horlent, Aldana Bini) for providing satellite data that is not easy to acquire.","","Zlatanova S.; Brovelli M.A.; Wu H.; Helmholz P.; Chen L.","International Society for Photogrammetry and Remote Sensing","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85212524063"
"Zhai Z.; Zhang Z.; Xu H.; Wang H.; Chen X.; Yang C.","Zhai, Zhaoyu (57202360240); Zhang, Zihan (59378322400); Xu, Huanliang (15521655900); Wang, Haiqing (55983780100); Chen, Xi (59207493600); Yang, Chenmin (59418470400)","57202360240; 59378322400; 15521655900; 55983780100; 59207493600; 59418470400","Review of Applying YOLO Family Algorithms to Analyze Animal and Plant Phenotype; [YOLO算法在动植物表型研究中应用综述]","2024","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","55","11","","1","20","19","0","10.6041/j.issn.1000-1298.2024.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209760377&doi=10.6041%2fj.issn.1000-1298.2024.11.001&partnerID=40&md5=087a91699b4222d11c969a2dd7251486","College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China","Zhai Z., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; Zhang Z., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; Xu H., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; Wang H., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; Chen X., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; Yang C., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China","Plant and animal phenotypes are quantitative descriptions of their characteristics and traits. Accurate analysis of phenotypic features is an important prerequisite for the development of digital agriculture. The traditional phenotypic analysis task heavily relies on manual identification and measurement by agricultural experts, which is labor-intensive, costly, and sensitive to subjective judgments. Also, the traditional approach can hardly process high-throughput data. Benefited by the rapid development of the deep learning technique, as one of the most representative computer vision models, the YOLO family algorithms have shown excellent performance and great potential in plant and animal phenotypic analysis tasks, including disease diagnosis, behavior quantification, biomass estimation, and so on. In this review, livestock, poultry, crops, fruits, vegetables, and other plants and animals were chosen as the research targets. The research progress of YOLO family algorithm applications was summarized from three aspects, namely, object detection, key point detection, and object segmentation. Along the same lines, some commonly used datasets for plant and animal phenotyping tasks for subsequent researchers were presented. Finally, the potential problems faced by current researching and the future development trend of YOLO family algorithms were highlighted, including lightweight architecture design, accurate detection of small targets, weakly supervised learning, complex scene deployment, and large model for target detection. The research aimed at providing summarization and guidance for plant and animal phenotypic analysis based on YOLO family algorithms and promoting the further development of digital agriculture. © 2024 Chinese Society of Agricultural Machinery. All rights reserved.","animal and plant phenotype; key-point detection; lightweight; object detection; object segmentation; YOLO","Deep learning; Object detection; Self-supervised learning; Supervised learning; Animal and plant phenotype; Digital agriculture; Key-point detection; Keypoints; Lightweight; Objects detection; Objects segmentation; Phenotypic analysis; Point detection; YOLO; Livestock","","","H. Xu; College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210095, China; email: huanliangxu@njau.edu.cn","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85209760377"
"Embaby M.G.; Sarker T.T.; AbuGhazaleh A.; Ahmed K.R.","Embaby, Mohamed G. (57211397813); Sarker, Toqi Tahamid (57215601896); AbuGhazaleh, Amer (6602381337); Ahmed, Khaled R. (57218488448)","57211397813; 57215601896; 6602381337; 57218488448","Optical gas imaging and deep learning for quantifying enteric methane emissions from rumen fermentation in vitro","2025","IET Image Processing","19","1","e13327","","","","0","10.1049/ipr2.13327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215619622&doi=10.1049%2fipr2.13327&partnerID=40&md5=833f1a3d4cec5bf2cfacc88e60d759ac","School of Agricultural Sciences, Southern Illinois University Carbondale, IL, United States; School of Computing, Southern Illinois University Carbondale, IL, United States","Embaby M.G., School of Agricultural Sciences, Southern Illinois University Carbondale, IL, United States; Sarker T.T., School of Computing, Southern Illinois University Carbondale, IL, United States; AbuGhazaleh A., School of Agricultural Sciences, Southern Illinois University Carbondale, IL, United States; Ahmed K.R., School of Computing, Southern Illinois University Carbondale, IL, United States","This study investigated the possibility of using a laser methane detector (LMD) and optical gas imaging (OGI) to detect and quantify enteric methane ((Formula presented.)) produced by ruminants in vitro. Four single-flow continuous fermenters were used for rumen culture incubation with four different treatment diets: Control (50:50 forage to concentrate [F:C] ratio), Control + Bromoform (CBR), Low Forage (LF; 20:80), and High Forage (HF; 80:20). After 10 days of incubation, all fermenter contents were transferred and used in a 24 h ANKOM batch culture to measure (Formula presented.) gas production with LMD and OGI. The authors introduce the Controlled Diet (CD) dataset, a large-scale collection of 4,885 (Formula presented.) plume images captured using an FLIR GF77 OGI camera under varying dietary conditions. The performance of six semantic segmentation models (FCN, U-Net, Vision Transformer, Swin Transformer, DeepLabv3+, and Gasformer) on the CD dataset is compared. Results showed that LMD data for (Formula presented.) followed a similar pattern to the gas chromatography (GC) instrument results. The in vitro results showed that different diets and F:C ratios had an impact on (Formula presented.) gas production and rumen fermentation characteristics. Adding bromoform to the control diet fully inhibited (Formula presented.) emission. The HF diet produced more (Formula presented.) compared to all treatments ((Formula presented.)) when measured with GC and LMD. CBR produced the lowest (Formula presented.) values when measured with GC and LMD. The Gasformer architecture achieved the highest performance with mean IoU of 85.1% and mean F-score of 91.72%. These findings demonstrate that OGI technology combined with advanced semantic segmentation models offers a promising solution for predicting and quantifying (Formula presented.) emissions in the livestock sector, potentially aiding in the development of mitigation strategies to combat climate change. © 2025 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","gasformer; methane quantification; optical gas imaging; semantic segmentation","Computerized tomography; Fluidized bed process; Gas chromatography; Hydraulic mining; Infrared absorption; Mining laws and regulations; Bromoform; Gas productions; Gasformer; In-vitro; Methane quantification; Optical gas imaging; Performance; Rumen fermentations; Segmentation models; Semantic segmentation; Semantic Segmentation","","","T.T. Sarker; School of Computing, Southern Illinois University, Carbondale, United States; email: toqitahamid.sarker@siu.edu","","John Wiley and Sons Inc","17519659","","","","English","IET Image Proc.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85215619622"
"Yin L.; Zheng W.; Shi H.; Wang M.; Wang W.; Wang Y.; Ding D.","Yin, Liting (57201796003); Zheng, Wei (56999543600); Shi, Honghua (8394080900); Wang, Ming (59256356100); Wang, Weimin (57201393381); Wang, Yongzhi (7601503414); Ding, Dewen (8986547800)","57201796003; 56999543600; 8394080900; 59256356100; 57201393381; 7601503414; 8986547800","A deep learning-based model for estimating pollution fluxes from rivers into the sea and its optimization","2024","Science of the Total Environment","951","","175434","","","","0","10.1016/j.scitotenv.2024.175434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201190282&doi=10.1016%2fj.scitotenv.2024.175434&partnerID=40&md5=0e9b231849485d2bf1c9ae27bc89b31a","College of Environmental Science and Engineering, Ocean University of China, Qingdao, 266100, China; First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China; Laoshan Laboratory, Qingdao, 266237, China","Yin L., College of Environmental Science and Engineering, Ocean University of China, Qingdao, 266100, China; Zheng W., First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China; Shi H., Laoshan Laboratory, Qingdao, 266237, China; Wang M., Laoshan Laboratory, Qingdao, 266237, China; Wang W., Laoshan Laboratory, Qingdao, 266237, China; Wang Y., First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China; Ding D., First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China","Pollution fluxes from rivers into the sea are currently the main source of pollutants in nearshore areas. Based on the source-sink process of the basin-estuary-coastal waters system, the pollution fluxes into the sea and their spatiotemporal heterogeneity were estimated. A deep learning-based model was established to simplify the estimation of pollution fluxes into the sea, with socio-economic drivers and meteorological data as input variables. A method for estimating the contribution rate of pollution fluxes from different spatial gradient was proposed. In this study, we found that (1) the pollution fluxes into the sea of total nitrogen (TN) and total phosphorus (TP) from the Bohai Sea Rim Basin (BSRB) in 1980, 1990, 2000, 2010, and 2020 were 25.38 × 104, 26.12 × 104, 27.27 × 104, 29.82 × 104, 25.31 × 104 and 1.32 × 104, 2.14 × 104, 2.09 × 104, 1.87 × 104, 1.68 × 104 tons, respectively. (2) The proportion of rural life and livestock to the TN was the highest, accounting for 39.18 % and 21.19 %, respectively. The proportion of livestock to the TP was the highest, accounting for 39.20 %, followed by rural life, accounting for 24.72 %. The results indicated that the pollution fluxes in the BSRB were related to human economic activities and relevant environmental protection measures. (3) The deep learning-based model established to estimate runoff pollution fluxes into the sea had the accuracy of over 90 %. (4) As for contribution rate, in terms of the elevation, the range of 0–100 m had the highest proportion, accounting for 39.65 %. The range of 50–100 km from the coastline had the highest proportion, accounting for 18.11 %. In terms of the district, coastal area has the highest proportion, accounting for 38.00 %. This study revealed the changing trends and driving mechanisms of pollution fluxes into the sea over the past 40 years and established a simplified deep learning-based model for estimating pollution fluxes into the sea. Then, we identified regions with high pollution contribution rate. The results can provide scientific references for the adaptive management of the nearshore areas based on the ecosystem. © 2024 Elsevier B.V.","Basin-estuary-coastal waters system; Convolutional neural networks; Export coefficient model; SWAT","Agricultural pollution; Convolutional neural networks; Deep learning; Economics; Learning systems; River pollution; Rural areas; nitrogen; phosphorus; Basin-estuary-coastal water system; Coastal waters; Contribution rate; Convolutional neural network; Export coefficient models; Learning Based Models; Nearshores; Total nitrogen; Total phosphorus; Water system; artificial neural network; coastal water; economic activity; optimization; pollutant source; river pollution; adsorption; Article; deep learning; environmental factor; evaporation; geographic distribution; livestock; mathematical model; pollution; precipitation; river; rural area; sea; sensitivity analysis; temperature; wettability; article; Bohai Sea; coastal waters; controlled study; convolutional neural network; human; nonhuman; runoff; seashore; SWAT","National Natural Science Foundation of China, NSFC, (U1806214); National Natural Science Foundation of China, NSFC; National Key Research and Development Program of China, NKRDPC, (2018YFD0900806); National Key Research and Development Program of China, NKRDPC","The manuscript was greatly improved by comments from anonymous reviewers. This study was supported by National Natural Science Foundation of China (No. U1806214 ), and National Key Research and Development Program of China (No. 2018YFD0900806 ). ","W. Zheng; First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China; email: zhengwei@fio.org.cn","","Elsevier B.V.","00489697","","STEVA","39128526","English","Sci. Total Environ.","Article","Final","","Scopus","2-s2.0-85201190282"
"Michielon A.; Litta P.; Bonelli F.; Don G.; Farisè S.; Giannuzzi D.; Milanesi M.; Pietrucci D.; Vezzoli A.; Cecchinato A.; Chillemi G.; Gallo L.; Mele M.; Furlanello C.","Michielon, Andrea (59489437500); Litta, Paolo (59489652500); Bonelli, Francesca (55240093800); Don, Gregorio (59489014800); Farisè, Stefano (59489652600); Giannuzzi, Diana (55312922600); Milanesi, Marco (56492690800); Pietrucci, Daniele (57201683476); Vezzoli, Angelica (59489652700); Cecchinato, Alessio (16743701500); Chillemi, Giovanni (57210768035); Gallo, Luigi (35310013200); Mele, Marcello (7006984765); Furlanello, Cesare (6701821823)","59489437500; 59489652500; 55240093800; 59489014800; 59489652600; 55312922600; 56492690800; 57201683476; 59489652700; 16743701500; 57210768035; 35310013200; 7006984765; 6701821823","Mind the Step: An Artificial Intelligence-Based Monitoring Platform for Animal Welfare †","2024","Sensors","24","24","8042","","","","0","10.3390/s24248042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213269156&doi=10.3390%2fs24248042&partnerID=40&md5=72120f1fa020a13545406764ec0fbae5","Orobix Life, Bergamo, 24121, Italy; Antares Vision, Travagliato, 25039, Italy; Department of Veterinary Sciences, University of Pisa, Pisa, 56124, Italy; Department of Agronomy, Animals, Food, Natural Resources, and Environment—DAFNAE, University of Padua, Legnaro, 35020, Italy; Department for Innovation in Biological, Agri-Food, and Forestry Systems—DIBAF, University of Tuscia, Viterbo, 01100, Italy; Department of Agriculture, Food and Environment—DAFE, Università di Pisa, Pisa, 56124, Italy; LIGHT Center Brescia, Brescia, 25123, Italy","Michielon A., Orobix Life, Bergamo, 24121, Italy, Antares Vision, Travagliato, 25039, Italy; Litta P., Orobix Life, Bergamo, 24121, Italy, Antares Vision, Travagliato, 25039, Italy; Bonelli F., Department of Veterinary Sciences, University of Pisa, Pisa, 56124, Italy; Don G., Department of Agronomy, Animals, Food, Natural Resources, and Environment—DAFNAE, University of Padua, Legnaro, 35020, Italy; Farisè S., Orobix Life, Bergamo, 24121, Italy, Antares Vision, Travagliato, 25039, Italy; Giannuzzi D., Department of Agronomy, Animals, Food, Natural Resources, and Environment—DAFNAE, University of Padua, Legnaro, 35020, Italy; Milanesi M., Department for Innovation in Biological, Agri-Food, and Forestry Systems—DIBAF, University of Tuscia, Viterbo, 01100, Italy; Pietrucci D., Department for Innovation in Biological, Agri-Food, and Forestry Systems—DIBAF, University of Tuscia, Viterbo, 01100, Italy; Vezzoli A., Orobix Life, Bergamo, 24121, Italy, Antares Vision, Travagliato, 25039, Italy; Cecchinato A., Department of Agronomy, Animals, Food, Natural Resources, and Environment—DAFNAE, University of Padua, Legnaro, 35020, Italy; Chillemi G., Department for Innovation in Biological, Agri-Food, and Forestry Systems—DIBAF, University of Tuscia, Viterbo, 01100, Italy; Gallo L., Department of Agronomy, Animals, Food, Natural Resources, and Environment—DAFNAE, University of Padua, Legnaro, 35020, Italy; Mele M., Department of Agriculture, Food and Environment—DAFE, Università di Pisa, Pisa, 56124, Italy; Furlanello C., Orobix Life, Bergamo, 24121, Italy, Antares Vision, Travagliato, 25039, Italy, LIGHT Center Brescia, Brescia, 25123, Italy","We present an artificial intelligence (AI)-enhanced monitoring framework designed to assist personnel in evaluating and maintaining animal welfare using a modular architecture. This framework integrates multiple deep learning models to automatically compute metrics relevant to assessing animal well-being. Using deep learning for AI-based vision adapted from industrial applications and human behavioral analysis, the framework includes modules for markerless animal identification and health status assessment (e.g., locomotion score and body condition score). Methods for behavioral analysis are also included to evaluate how nutritional and rearing conditions impact behaviors. These models are initially trained on public datasets and then fine-tuned on original data. We demonstrate the approach through two use cases: a health monitoring system for dairy cattle and a piglet behavior analysis system. The results indicate that scalable deep learning and edge computing solutions can support precision livestock farming by automating welfare assessments and enabling timely, data-driven interventions. © 2024 by the authors.","AI/ML; behavioral analysis; body condition score; locomotion score; precision livestock farming","Animal Welfare; Animals; Artificial Intelligence; Behavior, Animal; Cattle; Deep Learning; Humans; Monitoring, Physiologic; Swine; Electronic health record; Animal welfare; Artificial intelligence/ML; Behavioral analysis; Body condition score; Enhanced monitoring; Locomotion score; Modular architectures; Monitoring frameworks; Monitoring platform; Precision livestock farming; animal; animal behavior; animal welfare; artificial intelligence; bovine; deep learning; devices; human; physiologic monitoring; physiology; pig; procedures","European Commission, EC; European Union Next-Generation EU, (1032 17/06/2022, CN00000022)","This study was carried out within the AGRITECH National Research Center and received funding from the European Union Next-Generation EU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR)\u2014MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.4\u2014D.D. 1032 17/06/2022, CN00000022). This manuscript reflects only the authors\u2019 views and opinions, and neither the European Union nor the European Commission can be considered responsible for them.","A. Michielon; Orobix Life, Bergamo, 24121, Italy; email: andrea.michielon97@gmail.com; P. Litta; Orobix Life, Bergamo, 24121, Italy; email: paolo.litta@orobix.life","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","39771778","English","Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85213269156"
"Park M.; Kim H.-M.; Kim Y.; Bak S.; Kim T.-Y.; Jang S.W.","Park, Miso (58832341000); Kim, Heung-Min (57192656656); Kim, Youngmin (58700288900); Bak, Suho (57192652574); Kim, Tak-Young (59487262500); Jang, Seon Woong (57814293300)","58832341000; 57192656656; 58700288900; 57192652574; 59487262500; 57814293300","A Framework for Detecting and Managing Non-Point-Source Pollution in Agricultural Areas Using GeoAI and UAVs","2024","Drones","8","12","786","","","","0","10.3390/drones8120786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213079345&doi=10.3390%2fdrones8120786&partnerID=40&md5=af3fc7081d075e699fd191ad3fdd09ca","Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; Remote Sensing Department, IREMTECH Co., Ltd., Busan, 46027, South Korea","Park M., Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; Kim H.-M., Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; Kim Y., Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; Bak S., Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; Kim T.-Y., Remote Sensing Department, IREMTECH Co., Ltd., Busan, 46027, South Korea; Jang S.W., Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea","This study proposes a novel framework for detecting and managing non-point-source (NPS) pollution in agricultural areas using unmanned aerial vehicles (UAVs) and geospatial artificial intelligence (GeoAI). High-resolution UAV imagery, combined with the YOLOv8 instance segmentation model, was employed to accurately detect and classify various NPS sources, such as livestock barns, compost heaps, greenhouses, and mulching films. The spatial information, including the area and volume of detected objects, was analyzed to track temporal changes and evaluate management strategies. The framework integrates remote sensing, deep learning, and geographic information system (GIS) analysis to enhance decision-making processes, providing detailed insight into NPS pollution dynamics over time. This approach not only improves the efficiency of NPS monitoring but also facilitates proactive management by offering precise location and environmental impact data. The results indicate that this framework can significantly improve resource allocation and environmental management practices, particularly in agriculture-dominated regions susceptible to NPS pollution, thereby contributing to the sustainable development of these areas. © 2024 by the authors.","deep learning; environmental monitoring; geospatial artificial intelligence; non-point-source pollution; unmanned aerial vehicle","Decision making; Direct air capture; Farm buildings; Haze pollution; Pollution detection; Unmanned aerial vehicles (UAV); Aerial vehicle; Agricultural areas; Deep learning; Environmental Monitoring; Geo-spatial; Geospatial artificial intelligence; High resolution; Nonpoint sources; Nonpoint-source pollution (NPS); Unmanned aerial vehicle; Environmental monitoring","Ministry of Science and ICT, (RS-2023-00243061)","This research was funded by the Commercialization Promotion Agency for R&D Outcomes (COMPA), funded by the Ministry of Science and ICT (MSIT), grant number: RS-2023-00243061, and the project entitled \u201CDevelopment of an intelligent image automatic analysis system for river non-point pollution source management\u201D. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.","S.W. Jang; Research Institute, IREMTECH Co., Ltd., Busan, 46027, South Korea; email: jsw@iremtech.co.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","2504446X","","","","English","Drones","Article","Final","","Scopus","2-s2.0-85213079345"
"Li X.; Zhang Y.; Li S.","Li, Xiaopeng (57271095500); Zhang, Yichi (58611109700); Li, Shuqin (55490641400)","57271095500; 58611109700; 55490641400","Rethinking lightweight sheep face recognition via network latency-accuracy tradeoff","2024","Computers and Electronics in Agriculture","227","","109662","","","","1","10.1016/j.compag.2024.109662","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209376680&doi=10.1016%2fj.compag.2024.109662&partnerID=40&md5=dfdd164e7947c00c22b3cfa636ce42ce","College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Li X., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Zhang Y., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Li S., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Deep learning has greatly improved the performance of sheep face recognition, but existing recognition methods usually adopt deeper and wider networks to obtain better performance, resulting in heavy computational burden and slow inference speed. This paper proposes a very lightweight sheep face recognition network, referred to as VLFaceNet, which achieves state-of-the-art (SOTA) latency-accuracy tradeoff. The basic module of VLFaceNet is VL, which uses inexpensive linear operations to complement redundant features and reduces the model size and computational complexity through structural re-parameterization during inference, improving inference speed. VLDBlock is formed by concatenating VL and ECA channel attention to enhance the effectiveness of channel-level feature extraction. VLFaceNet is formed by stacking VL and VLDBlock. By fusing features of different scales of VLFaceNet, sheep faces of different scales can be recognized, improving the recognition performance of the model. To address the problem of high similarity and difficulty in distinguishing white sheep faces, this paper proposes a scaling feature enhancement method SFE, which changes the color distribution and texture of sheep face images, improving the distinguishability between sheep face images and thus the recognition performance of VLFaceNet. The recognition performance gains of multiple recognition models demonstrate the effectiveness of SFE. On a self-built dataset, VLFaceNet achieves the best latency-accuracy tradeoff with an inference latency of 2.58 ms and a recognition accuracy of 97.75 %. This research is expected to promote the application of deep learning-based recognition methods in livestock breeding. © 2024 Elsevier B.V.","Data augmentation; Latency-accuracy tradeoff; Lightweight model; Reparameterization; Sheep face recognition","Deep reinforcement learning; Macroinvertebrates; Computational burden; Data augmentation; Face images; Latency-accuracy tradeoff; Lightweight model; Network latencies; Performance; Recognition methods; Reparameterization; Sheep face recognition; accuracy assessment; complexity; machine learning; numerical model; parameterization; pattern recognition; research work; trade-off; Face recognition","","","S. Li; College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: lsq_cie@nwafu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85209376680"
"Bai L.; Guo C.; Song J.","Bai, Lili (58922823300); Guo, Chaopeng (56144833700); Song, Jie (55276320800)","58922823300; 56144833700; 55276320800","Cattle weight estimation model through readily photos","2025","Engineering Applications of Artificial Intelligence","143","","109976","","","","0","10.1016/j.engappai.2024.109976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214447282&doi=10.1016%2fj.engappai.2024.109976&partnerID=40&md5=5e915e1b860ebc671d0499985b652888","Software College, Northeastern University, Shenyang, 110819, China","Bai L., Software College, Northeastern University, Shenyang, 110819, China; Guo C., Software College, Northeastern University, Shenyang, 110819, China; Song J., Software College, Northeastern University, Shenyang, 110819, China","Cattle weight is a key indicator for assessing health, meat quality, productivity, and overall well-being. Manual measurement methods are time-consuming and induce stress reactions. In contrast, non-contact measurement methods based on computer vision can effectively and rapidly estimate cattle weight. These non-contact methods typically involve image processing and three-dimensional reconstruction, capturing image information with cameras and combining deep learning algorithms to estimate cattle weight. Despite their advantages, these methods are mainly designed for large-scale professional cattle farms and are characterized by high costs and complex operations, which do not meet the needs of individual cattle farmers. To address this issue, we propose the Lightweight Network-based Cattle Weight Estimation (LaWE) model, deployed on mobile phones and can measure cattle weight by capturing readily available photos. The LaWE model uses an optimized lightweight multi-scale fusion network to accurately extract keypoints from images and calculate relevant body measurements, which are then input into a specially designed deep regression network to estimate cattle weight. We conducted extensive experiments on both publicly available datasets and our self-collected dataset of 108 Horqin Yellow cattle. The results demonstrate that the LaWE model achieves an accuracy rate of over 97% in cattle weight estimation, meeting the requirements for low cost and real-time performance. Our research makes a significant contribution to the field of non-contact cattle weight estimation, providing practical technical support for smallholder farmers in the industry. © 2025 Elsevier Ltd","Automatic measurement; Computer vision for livestock; Multi-scale feature fusion; Non-contact weight estimation; Readily photos","Automatic measurements; Cattles; Computer vision for livestock; Features fusions; Multi-scale feature fusion; Multi-scale features; Non-contact; Non-contact weight estimation; Readily photo; Weights estimation","","","J. Song; Software College, Northeastern University, Shenyang, 110819, China; email: songjie@mail.neu.edu.cn","","Elsevier Ltd","09521976","","EAAIE","","English","Eng Appl Artif Intell","Article","Final","","Scopus","2-s2.0-85214447282"
"Yadav C.S.; Peixoto A.A.T.; Rufino L.A.L.; Silveira A.B.; de Alexandria A.R.","Yadav, Chandra Shekhar (57213555062); Peixoto, Antonio Augusto Teixeira (57223686281); Rufino, Luis Alberto Linhares (58706101000); Silveira, Aedo Braga (59390135300); de Alexandria, Auzuir Ripardo (26031321600)","57213555062; 57223686281; 58706101000; 59390135300; 26031321600","Intelligent Classifier for Identifying and Managing Sheep and Goat Faces Using Deep Learning","2024","AgriEngineering","6","4","","3586","3601","15","0","10.3390/agriengineering6040204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207836292&doi=10.3390%2fagriengineering6040204&partnerID=40&md5=c1bd4b63f78f684d8cd093149d097b77","School of Computer Applications, Noida Institute of Engineering and Technology, Uttar Pradesh, Greater Noida, 201306, India; Instituto Federal do Ceará, Jaguaribe, 63475-000, Brazil; Programa de Pós-Graduação em Ciências Veterinárias Brazil, Universidade Estadual do Ceará—UECE, Fortaleza, 60040-215, Brazil; Instituto Federal de Educação, Ciência e Tecnologia do Ceará—IFCE Brazil, Fortaleza, 60040-215, Brazil","Yadav C.S., School of Computer Applications, Noida Institute of Engineering and Technology, Uttar Pradesh, Greater Noida, 201306, India; Peixoto A.A.T., Instituto Federal do Ceará, Jaguaribe, 63475-000, Brazil; Rufino L.A.L., Programa de Pós-Graduação em Ciências Veterinárias Brazil, Universidade Estadual do Ceará—UECE, Fortaleza, 60040-215, Brazil; Silveira A.B., Instituto Federal de Educação, Ciência e Tecnologia do Ceará—IFCE Brazil, Fortaleza, 60040-215, Brazil; de Alexandria A.R., Instituto Federal de Educação, Ciência e Tecnologia do Ceará—IFCE Brazil, Fortaleza, 60040-215, Brazil","Computer vision, particularly in artificial intelligence (AI), is increasingly being applied in various industries, including livestock farming. Identifying and managing livestock through machine learning is essential to improve efficiency and animal welfare. The aim of this work is to automatically identify individual sheep or goats based on their physical characteristics including muzzle pattern, coat pattern, or ear pattern. The proposed intelligent classifier was built on the Roboflow platform using the YOLOv8 model, trained with 35,204 images. Initially, a Convolutional Neural Network (CNN) model was developed, but its performance was not optimal. The pre-trained VGG16 model was then adapted, and additional fine-tuning was performed using data augmentation techniques. The dataset was split into training (88%), validation (8%), and test (4%) sets. The performance of the classifier was evaluated using precision, recall, and F1-Score metrics, with comparisons against other pre-trained models such as EfficientNet. The YOLOv8 classifier achieved 95.8% accuracy in distinguishing between goat and sheep images. Compared to the CNN and VGG16 models, the YOLOv8-based classifier showed superior performance in terms of both accuracy and computational efficiency. The results confirm that deep learning models, particularly YOLOv8, significantly enhance the accuracy and efficiency of livestock identification and management. Future research could extend this technology to other livestock species and explore real-time monitoring through IoT integration. © 2024 by the authors. Licensee MDPI, Basel, Switzerland.","CNN; computer vision; deep learning; livestock face identification; VGG16; YOLO","","National Research Council, NRC; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (305359/ 2021-5, 442182/2023-6); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq; Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico, FUNCAP, (UNI-0210-00699.01.00/23, Edital 38/2022); Fundação Cearense de Apoio ao Desenvolvimento Científico e Tecnológico, FUNCAP","This work was supported by the National Research Council\u2014CNPq through call 305359/ 2021-5 and 442182/2023-6 and the Internal Simplified Call PRPI/Postgraduate\u2014Grant Support for IFCE Stricto Sensu Postgraduate Programs, FUNCAP (UNI-0210-00699.01.00/23, and Edital 38/2022), and CAPES.","C.S. Yadav; School of Computer Applications, Noida Institute of Engineering and Technology, Greater Noida, Uttar Pradesh, 201306, India; email: drcsyadav@niet.co.in","","Multidisciplinary Digital Publishing Institute (MDPI)","26247402","","","","English","AgriEng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207836292"
"Lee S.-B.; Park J.-B.; Cho H.-C.","Lee, Su-Bin (59505133700); Park, Jae-Beom (58753914100); Cho, Hyun-Chong (22233514800)","59505133700; 58753914100; 22233514800","Real-Time Detection of Cattle Behavior Using YOLOv9 with AutoAugment and Mixup Data Augmentation Techniques; [AutoAugment 와 Mixup 데이터 증대 기법을 적용한 YOLOv9 기반 실시간 소 행동 탐지]","2024","Transactions of the Korean Institute of Electrical Engineers","73","12","","2326","2332","6","0","10.5370/KIEE.2024.73.12.2326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214362900&doi=10.5370%2fKIEE.2024.73.12.2326&partnerID=40&md5=29cf5b676d962370d61d83cfb2a6bc78","Dept. of Electronics Engineering, Dept. of Data Science, Kangwon National University, South Korea; Dept. of Data Science, Kangwon National University, South Korea; Dept. of Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea","Lee S.-B., Dept. of Data Science, Kangwon National University, South Korea; Park J.-B., Dept. of Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea; Cho H.-C., Dept. of Electronics Engineering, Dept. of Data Science, Kangwon National University, South Korea","As the number of livestock farms in Korea decreases, the scale of individual farms has grown, necessitating advanced methods for efficient livestock management. Recent studies have increasingly applied computer vision and deep learning to monitor livestock behavior, health, and disease, aiming to enhance productivity and animal welfare. This study introduces a real-time cattle behavior detection system utilizing YOLOv9, a state-of-the-art object detection algorithm known for its high accuracy and efficiency. The YOLOv9-s model, optimized for real-time detection with minimal computational overhead, was employed. To further enhance detection accuracy, we incorporated AutoAugment, a data augmentation technique that automatically selects the optimal augmentation policies, and mixup, a method that improves model generalization by creating new training samples through linear interpolation of existing ones. The application of AutoAugment improved the mean Average Precision (mAP) from 0.926 to 0.941, while the addition of mixup raised the mAP to 0.947, representing a 2.1% performance increase. These results confirm the system's capability for accurate and efficient real-time detection of cattle behavior.  Copyright © The Korean Institute of Electrical Engineers.","Cattle Behaviors; Data Augmentation; Deep Learning; Real-time Object Detection; Smart Livestock Farming","Deep learning; Cattle behavior; Cattles; Data augmentation; Deep learning; Livestock farming; Objects detection; Real- time; Real-time detection; Real-time object detection; Smart livestock farming; Livestock","","","H.-C. Cho; Dept. of Electronics Engineering, Dept. of Data Science, Kangwon National University, South Korea; email: hyuncho@kangwon.ac.kr","","Korean Institute of Electrical Engineers","19758359","","","","Korean","Trans. Korean Inst. Electr. Eng.","Article","Final","","Scopus","2-s2.0-85214362900"
"Araújo V.M.; Rili I.; Gisiger T.; Gambs S.; Vasseur E.; Cellier M.; Diallo A.B.","Araújo, Voncarlos M. (57201293024); Rili, Ines (59506833800); Gisiger, Thomas (8374275000); Gambs, Sébastien (56025052200); Vasseur, Elsa (33267999700); Cellier, Marjorie (57220163298); Diallo, Abdoulaye Baniré (15044077500)","57201293024; 59506833800; 8374275000; 56025052200; 33267999700; 57220163298; 15044077500","AI-powered cow detection in complex farm environments","2025","Smart Agricultural Technology","10","","100770","","","","0","10.1016/j.atech.2025.100770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214528875&doi=10.1016%2fj.atech.2025.100770&partnerID=40&md5=1ccdfb201ad9e2ed721d0bc1db552549","Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada; McGill University, Montréal, Québec, Canada; Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada","Araújo V.M., Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Rili I., Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Gisiger T., Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Gambs S., Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Vasseur E., McGill University, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Cellier M., McGill University, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada; Diallo A.B., Département d'informatique, Université du Québec à Montréal, Montréal, Québec, Canada, Innovation Chair in Animal Welfare and Artificial Intelligence (WELL-E), Canada","Animal welfare has become a critical issue in contemporary society, emphasizing our ethical responsibilities toward animals, particularly within livestock farming. In addition, the advent of Artificial Intelligence (AI) technologies, specifically computer vision, offers a innovative approach to monitoring and enhancing animal welfare. Cows, as essential contributors to sustainable agriculture and climate management, being a central part of it. However, existing cow detection algorithms face significant challenges in real-world farming environments, such as complex lighting, occlusions, pose variations and background interference, which hinder accurate and reliable detection. Additionally, the model generalization power is highly desirable as it enables the model to adapt and perform well across different contexts and conditions, beyond its training environment or dataset. This study addresses these challenges in diverse cow dataset composed of six different environments, including indoor and outdoor scenarios. More precisely, we propose a novel detection model that combines YOLOv8 with the CBAM (Convolutional Block Attention Module) and assess its performance against baseline models, including Mask R-CNN, YOLOv5 and YOLOv8. Our findings indicate that while baseline models show promise, their performance degrades in complex real-world conditions, which our approach improves using the CBAM attention module. Overall, YOLOv8-CBAM outperformed YOLOv8 by 2.3% in mAP across all camera types, achieving a precision of 95.2% and an mAP@0.5:0.95 of 82.6%, demonstrating superior generalization and enhanced detection accuracy in complex backgrounds. Thus, the primary contributions of this research are: (1) providing an in-depth analysis of current limitations in cow detection under challenging indoor and outdoor environments, (2) proposing a robust general model that effectively detects cows in complex real-world conditions and (3) evaluating and benchmarking state-of-the-art detection algorithms. Potential application scenarios of the model include automated health monitoring, behavioral analysis and tracking within smart farm management systems, enabling precise detection of individual cows, even in challenging environments. By addressing these critical challenges, this study paves the way for future innovations in AI-driven livestock monitoring, aiming to improve the welfare and management of farm animals while advancing smart agriculture. © 2025 The Author(s)","Animal welfare; Cow detection; Deep learning; Livestock monitoring","","","","V.M. Araújo; Département d'informatique, Université du Québec à Montréal, Montréal, Canada; email: voncarlos.araujo@gmail.com","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Review","Final","","Scopus","2-s2.0-85214528875"
"Wang X.; Hu Y.; Wang M.; Li M.; Zhao W.; Mao R.","Wang, Xiaobo (58539626200); Hu, Yufan (59117476500); Wang, Meili (55694491200); Li, Mei (57199160408); Zhao, Wenxiao (59462811200); Mao, Rui (36167161700)","58539626200; 59117476500; 55694491200; 57199160408; 59462811200; 36167161700","A Real-Time Lightweight Behavior Recognition Model for Multiple Dairy Goats","2024","Animals","14","24","3667","","","","0","10.3390/ani14243667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213387868&doi=10.3390%2fani14243667&partnerID=40&md5=815239223ea4df38b7dd675372fb4412","College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Shaanxi Engineering Research Center of Agriculture Information Intelligent Perception and Analysis, Yangling, 712100, China; College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China","Wang X., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Hu Y., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Wang M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Shaanxi Engineering Research Center of Agriculture Information Intelligent Perception and Analysis, Yangling, 712100, China; Li M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Shaanxi Engineering Research Center of Agriculture Information Intelligent Perception and Analysis, Yangling, 712100, China; Zhao W., College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China; Mao R., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Shaanxi Engineering Research Center of Agriculture Information Intelligent Perception and Analysis, Yangling, 712100, China","Livestock behavior serves as a crucial indicator of physiological health. Leveraging deep learning techniques to automatically recognize dairy goat behaviors, particularly abnormal ones, enables early detection of potential health and environmental issues. To address the challenges of recognizing small-target behaviors in complex environments, a multi-scale and lightweight behavior recognition model for dairy goats called GSCW-YOLO was proposed. The model integrates Gaussian Context Transformation (GCT) and the Content-Aware Reassembly of Features (CARAFE) upsampling operator, enhancing the YOLOv8n framework’s attention to behavioral features, reducing interferences from complex backgrounds, and improving the ability to distinguish subtle behavior differences. Additionally, GSCW-YOLO incorporates a small-target detection layer and optimizes the Wise-IoU loss function, increasing its effectiveness in detecting distant small-target behaviors and transient abnormal behaviors in surveillance videos. Data for this study were collected via video surveillance under varying lighting conditions and evaluated on a self-constructed dataset comprising 9213 images. Experimental results demonstrated that the GSCW-YOLO model achieved a precision of 93.5%, a recall of 94.1%, and a mean Average Precision (mAP) of 97.5%, representing improvements of 3, 3.1, and 2 percentage points, respectively, compared to the YOLOv8n model. Furthermore, GSCW-YOLO is highly efficient, with a model size of just 5.9 MB and a frame per second (FPS) of 175. It outperforms popular models such as CenterNet, EfficientDet, and other YOLO-series networks, providing significant technical support for the intelligent management and welfare-focused breeding of dairy goats, thus advancing the modernization of the dairy goat industry. © 2024 by the authors.","abnormal behaviors; behavior recognition; dairy goat; deep learning; GSCW-YOLO","abnormal behavior; accuracy; animal experiment; animal welfare; Article; computer language; dairy goat; data processing; deep learning; drinking; eating; grooming; lightweight behavior recognition model; livestock; model; nonhuman; scratching; standing; training; video surveillance; videorecording","Key Research and Development Projects of Shaanxi Province, (2024NC-ZDCYL-05-11); Key Research and Development Projects of Shaanxi Province; National Key Research and Development Program of China, NKRDPC, (2022ZD04014); National Key Research and Development Program of China, NKRDPC; Shaanxi Provincial Science and Technology Department, (2022QFY11-03); Shaanxi Provincial Science and Technology Department","This study was financially supported by the Key Research and Development Program of Shaanxi (2024NC-ZDCYL-05-11); the National Key Research and Development Plan (2022ZD04014); and the Regional Innovation Capability Guidance Plan of Science and Technology Department of Shaanxi Province (2022QFY11-03).","R. Mao; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: maorui@nwafu.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85213387868"
"Hua Z.; Jiao Y.; Zhang T.; Wang Z.; Shang Y.; Song H.","Hua, Zhixin (57724662500); Jiao, Yitao (57577689200); Zhang, Tianyu (59392032700); Wang, Zheng (55352644900); Shang, Yuying (57215528335); Song, Huaibo (17342958900)","57724662500; 57577689200; 59392032700; 55352644900; 57215528335; 17342958900","Automatic location and recognition of horse freezing brand using rotational YOLOv5 deep learning network","2024","Artificial Intelligence in Agriculture","14","","","21","30","9","1","10.1016/j.aiia.2024.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207921848&doi=10.1016%2fj.aiia.2024.10.003&partnerID=40&md5=ac0a868e1ca72d4a3bdf83903fe4aaac","College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China; XilinGol League Animal Husbandry Station, XilinGol League, Inner Mongolia, 026000, China","Hua Z., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China; Jiao Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China; Zhang T., XilinGol League Animal Husbandry Station, XilinGol League, Inner Mongolia, 026000, China; Wang Z., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China; Shang Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China; Song H., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, and Rural Affairs, Shaanxi, Yangling, 712100, China","Individual livestock identification is of great importance to precision livestock farming. Liquid nitrogen freezing labeled horse brand is an effective way for livestock individual identification. Along with various technological developments, deep-learning-based methods have been applied in such individual marking recognition. In this research, a deep learning method for oriented horse brand location and recognition was proposed. Firstly, Rotational YOLOv5 (R-YOLOv5) was adopted to locate the oriented horse brand, then the cropped images of the brand area were trained by YOLOv5 for number recognition. In the first step, unlike classical detection methods, R-YOLOv5 introduced the orientation into the YOLO framework by integrating Circle Smooth Label (CSL). Besides, Coordinate Attention (CA) was added to raise the attention to positional information in the network. These improvements enhanced the accuracy of detecting oriented brands. In the second step, number recognition was considered as a target detection task because of the requirement of accurate recognition. Finally, the whole brand number was obtained according to the sequences of each detection box position. The experiment results showed that R-YOLOv5 outperformed other rotating target detection algorithms, and the AP (Average Accuracy) was 95.6 %, the FLOPs were 17.4 G, the detection speed was 14.3 fps. As for the results of number recognition, the mAP (mean Average Accuracy) was 95.77 %, the weight size was 13.71 MB, and the detection speed was 68.6 fps. The two-step method can accurately identify brand numbers with complex backgrounds. It also provides a stable and lightweight method for livestock individual identification. © 2024 The Authors","CSL; Horse brand; Number recognition; Rotating target detection; YOLOv5","Automatic target recognition; Deep reinforcement learning; Automatic location; Automatic recognition; Circle smooth label; Detection speed; Horse brand; Individual identification; Number recognition; Rotating target detection; Targets detection; YOLOv5; Deep learning","Northwest A and F University, NWAFU; National Key Research and Development Program of China, NKRDPC, (2023YFD1301801); National Key Research and Development Program of China, NKRDPC; National Natural Science Foundation of China, NSFC, (32272931); National Natural Science Foundation of China, NSFC","This work was supported by the National Key Research and Development Program of China (Grant No. 2023YFD1301801 ) and National Natural Science Foundation of China (No. 32272931 ). The authors appreciate the funding organization for its financial support. The study was approved by the Medical Ethics Committee (MEC) of NWAFU Experimental Animal Manage Committee of Northwest A&F University . The authors would also like to thank the helpful comments and suggestions provided by all the authors cited in this article and the anonymous reviewers.","H. Song; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: songhuaibo@nwsuaf.edu.cn","","KeAi Communications Co.","25897217","","","","English","Artif. Intell. Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207921848"
"Papa M.; Robson de Medeiros Oliveira S.; Bergier I.","Papa, Matheus (57219866988); Robson de Medeiros Oliveira, Stanley (59353553900); Bergier, Ivan (50461023200)","57219866988; 59353553900; 50461023200","Technologies in cattle traceability: A bibliometric analysis","2024","Computers and Electronics in Agriculture","227","","109459","","","","0","10.1016/j.compag.2024.109459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205563703&doi=10.1016%2fj.compag.2024.109459&partnerID=40&md5=b22566cc94e0821f5ad55859aa33c6ba","Faculty of Agricultural Engineering (FEAGRI), Unicamp, SP, Campinas, Brazil; Embrapa Digital Agriculture, SP, Campinas, Brazil","Papa M., Faculty of Agricultural Engineering (FEAGRI), Unicamp, SP, Campinas, Brazil; Robson de Medeiros Oliveira S., Faculty of Agricultural Engineering (FEAGRI), Unicamp, SP, Campinas, Brazil, Embrapa Digital Agriculture, SP, Campinas, Brazil; Bergier I., Embrapa Digital Agriculture, SP, Campinas, Brazil","It has been widely documented that livestock cattle can play a non-negligible role in natural landscapes due to climate change, deforestation, and enteric methane emissions. Alternatively, sustainable protocols and market digitalization are highlighted as promising tools to mitigate environmental cattle impacts by authenticated data in digital traceability systems. Digital inclusion, particularly for cattle breeders, can be a useful starting point for employing sustainable protocol in food chain production and management. This study analyzes the evolution of knowledge in the area of animal traceability to compare applied technologies found by a bibliometric analysis of articles published in Web of Science. The study evidences a clear change in thematic research over decades, currently culminating in technologies such as blockchain, IoT (Internet of Things), machine learning, and deep learning. These technologies emerge as the main research scopes in promoting transparency and reliability in the production chain, especially considering individual digital identification. However, challenges such as high investment requirements and difficulties in data accessibility, interoperability, privacy, and security implicate the low maturity level of available technologies and knowledge, therefore preventing further adoption and development of reliable worldwide animal traceability systems. © 2024 Elsevier B.V.","Agriculture 4.0; Animal identification Tag; Beef Chain; Technology adoption; Transparency","Low emission; Agriculture 4.0; Animal identification; Animal identification tag; Beef chain; Bibliometrics analysis; Cattles; Identification tags; Natural landscapes; Technology adoption; Traceability systems; agricultural technology; cattle; digitization; food chain; identification key; technology adoption; Kyoto Protocol","Web of Science; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, (88887.670488/2022-00, 88887, 670488/2022-00); Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2022/09319-9)","Funding text 1: We thank Dr. Cesar de Oliveira Ferreira Silva for sharing the procedures and scripts written in the R programming language for analyzing articles extracted from the Web of Science (WoS). We would also like to thank the support from the Coordination for the Improvement of Higher Education Personnel - Brazil (CAPES) - Financing Code 001, in addition to the support with the scholarship (process no. 88887.670488/2022-00). I.B. thanks to the support of Fapesp grant 2022/09319-9.; Funding text 2: We thank Dr. Cesar de Oliveira Ferreira Silva for sharing the procedures and scripts written in the R programming language for analyzing articles extracted from the Web of Science (WoS). We would also like to thank the support from the Coordination for the Improvement of Higher Education Personnel - Brazil (CAPES) - Financing Code 001, in addition to the support with the scholarship (process no. 88887. 670488/2022-00 ). ","M. Papa; Faculty of Agricultural Engineering (FEAGRI), Unicamp, Campinas, SP, Brazil; email: matheus.papa.almeida@gmail.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","","Scopus","2-s2.0-85205563703"
"Ashworth A.J.; Avila A.; Smith H.; Winzeler T.E.; Owens P.; Flynn C.; O'Brien P.; Philipp D.; Su J.","Ashworth, A.J. (56047858300); Avila, A. (58615625400); Smith, H. (57552310500); Winzeler, T.E. (59330532600); Owens, P. (14219522600); Flynn, C. (59330647300); O'Brien, P. (57190136503); Philipp, D. (57205771967); Su, J. (58599073900)","56047858300; 58615625400; 57552310500; 59330532600; 14219522600; 59330647300; 57190136503; 57205771967; 58599073900","Predicting spatiotemporal patterns of productivity and grazing from multispectral data using neural network analysis based on system complexity","2024","Agrosystems, Geosciences and Environment","7","3","e20571","","","","0","10.1002/agg2.20571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204137636&doi=10.1002%2fagg2.20571&partnerID=40&md5=d838e33eb1f944494505ce16330563f0","USDA-ARS, Poultry Production and Product Safety Research Unit, Fayetteville, AR, United States; Department of Mathematics, University of Texas at Arlington, Arlington, TX, United States; Environmental Dynamics Program, University of Arkansas, Fayetteville, AR, United States; USDA-ARS, Dale Bumpers Small Farms Research Center, Booneville, AR, United States; Grassland Soil and Water Research Laboratory, Temple, TX, United States; USDA ARS National Laboratory for Agriculture and The Environment, Ames, IA, United States; Animal Science Department, University of Arkansas, Fayetteville, AR, United States","Ashworth A.J., USDA-ARS, Poultry Production and Product Safety Research Unit, Fayetteville, AR, United States; Avila A., Department of Mathematics, University of Texas at Arlington, Arlington, TX, United States; Smith H., Environmental Dynamics Program, University of Arkansas, Fayetteville, AR, United States; Winzeler T.E., Department of Mathematics, University of Texas at Arlington, Arlington, TX, United States; Owens P., USDA-ARS, Dale Bumpers Small Farms Research Center, Booneville, AR, United States; Flynn C., Grassland Soil and Water Research Laboratory, Temple, TX, United States; O'Brien P., USDA ARS National Laboratory for Agriculture and The Environment, Ames, IA, United States; Philipp D., Animal Science Department, University of Arkansas, Fayetteville, AR, United States; Su J., Department of Mathematics, University of Texas at Arlington, Arlington, TX, United States","Remote sensing tools, along with Global Navigation Satellite System cattle collars and digital soil maps, may help elucidate spatiotemporal relationships among soils, terrain, forages, and animals. However, standard computational procedures preclude systems-level evaluations across this continuum due to data inoperability and processing limitations. Deep learning, a subset of neural network, may elucidate efficiency of livestock production and linkages within the livestock-grazing environment. Consequently, we applied deep learning to environmental remote sensing data to (1) develop predictive models for yield and forage nutrition based on vegetation indices and (2) at a pixel-level (per 55 m2), identify how grazing is linked to soil properties, forage growth and nutrition, and terrain attributes in silvopasture and pasture-only systems. Remotely sensed data rapidly and non-destructively estimated herbage mass and nutritive value for enhanced net and primary productivity management in livestock and grazing systems. Cattle grazed big bluestem (Andropogon gerardii ‘Vitman’) with 182% greater frequency than orchardgrass (Dactylis glomerata L.) in the pasture-only system. Real-time estimates of vegetative bands may assist in predicting grazing pressure for more efficient pasture resource management. Cattle grazing followed distinct soil-landscape patterns, namely reduced cattle grazing preference occurred in areas of water accumulation, which highlights linkages among terrain, soil-water movement, soil properties, forage nutrition, and animal grazing response spatially and temporally. Results from this study could be scaled up to improve grazing management among the largest land-use category in the United States, that is, grasslands, which would allow for sustainable intensification of forage-based livestock production to meet growing demands for environmentally responsible protein. © 2024 The Author(s). Agrosystems, Geosciences & Environment published by Wiley Periodicals LLC on behalf of Crop Science Society of America and American Society of Agronomy.","","","U.S. Department of Agriculture, USDA; Agricultural Research Service, ARS","Trade names or commercial products mentioned in this article are solely for the purpose of providing specific information and do not infer either recommendation or endorsement by the U.S. Department of Agriculture. Data collection and sample analysis were made possible by Taylor Adams with the USDA-ARS. Field management by Robert Rhein with University of Arkansas Animal Science is also gratefully acknowledged.","A.J. Ashworth; USDA-ARS, Poultry Production and Product Safety Research Unit, Fayetteville, 72701, United States; email: Amanda.Ashworth@usda.gov","","John Wiley and Sons Inc","26396696","","","","English","Agrosyst. Geosci. Environ.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85204137636"
"Walle Girmaw D.","Walle Girmaw, Dagne (58068753600)","58068753600","Livestock animal skin disease detection and classification using deep learning approaches","2025","Biomedical Signal Processing and Control","102","","107334","","","","0","10.1016/j.bspc.2024.107334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212219165&doi=10.1016%2fj.bspc.2024.107334&partnerID=40&md5=f006250baa46a73cf7fe1a6b786e7a83","Department of Information Technology, Haramaya University, Dire Dawa, Ethiopia","Walle Girmaw D., Department of Information Technology, Haramaya University, Dire Dawa, Ethiopia","The manual methods of identifying skin diseases in animals are labor intensive, time-consuming, and inaccurate. Because of the absence of medical materials, the veterinarian's skin knowledge is limited, the correctness of diagnosis is low, and the symptoms and characteristics of skin disease are complex due to this, the accuracy is low. Therefore, we used a deep learning approach for skin disease recognition that can accurately classify diseases with high performance. Currently, deep-learning approaches are important for detecting skin diseases in the medical field. The pre-trained models are initially trained on a captured dataset of animal skin disease. The color and texture of the animal skin are crucial for the deep learning models for feature extraction and to recognize the affected area. A total of 1405 diverse images of animals (cattle, sheep, and goats) were collected from Haramaya University, College of Veterinary Medicine by veterinarians. To develop the proposed work, we used data processing methods, data augmentation techniques, transfer learning, performance evaluation metrics, training, validation, and testing phases. We selected three recent deep-learning models (EfficentNetB7, MobileNetV2, and DenseNet201) to extract significant features from the input images. We used a SoftMax classifier for skin disease classification because we have multiple classes. According to the experimental results, EfficentNetB7 achieved an accuracy of 99.01% compared to the other models in classifying the four classes investigated. We also observed the locations of diseased areas of skin, the convolution layer, and the intermediate layers using visualization approaches. © 2024 The Author(s)","Animal Skin Disease; Deep Learning; Transfer Learning; Visualization","Animal skin disease; Animal skins; Deep learning; Disease classification; Disease detection; Learning approach; Learning models; Livestock animals; Skin disease; Transfer learning; animal disease; Article; bovine; classifier; comparative study; controlled study; convolutional neural network; data processing; deep learning; deep neural network; densenet; diagnostic accuracy; diagnostic test accuracy study; disease classification; efficientnet; feature extraction; goat; livestock; mobilenet model; mobilenetv2; nonhuman; random forest; receiver operating characteristic; sheep; skin disease; transfer of learning; validation study; veterinarian; veterinary medicine","","","D. Walle Girmaw; Department of Information Technology, Haramaya University, Dire Dawa, Ethiopia; email: dagnewalle143@gmail.com","","Elsevier Ltd","17468094","","","","English","Biomed. Signal Process. Control","Article","Final","","Scopus","2-s2.0-85212219165"
"Ruchay A.; Kolpakov V.; Guo H.; Pezzuolo A.","Ruchay, Alexey (57192592568); Kolpakov, Vladimir (58511094300); Guo, Hao (55331600800); Pezzuolo, Andrea (56023708900)","57192592568; 58511094300; 55331600800; 56023708900","On-barn cattle facial recognition using deep transfer learning and data augmentation","2024","Computers and Electronics in Agriculture","225","","109306","","","","0","10.1016/j.compag.2024.109306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200474714&doi=10.1016%2fj.compag.2024.109306&partnerID=40&md5=38ec0ad91666c4297a9bffb69ef98c29","Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; South Ural State University, Chelyabinsk, 454080, Russian Federation; Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation; Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Orenburg, 460000, Russian Federation; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Department of Land, Environment, Agriculture and Forestry, University of Padua, Legnaro, 35020, Italy; Department of Agronomy, Food, Natural Resources, Animals and Environment, University of Padua, Legnaro, 35020, Italy","Ruchay A., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, South Ural State University, Chelyabinsk, 454080, Russian Federation, Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation; Kolpakov V., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Orenburg, 460000, Russian Federation; Guo H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Pezzuolo A., Department of Land, Environment, Agriculture and Forestry, University of Padua, Legnaro, 35020, Italy, Department of Agronomy, Food, Natural Resources, Animals and Environment, University of Padua, Legnaro, 35020, Italy","Cattle face recognition technology is an innovative approach to herd management that allows farmers to effectively monitor the health of animals, their movement and behavior, for example, by tracking individual animals to determine which ones need extra care or treatment. Tobehaviour address the issue of cattle identification, especially non-invasive facial recognition, deep neural networks are currently being researched in the biometrics sector. However, their implementation presents significant challenges due to the small database of animal faces. The collected dataset contains 315 RGB images of 91 Aberdeen Angus cattle. This paper uses deep transfer learning to design a deep neural network. Based on the pre-trained models, VGGFACE and VGGFACE2, we propose a model for recognizing the faces of cattle. RGB images were used after preprocessed to improve the reliability of face recognition. RGB image preprocessing consists of several steps: contrast and contour enhancement methods, cattle face detection, image resizing, and normalisation. Data augmentation methods using transformation RGB images and neural network fine-tuning were used to train the deep-learning model. The recognition rate of the pre-trained VGGFACE2 model is 97.1% with the collected dataset. The proposed model can lead to increased efficiency and sustainability of livestock production and improved animal welfare conditions. © 2024 Elsevier B.V.","Deep Learning; Face detection; Face identification; Image preprocessing; Precision Livestock Farming","Animals; Deep neural networks; Farms; Image enhancement; Learning systems; Metadata; Data augmentation; Deep learning; Face identification; Face recognition technologies; Faces detection; Facial recognition; Image preprocessing; Precision livestock farming; RGB images; Transfer learning; animal welfare; artificial neural network; biometry; cattle; data processing; image analysis; image processing; livestock farming; pattern recognition; Face recognition","Russian Science Foundation, RSF, (21-76-20014); Russian Science Foundation, RSF","This work was supported by the Russian Science Foundation , grant \u2116. 21-76-20014 .","A. Ruchay; Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; email: ran@csu.ru","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85200474714"
"Fomina A.S.; Sklyarenko A.A.; Dolgov V.V.; Vasiliev P.V.; Zelenkova G.A.; Kochetkova N.A.","Fomina, A.S. (55929829400); Sklyarenko, A.A. (59539899300); Dolgov, V.V. (57422115300); Vasiliev, P.V. (57193327081); Zelenkova, G.A. (57212390928); Kochetkova, N.A. (58079668400)","55929829400; 59539899300; 57422115300; 57193327081; 57212390928; 58079668400","Assessing the motor activity of chickens based on artificial intelligence technology","2024","BIO Web of Conferences","149","","01017","","","","0","10.1051/bioconf/202414901017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216795305&doi=10.1051%2fbioconf%2f202414901017&partnerID=40&md5=73331518f4e458270212f6225ed0231c","Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation","Fomina A.S., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation; Sklyarenko A.A., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation; Dolgov V.V., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation; Vasiliev P.V., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation; Zelenkova G.A., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation; Kochetkova N.A., Don State Technical University, sq. Gagarina, 1, Rostov-on-Don, 344010, Russian Federation","The article describes an approach to automating the process of analyzing video recordings of bird behavior using artificial intelligence methods, with the aim of increasing production efficiency through the early detection of behavioral change markers in chickens. Currently, agricultural enterprises have a need for automatic assessment of the condition of livestock and poultry to identify signs of stress at an early stage. This approach will allow for minimizing losses and improving production indicators. In turn, the application of deep learning models will enhance the effectiveness of identifying animal behavior markers that may not be detected by a facility operator. © The Authors, published by EDP Sciences.","","","","","A.S. Fomina; Don State Technical University, Rostov-on-Don, sq. Gagarina, 1, 344010, Russian Federation; email: a_bogun@mail.ru","Kalashnikov V.V.; Zaitsev A.M.","EDP Sciences","22731709","","","","English","BIO. Web. Conf.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85216795305"
"Chen F.; Zhou B.; Yang L.; Zhuang J.; Chen X.","Chen, Fengxian (58043730400); Zhou, Bin (55882914500); Yang, Liqiong (55877449300); Zhuang, Jie (7102040150); Chen, Xijuan (28567443800)","58043730400; 55882914500; 55877449300; 7102040150; 28567443800","Assessing the risk of E. coli contamination from manure application in Chinese farmland by integrating machine learning and Phydrus","2024","Environmental Pollution","356","","124345","","","","0","10.1016/j.envpol.2024.124345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195413234&doi=10.1016%2fj.envpol.2024.124345&partnerID=40&md5=9590cc5afe9e03b7f286e9619a867021","Key Laboratory of Pollution Ecology and Environmental Engineering, Institute of Applied Ecology, Chinese Academy of Sciences, Liaoning, Shenyang, 110016, China; Chair of model-based environmental exposure science, Faculty of Medicine, University of Augsburg, Augsburg, 86159, Germany; Department of Biosystems Engineering and Soil Science, Institute for a Secure and Sustainable Environment, The University of Tennessee, Knoxville, 37996, TN, United States; Sino-Spain Joint Laboratory for Agricultural Environment Emerging Contaminants of Zhejiang Province, College of Environmental and Resource Sciences, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China","Chen F., Key Laboratory of Pollution Ecology and Environmental Engineering, Institute of Applied Ecology, Chinese Academy of Sciences, Liaoning, Shenyang, 110016, China; Zhou B., Chair of model-based environmental exposure science, Faculty of Medicine, University of Augsburg, Augsburg, 86159, Germany; Yang L., Key Laboratory of Pollution Ecology and Environmental Engineering, Institute of Applied Ecology, Chinese Academy of Sciences, Liaoning, Shenyang, 110016, China; Zhuang J., Department of Biosystems Engineering and Soil Science, Institute for a Secure and Sustainable Environment, The University of Tennessee, Knoxville, 37996, TN, United States; Chen X., Sino-Spain Joint Laboratory for Agricultural Environment Emerging Contaminants of Zhejiang Province, College of Environmental and Resource Sciences, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China","This study aims to present a comprehensive study on the risks associated with the residual presence and transport of Escherichia coli (E. coli) in soil following the application of livestock manure in Chinese farmlands by integrating machine learning algorithms with mechanism-based models (Phydrus). We initially review 28 published papers to gather data on E. coli's die-off and attachment characteristics in soil. Machine learning models, including deep learning and gradient boosting machine, are employed to predict key parameters such as the die-off rate of E. coli and first-order attachment coefficient in soil. Then, Phydrus was used to simulate E. coli transport and survival in 23692 subregions in China. The model considered regional differences in E. coli residual risk and transport, influenced by soil properties, soil depths, precipitation, seasonal variations, and regional disparities. The findings indicate higher residual risks in regions such as the Northeast China, Eastern Qinghai-Tibet Plateau, and pronounced transport risks in the fringe of the Sichuan Basin fringe, the Loess Plateau, the North China Plain, the Northeast Plain, the Shigatse Basin, and the Shangri-La region. The study also demonstrates a significant reduction in both residual and transport risks one month after manure application, highlighting the importance of timing manure application and implementing region-specific standards. This research contributes to the broader understanding of pathogen behavior in agricultural soils and offers practical guidelines for managing the risks associated with manure use. This study's comprehensive method offers a potentially valuable tool for evaluating microbial contaminants in agricultural soils across the globe. © 2024 Elsevier Ltd","Bacterial attachment; Bacterial die-off; E. coli; Integrated modeling; Machine learning","China; Qinghai-Xizang Plateau; Adaptive boosting; Deep learning; Farms; Fertilizers; Learning systems; Manures; Risk assessment; Soils; rain; soil organic matter; Agricultural soils; Bacterial attachment; Bacterial die-off; Die-off; Integrated modeling; Integrating machines; Machine-learning; Manure applications; Residual risk; Transport risks; agricultural land; agricultural soil; coliform bacterium; machine learning; manure; modeling; pathogen; risk assessment; soil pollution; agricultural worker; algorithm; aquatic environment; Article; bulk density; Chinese; colony forming unit; contamination; cross validation; deep learning; Escherichia coli; limit of detection; livestock; machine learning; nonhuman; scientific literature; simulation; soil; temperature; Escherichia coli","National Natural Science Foundation of China, NSFC, (52379051); National Natural Science Foundation of China, NSFC; Chinese Academy of Sciences, CAS, (XDA28090100); Chinese Academy of Sciences, CAS","This work was financially supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDA28090100), and National Natural Science Foundation of China (Grant number: 52379051). ","X. Chen; Sino-Spain Joint Laboratory for Agricultural Environment Emerging Contaminants of Zhejiang Province, College of Environmental and Resource Sciences, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; email: chenxj@iae.ac.cn","","Elsevier Ltd","02697491","","ENPOE","38852664","English","Environ. Pollut.","Article","Final","","Scopus","2-s2.0-85195413234"
"Fu C.; Ren L.; Wang F.","Fu, Chenfu (59137705100); Ren, Lisheng (22635327400); Wang, Fang (57199195670)","59137705100; 22635327400; 57199195670","Recognizing beef cattle behavior under automatic scene distinction using lightweight FABF-YOLOv8s; [自动化场景区分下 FABF-YOLOv8s 轻量化肉牛行为识别方法]","2024","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","40","15","","152","163","11","1","10.11975/j.issn.1002-6819.202404073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203104591&doi=10.11975%2fj.issn.1002-6819.202404073&partnerID=40&md5=92a5bdbbedb3bbec0ad0ff401c5d8c41","College of Information Science and Technology, Hebei Agricultural University, Baoding 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding 071001, China","Fu C., College of Information Science and Technology, Hebei Agricultural University, Baoding 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding 071001, China; Ren L., College of Information Science and Technology, Hebei Agricultural University, Baoding 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding 071001, China; Wang F., College of Information Science and Technology, Hebei Agricultural University, Baoding 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding 071001, China","Object detection has been widely applied in daily life in recent years, including pedestrian detection, face recognition, and item counting. Among them, deep learning has brought some breakthroughs in the field of object detection. Therefore, object detection can be expected to be applied to the livestock industry, such as cow farms. However, some challenges have been posed by the existing object detection models. Particularly, beef cattle behavior can be hardly recognized under natural weather conditions, due mainly to the susceptibility to complex background interference, large model parameter size, computational complexity, and substantial memory usage for weight files. In this study, a lightweight (FABF-YOLOv8s) model was proposed to recognize the multiple behaviors of beef cattle using automatic scene distinction. Video data sources were collected from the real locations of beef cattle breeding. An all-weather natural scene dataset was constructed with the varying weather conditions. The beef cattle behavior was labeled in the scene dataset using LabelImg software and semiautomatic system annotation tools, thereby constructing a beef cattle behavior dataset. The natural scene dataset was trained using the FasterNet model. The output categories were obtained to verify the scene, and then automatically divide the weather scenes. The YOLOv8 model was obtained to determine the scene features before the detection of beef cattle behavior. The lightweight YOLOv8s network was also selected. The FasterNet lightweight model was used as the Backbone of the YOLOv8s, thus eliminating the computational redundancy of complex models for simple tasks. The Intra-Scale Feature Interaction (AIFI) was combined with the FasterNet lightweight model, in order to enhance the network focus among the features at the same scale, and then to capture the important features. The Weighted Bidirectional Feature Pyramid (BiFPN) was used as the Neck end network. The learnable weights were introduced to learn different input features. The top-down and bottom-up bidirectional paths were repeatedly applied to carry out the multi-scale feature fusion. The C2f-Faster feature extraction network was selected as the node. The parameter size and computational complexity were reduced during convolution. The accuracy of the model was improved to recognize the beef cattle behavior and subsequent deployment. The MPDIoU function was selected to overcome some limitations, such as cross occlusion of beef cattle, and the similar comparison between the prediction and bounding box, particularly with/without the overlap in the bounding box. The accuracy was guaranteed to compress the model structure, parameter quantity, and memory size, in order to obtain the detection model with high accuracy, small size, and strong generalization. An accurate identification of beef cattle behavior was achieved, such as standing, lying, feeding, drinking water, and licking. Finally, the visualization interface was designed to input the dataset in the form of images and videos. The recognition of beef cattle behavior was more intuitively demonstrated by the visualization system. The experimental results show that the FABF-YOLOv8s model (FasterNet, AIFI, BiFPN, and C2f-Faster, FABF) was achieved in an average accuracy, parameter volume, and floating-point computation of 93.6%, 4.51M, and 16.0 GFLOPs, respectively, on the self-built dataset of beef cattle behavior. Consequently, the average accuracy increased by 1.1, 4.7, and 0.4 percentage points, respectively, compared with the YOLOv5s, YOLOv7, and original YOLOv8s models. The parameter volume and floating-point computation decreased by 59.48% and 43.66%, respectively, compared with the YOLOv8s. The accuracy of detection was improved to minimize the computational complexity. The FasterNet’s automated scene distinction was achieved in an Accuracy, Precision, and F1-Score of 99.1%, 98.3%, and 96.7%, respectively. The FasterNet-FABFYOLOv8s model introduced the natural scene factors, with an average accuracy of 94.6%. There were the smaller parameter size and floating-point computation, whereas, the precision was higher, compared with the Faster-RCNN, SSD, FCOS, and DETR models. A lightweight recognition of beef cattle behavior was constructed under an automated scene distinction system. The finding can provide technical support to monitor the health status of beef cattle and automated intelligent breeding. © 2024 Chinese Society of Agricultural Engineering. All rights reserved.","beef cattle; behavior recognition; object detection; scene distinction; YOLOv8","Fertilizers; Image annotation; Image segmentation; Image thinning; Input output programs; Large datasets; Optical flows; Scales (weighing instruments); Water towers; Beef cattle; Behaviour recognition; Bounding-box; Condition; Detection models; Floating-point computation; Natural scenes; Objects detection; Scene distinction; YOLOv8; Deep learning","","","F. Wang; email: 8335901@qq.com","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85203104591"
"Tanveer M.U.; Munir K.; Raza A.; Abualigah L.; Garay H.; Gonzalez L.E.P.; Ashraf I.","Tanveer, Muhammad Usama (59176850400); Munir, Kashif (57671857800); Raza, Ali (56640761700); Abualigah, Laith (57190984712); Garay, Helena (58970103500); Gonzalez, Luis Eduardo Prado (59500158000); Ashraf, Imran (57195478761)","59176850400; 57671857800; 56640761700; 57190984712; 58970103500; 59500158000; 57195478761","Novel Transfer Learning Approach for Detecting Infected and Healthy Maize Crop Using Leaf Images","2025","Food Science and Nutrition","13","1","e4655","","","","0","10.1002/fsn3.4655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214083818&doi=10.1002%2ffsn3.4655&partnerID=40&md5=3f4627f0c4ada92c3daa6a5498ee74ff","Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan; Department of Software Engineering, University of Lahore, Lahore, Pakistan; Computer Science Department, Al al-Bayt University, Mafraq, Jordan; Centre for Research Impact & Outcome, Chitkara University Institute of Engineering and Technology, Rajpura, India; Applied Science Research Center, Applied Science Private University, Amman, Jordan; Universidad Europea del Atlantico, Santander, Spain; Universidade Internacional Do Cuanza, Cuito, Angola; Universidad de La Romana, La Romana, Dominican Republic; Universidad Internacional Iberoamericana, Campeche, Mexico; Fundacion Universitaria Internacional de Colombia, Bogota, Colombia; Department of Information & Communication Engineering, Yeungnam University, Gyeongsan-si, South Korea","Tanveer M.U., Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan; Munir K., Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan; Raza A., Department of Software Engineering, University of Lahore, Lahore, Pakistan; Abualigah L., Computer Science Department, Al al-Bayt University, Mafraq, Jordan, Centre for Research Impact & Outcome, Chitkara University Institute of Engineering and Technology, Rajpura, India, Applied Science Research Center, Applied Science Private University, Amman, Jordan; Garay H., Universidad Europea del Atlantico, Santander, Spain, Universidade Internacional Do Cuanza, Cuito, Angola, Universidad de La Romana, La Romana, Dominican Republic; Gonzalez L.E.P., Universidad Europea del Atlantico, Santander, Spain, Universidad Internacional Iberoamericana, Campeche, Mexico, Fundacion Universitaria Internacional de Colombia, Bogota, Colombia; Ashraf I., Department of Information & Communication Engineering, Yeungnam University, Gyeongsan-si, South Korea","Maize is a staple crop worldwide, essential for food security, livestock feed, and industrial uses. Its health directly impacts agricultural productivity and economic stability. Effective detection of maize crop health is crucial for preventing disease spread and ensuring high yields. This study presents VG-GNBNet, an innovative transfer learning model that accurately detects healthy and infected maize crops through a two-step feature extraction process. The proposed model begins by leveraging the visual geometry group (VGG-16) network to extract initial pixel-based spatial features from the crop images. These features are then further refined using the Gaussian Naive Bayes (GNB) model and feature decomposition-based matrix factorization mechanism, which generates more informative features for classification purposes. This study incorporates machine learning models to ensure a comprehensive evaluation. By comparing VG-GNBNet's performance against these models, we validate its robustness and accuracy. Integrating deep learning and machine learning techniques allows VG-GNBNet to capitalize on the strengths of both approaches, leading to superior performance. Extensive experiments demonstrate that the proposed VG-GNBNet+GNB model significantly outperforms other models, achieving an impressive accuracy score of 99.85%. This high accuracy highlights the model's potential for practical application in the agricultural sector, where the precise detection of crop health is crucial for effective disease management and yield optimization. © 2025 The Author(s). Food Science & Nutrition published by Wiley Periodicals LLC.","feature extraction; plant disease detection; plant leaf detection; precision agriculture; transfer learning","","","","K. Munir; Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan, Pakistan; email: kashif.munir@kfueit.edu.pk; I. Ashraf; Department of Information & Communication Engineering, Yeungnam University, Gyeongsan-si, South Korea; email: imranashraf@ynu.ac.kr","","John Wiley and Sons Inc","20487177","","","","English","Food Sci. Nutr.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85214083818"
"Ocholla I.A.; Pellikka P.; Karanja F.; Vuorinne I.; Väisänen T.; Boitt M.; Heiskanen J.","Ocholla, Ian A. (58829676200); Pellikka, Petri (7007042259); Karanja, Faith (22134890000); Vuorinne, Ilja (57221478085); Väisänen, Tuomas (57222485852); Boitt, Mark (57193012292); Heiskanen, Janne (9842163500)","58829676200; 7007042259; 22134890000; 57221478085; 57222485852; 57193012292; 9842163500","Correction to: Livestock Detection and Counting in Kenyan Rangelands Using Aerial Imagery and Deep Learning Techniques (Remote Sensing, (2024), 16, 16, (2929), 10.3390/rs16162929)","2024","Remote Sensing","16","19","3656","","","","0","10.3390/rs16193656","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206495365&doi=10.3390%2frs16193656&partnerID=40&md5=1fcea877482d99cc42949f63aae6a2c1","Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland; Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Wangari Maathai Institute for Environmental and Peace Studies, University of Nairobi, P.O. Box 29053-00625, Nairobi, Kenya; Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197-00100, Nairobi, Kenya; Helsinki Institute of Sustainability Science, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Helsinki Institute of Urban and Regional Studies, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Institute of Geomatics, GIS and Remote Sensing, Dedan Kimathi University of Technology, Private Bag, Dedan Kimathi P.O. Box 10143-10100, Nyeri, Kenya; Finnish Meteorological Institute, P.O. Box 503, Helsinki, 00101, Finland","Ocholla I.A., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Pellikka P., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland, Wangari Maathai Institute for Environmental and Peace Studies, University of Nairobi, P.O. Box 29053-00625, Nairobi, Kenya; Karanja F., Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197-00100, Nairobi, Kenya; Vuorinne I., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Väisänen T., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Helsinki Institute of Sustainability Science, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland, Helsinki Institute of Urban and Regional Studies, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Boitt M., Institute of Geomatics, GIS and Remote Sensing, Dedan Kimathi University of Technology, Private Bag, Dedan Kimathi P.O. Box 10143-10100, Nyeri, Kenya; Heiskanen J., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Finnish Meteorological Institute, P.O. Box 503, Helsinki, 00101, Finland","The authors would like to make the following corrections to their published paper [1]. We found that an error was made in the text of the Introduction section, paragraph six, where we stated “[…] However, the experiment was conducted in homogeneous herds where animals were 2 to 3 m apart. This method may not be applicable in scenarios where animals are clustered in herds”. While the animal herds were homogeneous in terms of species, the spacing between the animals was not consistently 2 to 3 m apart. Some images featured animal herds with animals that were in close proximity of each other. Therefore, the following correction has been made to the Introduction section, paragraph six: “The experiment was conducted in homogeneous herds for dense, medium, and sparse animal herds”. In the Discussion section, Section 4.2, paragraph three, the values of the HerdNet model were wrongly cited from the validation results rather than the final test values of the cited study, as follows: “Compared to a similar study conducted in Chad [21], the YOLOv5m model used in our study outperformed the HerdNet model, which attained a Precision and total count error of 75.4% and −14.6%, respectively”. The following correction has been made to the Discussion section, Section 4.2, paragraph three: “Compared to a similar study conducted in Chad [21], the YOLOv5m model used in our study outperformed the HerdNet model, which attained a Precision and total count error of 77.5% and −9.4%, respectively”. The authors state that the scientific conclusions are unaffected. These corrections were approved by the Academic Editor. The original publication has also been updated. © 2024 by the authors.","","","","","I.A. Ocholla; Department of Geosciences and Geography, University of Helsinki, Helsinki, P.O. Box 64, 00014, Finland; email: ian.ocholla@helsinki.fi","","Multidisciplinary Digital Publishing Institute (MDPI)","20724292","","","","English","Remote Sens.","Erratum","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85206495365"
"Song H.; Wang Y.; Deng H.; Xu X.; Wen Y.; Zhang S.","Song, Huaibo (17342958900); Wang, Yunfei (57669107600); Deng, Hongxing (58849546000); Xu, Xingshi (57812538800); Wen, Yuchen (58316782800); Zhang, Shujin (58317637700)","17342958900; 57669107600; 58849546000; 57812538800; 58316782800; 58317637700","Research progress of intelligent monitoring technology for large-scale dairy cows based on video analysis; [基于视频分析的规模化奶牛智能监测技术研究进展]","2024","Journal of South China Agricultural University","45","5","","649","660","11","0","10.7671/j.issn.1001-411X.202403038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201237766&doi=10.7671%2fj.issn.1001-411X.202403038&partnerID=40&md5=2653b3f6a016e35ac62e87f2f7c92d77","Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China","Song H., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Wang Y., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Deng H., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Xu X., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Wen Y., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Zhang S., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China","Cow intelligent monitoring is an important link in large-scale dairy farming. Video analysis has the advantages of contactless, low-cost, and intelligent analysis, and has become a hot spot in the research of intelligent identification technology of large-scale dairy cows. Dairy cow target detection, target tracking, and the technologies of individual and behavior recognition are of great significance for large-scale dairy cow supervision. Lighting, day and night alternations, fence occlusion and mutual occlusion caused by large number of cows in complex breeding environment are serious factors affecting the intelligent monitoring of large-scale dairy cows. This paper summarized the depth models and practical application commonly used in cow intelligent monitoring. The problems and challenges faced in the current research were put forward. The analysis result showed that the attention mechanism and hybrid convolution were effective methods to improve the recognition accuracy of the model, and the lightweight modules were conducive to reducing the complexity and computation of the model. The factors that affected the current research to be practical were computational complexity, universality and accuracy. It is necessary to conduct specific analyses based on the dairy farming environment and the condition of dairy cows to continuously meet the needs of large-scale farming while applying this technology. © 2024 Editorial Department, Journal of South Agricultural University. All rights reserved.","Dairy cattle; Deep learning; Intelligent monitoring technology; Livestock breeding; Machine vision; Object identification","","","","","","Editorial  Department, Journal of South Agricultural University","1001411X","","","","Chinese","J. South China Agri.  Univ.","Review","Final","","Scopus","2-s2.0-85201237766"
"Pacheco V.M.; Brown-Brandl T.M.; de Sousa R.V.; Rohrer G.A.; Sharma S.R.; Martello L.S.","Pacheco, Verônica Madeira (57218882298); Brown-Brandl, Tami M. (57207606316); de Sousa, Rafael Vieira (57197806074); Rohrer, Gary A. (7102792700); Sharma, Sudhendu Raj (58030507200); Martello, Luciane Silva (6602090108)","57218882298; 57207606316; 57197806074; 7102792700; 58030507200; 6602090108","Deep learning-based sow posture classifier using colour and depth images","2024","Smart Agricultural Technology","9","","100563","","","","1","10.1016/j.atech.2024.100563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204010756&doi=10.1016%2fj.atech.2024.100563&partnerID=40&md5=7831da735da5c7c6772ab30d782902af","Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States; Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; United States Department of Agriculture (USDA)- Agricultural Research Service, US Meat Animal Research Center, Clay Center, 68933, NE, United States","Pacheco V.M., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States; Brown-Brandl T.M., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States; de Sousa R.V., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Rohrer G.A., United States Department of Agriculture (USDA)- Agricultural Research Service, US Meat Animal Research Center, Clay Center, 68933, NE, United States; Sharma S.R., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States; Martello L.S., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil","Assessing sow posture is essential for understanding their physiological condition and helping farmers improve herd productivity. Deep learning-based techniques have proven effective for image interpretation, offering a better alternative to traditional image processing methods. However, distinguishing transitional postures such as sitting and kneeling is challenging with only conventional top-view RGB images. This study aimed to develop and compare different deep learning-based sow posture classifiers using different architectures and image types. Using Kinect v.2 cameras, RGB and depth images were collected from 9 sows housed individually in farrowing crates. A total of 26,362 images were manually labelled by posture: “standing”, “kneeling”, “sitting”, “ventral recumbency” and “lateral recumbency”. Different deep learning algorithms were developed to detect sow postures from three types of images: colour (RGB), depth (depth image transformed into greyscale), and fused (colour-depth composite images). Results indicated that the ResNet-18 model presented the best results and that including depth information improved the performance of all models tested. Depth and fused models achieved higher accuracies than the models using only RGB images. The best model used only depth images as input and presented an accuracy of 98.3 %. The mean precision and recall values were 97.04 % and 97.32 %, respectively (F1-score = 97.2 %). The study shows improved posture classification using depth images. Future research can improve model accuracy and speed by expanding the database, exploring fused methods and computational models, considering different breeds of sows, and incorporating more postures. These models can be integrated into computer vision systems to automatically characterise sow behavior. © 2024","Computer vision; Multisource image; Precision livestock farming; Sow posture detection","","U.S. Department of Agriculture, USDA; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; National Pork Board, NPB, (19-119)","The authors thank the USMARC swine crew for caring for the animals and John Holman for helping with data collection. Mention of trade names or commercial products in this publication is solely for the purpose of providing specific information and does not imply recommendation or endorsement by the USDA. The USDA prohibits discrimination in all its programmes and activities on the basis of race, colour, national origin, age, disability, and where applicable, sex, marital status, familial status, parental status, religion, sexual orientation, genetic information, political beliefs, reprisal, or because all or part of an individual's income is derived from any public assistance programme (Not all prohibited bases apply to all programmes). USDA is an equal opportunity employer. This study was funded by National Pork Board (19-119) and part by the Coordena\u00E7\u00E3o de Aperfei\u00E7oamento de Pessoal de N\u00EDvel Superior \u2013 Brasil (CAPES) \u2013 Finance Code 001.","T.M. Brown-Brandl; Department of Biosystems Engineering, University of Nebraska-Lincoln, Lincoln, 1400 R Street, 68588, United States; email: tami.brownbrandl@unl.edu","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85204010756"
"Al-Lawati A.; Eshra E.; Mitra P.","Al-Lawati, Ali (58846719200); Eshra, Elsayed (57205196894); Mitra, Prasenjit (35582720000)","58846719200; 57205196894; 35582720000","WildGraph: Realistic Long-Horizon Trajectory Generation with Limited Sample Size","2024","32nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM SIGSPATIAL 2024","","","","247","258","11","0","10.1145/3678717.3691265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215116280&doi=10.1145%2f3678717.3691265&partnerID=40&md5=d9458d908ac4939eda13e8e24e2bf6e8","The Pennsylvania State University, University Park, PA, United States","Al-Lawati A., The Pennsylvania State University, University Park, PA, United States; Eshra E., The Pennsylvania State University, University Park, PA, United States; Mitra P., The Pennsylvania State University, University Park, PA, United States","Trajectory generation is an important task in movement studies. Generated trajectories augment the training corpus of deep learning applications, facilitate experimental and theoretical research, and mitigate the privacy concerns associated with real trajectories. This is especially significant in the wildlife domain, where trajectories are scarce due to the ethical and technical constraints of the collection process. In this paper, we consider the problem of generating long-horizon trajectories, akin to wildlife migration, based on a small set of real samples. We propose a hierarchical approach to learn the global movement characteristics of the real dataset, and recursively refine localized regions. Our solution, WildGraph discretizes the geographic path into a prototype network of H31 regions and leverages a novel recurrent VAE to probabilistically generate paths over the regions, based on occupancy. Experiments performed on two wildlife migration datasets demonstrate the remarkable capability of WildGraph to generate realistic months-long trajectories using a sample size as small as 60 while improving generalization compared to existing work. Moreover, WildGraph achieves superior or comparable performance on performance measures, including geographic imagery similarity. Our code is published on the following repository: https://github.com/aliwister/wildgraph. © 2024 Copyright held by the owner/author(s).","data mining; small data; trajectory generation; wildlife movement","Animals; Invertebrates; Spatio-temporal data; Experimental research; Generated trajectories; Geographics; Privacy concerns; Sample sizes; Small data; Theoretical research; Training corpus; Trajectory generation; Wildlife movement; Livestock","","","","Nascimento M.A.; Xiong L.; Zufle A.; Chiang Y.-Y.; Eldawy A.; Kroger P.","Association for Computing Machinery, Inc","","979-840071107-7","","","English","ACM SIGSPATIAL Int. Conf. Adv. Geogr. Inf. Syst., ACM SIGSPATIAL","Conference paper","Final","","Scopus","2-s2.0-85215116280"
"Ramachandran A.; Sendhil Kumar K.S.","Ramachandran, Anitha (57941261600); Sendhil Kumar, K.S. (59297924200)","57941261600; 59297924200","An efficient deep learning model for paddy growth stage classification using neural network pruning on UAV images","2024","Engineering Research Express","6","4","045252","","","","0","10.1088/2631-8695/ad9afe","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212228821&doi=10.1088%2f2631-8695%2fad9afe&partnerID=40&md5=a40fa22cfba5da9f4372a9156e0fe592","School of Computer Science and Engineering, Vellore Institute of Technology, Tamil Nadu, Vellore, India","Ramachandran A., School of Computer Science and Engineering, Vellore Institute of Technology, Tamil Nadu, Vellore, India; Sendhil Kumar K.S., School of Computer Science and Engineering, Vellore Institute of Technology, Tamil Nadu, Vellore, India","Crop phenology has a vital role in sustainable agriculture, facilitating farmers to make informed decisions throughout the crop-growing season. The traditional method of phenological detection relies on vegetation index calculations and time-series data, which can be extremely costly and difficult to obtain. In contrast, deep learning algorithms can estimate phenological stages directly from images, overcoming Vegetative Index (VI)-based limitations. Unmanned Aerial Vehicles (UAVs) offer high spatial and temporal resolution images at low cost, making them suitable for frequent field monitoring. This study focuses on the classification of rice seedling growth stages using deep learning techniques from images captured by UAVs. The proposed PaddyTrimNet model incorporates neural network pruning to classify paddy growth stages efficiently based on the BBCH (Biologische Bundesanstalt, Bundessortenamt und Chemische Industrie) scale. It focuses on the BBCH11, BBCH12, and BBCH13 using UAV images. PaddyTrimNet is an architecture based on ResNet50 modified specifically to classify rice development stages, incorporating separable convolutional layers to reduce parameters. The model is pruned using the Layer-wise Relevance Propagation method to enhance efficiency without compromising performance. It has demonstrated superior performance in paddy growth stage classification, achieving an accuracy of 96.97% while utilizing only 48.18 MFLOPS. It surpasses the existing pretrained deep learning classification models in terms of both accuracy and computational efficiency. This study contributes to precision agriculture and sustainable farming practices by leveraging deep learning and UAV imagery. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.","crop phenology; deep learning; neural network pruning; paddy; sustainable agriculture; UAVs","Livestock; Aerial vehicle; Crop phenology; Deep learning; Growth stages; Network pruning; Neural network pruning; Neural-networks; Paddy; Sustainable agriculture; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)","","","K.S. Sendhil Kumar; School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India; email: sendhilkumar.ks@vit.ac.in","","Institute of Physics","26318695","","","","English","Eng. Res. Exp.","Article","Final","","Scopus","2-s2.0-85212228821"
"Chen G.; Yuan Z.; Luo X.; Liang J.; Wang C.","Chen, Geng (58775423100); Yuan, Zhiyu (57221850414); Luo, Xinhui (57199846266); Liang, Jinxin (59323083900); Wang, Chunxin (57208033441)","58775423100; 57221850414; 57199846266; 59323083900; 57208033441","Research on Behavior Recognition and Online Monitoring System for Liaoning Cashmere Goats Based on Deep Learning","2024","Animals","14","22","3197","","","","0","10.3390/ani14223197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210234686&doi=10.3390%2fani14223197&partnerID=40&md5=d1593635013eeb03951498817eb22416","Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China; College of Animal Science and Technology, Jilin Agricultural University, Xincheng Street, Changchun, 130118, China","Chen G., Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China; Yuan Z., Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China; Luo X., Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China; Liang J., Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China, College of Animal Science and Technology, Jilin Agricultural University, Xincheng Street, Changchun, 130118, China; Wang C., Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China","Liaoning Cashmere Goats are a high-quality dual-purpose breed valued for both their cashmere and meat. They are also a key national genetic resource for the protection of livestock and poultry in China, with their intensive farming model currently taking shape. Leveraging new productivity advantages and reducing labor costs are urgent issues for intensive breeding. Recognizing goatbehavior in large-scale intelligent breeding not only improves health monitoring and saves labor, but also improves welfare standards by providing management insights. Traditional methods of goat behavior detection are inefficient and prone to cause stress in goats. Therefore, the development of a convenient and rapid detection method is crucial for the efficiency and quality improvement of the industry. This study introduces a deep learning-based behavior recognition and online detection system for Liaoning Cashmere Goats. We compared the convergence speed and detection accuracy of the two-stage algorithm Faster R-CNN and the one-stage algorithm YOLO in behavior recognition tasks. YOLOv8n demonstrated superior performance, converging within 50 epochs with an average accuracy of 95.31%, making it a baseline for further improvements. We improved YOLOv8n through dataset expansion, algorithm lightweighting, attention mechanism integration, and loss function optimization. Our improved model achieved the highest detection accuracy of 98.11% compared to other state-of-the-art (SOTA) target detection algorithms. The Liaoning Cashmere Goat Online Behavior Detection System demonstrated real-time detection capabilities, with a relatively low error rate compared to manual video review, and can effectively replace manual labor for online behavior detection. This study introduces detection algorithms and develops the Liaoning Cashmere Goat Online Behavior Detection System, offering an effective solution for intelligent goat management. © 2024 by the authors.","behavior recognition; deep learning; Liaoning Cashmere Goat; YOLOv8n","Article; behavior recognition; breeding; Cashmere goat; convolutional neural network; deep learning; deep neural network; detection algorithm; diagnostic imaging; drinking; eating; entropy; feeding; image analysis; image enhancement; intensive breeding; learning; liaoning cashmere goat; livestock; man machine interaction; meat; nonhuman; poultry; recognition; spatial attention; standing; total quality management","Institute of Animal Husbandry and Veterinary Medicine, Jilin Academy of Agricultural Sciences; Jilin Provincial Science and Technology Department Talent Special Project, (20240601054RC)","Funding text 1: We are deeply grateful for the generous financial support provided by Wang Chunxin from the Institute of Animal Husbandry and Veterinary Medicine, Jilin Academy of Agricultural Sciences. His support has been instrumental in enabling us to conduct this research and achieve our objectives. We would like to express our sincere thanks to Wang for his commitment to advancing scientific research and for his belief in our project\u2019s potential. ; Funding text 2: This research was funded by the Jilin Provincial Science and Technology Department Talent Special Project \u201CSheep Efficient Breeding Technology Innovation Team\u201D, grant number 20240601054RC.","C. Wang; Animal Husbandry and Veterinary Research Institute, Jilin Academy of Agricultural Sciences, Shengtai Street, Changchun, 130033, China; email: wcxjlsnky@163.com","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","","Scopus","2-s2.0-85210234686"
"Shi Y.; Li Q.; Wang G.; Wang M.","Shi, Yinghan (58087459100); Li, Qiqi (59422058900); Wang, Guorui (57828839500); Wang, Meili (55694491200)","58087459100; 59422058900; 57828839500; 55694491200","A Temporal Recognition Framework for Multi-sheep Behaviour Using ViTSORT and YOLOv8-MS","2025","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ","15043 LNCS","","","206","221","15","0","10.1007/978-981-97-8493-6_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210039829&doi=10.1007%2f978-981-97-8493-6_15&partnerID=40&md5=a249942157d06d6673aee646ba9cc340","College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Shi Y., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Li Q., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Wang G., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Wang M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Sheep behaviour can reflect their growth and health. However, the current sheep behaviour recognition studies have yet to focus on statistics on the duration of various behaviours for individual sheep. Traditional behavioural recognition methods can overlook abnormal sheep behaviour such as prolonged lying or not eating. Therefore, we propose an advanced framework for statistically analyzing the duration of sheep behaviours within a farm environment. This paper constructed a dataset of sheep behaviour images collected from a natural farm environment, including walking, standing, eating, lame, lying, licking, and attacking. Based on the Vision Transformer (ViT) method and the YOLOv8 model, a sheep tracking model, ViTSORT, and a sheep behaviour recognition model, YOLOv8-MS, are presented for the duration of sheep behaviours. The experimental results show that ViTSORT can solve the problem of tracking target loss when sheep cover each other. Meanwhile, the YOLOv8-MS model achieves a precision of 96.9% mAP on our sheep behaviour dataset, and the detection speed is 196 FPS, higher than previous methods. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","Classification; Deep learning; Detection; Sheep behaviour; Tracking","Invertebrates; 'current; Behavioral recognition; Behaviour recognition; Deep learning; Detection; Farm environments; Recognition methods; Sheep behaviour; Tracking; Tracking models; Livestock","Team Development of Scientists and Engineers in Shaanxi Province of China; National Key Research and Development Program of China, NKRDPC, (2022ZD04014); National Key Research and Development Program of China, NKRDPC; Scientist and Engineer"" team of Qin Chuang Yuan in Shaanxi Province of China, (2023KXJ-109, 2022KXJ-67)","This work was partially funded by the Construction of the National Key Research and Development Program of China (2022ZD04014), \""Scientist and Engineer\"" team of Qin Chuang Yuan in Shaanxi Province of China (2023KXJ-109), Qinchuangyuan Project for the Team Development of Scientists and Engineers in Shaanxi Province of China (2022KXJ-67).","M. Wang; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: wml@nwsuaf.edu.cn","Lin Z.; Zha H.; Cheng M.-M.; He R.; Liu C.-L.; Ubul K.; Silamu W.; Zhou J.","Springer Science and Business Media Deutschland GmbH","03029743","978-981978492-9","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85210039829"
"Xie T.; Huang Z.; Tan T.; Chen Y.","Xie, Tiantian (57218822503); Huang, Zetao (58002051600); Tan, Tao (58001829200); Chen, Yong (57213553753)","57218822503; 58002051600; 58001829200; 57213553753","Forecasting China's agricultural carbon emissions: A comparative study based on deep learning models","2024","Ecological Informatics","82","","102661","","","","4","10.1016/j.ecoinf.2024.102661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195377390&doi=10.1016%2fj.ecoinf.2024.102661&partnerID=40&md5=84d99590dd1fa02cd2c8ddfeab9d11b7","Institute of New Rural Development, South China Agricultural University, Guangzhou, 510642, China; Centre de Recherche Sur Les Liens Sociaux (CERLIS), Université Paris Cité, Paris, 75005, France; Institute of Biomass Engineering, South China Agricultural University, Guangzhou, 510642, China; School of Mechanical Engineering, Nanjing Tech University, Nanjing, 211816, China; Guangzhou Institute of Energy Conversion, Chinese Academy of Sciences, Guangzhou, 510640, China","Xie T., Institute of New Rural Development, South China Agricultural University, Guangzhou, 510642, China, Centre de Recherche Sur Les Liens Sociaux (CERLIS), Université Paris Cité, Paris, 75005, France; Huang Z., Institute of Biomass Engineering, South China Agricultural University, Guangzhou, 510642, China; Tan T., Institute of Biomass Engineering, South China Agricultural University, Guangzhou, 510642, China, School of Mechanical Engineering, Nanjing Tech University, Nanjing, 211816, China; Chen Y., Institute of Biomass Engineering, South China Agricultural University, Guangzhou, 510642, China, School of Mechanical Engineering, Nanjing Tech University, Nanjing, 211816, China, Guangzhou Institute of Energy Conversion, Chinese Academy of Sciences, Guangzhou, 510640, China","Given the critical urgency to combat the escalating climate crisis and the continuous rise in agricultural carbon emissions (ACE) in China, accurately forecasting their future trends is crucial. This research employs the emission factor method to assess ACE throughout mainland China from 1993 to 2021. To refine our forecasting approach, both statistical and neural network methodologies were utilized to pinpoint key factors influencing ACE. We crafted forecasting models incorporating both deep learning techniques and traditional methods. Notably, the Tree-structured Parzen Estimator Bayesian Optimization (TPEBO) algorithm was applied to optimize Long Short-Term Memory (LSTM) neural networks, culminating in the creation of a superior integrated TPEBO-LSTM model that demonstrated strong performance across various datasets. The forecasting outcomes suggest that ACE in 24 provinces are expected to reach their zenith before 2030, primarily driven by farm operations, as well as livestock and poultry manure management. The result provides a significant forecasting tool for assessing agricultural carbon emissions in different regions, offering insights crucial for targeted mitigation strategies. © 2023","Agricultural carbon emissions; Deep learning; Forecast; Long short-term memory neural network; Tree-structured Parzen estimator Bayesian optimization","China; agricultural emission; artificial neural network; Bayesian analysis; carbon emission; comparative study; forecasting method; optimization; performance assessment","National Key Research and Development Program of China, NKRDPC, (2023YFC3905802); National Key Research and Development Program of China, NKRDPC; China Scholarship Council, CSC, (201708070092); China Scholarship Council, CSC","The authors gratefully acknowledge the support of the National Key Research and Development Program of China (2023YFC3905802) and China Scholarship Council (201708070092).","T. Tan; Institute of Biomass Engineering, South China Agricultural University, Guangzhou, 510642, China; email: tantao@scau.edu.cn","","Elsevier B.V.","15749541","","","","English","Ecol. Informatics","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85195377390"
"Kou X.; Yang Y.; Xue H.; Wang L.; Li L.","Kou, Xupeng (57209418122); Yang, Yakun (57881066600); Xue, Hongcheng (57213519575); Wang, Longhe (57201406237); Li, Lin (57189531507)","57209418122; 57881066600; 57213519575; 57201406237; 57189531507","Revealing in vivo broiler chicken growth state: Integrating CT imaging and deep learning for non-invasive reproductive phenotypic measurement","2024","Computers and Electronics in Agriculture","227","","109477","","","","0","10.1016/j.compag.2024.109477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205145988&doi=10.1016%2fj.compag.2024.109477&partnerID=40&md5=e03e371a7c0ec90d8dc7421c4f6923ea","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Kou X., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Yang Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Xue H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Wang L., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Li L., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","The measurement of body dimensions in broiler chickens has long been of great interest to the poultry industry, especially regarding reproductive phenotypes closely linked to breeding traits and the growth status of roosters. Existing measurement methods suffer from subjectivity, destructiveness, and various limitations. Therefore, we propose a novel method for the in vivo measurement of broiler chicken reproductive phenotype based on a three-dimensional medical imaging segmentation network. Leveraging our proposed FDTR medical imaging segmentation algorithm, this method achieves robust segmentation performance on a broiler chicken dataset. However, due to the morphological diversity and extreme class imbalance of broiler chicken reproductive phenotypes, we introduce a meta-learning paradigm and design a specialized decoder path. Ultimately, we establish relationships between the predicted results obtained from the segmentation network and the reproductive phenotypes to achieve phenotypic measurement. To evaluate this method, we collected and constructed the YF2023 broiler chicken reproductive phenotype dataset. Extensive cross-validation experiments demonstrate that the proposed method effectively measures the reproductive phenotypes of live roosters using computed tomography (CT) medical imaging. The proposed segmentation model achieves a mean fold dice similarity coefficient (DSC) of 0.6353, a mean fold Hausdorff distance (HD) of 11.84 mm, and a mean fold average volume distance (AVD) of 1.72 mm3. In addition, we evaluated our model's generalization performance on unseen data, where the model achieved a DSC of 0.6825, an HD of 7.58 mm, and an AVD of 1.40 mm3. The predicted results exhibit a mean correlation coefficient of 66.18% with the weights of reproductive phenotype. This validates the enormous potential of the method in broiler chicken breeding and provides new insights and solutions for the accurate segmentation of broiler chicken reproductive phenotype. The code is available at https://github.com/Github-XKou/MetaFDTR. © 2024 Elsevier B.V.","Broiler chicken; Classical imbalance; Image segmentation; Non-invasive measurement; Reproductive phenotype","Computerized tomography; Image coding; Image segmentation; Macroinvertebrates; Medical imaging; Average volume; Broiler chickens; Classical imbalance; Hausdorff distance; Images segmentations; In-vivo; Measurements of; Non- invasive measurements; Reproductive phenotype; Similarity coefficients; machine learning; phenotype; poultry; reproductive behavior; segmentation; tomography; Livestock","National Key Research and Development Program of China, NKRDPC, (2021YFD1300100); National Key Research and Development Program of China, NKRDPC","The study is supported by the National Key R&D Program of China , Grant No. 2021YFD1300100 . ","L. Wang; College of Information and Electrical Engineering, Beijing, China Agricultural University, 100083, China; email: Phil.wang@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85205145988"
"Devi I.; Singh N.; Dudi K.; Ranjan R.; Lathwal S.S.; Tomar D.S.; Nagar H.","Devi, Indu (56728343200); Singh, Naseeb (57225962597); Dudi, Kuldeep (57191854426); Ranjan, Rakesh (57205706871); Lathwal, Surender Singh (6508046095); Tomar, Divyanshu Singh (58986088700); Nagar, Harsh (57835570900)","56728343200; 57225962597; 57191854426; 57205706871; 6508046095; 58986088700; 57835570900","Deep learning aided computer vision system for automated linear type trait evaluation in dairy cows","2024","Smart Agricultural Technology","8","","100509","","","","1","10.1016/j.atech.2024.100509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199108992&doi=10.1016%2fj.atech.2024.100509&partnerID=40&md5=306654c55e165f04358b258288417492","Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Division of System Research and Engineering, ICAR RC for NEH Region, Meghalaya, Umiam, 793103, India; Krishi Vigyan Kendra (under Haryana Agricultural University, Hisar), Haryana, Panipat, 132103, India; The Conservation Fund Freshwater Institute, Shepherdstown, 25443, WV, United States; Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721 302, India","Devi I., Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Singh N., Division of System Research and Engineering, ICAR RC for NEH Region, Meghalaya, Umiam, 793103, India; Dudi K., Krishi Vigyan Kendra (under Haryana Agricultural University, Hisar), Haryana, Panipat, 132103, India; Ranjan R., The Conservation Fund Freshwater Institute, Shepherdstown, 25443, WV, United States; Lathwal S.S., Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Tomar D.S., Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Nagar H., Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721 302, India","The assessment of traits is important in determining production potential, reproductive performance, and overall health of dairy cows. The assessment of these traits typically involves visual inspection and manual measurement, which can be time-consuming, subject to bias, and potentially distressing for the animals. To address these challenges, convolutional neural networks (CNNs)-aided non-invasive computer vision system was developed in the present study. This system consists of a depth camera to acquire the RGB images and depth information of cows. The DeepLabV3+ model, having the ResNet50 model as a backbone, was utilized to segment the body parts of cows from RGB images. Image processing-based algorithms were developed to extract key pixel locations for each trait from these segmented images. The system estimated trait dimensions utilizing 3D data of respective key points. The mean-IoU (intersection-over-union) values for the developed segmentation models were 93.46%, 91.25%, and 99.27% for side-view, back-view traits, and stature, respectively. Additionally, the vision system was able to estimate the trait dimensions with mean absolute percentage error (MAPE) below 6.0%. For a few traits, MAPE, however, exceeded 10.0%, indicating higher error. Inaccurate segmentation, imprecise key point extraction, visual overlaps of specific body parts, and variations in cow postures contribute to such errors. The developed system attained a Ratio of Performance to Deviation (RPD) above 1.2 for all traits, indicating its ability to estimate the dimensions of traits efficaciously. Thus, the present study demonstrated the potential of a CNN-based computer vision-based system for automating the trait measurement process in cows. © 2024","Automatic measurement; Computer vision; Dairy cattle; Deep learning; Precision livestock farming","","ICAR-National Dairy Research Institute, Karnal, NDRI; Indian Council of Agricultural Research, ICAR; Science and Engineering Research Board, SERB, (SRG/2020/001804); Science and Engineering Research Board, SERB","The authors express gratitude to the Science Engineering Research Board (SERB), New Delhi, India, for the financial grant (SRG/2020/001804) that supported this study. Additionally, we are thankful for the support and facilities extended by the Director of ICAR-NDRI and the workforce involved in data collection, which contributed towards the successful completion of this research. Our special thanks are also due to the Director of ICAR - Research Complex for North Eastern Hilly Region, Umiam, Meghalaya, India, for providing essential facilities for conducting the study.","N. Singh; Division of System Research and Engineering, ICAR RC for NEH Region, Umiam, Meghalaya, 793103, India; email: naseeb501@gmail.com","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85199108992"
"Shen P.; Wang F.; Luo W.; Zhao Y.; Li L.; Zhang G.; Zhu Y.","Shen, Peng (59348892900); Wang, Fulong (59047511300); Luo, Wei (57677752400); Zhao, Yongxiang (57888749800); Li, Lin (59058250800); Zhang, Guoqing (58073513000); Zhu, Yuchen (57204587499)","59348892900; 59047511300; 57677752400; 57888749800; 59058250800; 58073513000; 57204587499","Based on improved joint detection and tracking of UAV for multi-target detection of livestock","2024","Heliyon","10","19","e38316","","","","1","10.1016/j.heliyon.2024.e38316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205297594&doi=10.1016%2fj.heliyon.2024.e38316&partnerID=40&md5=1c8af177cf8595cb5e61fad96b73b272","North China Institute of Aerospace Engineering, Langfang, 065000, China; Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China; Fujian Provincial Key Laboratory of Water Cycling and Eco-Geological Processes, Xiamen, 361021, China; Institute of Hydrogeology and Environmental Geology, Chinese Academy of Geological Science, Shijiazhuang, 050061, China","Shen P., North China Institute of Aerospace Engineering, Langfang, 065000, China; Wang F., North China Institute of Aerospace Engineering, Langfang, 065000, China; Luo W., North China Institute of Aerospace Engineering, Langfang, 065000, China, Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China; Zhao Y., North China Institute of Aerospace Engineering, Langfang, 065000, China; Li L., North China Institute of Aerospace Engineering, Langfang, 065000, China; Zhang G., North China Institute of Aerospace Engineering, Langfang, 065000, China; Zhu Y., Fujian Provincial Key Laboratory of Water Cycling and Eco-Geological Processes, Xiamen, 361021, China, Institute of Hydrogeology and Environmental Geology, Chinese Academy of Geological Science, Shijiazhuang, 050061, China","In agriculture, specifically livestock monitoring, drones' ability to track multiple targets is essential for advancing the field. However, limited computing resources and unpredictable drone movements often cause issues like ambiguous video frames, object obstructions, and size deviations. These inconsistencies reduce tracking accuracy, making traditional algorithms inadequate for handling drone footage. This study introduces an enhanced deep learning-based multi-target drone tracker framework that enables real-time processing. The proposed method combines object detection and tracking by leveraging consecutive frame pairs to extract and share features, enhancing computational efficiency. It employs diverse loss functions to address class and sample distribution imbalances and includes a composite deblurring module to enhance detection accuracy. Object association utilizes a dual regress bounding box technique, aiding in object identification verification and predictive motion. Live tracking is achieved by predicting object locations in subsequent frames, enabling real-time tracking. Evaluation against leading benchmarks shows that the system improves precision and speed, achieving a 4.3 % increase in Multi-Object Tracking Accuracy (MOTA) and a 7.7 % boost in F1 score. © 2024 The Authors","Animal monitoring; Multi-object detection; UAV","","Study and Research in Chinese Universities, (2021ZYA08001)","This research was funded by the central government guides local funds for science and technology development (No. 236Z7201G); Study and Research in Chinese Universities (NO.2021ZYA08001). ","Y. Zhu; Fujian Provincial Key Laboratory of Water Cycling and Eco-Geological Processes, Xiamen, 361021, China; email: zhuyuchen413@163.com","","Elsevier Ltd","24058440","","","","English","Heliyon","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85205297594"
"Nie L.; Li B.; Du Y.; Jiao F.; Song X.; Liu Z.","Nie, Lili (58074459800); Li, Bugao (35201007400); Du, Yihan (58491276200); Jiao, Fan (58075250300); Song, Xinyue (58524790500); Liu, Zhenyu (55714953500)","58074459800; 35201007400; 58491276200; 58075250300; 58524790500; 55714953500","Deep learning strategies with CReToNeXt-YOLOv5 for advanced pig face emotion detection","2024","Scientific Reports","14","1","1679","","","","9","10.1038/s41598-024-51755-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182712213&doi=10.1038%2fs41598-024-51755-8&partnerID=40&md5=8df160417ad8326d8f1c1d0ace00202a","College of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; College of Animal Science, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; College of Agricultural Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China","Nie L., College of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Li B., College of Animal Science, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Du Y., College of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Jiao F., College of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Song X., College of Information Science and Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China; Liu Z., College of Agricultural Engineering, Shanxi Agricultural University, Shanxi, Taigu, 030801, China","This study underscores the paramount importance of facial expressions in pigs, serving as a sophisticated mode of communication to gauge their emotions, physical well-being, and intentions. Given the inherent challenges in deciphering such expressions due to pigs' rudimentary facial muscle structure, we introduced an avant-garde pig facial expression recognition model named CReToNeXt-YOLOv5. The proposed model encompasses several refinements tailored for heightened accuracy and adeptness in detection. Primarily, the transition from the CIOU to the EIOU loss function optimized the training dynamics, leading to precision-driven regression outcomes. Furthermore, the incorporation of the Coordinate Attention mechanism accentuated the model's sensitivity to intricate expression features. A significant innovation was the integration of the CReToNeXt module, fortifying the model's prowess in discerning nuanced expressions. Efficacy trials revealed that CReToNeXt-YOLOv5 clinched a mean average precision (mAP) of 89.4%, marking a substantial enhancement by 6.7% relative to the foundational YOLOv5. Crucially, this advancement holds profound implications for animal welfare monitoring and research, as our findings underscore the model's capacity to revolutionize the accuracy of pig facial expression recognition, paving the way for more humane and informed livestock management practices. © 2024, The Author(s).","","Animal Welfare; Animals; Communication; Deep Learning; Emotions; Facial Muscles; Swine; animal welfare; article; controlled study; deep learning; emotion; face muscle; facial expression; facial recognition; female; human; male; nonhuman; physical well-being; pig; animal; emotion; interpersonal communication; pig","Germplasm Innovation, and Demonstration of Jinfen; Intelligent Production Performance Measurement Technology of Jinfen, (202102140601005, 202102140604005-2, 2023-092); Shanxi Key R&D Program","This work was supported by the Shanxi Key R&D Program under the topic: Genetic Enhancement, Germplasm Innovation, and Demonstration of Jinfen White Pigs with a focus on the Research on Intelligent Production Performance Measurement Technology of Jinfen White Pigs [project number: 202102140601005, topic number: 202102140604005-2]; and the study on the multiphase reaction mechanism of bacterial biofilm attachment in livestock and poultry houses mediated by high-voltage pulsed electric fields [project number: 2023-092]. ","Z. Liu; College of Agricultural Engineering, Shanxi Agricultural University, Taigu, Shanxi, 030801, China; email: lzysyb@126.com","","Nature Research","20452322","","","38242984","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85182712213"
"Aryawan P.O.W.; Prabaswara I.D.G.W.; Husain A.; Akbar I.; Jannah N.; Supriyanto S.; Ulum M.F.","Aryawan, Putu Oki Wiradita (59218819300); Prabaswara, I. Dewa Gede Wicaksana (59218819200); Husain, Altaf (59325743100); Akbar, Ilham (58989538100); Jannah, Nor (59326115200); Supriyanto, Supriyanto (57190128799); Ulum, Mokhamad Fakhrul (54896746700)","59218819300; 59218819200; 59325743100; 58989538100; 59326115200; 57190128799; 54896746700","Real-time estrus detection in cattle using deep learning-based pose estimation","2024","BIO Web of Conferences","123","","04009","","","","0","10.1051/bioconf/202412304009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203810663&doi=10.1051%2fbioconf%2f202412304009&partnerID=40&md5=d253b5749488105c4ed32165420a6645","Department of Mechanical and Biosystem Engineering, Faculty of Agricultural Technology IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Division of Veterinary Reproduction and Obstetrics, School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia","Aryawan P.O.W., Department of Mechanical and Biosystem Engineering, Faculty of Agricultural Technology IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Prabaswara I.D.G.W., School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Husain A., Department of Mechanical and Biosystem Engineering, Faculty of Agricultural Technology IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Akbar I., School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Jannah N., School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Supriyanto S., Department of Mechanical and Biosystem Engineering, Faculty of Agricultural Technology IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia; Ulum M.F., Division of Veterinary Reproduction and Obstetrics, School of Veterinary Medicine and Biomedical Sciences, IPB University, Jl. Agatis Campus IPB Dramaga, West Java, Bogor, Indonesia","Accurate estrus detection is of paramount importance for optimizing the reproductive efficiency of livestock. Traditional methods are often labor-intensive and subjective. The cow estrus period, which only lasts 12-24 hours in a cycle that repeats every 18-24 days, causes the opportunity to mate or perform artificial insemination to be missed. This study proposes a novel approach that utilizes pose estimation with a deep learning model for real-time estrus detection in female cows. We collected a dataset of annotated images of cows at different estrus stages and developed a deep learning model based on the EfficientPose architecture. The cow estrus parameter analyzed was locomotion activity, which was categorized into lying down and standing classes with an integrated system and LCD-displayed detection results. The Jetson Nano and YOLOv5 algorithms processed the input parameter data with a mean average precision (mAP) of 0.8 and a final loss prediction value of 0.01. If the female cow is classified as active (number of lying down classes < 57,600 classes/h), then the cow is considered to be in the estrus period. This system provides reliable and noninvasive estrus detection, enabling timely intervention for improved reproductive management in cattle farming. © The Authors, published by EDP Sciences.","","","Balai Pengembangan Talenta Indonesia; Kementerian Pendidikan, Kebudayaan, Riset, dan Teknologi, MECRT, (2489/E2/KM.05.01/2022); Kementerian Pendidikan, Kebudayaan, Riset, dan Teknologi, MECRT","The authors would like to thank Balai Pengembangan Talenta Indonesia (BPTI), Ministry of Education, Culture, Research, and Technology, Indonesia, for funding this research (contract: 2489/E2/KM.05.01/2022). We thank the anonymous reviewers for their valuable comments and suggestions during the review process.","M.F. Ulum; Division of Veterinary Reproduction and Obstetrics, School of Veterinary Medicine and Biomedical Sciences, IPB University, Bogor, Jl. Agatis Campus IPB Dramaga, West Java, Indonesia; email: ulum@apps.ipb.ac.id","Jakaria null","EDP Sciences","22731709","","","","English","BIO. Web. Conf.","Conference paper","Final","","Scopus","2-s2.0-85203810663"
"de Paula T.M.C.G.; de Sousa R.V.; Sarmiento M.P.; Kramer T.; de Souza Sardinha E.J.; Sabei L.; Machado J.S.; Vilioti M.; Zanella A.J.","de Paula, Tauana Maria Carlos Guimarães (58981631500); de Sousa, Rafael Vieira (57197806074); Sarmiento, Marisol Parada (58795928400); Kramer, Ton (57212644121); de Souza Sardinha, Edson José (57338317200); Sabei, Leandro (57673546200); Machado, Júlia Silvestrini (58606762500); Vilioti, Mirela (58796554200); Zanella, Adroaldo José (7102861452)","58981631500; 57197806074; 58795928400; 57212644121; 57338317200; 57673546200; 58606762500; 58796554200; 7102861452","Deep learning pose detection model for sow locomotion","2024","Scientific Reports","14","1","16401","","","","0","10.1038/s41598-024-62151-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198618547&doi=10.1038%2fs41598-024-62151-7&partnerID=40&md5=6b7c4ce09daf036a7066d16547056cd9","Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; Robotics and Automation Group for Biosystems Engineering, Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), SP, Pirassununga, 13635-900, Brazil; Zinpro Corporation, SP, Piracicaba, Brazil","de Paula T.M.C.G., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; de Sousa R.V., Robotics and Automation Group for Biosystems Engineering, Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), SP, Pirassununga, 13635-900, Brazil; Sarmiento M.P., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; Kramer T., Zinpro Corporation, SP, Piracicaba, Brazil; de Souza Sardinha E.J., Robotics and Automation Group for Biosystems Engineering, Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), SP, Pirassununga, 13635-900, Brazil; Sabei L., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; Machado J.S., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; Vilioti M., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil; Zanella A.J., Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, SP, Pirassununga, 13635-900, Brazil","Lameness affects animal mobility, causing pain and discomfort. Lameness in early stages often goes undetected due to a lack of observation, precision, and reliability. Automated and non-invasive systems offer precision and detection ease and may improve animal welfare. This study was conducted to create a repository of images and videos of sows with different locomotion scores. Our goal is to develop a computer vision model for automatically identifying specific points on the sow's body. The automatic identification and ability to track specific body areas, will allow us to conduct kinematic studies with the aim of facilitating the detection of lameness using deep learning. The video database was collected on a pig farm with a scenario built to allow filming of sows in locomotion with different lameness scores. Two stereo cameras were used to record 2D videos images. Thirteen locomotion experts assessed the videos using the Locomotion Score System developed by Zinpro Corporation. From this annotated repository, computational models were trained and tested using the open-source deep learning-based animal pose tracking framework SLEAP (Social LEAP Estimates Animal Poses). The top-performing models were constructed using the LEAP architecture to accurately track 6 (lateral view) and 10 (dorsal view) skeleton keypoints. The architecture achieved average precisions values of 0.90 and 0.72, average distances of 6.83 and 11.37 in pixel, and similarities of 0.94 and 0.86 for the lateral and dorsal views, respectively. These computational models are proposed as a Precision Livestock Farming tool and method for identifying and estimating postures in pigs automatically and objectively. The 2D video image repository with different pig locomotion scores can be used as a tool for teaching and research. Based on our skeleton keypoint classification results, an automatic system could be developed. This could contribute to the objective assessment of locomotion scores in sows, improving their welfare. © The Author(s) 2024.","","Animals; Biomechanical Phenomena; Deep Learning; Female; Lameness, Animal; Locomotion; Swine; Swine Diseases; Video Recording; animal; animal lameness; biomechanics; deep learning; diagnosis; female; locomotion; pathophysiology; physiology; pig; procedures; swine disease; videorecording","Center for Comparative Studies in Health, Welfare and Sustainability; CECSBE; Robotics and Automation Group for Biosystems Engineering; RAEB; Zinpro; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (161577/2021-0)","Funding text 1: The authors acknowledge support from the Center for Comparative Studies in Health, Welfare and Sustainability (CECSBE)\u2014FMVZ/USP and Robotics and Automation Group for Biosystems Engineering (RAEB)\u2014FZEA/USP. Thirteen locomotion experts assessed the videos. Additionally, the authors acknowledge the pig farm, Topgen, for supplying the experimental animals and recording location. This study was financed in part by the National Council for Scientific and Technological Development (CNPq)\u2014161577/2021-0 and Zinpro Corporation. Figure 1 was partially made in the BioRender software.; Funding text 2: The authors acknowledge support from the Center for Comparative Studies in Health, Welfare and Sustainability (CECSBE)\u2014FMVZ/USP and Robotics and Automation Group for Biosystems Engineering (RAEB)\u2014FZEA/USP. Thirteen locomotion experts assessed the videos. Additionally, the authors acknowledge the pig farm, Topgen, for supplying the experimental animals and recording location. This study was financed in part by the National Council for Scientific and Technological Development (CNPq)\u2014161577/2021-0 and Zinpro Corporation. Figure was partially made in the BioRender software. ","T.M.C.G. de Paula; Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, Pirassununga, SP, 13635-900, Brazil; email: tauanamariapaula@gmail.com; A.J. Zanella; Department of Preventive Veterinary Medicine and Animal Health, School of Veterinary Medicine and Animal Science, Center for Comparative Studies in Sustainability, Health and Welfare, University of São Paulo, Pirassununga, SP, 13635-900, Brazil; email: adroaldo.zanella@usp.br","","Nature Research","20452322","","","39013897","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85198618547"
"Myint B.B.; Onizuka T.; Tin P.; Aikawa M.; Kobayashi I.; Zin T.T.","Myint, Bo Bo (59173041400); Onizuka, Tsubasa (58544593700); Tin, Pyke (24923729700); Aikawa, Masaru (36663478200); Kobayashi, Ikuo (24174822700); Zin, Thi Thi (6506258245)","59173041400; 58544593700; 24923729700; 36663478200; 24174822700; 6506258245","Development of a real-time cattle lameness detection system using a single side-view camera","2024","Scientific Reports","14","1","13734","","","","2","10.1038/s41598-024-64664-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196033707&doi=10.1038%2fs41598-024-64664-7&partnerID=40&md5=96b15e1b3ed5da9de082965387305136","Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Organization for Learning and Student Development, University of Miyazaki, Miyazaki, 889-2192, Japan; Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, 889-0121, Japan","Myint B.B., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Onizuka T., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Tin P., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Aikawa M., Organization for Learning and Student Development, University of Miyazaki, Miyazaki, 889-2192, Japan; Kobayashi I., Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, 889-0121, Japan; Zin T.T., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan","Recent advancements in machine learning and deep learning have revolutionized various computer vision applications, including object detection, tracking, and classification. This research investigates the application of deep learning for cattle lameness detection in dairy farming. Our study employs image processing techniques and deep learning methods for cattle detection, tracking, and lameness classification. We utilize two powerful object detection algorithms: Mask-RCNN from Detectron2 and the popular YOLOv8. Their performance is compared to identify the most effective approach for this application. Bounding boxes are drawn around detected cattle to assign unique local IDs, enabling individual tracking and isolation throughout the video sequence. Additionally, mask regions generated by the chosen detection algorithm provide valuable data for feature extraction, which is crucial for subsequent lameness classification. The extracted cattle mask region values serve as the basis for feature extraction, capturing relevant information indicative of lameness. These features, combined with the local IDs assigned during tracking, are used to compute a lameness score for each cattle. We explore the efficacy of various established machine learning algorithms, such as Support Vector Machines (SVM), AdaBoost and so on, in analyzing the extracted lameness features. Evaluation of the proposed system was conducted across three key domains: detection, tracking, and lameness classification. Notably, the detection module employing Detectron2 achieved an impressive accuracy of 98.98%. Similarly, the tracking module attained a high accuracy of 99.50%. In lameness classification, AdaBoost emerged as the most effective algorithm, yielding the highest overall average accuracy (77.9%). Other established machine learning algorithms, including Decision Trees (DT), Support Vector Machines (SVM), and Random Forests, also demonstrated promising performance (DT: 75.32%, SVM: 75.20%, Random Forest: 74.9%). The presented approach demonstrates the successful implementation for cattle lameness detection. The proposed system has the potential to revolutionize dairy farm management by enabling early lameness detection and facilitating effective monitoring of cattle health. Our findings contribute valuable insights into the application of advanced computer vision methods for livestock health management. © The Author(s) 2024.","","Algorithms; Animals; Cattle; Cattle Diseases; Deep Learning; Image Processing, Computer-Assisted; Lameness, Animal; Machine Learning; Support Vector Machine; Video Recording; algorithm; animal; animal lameness; bovine; cattle disease; deep learning; diagnosis; image processing; machine learning; procedures; support vector machine; videorecording","Ministry of Internal Affairs and Communications, MIC; Ministry of Agriculture, Forestry and Fisheries, MAFF; National Agriculture and Food Research Organization, NARO","This publication was subsidized by JKA through its promotion funds from KEIRIN RACE. This work was supported in part by \u201CThe Development and demonstration for the realization of problem-solving local 5G\u201D from the Ministry of Internal Affairs and Communications and the Project of \u201Cthe On-farm Demonstration Trials of Smart Agriculture\u201D from the Ministry of Agriculture, Forestry and Fisheries (funding agency: NARO). ","T.T. Zin; Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; email: thithi@cc.miyazaki-u.ac.jp","","Nature Research","20452322","","","38877097","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85196033707"
"Tian X.; Afrin M.; Mistry S.; Mahmud R.; Krishna A.; Li Y.","Tian, Xinyu (59238528100); Afrin, Mahbuba (56592961900); Mistry, Sajib (57027338000); Mahmud, Redowan (57200419245); Krishna, Aneesh (57209052897); Li, Yan (57294903300)","59238528100; 56592961900; 57027338000; 57200419245; 57209052897; 57294903300","MURE: Multi-layer real-time livestock management architecture with unmanned aerial vehicles using deep reinforcement learning","2024","Future Generation Computer Systems","161","","","454","466","12","2","10.1016/j.future.2024.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199879001&doi=10.1016%2fj.future.2024.07.038&partnerID=40&md5=537a59ca8802fb81861359beaa4353c8","Australian National University, Canberra, 2601, ACT, Australia; School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; School of Mathematics, Physics and Computing, University of Southern Queensland, 4350, QLD, Australia","Tian X., Australian National University, Canberra, 2601, ACT, Australia; Afrin M., School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; Mistry S., School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; Mahmud R., School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; Krishna A., School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; Li Y., School of Mathematics, Physics and Computing, University of Southern Queensland, 4350, QLD, Australia","In recent years, the combination of unmanned aerial vehicles (UAVs) and wireless sensor networks (WSNs) has gained popularity in livestock management (LM) due to energy constraints and network instability. Limited energy storage of sensor nodes (SNs) and the possibility of packet loss contribute to fast energy consumption and unstable networks, respectively. UAVs serve as relay nodes and data sinks, addressing these issues by temporarily storing data to reduce SN workload and establishing mobile nodes for network stability. We propose two innovations based on previous work: 1) We introduce a multi-layer wireless network architecture, categorizing UAVs into two layers based on their functions including data collection and data processing. This enhances task parallelization, bridging performance gaps among multiple UAVs; 2) We overcome the mobility limitation of SNs, considering their real-time movement in the network. Through deep reinforcement learning, UAVs learn to cooperatively locate moving SNs. This accounts for the inevitable mobility of livestock in the industry. Additionally, we simulate the environment and compare our approach to traditional methods, evaluating metrics such as collected data per timestep (DCPS), energy consumed per timestep (ECPS), and network stability (NS). Experimental results demonstrate that our method outperforms traditional approaches, achieving a data collecting gain of 4.84% and 8.20% compared to the methods without considering SN mobility or the multi-layer characteristics of WSNs, respectively. Under energy consumption limits, our method yields energy savings of 3.00% and 1.35% respectively. Furthermore, we extensively study and validate our method against other path planning algorithms, including genetic particle swarm optimization (GPSO), modified central force optimization (MCFO), and rapidly-exploring random trees (RRT). Our approach surpasses these methods in terms of data collecting efficiency and network stability. © 2024 The Author(s)","Deep reinforcement learning; Internet of Things; Precision livestock management; Unmanned aerial vehicle; Wireless sensor network","Agriculture; Antennas; Data acquisition; Data handling; Deep learning; Digital storage; Energy conservation; Energy utilization; Motion planning; Network architecture; Network layers; Particle swarm optimization (PSO); Reinforcement learning; Sensor nodes; Unmanned aerial vehicles (UAV); Vehicle to vehicle communications; Aerial vehicle; Deep reinforcement learning; Energy-consumption; Multi-layers; Network stability; Precision livestock management; Real- time; Reinforcement learnings; Time step; Unmanned aerial vehicle; Internet of things","Australian Research Council, ARC; NSW State Emergency Service; Western Australia Dementia Training Center; Andrew Corporation","Dr. Aneesh Krishna is currently an Associate Professor with the School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Australia. He holds a Ph.D. in computer science from the University of Wollongong, Australia. He was a lecturer in software engineering at the School of Computer Science and Software Engineering, University of Wollongong, Australia (from February 2006 - June 2009). His research interests include AI for software engineering, model-driven development/evolution, requirements engineering, agent systems, formal methods, data mining, computer vision, machine learning, bio-informative and renewable energy systems. He has published more than 130 articles in reputed journals and international conferences. His research is (or has been) funded by the Australian Research Council (ARC), and various Australian government agencies (like NSW State Emergency Service) as well as companies such as Woodside Energy, Amristar Solutions, Autism West Incorporated, BW Solar Australia, Western Australia Dementia Training Center and Andrew Corporation. He serves as an assessor (Ozreader) for the ARC. He has been on the organising committee, served as invited technical program committee member of many conferences and workshops in the areas related to his research. ","M. Afrin; School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, 6102, WA, Australia; email: mahbuba.afrin@curtin.edu.au","","Elsevier B.V.","0167739X","","FGCSE","","English","Future Gener Comput Syst","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85199879001"
"Chen T.; Lv L.; Wang D.; Zhang J.; Yang Y.; Zhao Z.; Wang C.; Guo X.; Chen H.; Wang Q.; Xu Y.; Zhang Q.; Du B.; Zhang L.; Tao D.","Chen, Tao (56178227200); Lv, Liang (57446997400); Wang, Di (56103027500); Zhang, Jing (56645651600); Yang, Yue (57486934900); Zhao, Zeyang (57223871753); Wang, Chen (57192598512); Guo, Xiaowei (58299654700); Chen, Hao (59308445800); Wang, Qingye (58299950200); Xu, Yufei (57274183600); Zhang, Qiming (57211029834); Du, Bo (57217375214); Zhang, Liangpei (8359720900); Tao, Dacheng (7102600334)","56178227200; 57446997400; 56103027500; 56645651600; 57486934900; 57223871753; 57192598512; 58299654700; 59308445800; 58299950200; 57274183600; 57211029834; 57217375214; 8359720900; 7102600334","Empowering Agrifood System with Artificial Intelligence: A Survey of the Progress, Challenges and Opportunities","2024","ACM Computing Surveys","57","2","49","","","","1","10.1145/3698589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210117388&doi=10.1145%2f3698589&partnerID=40&md5=d0535457bf21c1d6832d00adb3a4a28a","China University of Geosciences, Wuhan, 60006019, China; Wuhan University, Wuhan, 60029306, China; The University of Sydney, Sydney, 60025709, Australia; School of Computer Science, Wuhan University, Wuhan, 60029306, China; Nanyang Technological University, Singapore, 60005510, Singapore","Chen T., China University of Geosciences, Wuhan, 60006019, China; Lv L., China University of Geosciences, Wuhan, 60006019, China; Wang D., Wuhan University, Wuhan, 60029306, China; Zhang J., Wuhan University, Wuhan, 60029306, China, The University of Sydney, Sydney, 60025709, Australia; Yang Y., China University of Geosciences, Wuhan, 60006019, China; Zhao Z., China University of Geosciences, Wuhan, 60006019, China; Wang C., China University of Geosciences, Wuhan, 60006019, China; Guo X., China University of Geosciences, Wuhan, 60006019, China; Chen H., China University of Geosciences, Wuhan, 60006019, China; Wang Q., China University of Geosciences, Wuhan, 60006019, China; Xu Y., The University of Sydney, Sydney, 60025709, Australia; Zhang Q., The University of Sydney, Sydney, 60025709, Australia; Du B., School of Computer Science, Wuhan University, Wuhan, 60029306, China; Zhang L., Wuhan University, Wuhan, 60029306, China; Tao D., Nanyang Technological University, Singapore, 60005510, Singapore","With the world population rapidly increasing, transforming our agrifood systems to be more productive, efficient, safe, and sustainable is crucial to mitigate potential food shortages. Recently, artificial intelligence (AI) techniques such as deep learning (DL) have demonstrated their strong abilities in various areas, including language, vision, remote sensing (RS), and agrifood systems applications. However, the overall impact of AI on agrifood systems remains unclear. In this article, we thoroughly review how AI techniques can transform agrifood systems and contribute to the modern agrifood industry. First, we summarize the data acquisition methods in agrifood systems, including acquisition, storage, and processing techniques. Second, we present a progress review of AI methods in agrifood systems, specifically in agriculture, animal husbandry, and fishery, covering topics such as agrifood classification, growth monitoring, yield prediction, and quality assessment. Furthermore, we highlight potential challenges and promising research opportunities for transforming modern agrifood systems with AI. We hope this survey can offer an overall picture to newcomers in the field and serve as a starting point for their further research. The project website is https://github.com/Frenkie14/Agrifood-Survey.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Agrifood Systems; artificial intelligence; computer vision; machine learning; remote sensing","Adversarial machine learning; Deep learning; Livestock; Agri-food industry; Agri-food system; Artificial intelligence techniques; Machine-learning; Processing technique; Remote sensing system; Remote-sensing; Storage technique; System applications; World population","National Natural Science Foundation of China, NSFC, (62071439, 62371430, 62225113); National Natural Science Foundation of China, NSFC; Innovative Research Group Project of Hubei Province, (2024AFA017); National Key Research and Development Program of China, NKRDPC, (2022YFB3903405); National Key Research and Development Program of China, NKRDPC","This work was supported in part by the National Natural Science Foundation of China under Grants 62371430, 62071439, and 62225113, in part by the Innovative Research Group Project of Hubei Province under Grant 2024AFA017, and in part by the National Key Research and Development Program of China under Grant 2022YFB3903405.","J. Zhang; Wuhan University, Wuhan, 60029306, China; email: jingzhang.cv@gmail.com","","Association for Computing Machinery","03600300","","ACSUE","","English","ACM Comput Surv","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85210117388"
"Hou Z.; Zhang Q.; Zhang B.; Zhang H.; Huang L.; Wang M.","Hou, Zixia (57211969060); Zhang, Qi (58376001300); Zhang, Bin (59544283000); Zhang, Hongming (55685512300); Huang, Lyuwen (58149618000); Wang, Meili (55694491200)","57211969060; 58376001300; 59544283000; 55685512300; 58149618000; 55694491200","CattlePartNet: An identification approach for key region of body size and its application on body measurement of beef cattle","2025","Computers and Electronics in Agriculture","232","","110013","","","","0","10.1016/j.compag.2025.110013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217025916&doi=10.1016%2fj.compag.2025.110013&partnerID=40&md5=c783731644996743ac7a4d6855a01d35","College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi, Xianyang, 712100, China; Shaanxi Engineering Research Center for Intelligent Perception and Analysis of Agricultural Information, Yangling, Shaanxi, Xianyang, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, Shaanxi, Xianyang, 712100, China","Hou Z., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China; Zhang Q., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China; Zhang B., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China; Zhang H., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi, Xianyang, 712100, China, Shaanxi Engineering Research Center for Intelligent Perception and Analysis of Agricultural Information, Yangling, Shaanxi, Xianyang, 712100, China; Huang L., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi, Xianyang, 712100, China, Shaanxi Engineering Research Center for Intelligent Perception and Analysis of Agricultural Information, Yangling, Shaanxi, Xianyang, 712100, China; Wang M., College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, Xianyang, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi, Xianyang, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, Shaanxi, Xianyang, 712100, China","Addressing the challenges associated with manual measurement of body sizes for beef cattle, the potential for inducing stress responses in animals, and the inefficiencies inherent in such labor-intensive tasks, a novel three-dimensional (3D) point-based deep learning (DL) network named CattlePartNet is proposed. This innovative network is redesigned to segment point cloud data (PCD) of cattle into crucial body regions, thereby facilitating and optimizing the measurement process. The newly proposed CattlePartNet adopts the base network of PointNet++ as its backbone network, where the parallelized patch-aware attention module and depth-wise separable convolutions are freshly incorporated to lower the risk of overfitting. Additionally, it incorporates the Sophia optimizer, a novel and highly efficient optimization algorithm, instead of the Adam optimizer in PointNet++. Impressively, CattlePartNet outperforms the PointNet++ backbone network with a 2.4 % improvement in mean Intersection over Union (mIoU), as demonstrated on the ShapeNetPart dataset, which is widely regarded as an established benchmark for part segmentation tasks within the domain of PCD analysis. Leveraging the established network, CattlePartNet is trained on a dataset of cattle PCD for automated segmentation of key body regions, achieving an impressive mIoU of 91.7 %. The body sizes of cattle are categorized into two types: linear and curvilinear. For linear body sizes: body height (BH), body length (BL), and hip height (HH), measurement points are identified and extracted using sophisticated contour extraction techniques such as Alpha Shapes, mean curvature, and Gaussian curvature. Curvilinear body sizes: chest girth (CG) and abdominal circumference (AC) are measured through slice interception and cubic B-spline curve methods. The mean relative errors for the 5 body sizes—BH, BL, CG, AC, and HH—are reported as 4.96 %, 5.47 %, 6.04 %, 5.68 %, and 5.49 %, respectively. In comparison to traditional measurement of beef cattle, CattlePartNet precisely extracts key regions of body size from PCD, demonstrating robustness in generalizing segmentation across diverse cattle breeds and showing potential for segmenting PCD of other large livestock species. Furthermore, the measurement algorithm of 5 body sizes accurately localizes key measurement points within each region, providing essential support for breeding applications such as health assessment and production performance measurement. © 2025 Elsevier B.V.","Beef cattle; Deep learning; Non-contact measurement; Point cloud; Regional segmentation","Beef; Beef cattle; Body regions; Body sizes; Cattles; Deep learning; Measurements of; Noncontact measurements; Point cloud data; Point-clouds; Regional segmentation","","","L. Huang; College of Information Engineering, Northwest A&F University, Xianyang, Yangling, Shaanxi, 712100, China; email: huanglvwen@nwafu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85217025916"
"Ma W.; Wang X.; Yang S.X.; Song L.; Li Q.","Ma, Weihong (55524058400); Wang, Xingmeng (59004650300); Yang, Simon X. (59449721600); Song, Lepeng (36614638900); Li, Qifeng (57205964175)","55524058400; 59004650300; 59449721600; 36614638900; 57205964175","Leveraging Thermal Infrared Imaging for Pig Ear Detection Research: The TIRPigEar Dataset and Performances of Deep Learning Models","2025","Animals","15","1","41","","","","0","10.3390/ani15010041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214456286&doi=10.3390%2fani15010041&partnerID=40&md5=792a90b5140ef57c3f87a0c1e96bcf85","Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; School of Electronic and Electrical Engineering, Chongqing University of Science & Technology, Chongqing 401331, China; Advanced Robotics and Intelligent Systems Laboratory, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada","Ma W., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, School of Electronic and Electrical Engineering, Chongqing University of Science & Technology, Chongqing 401331, China; Wang X., School of Electronic and Electrical Engineering, Chongqing University of Science & Technology, Chongqing 401331, China; Yang S.X., Advanced Robotics and Intelligent Systems Laboratory, School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Song L., School of Electronic and Electrical Engineering, Chongqing University of Science & Technology, Chongqing 401331, China; Li Q., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China","The stable physiological structure and rich vascular network of pig ears contribute to distinct thermal characteristics, which can reflect temperature variations. While the temperature of the pig ear does not directly represent core body temperature due to the ear’s role in thermoregulation, thermal infrared imaging offers a feasible approach to analyzing individual pig status. Based on this background, a dataset comprising 23,189 thermal infrared images of pig ears (TIRPigEar) was established. The TIRPigEar dataset was obtained through a pig house inspection robot equipped with an infrared thermal imaging device, with post-processing conducted via manual annotation. By labeling pig ears within these images, a total of 69,567 labeled files were generated, which can be directly used for training pig ear detection models and enabling the analysis of pig temperature information by integrating the corresponding thermal imaging data. To validate the dataset’s utility, it was evaluated across various object detection algorithms. Experimental results show that the dataset achieves the highest precision, recall, and mAP50 on the YOLOv9m model, reaching 97.35%, 98.1%, and 98.6%, respectively. Overall, the TIRPigEar dataset demonstrates optimal performance when applied to the YOLOv9m algorithm. Utilizing thermal infrared imaging technology to detect pig ear information provides a non-contact, rapid, and effective method. Establishing the TIRPigEar dataset is highly significant, as it allows for a valuable resource for AI and precision livestock farming researchers to validate and improve their algorithms. This dataset will support many researchers in advancing precision livestock farming by enabling an efficient way for pig ear temperature analysis. © 2024 by the authors.","deep learning for object detection; pig state monitoring; precision livestock farming; thermal infrared imaging","algorithm; animal experiment; Article; behavior; body temperature; controlled study; deep learning; detection algorithm; diagnostic test accuracy study; humidity; livestock; nonhuman; pig; temperature; thermography; thermoregulation","","","Q. Li; Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; email: liqf@nercita.org.cn; L. Song; School of Electronic and Electrical Engineering, Chongqing University of Science & Technology, Chongqing 401331, China; email: slphq@cqust.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85214456286"
"Li Y.; Dai X.; Dai B.; Song P.; Wang X.; Chen X.; Li Y.; Shen W.","Li, Yanxing (57227798600); Dai, Xin (58913616200); Dai, Baisheng (57225754325); Song, Peng (57415196000); Wang, Xinjie (58112052900); Chen, Xinchao (59466351200); Li, Yang (57872816500); Shen, Weizheng (23390072800)","57227798600; 58913616200; 57225754325; 57415196000; 58112052900; 59466351200; 57872816500; 23390072800","Cow depth image restoration method based on RGB guided network with modulation branch in the cowshed environment","2025","Computers and Electronics in Agriculture","229","","109773","","","","0","10.1016/j.compag.2024.109773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211640793&doi=10.1016%2fj.compag.2024.109773&partnerID=40&md5=c39160a968df4b50c321fa94ed378fa6","College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China","Li Y., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Dai X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Dai B., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Song P., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Wang X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Chen X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Li Y., College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China; Shen W., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China","Depth images were widely applied in smart animal husbandry. The raw depth images collected by the RGB-D cameras generally existed amount of missing depth values due to the light reflected from white pattern of cows and direct sunlight in the cowshed. The incomplete cows in depth images would affect the application of depth images in health monitoring. This study proposed a cow depth image restoration method based on RGB guided network with a modulation branch. Firstly, removing the outliers caused by light from the depth image and determining the depth value missing area of the cow's body. Second, RGB and depth features were extracted through multiple convolutions and fused in the S-C (Self-attention and Convolution attention) fusion module of encoder. Then, the prediction head generated a coarsely predicted depth image after deconvolution combined with a modulation branch. Finally, the repaired depth image was generated in the SPN (Spatial Propagation Network) refinement module of the decoder. In terms of dataset construction, 7260 depth images were collected in a commercial dairy farm. To make up for lacking ground truth complete depth images corresponded to the raw depth images with missing value, two ways for generating missing depth images were designed. The experimental results shown that the method had improved restoration quality of cow's incomplete body in depth images. By comparing with other depth restoration works, the proposed method achieved significantly superior performance on RMSE = 36.32 and MAE = 12.77, and the percentage of predicted pixels within the error range at 1.25 reached 0.999. Additionally, a smoother transition between missing and restoration regions was demonstrated in the repaired depth images and point cloud results. And compared with the depth images with missing regions, the Precision, Recall rate and F1-score of the repaired depth images were improved for cow body condition scoring. This study could improve the effectiveness of the collected data and make the depth images more practical for smart animal husbandry. © 2024 Elsevier B.V.","Body condition score; Dairy cow; Deep learning; Depth image restoration; RGB guidance","Image denoising; Image enhancement; Image reconstruction; Animal husbandry; Body condition score; Dairy cow; Deep learning; Depth image; Depth image restoration; Depth value; Health monitoring; Restoration methods; RGB guidance; animal husbandry; dairy farming; detection method; image analysis; performance assessment; simulation; Livestock","Modern Agricultural Industry Technology Collaborative Innovation Promotion System Construction Project of Heilongjiang Province; Department of Agriculture and Rural Affairs of Heilongjiang Province, (1492); Key Research and Development Program of Heilongjiang, (2022ZX01A24); Key Research and Development Program of Heilongjiang; earmarked fund for CARS, (CARS-36); National Key Research and Development Program of China, NKRDPC, (2023YFD2000700); National Key Research and Development Program of China, NKRDPC; National Natural Science Foundation of China, NSFC, (32072788); National Natural Science Foundation of China, NSFC","This work was supported in part by the National Key Research and Development Program of China (Grant No. 2023YFD2000700 ), in part by the Key Research and Development Program of Heilongjiang Province (Grant No. 2022ZX01A24 ), in part by the National Natural Science Foundation of China (Grant No. 32072788 ), in part by the Modern Agricultural Industry Technology Collaborative Innovation Promotion System Construction Project of Heilongjiang Province (the Letter of Department of Agriculture and Rural Affairs of Heilongjiang Province (2021) No. 1492) , and in part by the earmarked fund for CARS (Grant No. CARS-36 ).","B. Dai; College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; email: bsdai@neau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85211640793"
"Polat H.E.; Koc D.G.; Ertugrul Ö.; Koc C.; Ekinci K.","Polat, Havva Eylem (55928558800); Koc, Dilara Gerdan (59534303900); Ertugrul, Ömer (54785883800); Koc, Caner (16642988900); Ekinci, Kamil (57188836659)","55928558800; 59534303900; 54785883800; 16642988900; 57188836659","Deep Learning based Individual Cattle Face Recognition using Data Augmentation and Transfer Learning","2025","Tarim Bilimleri Dergisi","31","1","","137","150","13","0","10.15832/ankutbd.1509798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216401595&doi=10.15832%2fankutbd.1509798&partnerID=40&md5=6acd73a52a62453e9804ef81f7a4a443","Ankara University, Faculty of Agriculture, Agricultural Structures and Irrigation Department, Diskapi, Ankara, 06110, Türkiye; Ankara University, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Ankara, Türkiye; Kırşehir Ahi Evran University, Faculty of Agriculture, Department of Biosystems Engineering, Kırşehir, Türkiye; Isparta University of Applied Sciences, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Isparta, Türkiye","Polat H.E., Ankara University, Faculty of Agriculture, Agricultural Structures and Irrigation Department, Diskapi, Ankara, 06110, Türkiye; Koc D.G., Ankara University, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Ankara, Türkiye; Ertugrul Ö., Kırşehir Ahi Evran University, Faculty of Agriculture, Department of Biosystems Engineering, Kırşehir, Türkiye; Koc C., Ankara University, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Ankara, Türkiye; Ekinci K., Isparta University of Applied Sciences, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Isparta, Türkiye","Accurate identification of cattle is essential for monitoring ownership, controlling production supply, preventing disease, and ensuring animal welfare. Despite the widespread use of ear tag-based techniques in livestock farm management, large-scale farms encounter challenges in identifying individual cattle. The process of identifying individual animals can be hindered by ear tags that fall off, and the ability to identify them over a long period of time becomes impossible when tags are missing. A dataset was generated by capturing images of cattle in their native environment to tackle this issue. The dataset was divided into three segments: training, validation, and testing. The dataset consisted of 15 000 records, each pertaining to a distinct bovine specimen from a total of 30 different cattle. To identify specific cattle faces in this study, deep learning algorithms such as InceptionResNetV2, MobileNetV2, DenseNet201, Xception, and NasNetLarge were utilized. The DenseNet201 algorithm attained a peak test accuracy of 99.53% and a validation accuracy of 99.83%. Additionally, this study introduces a novel approach that integrates advanced image processing techniques with deep learning, providing a robust framework that can potentially be applied to other domains of animal identification, thus enhancing overall farm management and biosecurity. © 2025 The Author(s).","Cattle identification; Deep learning; Face detection; Smart farming","","","","C. Koc; Ankara University, Faculty of Agriculture, Department of Agricultural Machinery and Technologies Engineering, Ankara, Türkiye; email: ckoc@ankara.edu.tr","","Ankara University","13007580","","","","English","Tarim Bilimleri Dergisi","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85216401595"
"Ramesh K.; Darwish M.; Zibli A.S.A.; Miller N.C.; Sajun A.R.; Zualkernan I.; Habib A.; Gardner A.","Ramesh, Keshav (59521307300); Darwish, Mahmoud (59521625000); Zibli, Ahmed Sharafath Ahamed (59521307400); Miller, Nikita Christ (59522420400); Sajun, Ali Reza (57222002808); Zualkernan, Imran (6602818774); Habib, Altaf (59522107000); Gardner, Andrew (59521625100)","59521307300; 59521625000; 59521307400; 59522420400; 57222002808; 6602818774; 59522107000; 59521625100","Exploring the Generalizability of Transfer Learning for Camera Trap Animal Image Classification","2025","Communications in Computer and Information Science","2134 CCIS","","","212","227","15","0","10.1007/978-3-031-74627-7_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215573611&doi=10.1007%2f978-3-031-74627-7_15&partnerID=40&md5=e5eccb7a684b8d5fdda3fb8a781858b1","American University of Sharjah, Sharjah, United Arab Emirates; Emirates Nature-WWF, Dubai, United Arab Emirates","Ramesh K., American University of Sharjah, Sharjah, United Arab Emirates; Darwish M., American University of Sharjah, Sharjah, United Arab Emirates; Zibli A.S.A., American University of Sharjah, Sharjah, United Arab Emirates; Miller N.C., American University of Sharjah, Sharjah, United Arab Emirates; Sajun A.R., American University of Sharjah, Sharjah, United Arab Emirates; Zualkernan I., American University of Sharjah, Sharjah, United Arab Emirates; Habib A., Emirates Nature-WWF, Dubai, United Arab Emirates; Gardner A., Emirates Nature-WWF, Dubai, United Arab Emirates","Animal extinction and biodiversity loss are critical issues impacting our planet today. This paper presents an interdisciplinary approach that merges the power of deep learning with the urgent necessity of wildlife conservation to aid ecologists in their fight against animal extinction. Utilizing a labeled and unlabeled dataset obtained from camera trap images from the Hajar Mountains of the United Arab Emirates, we trained and evaluated six different pre-trained deep learning models to assess how well they can be applied in real-world scenarios. Our analysis yielded promising results in accuracy and generalizability, with Adjusted Rand Index (ARI) scores comparing model predictions exceeding 0.6 for all models. Despite the substantial challenge presented by the imbalanced nature of our dataset, our models successfully classified a wide array of species, with our best model (DenseNet121) averaging a weighted accuracy of 72.78% with an F1 score of 0.957. The robustness of our models was further validated by a t-SNE analysis, which revealed coherent clustering of high-dimensional data. We developed a Graphical User Interface (GUI) application to bring our technology to non-technical users, allowing ecologists to classify images easily and leverage the power of AI in their conservation efforts. This study is a stride forward in leveraging artificial intelligence to aid ecological conservation, demonstrating the potential for machine learning to provide practical, effective solutions in real-world scenarios. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Generalizability; Image Classification; Imbalanced Data; Wildlife Conservation","Contrastive Learning; Federated learning; Invertebrates; Livestock; Macroinvertebrates; Animal images; Biodiversity loss; Critical issues; Generalizability; Images classification; Imbalanced data; Power; Real-world scenario; Transfer learning; Wildlife conservation; Adversarial machine learning","","","A.R. Sajun; American University of Sharjah, Sharjah, United Arab Emirates; email: b00068908@aus.edu","Meo R.; Silvestri F.","Springer Science and Business Media Deutschland GmbH","18650929","978-303174626-0","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85215573611"
"Vayssade J.-A.; Bonneau M.","Vayssade, Jehan-Antoine (57208753084); Bonneau, Mathieu (55759851900)","57208753084; 55759851900","Puzzle: taking livestock tracking to the next level","2024","Scientific Reports","14","1","18348","","","","0","10.1038/s41598-024-69058-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200656821&doi=10.1038%2fs41598-024-69058-3&partnerID=40&md5=0f35f279f0306e6a019d47c33a72fbb5","UR143 ASSET, INRAE, Guadeloupe, Petit-Bourg, 97170, France","Vayssade J.-A., UR143 ASSET, INRAE, Guadeloupe, Petit-Bourg, 97170, France; Bonneau M., UR143 ASSET, INRAE, Guadeloupe, Petit-Bourg, 97170, France","Animal behavior is a critical aspect for a better understanding and management of animal health and welfare. The combination of cameras with artificial intelligence holds significant potential, particularly as it eliminates the need to handle animals and allows for the simultaneous measurement of various traits, including activity, space utilization, and inter-individual distance. The primary challenge in using these techniques is dealing with the individualization of data, known as the multiple object tracking problem in computer science. In this article, we propose an original solution called “Puzzle.” Similar to solving a puzzle, where you start with the border pieces that are easy to position, our approach involves commencing with video sequences where tracking is straightforward. This initial phase aims to train a Convolutional Neural Network (CNN) capable of deriving the appearance clues of each animal. The CNN is then used on the entire video, together with distance-based metrics, in order to associate detections and animal id. We illustrated our method in the context of outdoor goat tracking, achieving a high percentage of good tracking, exceeding 90%. We discussed the impact of different criteria used for animal ID association, considering whether they are based solely on location, appearance, or a combination of both. Our findings indicate that, by adopting the puzzle paradigm and tailoring the appearance CNN to the specific video, relying solely on appearance can yield satisfactory results. Finally, we explored the influence of tracking efficacy on two behavioral studies, estimating space utilization and activity. The results demonstrated that the estimation error remained below 10%. The code is entirely open-source and extensively documented. Additionally, it is linked to a data-paper to facilitate the training of any automatic detection algorithm for goats, with the goal of fostering open access within the deep-learning livestock community. © The Author(s) 2024.","Computer vision; Goats; Monitoring; Monitoring; Tracking","Algorithms; Animals; Artificial Intelligence; Behavior, Animal; Goats; Livestock; Neural Networks, Computer; Video Recording; algorithm; animal; animal behavior; artificial intelligence; artificial neural network; goat; livestock; videorecording","Rémy Arquet; French Research National Research Institute for Agriculture, Food and Environment; Agence Nationale de la Recherche, ANR, (ANR-22-CE32-0005-01); Agence Nationale de la Recherche, ANR","Funding text 1: We would like to thank R\u00E9my Arquet and all the members of PTEA staff.; Funding text 2: The first author received financial support from the French Research National Research Institute for Agriculture, Food and Environment (INRAE-Animal Genetics division). This work was supported by the French National Research Agency, under the project HealtHavior ANR-22-CE32-0005-01. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. ","M. Bonneau; UR143 ASSET, INRAE, Petit-Bourg, Guadeloupe, 97170, France; email: mathieu.bonneau@inrae.fr","","Nature Research","20452322","","","39112541","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85200656821"
"Mao Y.; Turner R.D.R.; McMahon J.M.; Correa D.F.; Chamberlain D.A.; Warne M.S.J.","Mao, Yongjing (57209077335); Turner, Ryan D. R. (49964566700); McMahon, Joseph M. (57044163400); Correa, Diego F. (56511426100); Chamberlain, Debbie A. (57216582739); Warne, Michael St. J. (57153693100)","57209077335; 49964566700; 57044163400; 56511426100; 57216582739; 57153693100","Predicting Ground Cover with Deep Learning Models—An Application of Spatio-Temporal Prediction Methods to Satellite-Derived Ground Cover Maps in the Great Barrier Reef Catchments","2024","Remote Sensing","16","17","3193","","","","1","10.3390/rs16173193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203853385&doi=10.3390%2frs16173193&partnerID=40&md5=aaf4b05216cb0d4694a282f8165c511b","Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia; Water Research Laboratory, University of New South Wales, Sydney, 2093, NSW, Australia; Water Quality and Investigations, Department of Environment and Science, Brisbane, 4102, QLD, Australia; Centre for Agroecology, Water and Resilience, Coventry University, Coventry, CV8 3LG, United Kingdom","Mao Y., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia, Water Research Laboratory, University of New South Wales, Sydney, 2093, NSW, Australia; Turner R.D.R., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia, Water Quality and Investigations, Department of Environment and Science, Brisbane, 4102, QLD, Australia; McMahon J.M., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia; Correa D.F., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia; Chamberlain D.A., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia; Warne M.S.J., Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, QLD, Australia, Centre for Agroecology, Water and Resilience, Coventry University, Coventry, CV8 3LG, United Kingdom","Livestock grazing is a major land use in the Great Barrier Reef Catchment Area (GBRCA). Heightened grazing density coupled with inadequate land management leads to accelerated soil erosion and increased sediment loads being transported downstream. Ultimately, these increased sediment loads impact the water quality of the Great Barrier Reef (GBR) lagoon. Ground cover mapping has been adopted to monitor and assess the land condition in the GBRCA. However, accurate prediction of ground cover remains a vital knowledge gap to inform proactive approaches for improving land conditions. Herein, we explored two deep learning-based spatio-temporal prediction models, including convolutional LSTM (ConvLSTM) and Predictive Recurrent Neural Network (PredRNN), to predict future ground cover. The two models were evaluated on different spatial scales, ranging from a small site (i.e., <5 km2) to the entire GBRCA, with different quantities of training data. Following comparisons against 25% withheld testing data, we found the following: (1) both ConvLSTM and PredRNN accurately predicted the next-season ground cover for not only a single site but also the entire GBRCA. They achieved this with a Mean Absolute Error (MAE) under 5% and a Structural Similarity Index Measure (SSIM) exceeding 0.65; (2) PredRNN superseded ConvLSTM by providing more accurate next-season predictions with better training efficiency; (3) The accuracy of PredRNN varies seasonally and spatially, with lower accuracy observed for low ground cover, which is underestimated. The models assessed in this study can serve as an early-alert tool to produce high-accuracy and high-resolution ground cover prediction one season earlier than observation for the entire GBRCA, which enables local authorities and grazing property owners to take preventive measures to improve land conditions. This study also offers a new perspective on the future utilization of predictive spatio-temporal models, particularly over large spatial scales and across varying environmental sites. © 2024 by the authors.","Great Barrier Reef Catchments; spatio-temporal prediction; time series analysis","Catchments; Prediction models; Recurrent neural networks; Catchment area; Great Barrier Reef; Great barrier reef catchment; Ground covers; Land condition; Neural-networks; Reef catchments; Sediment loads; Spatio-temporal prediction; Time-series analysis; Reefs","University of Queensland, UQ; Department of Environment and Science, Queensland Government, DES","This research was supported by the Reef Catchment Science Partnership jointly funded by the University of Queensland (UQ) and the Department of Environment and Science (DES), Queensland Government.","Y. Mao; Reef Catchments Science Partnership, School of the Environment, University of Queensland, Brisbane, 4108, Australia; email: yongjing.mao@unsw.edu.au","","Multidisciplinary Digital Publishing Institute (MDPI)","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85203853385"
"Huang X.; Huang F.; Hu J.; Zheng H.; Liu M.; Dou Z.; Jiang Q.","Huang, Xiaoping (57208149375); Huang, Fei (59294213300); Hu, Jiahui (57216826237); Zheng, Huanyu (59293570700); Liu, Mengyi (59293728800); Dou, Zihao (59293728900); Jiang, Qing (57200096209)","57208149375; 59294213300; 57216826237; 59293570700; 59293728800; 59293728900; 57200096209","Automatic Face Detection of Farm Images Based on an Enhanced Lightweight Deep Learning Model","2024","International Journal of Pattern Recognition and Artificial Intelligence","38","12","2456009","","","","0","10.1142/S0218001424560093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201783506&doi=10.1142%2fS0218001424560093&partnerID=40&md5=0ab2879434e909fa279aeabddf579bf5","School of Internet, Anhui University, Anhui, Hefei, 230039, China; Institute of Plasma Physics, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, 230039, China; Faculty of Electronic and Information Engineering, West Anhui University, Lu'an, 237012, China; Anhui Undergrowth Crop Intelligent Equipment Engineering Research Center, West Anhui University, Anhui, Lu'an, 237012, China","Huang X., School of Internet, Anhui University, Anhui, Hefei, 230039, China; Huang F., School of Internet, Anhui University, Anhui, Hefei, 230039, China; Hu J., Institute of Plasma Physics, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, 230039, China; Zheng H., School of Internet, Anhui University, Anhui, Hefei, 230039, China; Liu M., School of Internet, Anhui University, Anhui, Hefei, 230039, China; Dou Z., School of Internet, Anhui University, Anhui, Hefei, 230039, China; Jiang Q., Faculty of Electronic and Information Engineering, West Anhui University, Lu'an, 237012, China, Anhui Undergrowth Crop Intelligent Equipment Engineering Research Center, West Anhui University, Anhui, Lu'an, 237012, China","In the realm of precise management, artificial intelligence has garnered significant attention and adoption, particularly within the domain of smart agriculture. In modern animal husbandry, animal face detection is conducive to individual identification, expression detection and behavior analysis of animals, and this technological advancement holds immense importance in fostering the advancement of intelligent farming practices. In order to solve the challenge of face detection caused by similar appearance features (color, texture, etc.) and no obvious feature differences between the solid-color goats and sheep in the natural environment, this research introduces a novel approach for face detection by combining the capabilities of YOLOv5 and a convolutional block attention module (CBAM). First, datasets of goats and sheep with different angles, scales and densities were constructed. Second, the basic framework of YOLOv5 was used for object detection. To address the obstacle posed by the limited presence of distinguishing features on the faces of goats and sheep, this study aims to overcome the challenge of extracting informative facial characteristics. The CBAM block was introduced to construct the YOLOv5-CBAM model to improve the feature extraction ability. Finally, 2412 images were selected and divided into training set and verification set according to 8:1. The experimental results of this dataset show that the proposed YOLOv5-CBAM model yielded remarkable results with a precision rate of 0.970, a recall rate of 0.890, a mAP@0.5 score of 0.935, an frames per second (FPS) of 140.845, and a model size of 14.680 MB. In comparison to other approaches such as Faster R-CNN, SSD, YOLOv3, and YOLOv5, the proposed model demonstrated superior performance in some aspects. In addition, it excelled in both lightweight design and overall effectiveness, and it is well-suited for real-time detection of animal faces in real-world farming settings, ensuring efficient identification and monitoring of animals within practical agricultural environments.  © 2024 World Scientific Publishing Company.","attention mechanism; deep learning; Face detection; livestock farming; YOLOv5","Deep learning; Animal husbandry; Attention mechanisms; Automatic face detection; Deep learning; Faces detection; Image-based; Learning models; Livestock farming; Smart agricultures; YOLOv5","","","J. Hu; Institute of Plasma Physics, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, 230039, China; email: jhhu@ipp.ac.cn","","World Scientific","02180014","","IJPIE","","English","Int J Pattern Recognit Artif Intell","Article","Final","","Scopus","2-s2.0-85201783506"
"Amirivojdan A.; Nasiri A.; Zhou S.; Zhao Y.; Gan H.","Amirivojdan, Ahmad (58530813700); Nasiri, Amin (57208238617); Zhou, Shengyu (58864114000); Zhao, Yang (35101248700); Gan, Hao (57201637937)","58530813700; 57208238617; 58864114000; 35101248700; 57201637937","ChickenSense: A Low-Cost Deep Learning-Based Solution for Poultry Feed Consumption Monitoring Using Sound Technology","2024","AgriEngineering","6","3","","2115","2129","14","3","10.3390/agriengineering6030124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202600925&doi=10.3390%2fagriengineering6030124&partnerID=40&md5=4a90fd70fefb3cf6e5a690dd01e7baae","Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Department of Animal Science, University of Tennessee, Knoxville, 37996, TN, United States","Amirivojdan A., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Nasiri A., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Zhou S., Department of Animal Science, University of Tennessee, Knoxville, 37996, TN, United States; Zhao Y., Department of Animal Science, University of Tennessee, Knoxville, 37996, TN, United States; Gan H., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States","This research proposes a low-cost system consisting of a hardware setup and a deep learning-based model to estimate broiler chickens’ feed intake, utilizing audio signals captured by piezoelectric sensors. The signals were recorded 24/7 for 19 consecutive days. A subset of the raw data was chosen, and events were labeled in two classes, feed-pecking and non-pecking (including singing, anomaly, and silence samples). Next, the labeled data were preprocessed through a noise removal algorithm and a band-pass filter. Then, the spectrogram and the signal envelope were extracted from each signal and fed as inputs to a VGG-16-based convolutional neural network (CNN) with two branches for 1D and 2D feature extraction followed by a binary classification head to classify feed-pecking and non-pecking events. The model achieved 92% accuracy in feed-pecking vs. non-pecking events classification with an f1-score of 91%. Finally, the entire raw dataset was processed utilizing the developed model, and the resulting feed intake estimation was compared with the ground truth data from scale measures. The estimated feed consumption showed an 8 ± 7% mean percent error on daily feed intake estimation with a 71% R2 score and 85% Pearson product moment correlation coefficient (PPMCC) on hourly intake estimation. The results demonstrate that the proposed system estimates broiler feed intake at each feeder and has the potential to be implemented in commercial farms. © 2024 by the authors.","audio classification; broiler; feed consumption estimation; feed intake; feeding behavior; pecking detection; precision livestock farming","","","","H. Gan; Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, United States; email: hgan1@utk.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","26247402","","","","English","AgriEng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85202600925"
"Matlala B.; van der Haar D.; Vandapalli H.","Matlala, Boitumelo (54885265500); van der Haar, Dustin (55930315300); Vandapalli, Hima (59469782400)","54885265500; 55930315300; 59469782400","A Novel Approach To Lion Re-Identification Using Vision Transformers","2025","Communications in Computer and Information Science","2326 CCIS","","","270","281","11","0","10.1007/978-3-031-78255-8_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211787248&doi=10.1007%2f978-3-031-78255-8_16&partnerID=40&md5=b7efcefa9402596e4e0ec4005e28ee12","University of Johannesburg, Johannesburg, South Africa","Matlala B., University of Johannesburg, Johannesburg, South Africa; van der Haar D., University of Johannesburg, Johannesburg, South Africa; Vandapalli H., University of Johannesburg, Johannesburg, South Africa","In recent years, technology has played a crucial role in wildlife and ecosystem conservation, significantly decreasing the time and effort required for wildlife monitoring, population estimation, poaching prevention, habitat mapping and, specifically, animal re-identification. This study proposes a novel approach that can be used in wildlife research and conservation to track individual animals over time by employing deep learning techniques. The challenges associated with animal re-identification in diverse natural environments such as variability in appearance, species diversity, accuracy and reliability can be addressed by leveraging the capabilities of advanced deep learning models, namely Vision Transformers and Convolutional Neural Networks. When trained on a Lion wildlife dataset, the Vision Transformer demonstrated significantly better performance compared to the Convolutional Neural Network in terms of accuracy, precision, recall and training time. This research contributes towards ongoing ecological initiatives to improve population monitoring, anti-poaching efforts, and habitat protection. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Animal Re-identification; Tracking; Vision Transformer","Distribution transformers; Livestock; Animal re-identification; Convolutional neural network; Habitat mapping; Learning techniques; Natural environments; Population estimations; Re identifications; Tracking; Vision transformer; Wildlife monitoring; Invertebrates","","","H. Vandapalli; University of Johannesburg, Johannesburg, South Africa; email: himav@uj.ac.za","Gerber A.; Maritz J.; Pillay A.W.","Springer Science and Business Media Deutschland GmbH","18650929","978-303178254-1","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85211787248"
"Wee J.; Chen J.; Wei G.-W.","Wee, JunJie (57221323056); Chen, Jiahui (57190345974); Wei, Guo-Wei (57210276955)","57221323056; 57190345974; 57210276955","Preventing future zoonosis: SARS-CoV-2 mutations enhance human–animal cross-transmission","2024","Computers in Biology and Medicine","182","","109101","","","","0","10.1016/j.compbiomed.2024.109101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203185107&doi=10.1016%2fj.compbiomed.2024.109101&partnerID=40&md5=a5bf8183d0752c31d7255840d7bcb656","Department of Mathematics, Michigan State University, East Lansing, 48824, MI, United States; Department of Mathematical Sciences, University of Arkansas, Fayetteville, 72701, AR, United States; Department of Biochemistry and Molecular Biology, Michigan State University, East Lansing, 48824, MI, United States; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States","Wee J., Department of Mathematics, Michigan State University, East Lansing, 48824, MI, United States; Chen J., Department of Mathematical Sciences, University of Arkansas, Fayetteville, 72701, AR, United States; Wei G.-W., Department of Mathematics, Michigan State University, East Lansing, 48824, MI, United States, Department of Biochemistry and Molecular Biology, Michigan State University, East Lansing, 48824, MI, United States, Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States","The COVID-19 pandemic has driven substantial evolution of the SARS-CoV-2 virus, yielding subvariants that exhibit enhanced infectiousness in humans. However, this adaptive advantage may not universally extend to zoonotic transmission. In this work, we hypothesize that viral adaptations favoring animal hosts do not necessarily correlate with increased human infectivity. In addition, we consider the potential for gain-of-function mutations that could facilitate the virus's rapid evolution in humans following adaptation in animal hosts. Specifically, we identify the SARS-CoV-2 receptor-binding domain (RBD) mutations that enhance human–animal cross-transmission. To this end, we construct a multitask deep learning model, MT-TopLap trained on multiple deep mutational scanning datasets, to accurately predict the binding free energy changes upon mutation for the RBD to ACE2 of various species, including humans, cats, bats, deer, and hamsters. By analyzing these changes, we identified key RBD mutations such as Q498H in SARS-CoV-2 and R493K in the BA.2 variant that are likely to increase the potential for human–animal cross-transmission. © 2024","BA.2; COVID-19; Deep mutational scanning; SARS-coV-2; Topological deep learning; Zoonosis","Angiotensin-Converting Enzyme 2; Animals; Cats; Chiroptera; COVID-19; Cricetinae; Deer; Humans; Mutation; SARS-CoV-2; Spike Glycoprotein, Coronavirus; Zoonoses; Binding energy; Invertebrates; Livestock; SARS; angiotensin converting enzyme 2; coronavirus spike glycoprotein; SARS-CoV-2 antibody; ACE2 protein, human; angiotensin converting enzyme 2; coronavirus spike glycoprotein; spike protein, SARS-CoV-2; Ba.2; Binding free energy; Deep mutational scanning; Gain-of function; Human following; Learning models; Receptor-binding domains; SARS-cov-2; Topological deep learning; Zoonosis; Article; bat; cat; coronavirus disease 2019; deep learning; deer; feed forward neural network; gene mutation; hamster; human; human-animal interaction; nonhuman; revertant; Severe acute respiratory syndrome coronavirus 2; virus transmission; zoonosis; zoonotic transmission; animal; genetics; metabolism; mutation; prevention and control; virology; zoonosis; COVID-19","Pfizer; Michigan State University Research Foundation, MSUF; National Science Foundation, NSF, (IIS-1900473, DMS-2052983, DMS-1761320); National Science Foundation, NSF; National Institutes of Health, NIH, (R01AI146210, R01AI164266, R01GM126189); National Institutes of Health, NIH; Bristol-Myers Squibb, BMS, (65109); Bristol-Myers Squibb, BMS; National Aeronautics and Space Administration, NASA, (80NSSC21M0023); National Aeronautics and Space Administration, NASA","Funding text 1: This work was supported in part by NIH grants R01GM126189, R01AI164266, and R01AI146210, National Science Foundation grants DMS-2052983, DMS-1761320, and IIS-1900473, NASA grant 80NSSC21M0023, MSU Foundation, Bristol-Myers Squibb, United States 65109, and Pfizer.; Funding text 2: This work was supported in part by NIH grants R01GM126189 , R01AI164266 , and R01AI146210 , NSF grants DMS-2052983 , DMS-1761320 , and IIS-1900473 , NASA grant 80NSSC21M0023 , MSU Foundation , Bristol-Myers Squibb 65109 , and Pfizer . ","G.-W. Wei; Department of Mathematics, Michigan State University, East Lansing, 48824, United States; email: weig@msu.edu","","Elsevier Ltd","00104825","","CBMDA","39243518","English","Comput. Biol. Med.","Article","Final","","Scopus","2-s2.0-85203185107"
"Song X.; Zhang W.; Pan W.; Liu P.; Wang C.","Song, Xubin (57901176000); Zhang, Wanteng (58081591900); Pan, Weiting (58087668400); Liu, Ping (57191833487); Wang, Chunying (57203018569)","57901176000; 58081591900; 58087668400; 57191833487; 57203018569","Real-time monitor heading dates of wheat accessions for breeding in-field based on DDEW-YOLOv7 model and BotSort algorithm","2025","Expert Systems with Applications","267","","126140","","","","0","10.1016/j.eswa.2024.126140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212540196&doi=10.1016%2fj.eswa.2024.126140&partnerID=40&md5=9cf86c43157da77f6b433d1388528acd","State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China","Song X., State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China; Zhang W., State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China; Pan W., State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China; Liu P., State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China; Wang C., State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China","Accurately and swiftly measuring plant traits in large populations has become a curial bottleneck in connecting genotype to phenotype in breeding. Timely monitoring of wheat heading in-field is essential to estimating growth status and yield in wheat breeding. Therefore, this study aimed to develop a deep learning method to directly monitor wheat heading in-situ using a phenotype identification robot. We proposed a real-time monitoring method to estimate heading dates of wheat accessions for breeding in-field based on DDEW-YOLOv7 model and BotSort algorithm. This method first accomplished real-time detection and counting of wheat spikes using the constructed DDEW-YOLOv7 model and BotSort tracking algorithm. Subsequently, spike growth curves and increment graphs were generated using the acquired temporal spike data to determine the heading date for wheat F1 hybrid varieties and estimate spike density per unit area. The result showed that the proposed method achieved a root mean square error (RMSE) of 0.3 days in heading date determination across 11 wheat plots with different varieties and planting densities. Additionally, by employing the proposed method, it was observed that low temperatures caused a delay in wheat heading which validated the feasibility of dynamically real-time monitoring wheat heading. Furthermore, the RMSE between the estimated spike density per unit area, based on the constructed spike growth curve, the ground truth value is approximately 6.10. Therefore, the proposed method enables real-time monitoring of heading dates and estimating unit spike numbers for different wheat varieties that could serve as a high-throughput screening technique to accelerate wheat breeding. © 2024 Elsevier Ltd","BotSort; Spike detection; Wheat heading date; Wheat spike per unit area; YOLOv7","Livestock; Botsort; Heading date; In-field; Per unit; Real time monitoring; Spike detection; Wheat breeding; Wheat heading date; Wheat spike per unit area; YOLOv7; Plant diseases","Key Technology Research and Development Program of Shandong Province, (2022LZGCQY002, 2023TZXD004, 2021LZGC013); Key Technology Research and Development Program of Shandong Province","This research was supported partially by Shandong Provincial Key Research and Development Program (Nos. 2022LZGCQY002, 2023TZXD004 and 2021LZGC013). The authors are grateful to all study participants.","P. Liu; State Key Laboratory of Wheat Improvement, College of Mechanical and Electronic Engineering, Shandong Agricultural University, Tai'an, 271018, China; email: liuping@sdau.edu.cn","","Elsevier Ltd","09574174","","ESAPE","","English","Expert Sys Appl","Article","Final","","Scopus","2-s2.0-85212540196"
"Kawamura K.; Kato Y.; Yasuda T.; Aozasa E.; Yayota M.; Kitagawa M.; Kunishige K.","Kawamura, Kensuke (8599559800); Kato, Yura (57742809400); Yasuda, Taisuke (8732081000); Aozasa, Eriko (59328407400); Yayota, Masato (16308085700); Kitagawa, Miya (55807382900); Kunishige, Kyoko (59327900100)","8599559800; 57742809400; 8732081000; 59328407400; 16308085700; 55807382900; 59327900100","Cattle dung detection in pastures from drone images using YOLOv5","2024","Grassland Science","70","4","","168","174","6","1","10.1111/grs.12429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203981529&doi=10.1111%2fgrs.12429&partnerID=40&md5=a7d4d6037d3cc87d20c3a04cb89b128b","Obihiro University of Agriculture and Veterinary Medicine, Hokkaido, Japan; Mount Fuji Research Institute, Yamanashi Prefectural Government, Yamanashi, Japan; Faculty of Applied Biological Sciences, Gifu University, Gifu, Japan; Institute of Livestock and Grassland Science, NARO, Nagano, Japan; HRO Agricultural Research Department Animal Research Center, Hokkaido, Japan","Kawamura K., Obihiro University of Agriculture and Veterinary Medicine, Hokkaido, Japan; Kato Y., Obihiro University of Agriculture and Veterinary Medicine, Hokkaido, Japan; Yasuda T., Mount Fuji Research Institute, Yamanashi Prefectural Government, Yamanashi, Japan; Aozasa E., Faculty of Applied Biological Sciences, Gifu University, Gifu, Japan; Yayota M., Faculty of Applied Biological Sciences, Gifu University, Gifu, Japan; Kitagawa M., Institute of Livestock and Grassland Science, NARO, Nagano, Japan; Kunishige K., HRO Agricultural Research Department Animal Research Center, Hokkaido, Japan","Livestock excretions are crucial for nutrient cycling in pasture ecosystems. However, conventional methods based on field observations require significant human power and are time-consuming. This study developed a model, ‘Dung Detector (DD)’, for detecting cattle dung in pastures from drone images using the You Only Look Once (YOLO) v5 algorithm. The DD model was trained using our custom dataset including 1,504 split images from drone orthomosaic images in five paddocks: Obihiro (OBH), Shintoku (STK), Minokamo (MNO), Miyota (MYT), and Yatsugatake (YGK). The detection accuracy was evaluated using ground-truth data acquired in two quadrats within paddocks. The DD model performed well for OBH and STK (F-score = 0.861 and 0.835) paddocks with simple grass species and low surface sward height (SSH). Although the MNO and MYT, with complex vegetation and high SSH, showed few false positives (precision >0.9), some cattle dung pats were undetectable, presumably due to grass height (Recall = 0.500 and 0.276). © 2024 Japanese Society of Grassland Science.","deep learning; grazing management; unmanned aerial vehicle (UAV)","aerial survey; algorithm; cattle; detection method; feces; grazing management; image analysis; livestock farming; machine learning; nutrient cycling; precision; vegetation cover","Japan Society for the Promotion of Science, JSPS, (22H02470); Japan Society for the Promotion of Science, JSPS","We would like to give our special thanks to Ms. Sachiko Uesugi, Mr. Tsutomu Hotta, Mr. Takashi Tsukamoto of the Field Center of Animal Science and Agriculture, Obihiro University of Agriculture and Veterinary Medicine, Japan, Mr. Takashi Kato of Gifu Field Science Center, Gifu University, Japan for support in the field experiments. We are also grateful to all the staff members of the HRO Agricultural Research Department Animal Research Center, Japan, NARO Institute of Livestock and Grassland Science, Miyota, Japan, the Yatsugatake Farm, Yamanashi Prefectural Government, Japan, for their support in grazing management. We would like to thank Editage ( www.editage.com ) for English language editing. This work was supported by JSPS KAKENHI Grant Number 22H02470.","K. Kawamura; Department of Agro-environmental Science, Obihiro University of Agriculture and Veterinary Medicine, Hokkaido, 080-8555, Japan; email: kamuken@obihiro.ac.jp","","John Wiley and Sons Inc","17446961","","","","English","Grassl. Sci.","Article","Final","","Scopus","2-s2.0-85203981529"
"Bati C.T.; Ser G.","Bati, Cafer Tayyar (57211336993); Ser, Gazel (55372727900)","57211336993; 55372727900","Improved sheep identification and tracking algorithm based on YOLOv5 + SORT methods","2024","Signal, Image and Video Processing","18","10","","6683","6694","11","1","10.1007/s11760-024-03344-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195845399&doi=10.1007%2fs11760-024-03344-5&partnerID=40&md5=faaef00c4ca10bd3f8d493b50dc3cdf6","Department of Animal Science, Faculty of Agriculture, Van Yuzuncu Yil University, Van, Turkey","Bati C.T., Department of Animal Science, Faculty of Agriculture, Van Yuzuncu Yil University, Van, Turkey; Ser G., Department of Animal Science, Faculty of Agriculture, Van Yuzuncu Yil University, Van, Turkey","This research emphasises the importance of sheep identification and tracking in precision livestock farming and investigates the use of deep learning techniques for this purpose. Since traditional identification methods are time consuming and limiting, it is hypothesised that deep learning based models can make this process more efficient. However, although deep learning-based methods have achieved remarkable results in the field of animal recognition, some problems can be encountered that limit their practical application. Generally, these networks are tested on similar images taken from the dataset on which they are trained. Although the test performance of these models is high, they may perform poorly on images with different features. For this reason, in the present study on the YOLOv5 model, a number of effective preprocesses are included for the model’s ability to identify and track sheep from sheep images with different traits from the training data. In addition, some adaptive adjustments were made to the YOLOv5 model to increase its effectiveness in practical applications. According to the experimental results of this study, in which videos of 20 Norduz sheep in the scale and arena tests were used, the YOLOv5l model trained on the scale test reached a mAP value of 0.99. Although the model performed the task of identifying and tracking the sheep in the scale test, it was observed that it could not perform the task of identifying and tracking the sheep in the arena test. Therefore, YOLOV5l (Model II), which was retrained on the scale images segmented from the background, gained the ability to identify and track the sheep in the arena test with some various pre-tuning. The findings of the study indicate the potential of deep learning-based models to improve the effectiveness of animal identification and tracking procedures in precision livestock farming. At the same time, the developmental stages outlined in this study provide a reference for the identification and tracking of sheep or alternative livestock in real-life situations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.","Deep learning; Precision livestock farming; Sheep identification; Sheep tracking; YOLO","Animals; Farms; Learning systems; Tracking (position); Deep learning; Identification algorithms; Identification method; Learning Based Models; Learning techniques; Precision livestock farming; Sheep identification; Sheep tracking; Tracking algorithm; YOLO; Deep learning","","","C.T. Bati; Department of Animal Science, Faculty of Agriculture, Van Yuzuncu Yil University, Van, Turkey; email: cafertayyarbati@gmail.com","","Springer Science and Business Media Deutschland GmbH","18631703","","","","English","Signal Image Video Process.","Article","Final","","Scopus","2-s2.0-85195845399"
"Myint B.B.; Onizuka T.; Tin P.; Aikawa M.; Kobayashi I.; Zin T.T.","Myint, Bo Bo (59173041400); Onizuka, Tsubasa (58544593700); Tin, Pyke (24923729700); Aikawa, Masaru (36663478200); Kobayashi, Ikuo (24174822700); Zin, Thi Thi (6506258245)","59173041400; 58544593700; 24923729700; 36663478200; 24174822700; 6506258245","AUTOMATED CATTLE DETECTION USING MASK R-CNN AND IOU-BASED TRACKING WITH A SINGLE SIDE-VIEW CAMERA","2024","International Journal of Innovative Computing, Information and Control","20","5","","1439","1447","8","0","10.24507/ijicic.20.05.1439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207494316&doi=10.24507%2fijicic.20.05.1439&partnerID=40&md5=6866322236e885ad1f03d8cfb4a33bb2","Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Organization for Learning and Student Development, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan","Myint B.B., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Onizuka T., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Tin P., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Aikawa M., Organization for Learning and Student Development, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Kobayashi I., Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan; Zin T.T., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuen kibanadai-Nishi, Miyazaki, 889-2192, Japan","In precision livestock farming, the early detection of lameness in cattle is an extremely important aspect of effective breeding management. Timely identification of lameness not only facilitates prompt and cost-efficient treatment but also plays a crucial role in avoiding possible future diseases. This study emphasizes the significance of intelligent visual perception systems for lameness detection in dairy cattle, particularly in the lane between from Milking Parlor to Cattle Barn. To address the cattle lameness issue, we employ an advanced deep learning, and image processing technique, i.e., Mask R-CNN from Detectron2 to detect and identify cattle regions for feature extraction of lameness detection. On the other hand, cattle tracking using IoU is also an important part of data accumulation for lameness classification. The results of this study contribute to ongoing efforts in precision animal husbandry and demonstrate the potential of intelligent visual recognition systems for early lameness detection. © 2024, ICIC International. All rights reserved.","Cattle detection; Cattle tracking; Detectron2; Mask R-CNN","Farm buildings; Cattle detection; Cattle tracking; Cattles; Cost-efficient; Detectron2; Lameness detection; Mask R-CNN; Precision livestock farming; Side view; Timely identification; Deep learning","JKA","We would like to thank the people at the Kunneppu Demonstration Farm for the great accommodation and valuable advice for our research at the farm. This publication was subsidized by JKA through its promotion funds from KEIRIN RACE.","T.T. Zin; Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, Miyazaki, 1-1, Gakuen kibanadai-Nishi, 889-2192, Japan; email: thithi@cc.miyazaki-u.ac.jp","","ICIC International","13494198","","","","English","Int. J. Innov. Comput. Inf. Control","Article","Final","","Scopus","2-s2.0-85207494316"
"Hasan M.K.; Mun H.-S.; Ampode K.M.B.; Lagua E.B.; Park H.-R.; Kim Y.-H.; Sharifuzzaman M.; Yang C.-J.","Hasan, Md Kamrul (59496512600); Mun, Hong-Seok (56009158500); Ampode, Keiven Mark Bigtasin (57224728251); Lagua, Eddiemar Baguio (57226493132); Park, Hae-rang (57971856800); Kim, Young-Hwa (55699491000); Sharifuzzaman, Md (57193158632); Yang, Chul-Ju (16048451400)","59496512600; 56009158500; 57224728251; 57226493132; 57971856800; 55699491000; 57193158632; 16048451400","A Systematic Literature Review on the Uses, Benefits, Challenges, and Prospects of Digital Twins in Livestock Farm Management","2024","Advances in Animal and Veterinary Sciences","12","12","","2301","2314","13","0","10.17582/journal.aavs/2024/12.12.2301.2314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207342051&doi=10.17582%2fjournal.aavs%2f2024%2f12.12.2301.2314&partnerID=40&md5=20c98757457f993cde8ac24d8b2d4ff8","Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea; Department of Poultry Science, Sylhet Agricultural University, Sylhet, Bangladesh; Department of Multimedia Engineering, Sunchon National University, Suncheon, South Korea; Department of Animal Science, College of Agriculture, Sultan Kudarat State University, Tacurong, Philippines; Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Sunchon National University, Suncheon, South Korea; Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Chonnam National University, Gwangju, South Korea; Department of Animal Science and Veterinary Medicine, Bangabandhu Sheikh Mujibur Rahman Science and Technology University, Gopalganj, Bangladesh","Hasan M.K., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Department of Poultry Science, Sylhet Agricultural University, Sylhet, Bangladesh; Mun H.-S., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Department of Multimedia Engineering, Sunchon National University, Suncheon, South Korea; Ampode K.M.B., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Department of Animal Science, College of Agriculture, Sultan Kudarat State University, Tacurong, Philippines; Lagua E.B., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Sunchon National University, Suncheon, South Korea; Park H.-R., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Sunchon National University, Suncheon, South Korea; Kim Y.-H., Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Chonnam National University, Gwangju, South Korea; Sharifuzzaman M., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Department of Animal Science and Veterinary Medicine, Bangabandhu Sheikh Mujibur Rahman Science and Technology University, Gopalganj, Bangladesh; Yang C.-J., Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea, Interdisciplinary Program in IT-Bio Convergence System (BK21 Plus), Sunchon National University, Suncheon, South Korea","Digital twin (DT), an artificial intelligence (AI) technology, has garnered significant attention recently due to its potential applications across various industries, including the livestock sector. To facilitate the adoption and integration of DT technologies in livestock farming systems, this study aims to discern and appraise the most recent advancements by assessing scientific and technological progressions. The objective of this study is to conduct a comprehensive review of the literature to determine the status of DT application in livestock farm management with its benefits, challenges, and prospects. Scientific databases, PRISMA guidelines, and systematic bibliometric methodologies were used for searching and sorting the most fit articles. This study found that DT technology is new to the livestock industry and mainly used for environmental control, feed management, farm planning, greenhouse gas reduction, and behavior monitoring. Key barriers to DT deployment include high establishment costs, risk of market monopolization, and technical knowledge gaps. The amalgamation of government subsidies, localized technology production, and the provision of training facilities can be instrumental in enabling farmers to overcome the obstacles. The application of DT is limited in monitoring animal behavior and farm environment. However, DT technology has the potential for broader use, such as predicting growth, feed consumption, and improving the supply chain. Although these new areas are yet to be explored in real farm conditions. Along with expanding the new area of using DT in livestock farm management, future collaborative research should concentrate on creating new machine learning and/or deep learning models. © 2024 by the authors. Licensee ResearchersLinks Ltd, England, UK. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).","Animal behavior monitoring; Digital twin; Environmental control; Farm operation; Livestock supply chain; Precision agriculture","","Korea Smart Farm R&D Foundation; Ministry of Science and ICT; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421023-04); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET, (RS-2023-00231738); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","The authors would like to acknowledge the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation through the Smart Farm Innovation Technology Development Program, funded by the Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT) and Rural Development Administration (RDA) (421023-04). Also, work was supported by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry (IPET) through Agri-Food Export Enhancement Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) (RS-2023-00231738).","C.-J. Yang; Animal Nutrition and Feed Science Laboratory, Department of Animal Science and Technology, Sunchon National University, Suncheon, South Korea; email: yangcj@scnu.ac.kr","","ResearchersLinks Ltd","23093331","","","","English","Adv. Anim. Vet. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207342051"
"Moser A.; van Vliet J.; Wissen Hayek U.; Grêt-Regamey A.","Moser, Andreas (57199167421); van Vliet, Jasper (55098762600); Wissen Hayek, Ulrike (36894532200); Grêt-Regamey, Adrienne (16024264700)","57199167421; 55098762600; 36894532200; 16024264700","Analyzing the extent and use of impervious land in rural landscapes","2024","Geography and Sustainability","5","4","","625","636","11","0","10.1016/j.geosus.2024.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204190280&doi=10.1016%2fj.geosus.2024.08.004&partnerID=40&md5=344c20c8e231912199cb8a7e6734399e","Planning of Landscape and Urban Systems (PLUS), ETH Zurich, Stefano-Franscini-Platz 5, Zurich, Switzerland; Institute for Environmental Studies (IVM), Vrije Universiteit Amsterdam, De Boelelaan 1111, Amsterdam, Netherlands","Moser A., Planning of Landscape and Urban Systems (PLUS), ETH Zurich, Stefano-Franscini-Platz 5, Zurich, Switzerland; van Vliet J., Institute for Environmental Studies (IVM), Vrije Universiteit Amsterdam, De Boelelaan 1111, Amsterdam, Netherlands; Wissen Hayek U., Planning of Landscape and Urban Systems (PLUS), ETH Zurich, Stefano-Franscini-Platz 5, Zurich, Switzerland; Grêt-Regamey A., Planning of Landscape and Urban Systems (PLUS), ETH Zurich, Stefano-Franscini-Platz 5, Zurich, Switzerland","The amount of impervious surface is increasing rapidly worldwide. Although urban expansion has been studied extensively, the alteration of impervious land cover in rural regions remains under-examined. In particular, insights into the utilization of these sealed surfaces are crucially needed to unravel the underlying dynamics of land use changes beyond urban areas. This study focuses on rural regions from a Swiss case study and presents an analysis of the use of sealed surfaces in such regions, rather than solely quantifying the extent of sealed surfaces. Utilizing a synergistic approach that merges detailed cadastral plans with very-high-resolution remote sensing imagery and sophisticated deep learning algorithms, we characterized the uses of sealed surfaces, including buildings and their surroundings. Our findings reveal that 2.1 % of the study area's rural regions comprises sealed surfaces - an area comparable to the sealed surfaces in the urban regions. Within these rural regions, transport infrastructure represents 68 % of this impervious surface. Buildings account for 12 %, and their surroundings, constituting 13 %, are utilized primarily for agricultural purposes, including farming and livestock activities. The deep learning approach achieved a classification accuracy of 72 % for a shallow model and 79 % for a deeper model, indicating that mapping building types is possible with reasonable accuracy. The outcomes of this study underscore the critical need to factor in the presence and utilization of impervious land cover within rural regions for the sustainable management of land resources. © 2024","Building types; Cadastral data; Convolutional neural networks; Rural areas; Soil sealing; Very-high-resolution aerial imagery","artificial neural network; cadastre; data set; image analysis; image classification; remote sensing; rural area; sealing","Canton of Appenzell Ausserrhoden; ARE; FOAG; Amt für Gemeinden und Raumordnung; Canton of Appenzell Innerrhoden; Swiss Federal Office for Spatial Development; Amt für Raum und Wald; Kanton St. Gallen; Bundesamt für Landwirtschaft, BLW; Canton of Glarus; Canton of Vaud; Swiss Federal Office for Environment; FOEN; Kanton Bern; Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO, (VI.Vidi, 198.008, VI.Vidi.198.008); Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO","Funding text 1: This research is part of the project \u201CInterkantonal koordiniertes Monitoring Bauen ausserhalb Bauzonen\u201D and was funded by the Swiss Federal Office for Spatial Development (ARE), Swiss Federal Office for Environment (FOEN), Swiss Federal Office for Agriculture (FOAG), Canton of Bern (Amt f\u00FCr Gemeinden und Raumordnung), Canton of St. Gallen (Amt f\u00FCr Raumentwicklung und Geoinformation), Canton of Appenzell Ausserrhoden (Amt f\u00FCr Raum und Wald), Canton of Appenzell Innerrhoden (Amt f\u00FCr Raumentwicklung), Canton of Glarus (Abteilung Raumentwicklung und Geoinformation), and Canton of Vaud (Direction g\u00E9n\u00E9rale du territoire et du logement). JvV was supported by the Netherlands Organization for Scientific Research NWO in the form of a VIDI grant (Grant No VI.Vidi. 198.008 ). ; Funding text 2: This research is part of the project \u201CInterkantonal koordiniertes Monitoring Bauen ausserhalb Bauzonen\u201D and was funded by the Swiss Federal Office for Spatial Development (ARE), Swiss Federal Office for Environment (FOEN), Swiss Federal Office for Agriculture (FOAG), Canton of Bern (Amt f\u00FCr Gemeinden und Raumordnung), Canton of St. Gallen (Amt f\u00FCr Raumentwicklung und Geoinformation), Canton of Appenzell Ausserrhoden (Amt f\u00FCr Raum und Wald), Canton of Appenzell Innerrhoden (Amt f\u00FCr Raumentwicklung), Canton of Glarus (Abteilung Raumentwicklung und Geoinformation), and Canton of Vaud (Direction g\u00E9n\u00E9rale du territoire et du logement). JvV was supported by the Netherlands Organization for Scientific Research NWO in the form of a VIDI grant (Grant No. VI.Vidi.198.008).","A. Moser; ETH Zürich - Planning of Landscape and Urban Systems (PLUS), Stefano-Franscini-Platz 5, HIL H 41.2, Zürich, 8093, Switzerland; email: moserandreas@ethz.ch","","Beijing Normal University Press","20967438","","","","English","Geo. Sustain.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85204190280"
"Bai L.; Zhang Z.; Song J.","Bai, Lili (58922823300); Zhang, Zhe (59006789600); Song, Jie (55276320800)","58922823300; 59006789600; 55276320800","Image dataset for cattle biometric detection and analysis","2024","Data in Brief","56","","110835","","","","1","10.1016/j.dib.2024.110835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201724464&doi=10.1016%2fj.dib.2024.110835&partnerID=40&md5=b4be6f962f872ac06023e66dc06b7ca7","Software College, Northeastern University, Shenyang, 110819, China","Bai L., Software College, Northeastern University, Shenyang, 110819, China; Zhang Z., Software College, Northeastern University, Shenyang, 110819, China; Song J., Software College, Northeastern University, Shenyang, 110819, China","The dataset of cattle biometric features is a pivotal asset for improving livestock management and promoting smart agriculture innovation. We obtained a dataset of images capturing the side and back views of Horqin yellow cattle from a farm in eastern Inner Mongolia, China. These data consist of images of 72 free-range Horqin yellow cattle taken with a mobile camera on the grasslands. Each cattle is accompanied by detailed annotations, including oblique body length, withers height, heart girth, hip length, as well as body weight among other crucial data points. This information is considered as high-quality biological feature data. In the field of computer vision, utilizing this dataset can facilitate the construction of deep learning models to develop an automated livestock monitoring system. The aim is to enhance management efficiency and operational effectiveness within the livestock industry. By integrating biological feature information, specific model tools can be employed for body condition assessment and health monitoring research. This approach enables the effective identification and prevention of disease conditions, ultimately providing a deeper level of care and support for livestock welfare and health. The cattle dataset offers support for smart agriculture by enabling the development of intelligent farm management systems. These systems facilitate real-time alerts for livestock health and environmental monitoring. This advancement will drive the modernization and digitization of animal husbandry, fostering agricultural intelligence and sustainable development. © 2024 The Author(s)","Automatic measurement; Body measurements; Intelligent animal husbandry; Livestock husbandry","Animal husbandry; Automatic measurements; Biological features; Body measurements; Cattles; Health monitoring; Image datasets; Intelligent animal husbandry; Livestock husbandry; Smart agricultures","National Natural Science Foundation of China, NSFC, (62302086); Fundamental Research Funds for the Central Universities, (N2317005)","This paper is supported by the National Natural Science Foundation of China (Grant No. 62302086) and the Fundamental Research Funds for the Central Universities (Grant No. N2317005). The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.","J. Song; Software College, Northeastern University, Shenyang, 110819, China; email: songjie@mail.neu.edu.cn","","Elsevier Inc.","23523409","","","","English","Data Brief","Data paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85201724464"
"Senthilkumar C.; C S.; Vadivu G.; Neethirajan S.","Senthilkumar, Chamirti (59384570800); C, Sindhu (59384117400); Vadivu, G. (55357629600); Neethirajan, Suresh (57217318680)","59384570800; 59384117400; 55357629600; 57217318680","Early Detection of Lumpy Skin Disease in Cattle Using Deep Learning—A Comparative Analysis of Pretrained Models","2024","Veterinary Sciences","11","10","510","","","","0","10.3390/vetsci11100510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207457230&doi=10.3390%2fvetsci11100510&partnerID=40&md5=5834983fc671dde2ea0aa1b26c1c3be7","Department of Computing Technologies, School of Computing, SRM Institute of Science and Technology, Kattankulathur, 603203, India; Department of Data Science and Business Systems, School of Computing, SRM Institute of Science and Technology, Kattankulathur, 603203, India; Department of Animal Science and Aquaculture, Faculty of Agriculture, Dalhousie University, P.O. Box 550, Truro, B2N 5E3, NS, Canada; Faculty of Computer Science, Dalhousie University, 6050 University Ave, Halifax, B3H 1W5, NS, Canada","Senthilkumar C., Department of Computing Technologies, School of Computing, SRM Institute of Science and Technology, Kattankulathur, 603203, India; C S., Department of Computing Technologies, School of Computing, SRM Institute of Science and Technology, Kattankulathur, 603203, India; Vadivu G., Department of Data Science and Business Systems, School of Computing, SRM Institute of Science and Technology, Kattankulathur, 603203, India; Neethirajan S., Department of Animal Science and Aquaculture, Faculty of Agriculture, Dalhousie University, P.O. Box 550, Truro, B2N 5E3, NS, Canada, Faculty of Computer Science, Dalhousie University, 6050 University Ave, Halifax, B3H 1W5, NS, Canada","Lumpy Skin Disease (LSD) is a highly contagious viral infection in cattle that poses a significant threat to agricultural economies, especially in countries like India. Early and accurate detection is vital to prevent widespread outbreaks and reduce economic losses. Our research leverages advancements in artificial intelligence (AI), specifically deep learning, to develop an automated system for detecting LSD in cattle. We utilized publicly available datasets containing images of healthy cattle, those affected by LSD, and, importantly, cattle with other skin diseases to ensure our model specifically identifies LSD rather than general illness signs. We evaluated over ten pretrained deep learning models, including VGG16 and MobileNetV2, using transfer learning techniques. Through detailed data preprocessing, augmentation, and balancing, we enhanced model performance and generalizability. We assessed the models using crucial medical diagnostics metrics like sensitivity and specificity to minimize false negatives and positives. VGG16 and MobileNetV2 stood out, achieving high accuracy along with excellent sensitivity and specificity, effectively detecting LSD without misclassifying other conditions. This study provides valuable insights into applying deep learning in veterinary diagnostics, contributing to developing reliable AI tools for early LSD detection, ultimately improving animal health management and safeguarding the agricultural economy. © 2024 by the authors.","artificial intelligence; automated disease detection; bovine health management; deep learning; digital livestock farming; lumpy skin disease; veterinary diagnostics","accuracy; area under the curve; Article; artificial intelligence; artificial neural network; comparative study; controlled study; convolutional neural network; deep learning; diagnostic accuracy; diagnostic test accuracy study; image processing; information processing; learning algorithm; livestock; lumpy skin disease; model; nonhuman; pretrained model; receiver operating characteristic; sensitivity analysis; sensitivity and specificity; transfer of learning; veterinary medicine","","","S. Neethirajan; Department of Animal Science and Aquaculture, Faculty of Agriculture, Dalhousie University, Truro, P.O. Box 550, B2N 5E3, Canada; email: sneethir@gmail.com","","Multidisciplinary Digital Publishing Institute (MDPI)","23067381","","","","English","Vet. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85207457230"
"Rebez E.B.; Sejian V.; Silpa M.V.; Kalaignazhal G.; Thirunavukkarasu D.; Devaraj C.; Nikhil K.T.; Ninan J.; Sahoo A.; Lacetera N.; Dunshea F.R.","Rebez, Ebenezer Binuni (58072274800); Sejian, Veerasamy (24768193500); Silpa, Mullakkalparambil Velayudhan (57196190750); Kalaignazhal, Gajendirane (58411151100); Thirunavukkarasu, Duraisamy (8662420400); Devaraj, Chinnasamy (57206730749); Nikhil, Kumar Tej (59328015200); Ninan, Jacob (59328269700); Sahoo, Artabandhu (7006276668); Lacetera, Nicola (6701607005); Dunshea, Frank Rowland (7005947650)","58072274800; 24768193500; 57196190750; 58411151100; 8662420400; 57206730749; 59328015200; 59328269700; 7006276668; 6701607005; 7005947650","Applications of Artificial Intelligence for Heat Stress Management in Ruminant Livestock","2024","Sensors","24","18","5890","","","","0","10.3390/s24185890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205238679&doi=10.3390%2fs24185890&partnerID=40&md5=be57a28e1cc470aab8c6c96c5aa2d0ac","Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India; ICAR-National Institute of Animal Nutrition and Physiology, Adugodi, Bangalore, 560030, India; Department of Animal Breeding and Genetics, College of Veterinary Science and Animal Husbandry, Odisha University of Agriculture and Technology, Bhubaneshwar, 751003, India; Department of Veterinary and Animal Husbandry Extension Education, Veterinary College and Research Institute, Tamil Nadu Veterinary and Animal Sciences University, Namakkal, 637002, India; Department of Agriculture and Forest Sciences, University of Tuscia, Viterbo, 01100, Italy; School of Agriculture, Food and Ecosystem Sciences, Faculty of Science, The University of Melbourne, Parkville, Melbourne, 3010, VIC, Australia","Rebez E.B., Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India, ICAR-National Institute of Animal Nutrition and Physiology, Adugodi, Bangalore, 560030, India; Sejian V., Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India, ICAR-National Institute of Animal Nutrition and Physiology, Adugodi, Bangalore, 560030, India; Silpa M.V., Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India; Kalaignazhal G., Department of Animal Breeding and Genetics, College of Veterinary Science and Animal Husbandry, Odisha University of Agriculture and Technology, Bhubaneshwar, 751003, India; Thirunavukkarasu D., Department of Veterinary and Animal Husbandry Extension Education, Veterinary College and Research Institute, Tamil Nadu Veterinary and Animal Sciences University, Namakkal, 637002, India; Devaraj C., ICAR-National Institute of Animal Nutrition and Physiology, Adugodi, Bangalore, 560030, India; Nikhil K.T., Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India; Ninan J., Rajiv Gandhi Institute of Veterinary Education and Research, Puducherry, Kurumbapet, 605009, India; Sahoo A., ICAR-National Institute of Animal Nutrition and Physiology, Adugodi, Bangalore, 560030, India; Lacetera N., Department of Agriculture and Forest Sciences, University of Tuscia, Viterbo, 01100, Italy; Dunshea F.R., School of Agriculture, Food and Ecosystem Sciences, Faculty of Science, The University of Melbourne, Parkville, Melbourne, 3010, VIC, Australia","Heat stress impacts ruminant livestock production on varied levels in this alarming climate breakdown scenario. The drastic effects of the global climate change-associated heat stress in ruminant livestock demands constructive evaluation of animal performance bordering on effective monitoring systems. In this climate-smart digital age, adoption of advanced and developing Artificial Intelligence (AI) technologies is gaining traction for efficient heat stress management. AI has widely penetrated the climate sensitive ruminant livestock sector due to its promising and plausible scope in assessing production risks and the climate resilience of ruminant livestock. Significant improvement has been achieved alongside the adoption of novel AI algorithms to evaluate the performance of ruminant livestock. These AI-powered tools have the robustness and competence to expand the evaluation of animal performance and help in minimising the production losses associated with heat stress in ruminant livestock. Advanced heat stress management through automated monitoring of heat stress in ruminant livestock based on behaviour, physiology and animal health responses have been widely accepted due to the evolution of technologies like machine learning (ML), neural networks and deep learning (DL). The AI-enabled tools involving automated data collection, pre-processing, data wrangling, development of appropriate algorithms, and deployment of models assist the livestock producers in decision-making based on real-time monitoring and act as early-stage warning systems to forecast disease dynamics based on prediction models. Due to the convincing performance, precision, and accuracy of AI models, the climate-smart livestock production imbibes AI technologies for scaled use in the successful reducing of heat stress in ruminant livestock, thereby ensuring sustainable livestock production and safeguarding the global economy. © 2024 by the authors.","artificial intelligence; deep learning; heat stress; machine learning; neural networks; ruminant livestock","Algorithms; Animals; Artificial Intelligence; Climate Change; Heat Stress Disorders; Livestock; Machine Learning; Neural Networks, Computer; Ruminants; Deep neural networks; Animal performance; Artificial intelligence technologies; Deep learning; Heat stress; Livestock production; Machine-learning; Neural-networks; Ruminant livestock; Stress management; algorithm; animal; artificial intelligence; artificial neural network; climate change; heat injury; livestock; machine learning; physiology; prevention and control; ruminant; veterinary medicine; Decision making","University of Melbourne, UNIMELB; APC Europe; National Research Centre for Agricultural Technologies; European Union Next-GenerationEU, (J83C22000830005)","The APC for this manuscript was funded by Frank Dunshea, The University of Melbourne. This work also was partly supported by the European Union Next-GenerationEU (grant number J83C22000830005) within the \u201CNational Research Centre for Agricultural Technologies\u201D research programme.","V. Sejian; Rajiv Gandhi Institute of Veterinary Education and Research, Kurumbapet, Puducherry, 605009, India; email: drsejian@gmail.com","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","39338635","English","Sensors","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85205238679"
"Hossain M.R.H.; Islam R.; McGrath S.R.; Islam M.Z.; Lamb D.","Hossain, Muhammad Riaz Hasib (58161170100); Islam, Rafiqul (57209132403); McGrath, Shawn R. (55934587600); Islam, Md Zahidul (57198634079); Lamb, David (7202727562)","58161170100; 57209132403; 55934587600; 57198634079; 7202727562","Learning-based estimation of cattle weight gain and its influencing factors","2025","Computers and Electronics in Agriculture","231","","110033","","","","0","10.1016/j.compag.2025.110033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216749016&doi=10.1016%2fj.compag.2025.110033&partnerID=40&md5=d8e529c33d5d500f35dcab5d2ad13b4d","School of Computing, Mathematics and Engineering, Charles Sturt University, Wagga Wagga, 2650, NSW, Australia; School of Computing, Mathematics and Engineering, Charles Sturt University, Albury, 2640, NSW, Australia; Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia; School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia; Precision Agriculture Research Group, University of New England NSW, 2351, Australia; Food Agility CRC Ltd, Sydney, 2000, NSW, Australia","Hossain M.R.H., School of Computing, Mathematics and Engineering, Charles Sturt University, Wagga Wagga, 2650, NSW, Australia; Islam R., School of Computing, Mathematics and Engineering, Charles Sturt University, Albury, 2640, NSW, Australia; McGrath S.R., Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia; Islam M.Z., School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia; Lamb D., Precision Agriculture Research Group, University of New England NSW, 2351, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia","Many cattle farmers still depend on manual methods to measure the live weight gain of cattle at set intervals, which is time-consuming, labour-intensive, and stressful for both the animals and handlers. A remote and autonomous monitoring system using machine learning (ML) or deep learning (DL) can provide a more efficient and less invasive method and also predictive capabilities for future cattle weight gain (CWG). This system allows continuous monitoring and estimation of individual cattle's live weight gain, growth rates and weight fluctuations considering various factors like environmental conditions, genetic predispositions, feed availability, movement patterns and behaviour. Several researchers have explored the efficiency of estimating CWG using ML and DL algorithms. However, estimating CWG suffers from a lack of consistency in its application. Moreover, ML or DL can provide weight gain estimations based on several features that vary in existing research. Additionally, previous studies have encountered various data-related challenges when estimating CWG. This paper presents a comprehensive investigation in estimating CWG using advanced ML techniques based on research articles (2004–2024). This study investigates the current tools, methods, and features used in CWG estimation, as well as their strengths and weaknesses. The findings highlight the significance of using advanced ML approaches in CWG estimation and its critical influence on factors. Furthermore, this study identifies potential research gaps and provides research direction on CWG prediction, which serves as a reference for future research in this area. © 2025 The Author(s)","Cattle farming; Cattle weight gain; Deep learning; Machine learning; Systematic literature review","Adversarial machine learning; Contrastive Learning; Deep reinforcement learning; Cattle farming; Cattle weight gain; Cattles; Deep learning; Gain estimation; Machine-learning; Manual methods; Systematic literature review; Weight gain; cattle; detection method; environmental conditions; environmental factor; estimation method; growth rate; learning; literature review; machine learning; Livestock","","","M.R.H. Hossain; School of Computing, Mathematics and Engineering, Charles Sturt University, Wagga Wagga, 2650, Australia; email: muhossain@csu.edu.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85216749016"
"Najjar H.; Miranda M.; Nuske M.; Roscher R.; Dengel A.","Najjar, Hiba (57226769364); Miranda, Miro (57763934800); Nuske, Marlon (56648187900); Roscher, Ribana (37113871300); Dengel, Andreas (6603764314)","57226769364; 57763934800; 56648187900; 37113871300; 6603764314","Explainability of Subfield Level Crop Yield Prediction Using Remote Sensing","2025","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","18","","","4141","4161","20","0","10.1109/JSTARS.2025.3528068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214820612&doi=10.1109%2fJSTARS.2025.3528068&partnerID=40&md5=694a47be4e54e04264b2f60208d6fea7","RPTU Kaiserslautern-Landau, The German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany; Bundesanstalt für Landwirtschaft und Ernährung, Bonn, 53179, Germany; Forschungszentrum Jülich GmbH, Jülich, 52428, Germany; University of Bonn, Bonn, 53113, Germany","Najjar H., RPTU Kaiserslautern-Landau, The German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany; Miranda M., RPTU Kaiserslautern-Landau, The German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany; Nuske M., Bundesanstalt für Landwirtschaft und Ernährung, Bonn, 53179, Germany; Roscher R., Forschungszentrum Jülich GmbH, Jülich, 52428, Germany, University of Bonn, Bonn, 53113, Germany; Dengel A., RPTU Kaiserslautern-Landau, The German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany","Crop yield forecasting plays a significant role in addressing growing concerns about food security and guiding decision-making for policymakers and farmers. When deep learning is employed, understanding the learning and decision-making processes of the models, as well as their interaction with the input data, is crucial for establishing trust in the models and gaining insight into their reliability. In this study, we focus on the task of crop yield prediction, specifically for soybean, wheat, and rapeseed crops in Argentina, Uruguay, and Germany. Our goal is to develop and explain predictive models for these crops, using a large dataset of satellite images, additional data modalities, and crop yield maps. We employ a long short-term memory network and investigate the impact of using different temporal samplings of the satellite data and the benefit of adding more relevant modalities. For model explainability, we utilize feature attribution methods to quantify input feature contributions, identify critical growth stages, analyze yield variability at the field level, and explain less accurate predictions. The modeling results show an improvement when adding more modalities or using all available instances of satellite data. The explainability results reveal distinct feature importance patterns for each crop and region. We further found that the most influential growth stages on the prediction are dependent on the temporal sampling of the input data. We demonstrated how these critical growth stages, which hold significant agronomic value, closely align with the existing literature in agronomy and crop development biology.  © 2008-2012 IEEE.","Explainability; feature attribution; machine learning (ML); temporal analysis; yield prediction","Argentina; Germany; Uruguay; Livestock; Prediction models; Crop yield; Explainability; Feature attribution; Field level; Growth stages; Machine-learning; Satellite data; Temporal analysis; Temporal sampling; Yield prediction; crop production; crop yield; decision making; machine learning; prediction; remote sensing; Spatio-temporal data","","","H. Najjar; RPTU Kaiserslautern-Landau, The German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, 67663, Germany; email: najjar@rptu.de","","Institute of Electrical and Electronics Engineers Inc.","19391404","","","","English","IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85214820612"
"Bao Y.-C.; Shi C.-X.; Zhang C.-Q.; Gu M.-J.; Zhu L.; Liu Z.-X.; Zhou L.; Ma F.-Y.; Na R.-S.; Zhang W.-G.","Bao, Yan-Chun (58671906300); Shi, Cai-Xia (57897616600); Zhang, Chuan-Qiang (59330989700); Gu, Ming-Juan (57215924073); Zhu, Lin (57203842731); Liu, Zai-Xia (57248719400); Zhou, Le (57247852300); Ma, Feng-Ying (58117293000); Na, Ri-Su (56583894600); Zhang, Wen-Guang (54394758100)","58671906300; 57897616600; 59330989700; 57215924073; 57203842731; 57248719400; 57247852300; 58117293000; 56583894600; 54394758100","Progress on deep learning in genomics","2024","Yi chuan = Hereditas","46","9","","701","715","14","0","10.16288/j.yczz.24-151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204167869&doi=10.16288%2fj.yczz.24-151&partnerID=40&md5=19e2806337b1dd284cef1c5b6989218f","College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China; Inner Mongolia Engineering Research Center of Genomic Big Data for Agriculture, Hohhot, 010018, China; Inner Mongolia Saikexing Institute of Breeding and Reproductive Biotechnology in Domestic Animal, Hohhot, 011517, China; National Center of Technology Innovation for Dairy Industry, Hohhot, 010080, China; College of Life Sciences, Inner Mongolia Agricultural University, Hohhot, 010021, China","Bao Y.-C., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China, Inner Mongolia Engineering Research Center of Genomic Big Data for Agriculture, Hohhot, 010018, China; Shi C.-X., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China; Zhang C.-Q., Inner Mongolia Saikexing Institute of Breeding and Reproductive Biotechnology in Domestic Animal, Hohhot, 011517, China, National Center of Technology Innovation for Dairy Industry, Hohhot, 010080, China; Gu M.-J., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China; Zhu L., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China; Liu Z.-X., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China, Inner Mongolia Engineering Research Center of Genomic Big Data for Agriculture, Hohhot, 010018, China; Zhou L., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China, Inner Mongolia Engineering Research Center of Genomic Big Data for Agriculture, Hohhot, 010018, China; Ma F.-Y., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China, Inner Mongolia Engineering Research Center of Genomic Big Data for Agriculture, Hohhot, 010018, China; Na R.-S., College of Animal Science and Technology, Inner Mongolia Agricultural University, Hohhot, 010018, China; Zhang W.-G., College of Life Sciences, Inner Mongolia Agricultural University, Hohhot, 010021, China","随着高通量测序技术的迅猛发展，基因组学领域迎来了数据量的爆炸性增长，这对传统生物信息学处理复杂数据模式的能力构成了严峻挑战。在此技术革新的关键时刻，深度学习作为人工智能领域的前沿技术，以其强大的数据解析与模式识别能力，为基因组学研究注入了新的活力。本文聚焦于4种核心深度学习模型——卷积神经网络(convolution neural network，CNN)、循环神经网络(recurrent neural network，RNN)、长短期记忆网络(long short term memory，LSTM)及生成对抗网络(generative adversarial network，GAN)，系统阐述了它们的基础原理，重点回顾了这些模型近5年在DNA、RNA和蛋白质研究领域的广泛应用。此外，文章进一步探讨了深度学习在畜禽基因组学中的应用案例，揭示了其在遗传特征解析、疾病预防以及遗传改良等领域的潜在应用价值与面临的挑战。通过深入分析，本文旨在阐述深度学习技术在增强基因组数据分析的准确性和处理能力方面的作用，并构建一个概念性框架，以指导畜禽基因组学研究策略的发展及其在具体场景下的应用，进而推动精准农业和遗传改良技术的发展。.; With the rapid growth of data driven by high-throughput sequencing technologies, genomics has entered an era characterized by big data, which presents significant challenges for traditional bioinformatics methods in handling complex data patterns. At this critical juncture of technological progress, deep learning-an advanced artificial intelligence technology-offers powerful capabilities for data analysis and pattern recognition, revitalizing genomic research. In this review, we focus on four major deep learning models: Convolutional Neural Network(CNN), Recurrent Neural Network(RNN), Long Short-Term Memory(LSTM), and Generative Adversarial Network(GAN). We outline their core principles and provide a comprehensive review of their applications in DNA, RNA, and protein research over the past five years. Additionally, we also explore the use of deep learning in livestock genomics, highlighting its potential benefits and challenges in genetic trait analysis, disease prevention, and genetic enhancement. By delivering a thorough analysis, we aim to enhance precision and efficiency in genomic research through deep learning and offer a framework for developing and applying livestock genomic strategies, thereby advancing precision livestock farming and genetic breeding technologies.","CNN; deep learning; GAN; genome; LSTM; RNN","Animals; Computational Biology; Deep Learning; Genomics; Humans; Livestock; Neural Networks, Computer; animal; artificial neural network; bioinformatics; deep learning; genetics; genomics; human; livestock; procedures","","","","","","02539772","","","39275870","English","Yi Chuan","Review","Final","","Scopus","2-s2.0-85204167869"
"Rohan A.; Rafaq M.S.; Hasan M.J.; Asghar F.; Bashir A.K.; Dottorini T.","Rohan, Ali (57200014413); Rafaq, Muhammad Saad (57188699438); Hasan, Md. Junayed (57213529645); Asghar, Furqan (57208732896); Bashir, Ali Kashif (57193954653); Dottorini, Tania (6602762132)","57200014413; 57188699438; 57213529645; 57208732896; 57193954653; 6602762132","Application of deep learning for livestock behaviour recognition: A systematic literature review","2024","Computers and Electronics in Agriculture","224","","109115","","","","8","10.1016/j.compag.2024.109115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197949033&doi=10.1016%2fj.compag.2024.109115&partnerID=40&md5=75f41959effbc6403ed6be5a78c0ec28","National Subsea Centre, School of Computing, Robert Gordon University, 3 International Ave, Dyce, Aberdeen, AB21 0BH, United Kingdom; Wolfson School of MEME, Loughborough University, Loughborough, LE11 3TU, United Kingdom; Department of Energy Systems Engineering, University of Agriculture, Faisalabad, 38000, Pakistan; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, M15 6BY, United Kingdom; School of Veterinary Medicine and Sciences, University of Nottingham, Sutton Bonington Campus, Loughborough, LE12 5RD, United Kingdom","Rohan A., National Subsea Centre, School of Computing, Robert Gordon University, 3 International Ave, Dyce, Aberdeen, AB21 0BH, United Kingdom, School of Veterinary Medicine and Sciences, University of Nottingham, Sutton Bonington Campus, Loughborough, LE12 5RD, United Kingdom; Rafaq M.S., Wolfson School of MEME, Loughborough University, Loughborough, LE11 3TU, United Kingdom; Hasan M.J., National Subsea Centre, School of Computing, Robert Gordon University, 3 International Ave, Dyce, Aberdeen, AB21 0BH, United Kingdom; Asghar F., Department of Energy Systems Engineering, University of Agriculture, Faisalabad, 38000, Pakistan; Bashir A.K., Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, M15 6BY, United Kingdom; Dottorini T., School of Veterinary Medicine and Sciences, University of Nottingham, Sutton Bonington Campus, Loughborough, LE12 5RD, United Kingdom","Livestock health and welfare monitoring is a tedious and labour-intensive task previously performed manually by humans. However, with recent technological advancements, the livestock industry has adopted the latest AI and computer vision-based techniques empowered by deep learning (DL) models that, at the core, act as decision-making tools. These models have previously been used to address several issues, including individual animal identification, tracking animal movement, body part recognition, and species classification. However, over the past decade, there has been a growing interest in using these models to examine the relationship between livestock behaviour and associated health problems. Several DL-based methodologies have been developed for livestock behaviour recognition, necessitating surveying and synthesising state-of-the-art. Previously, review studies were conducted in a very generic manner and did not focus on a specific problem, such as behaviour recognition. To the best of our knowledge, there is currently no review study that focuses on the use of DL specifically for livestock behaviour recognition. As a result, this systematic literature review (SLR) is being carried out. The review was performed by initially searching several popular electronic databases, resulting in 1101 publications. Further assessed through the defined selection criteria, 126 publications were shortlisted. These publications were filtered using quality criteria that resulted in the selection of 44 high-quality primary studies, which were analysed to extract the data to answer the defined research questions. According to the results, DL solved 13 behaviour recognition problems involving 44 different behaviour classes. 23 DL models and 24 networks were employed, with CNN, Faster R-CNN, YOLOv5, and YOLOv4 being the most common models, and VGG16, CSPDarknet53, GoogLeNet, ResNet101, and ResNet50 being the most popular networks. Ten different matrices were utilised for performance evaluation, with precision and accuracy being the most commonly used. Occlusion and adhesion, data imbalance, and the complex livestock environment were the most prominent challenges reported by the primary studies. Finally, potential solutions and research directions were discussed in this SLR study to aid in developing autonomous livestock behaviour recognition systems. © 2024 The Author(s)","Artificial Intelligence (AI); Behaviour recognition; Deep learning (DL); Precision agriculture; Precision livestock farming","Animals; Behavioral research; Decision making; Deep learning; Farms; Artificial intelligence; Behaviour recognition; Deep learning; Labour-intensive; Learning models; Precision Agriculture; Precision livestock farming; Systematic literature review; Technological advancement; Vision based; animal welfare; behavioral response; computer vision; literature review; livestock; livestock farming; machine learning; pattern recognition; performance assessment; precision; precision agriculture; Precision agriculture","Innovate UK, (107458); Innovate UK","This research was financially supported by Innovate UK (Project name: Towards Net Zero Dairy Farming through AI and Machine Vision (DAIRYVISION), Project reference: 107458 )","A. Rohan; National Subsea Centre, School of Computing, Robert Gordon University, Aberdeen, 3 International Ave, Dyce, AB21 0BH, United Kingdom; email: a.rohan@rgu.ac.uk; T. Dottorini; School of Veterinary Medicine and Sciences, University of Nottingham, Loughborough, Sutton Bonington Campus, LE12 5RD, United Kingdom; email: tania.dottorini@nottingham.ac.uk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85197949033"
"Wu D.; Ying Y.; Zhou M.; Pan J.; Cui D.","Wu, Dihua (59350872700); Ying, Yibin (55120143200); Zhou, Mingchuan (55656604800); Pan, Jinming (12773017600); Cui, Di (13905114600)","59350872700; 55120143200; 55656604800; 12773017600; 13905114600","YOLO-Claw: A fast and accurate method for chicken claw detection","2024","Engineering Applications of Artificial Intelligence","136","","108919","","","","3","10.1016/j.engappai.2024.108919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197747249&doi=10.1016%2fj.engappai.2024.108919&partnerID=40&md5=e35a1e1239386befc150694ce02254b9","College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China; Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China; Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China","Wu D., College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China, Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China, Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China; Ying Y., College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China, Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China, Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China; Zhou M., College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China, Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China, Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China; Pan J., College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China, Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China, Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China; Cui D., College of Biosystems Engineering and Food Science, Zhejiang University, Zhejiang, Hangzhou, 310058, China, Key Laboratory of Intelligent Equipment and Robotics for Agriculture of Zhejiang Province, Science Technology Department of Zhejiang Province, China, Key Laboratory of On-Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, China","Reliable chicken claw detection is the technical prerequisite for further intelligent sensing of the health status and physiological information of their feet in precision livestock farming (PLF). Towards commercial farm scenarios, YOLO-Claw automatic chicken claw detection algorithm was proposed in this study, which was inspired by YOLOv5 deep learning framework. Specifically, the Tanh-aided Channel Attention (TCA) and C3TCA modules were designed firstly to enhance feature extraction. Moreover, to enrich useful features for YOLO-Claw detection head, a new AFPN-Neck was proposed, which can progressively fuse non-adjacent layer low-level features with high-level features and avoid semantic gaps. Furthermore, a total of 7170 images containing real-world disturbances were obtained from a monitoring platform built on a commercial layer farm for model training and testing. Both comparative experiments and ablation tests were performed to validate the effectiveness of the TCA as well as the contribution of C3TCA, and AFPN-Neck. In addition, the proposed YOLO-Claw was compared with four popular object detection algorithms of the YOLO family including YOLOv5, YOLOv6, YOLOv7, and YOLOv8, as well as six State-of-the-Art (SOTA) methods. The results showed that the proposed TCA contributed 0.5%–1.3% more than Squeeze and Excitation (SE), Convolutional Block Attention Module (CBAM), Efficient Channel Attention (ECA), and Pyramid Split Attention (PSA) to the mAP with minimal size and computations. In addition, C3TCA and AFPN-Neck enhanced the mAP by 0.22% and 0.30%, respectively. Further, the proposed YOLO-Claw outperformed the ten comparison methods with Precision, Recall, mAP, Speed, Model Size, and Computation of 95.3%, 93.6%, and 97.1%, 7.6 ms/frame, 12.1 MB, and 14.3 GFLOPs, respectively. Meanwhile, the test results on illumination condition, claw size and morphology variations revealed that YOLO-Claw could detect claws well in commercial farm scenarios. The code will be released on GitHub (https://github.com/PuristWu/YOLO-Claw). A video demo is provided to facilitate the reviewers’ review (YouTube:https://www.youtube.com/watch?v=f_NXkoem-bc BiliBili: https://www.bilibili.com/video/BV13N41137Ko). © 2024 Elsevier Ltd","Computer vision; Deep learning; Layer claw detection; Precision livestock farming; YOLO-Claw","Animals; Computer vision; Deep learning; Farms; Learning systems; Object detection; Semantics; Signal detection; Commercial farms; Deep learning; Detection algorithm; Health status; Intelligent sensing; Layer claw detection; Physiological informations; Precision livestock farming; Status informations; YOLO-claw; Feature extraction","CARS-40; Agriculture Research System of China; Fundamental Research Funds for the Central Universities; Key Research and Development Program of Zhejiang Province, (2021C02026); Key Research and Development Program of Zhejiang Province","This work was supported by the Key R&D Program of Zhejiang Province (No. 2021C02026), the Fundamental Research Funds for the Central University (No.226-2022-00067), and the China Agriculture Research System (CARS-40). The authors appreciate the funding organization for its financial support. The authors would also like to thank Mr. Junyue Yang for providing the experimental site, Dr. Jintian Chen for his help in setting up the monitoring platform, and all the authors cited in this paper and the anonymous reviewers for their helpful comments and suggestions.","D. Cui; College of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, Zhejiang, 310058, China; email: dicui@zju.edu.cn","","Elsevier Ltd","09521976","","EAAIE","","English","Eng Appl Artif Intell","Article","Final","","Scopus","2-s2.0-85197747249"
"Ocholla I.A.; Pellikka P.; Karanja F.; Vuorinne I.; Väisänen T.; Boitt M.; Heiskanen J.","Ocholla, Ian A. (58829676200); Pellikka, Petri (7007042259); Karanja, Faith (22134890000); Vuorinne, Ilja (57221478085); Väisänen, Tuomas (57222485852); Boitt, Mark (57193012292); Heiskanen, Janne (9842163500)","58829676200; 7007042259; 22134890000; 57221478085; 57222485852; 57193012292; 9842163500","Livestock Detection and Counting in Kenyan Rangelands Using Aerial Imagery and Deep Learning Techniques","2024","Remote Sensing","16","16","2929","","","","1","10.3390/rs16162929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202453125&doi=10.3390%2frs16162929&partnerID=40&md5=dd9f2500b6c26b67a98ee6985feeed6b","Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland; Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Wangari Maathai Institute for Environmental and Peace Studies, University of Nairobi, P.O. Box 29053-00625, Nairobi, Kenya; Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197-00100, Nairobi, Kenya; Helsinki Institute of Sustainability Science, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Helsinki Institute of Urban and Regional Studies, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Institute of Geomatics, GIS and Remote Sensing, Dedan Kimathi University of Technology, Private Bag, Dedan KimathiP.O. Box 10143-10100, Nyeri, Kenya; Finnish Meteorological Institute, P.O. Box 503, Helsinki, 00101, Finland","Ocholla I.A., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Pellikka P., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland, Wangari Maathai Institute for Environmental and Peace Studies, University of Nairobi, P.O. Box 29053-00625, Nairobi, Kenya; Karanja F., Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197-00100, Nairobi, Kenya; Vuorinne I., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Institute for Atmospheric and Earth System Research, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Väisänen T., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Helsinki Institute of Sustainability Science, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland, Helsinki Institute of Urban and Regional Studies, University of Helsinki, P.O. Box 4, Helsinki, 00014, Finland; Boitt M., Institute of Geomatics, GIS and Remote Sensing, Dedan Kimathi University of Technology, Private Bag, Dedan KimathiP.O. Box 10143-10100, Nyeri, Kenya; Heiskanen J., Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, Helsinki, 00014, Finland, Finnish Meteorological Institute, P.O. Box 503, Helsinki, 00101, Finland","Accurate livestock counts are essential for effective pastureland management. High spatial resolution remote sensing, coupled with deep learning, has shown promising results in livestock detection. However, challenges persist, particularly when the targets are small and in a heterogeneous environment, such as those in African rangelands. This study evaluated nine state-of-the-art object detection models, four variants each from YOLOv5 and YOLOv8, and Faster R-CNN, for detecting cattle in 10 cm resolution aerial RGB imagery in Kenya. The experiment involved 1039 images with 9641 labels for training from sites with varying land cover characteristics. The trained models were evaluated on 277 images and 2642 labels in the test dataset, and their performance was compared using Precision, Recall, and Average Precision (AP0.5–0.95). The results indicated that reduced spatial resolution, dense shrub cover, and shadows diminish the model’s ability to distinguish cattle from the background. The YOLOv8m architecture achieved the best AP0.5–0.95 accuracy of 39.6% with Precision and Recall of 91.0% and 83.4%, respectively. Despite its superior performance, YOLOv8m had the highest counting error of −8%. By contrast, YOLOv5m with AP0.5–0.95 of 39.3% attained the most accurate cattle count with RMSE of 1.3 and R2 of 0.98 for variable cattle herd densities. These results highlight that a model with high AP0.5–0.95 detection accuracy may struggle with counting cattle accurately. Nevertheless, these findings suggest the potential to upscale aerial-imagery-trained object detection models to satellite imagery for conducting cattle censuses over large areas. In addition, accurate cattle counts will support sustainable pastureland management by ensuring stock numbers do not exceed the forage available for grazing, thereby mitigating overgrazing. © 2024 by the authors.","aerial survey; deep learning; livestock census; object detection; remote sensing","Remote sensing; Aerial imagery; Aerial surveys; Cattles; Deep learning; Detection models; Learning techniques; Livestock census; Objects detection; Performance; Remote-sensing; Aerial photography","Helsingin Yliopisto, HY; European Commission, EC, (FOOD/2020/418-132); European Commission, EC","This research was funded by European Union DG International Partnerships under DeSIRA (Development of Smart Innovation through Research in Agriculture) programme (FOOD/2020/418-132) through the Earth observation and environmental sensing for climate-smart sustainable agropastoral ecosystem transformation in East Africa (ESSA) project. Open access funding provided by University of Helsinki.","I.A. Ocholla; Department of Geosciences and Geography, University of Helsinki, Helsinki, P.O. Box 64, 00014, Finland; email: ian.ocholla@helsinki.fi","","Multidisciplinary Digital Publishing Institute (MDPI)","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85202453125"
"Ahmed M.T.; Ahmed M.W.; Monjur O.; Emmert J.L.; Chowdhary G.; Kamruzzaman M.","Ahmed, Md. Toukir (57679639500); Ahmed, Md Wadud (57387937600); Monjur, Ocean (57254194500); Emmert, Jason Lee (7005159616); Chowdhary, Girish (8724723800); Kamruzzaman, Mohammed (56400584900)","57679639500; 57387937600; 57254194500; 7005159616; 8724723800; 56400584900","Hyperspectral image reconstruction for predicting chick embryo mortality towards advancing egg and hatchery industry","2024","Smart Agricultural Technology","9","","100533","","","","4","10.1016/j.atech.2024.100533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201392052&doi=10.1016%2fj.atech.2024.100533&partnerID=40&md5=70cbf2d6fa88988d5d56ebb22a8fc286","Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Department of Animal Sciences, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States","Ahmed M.T., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Ahmed M.W., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Monjur O., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Emmert J.L., Department of Animal Sciences, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Chowdhary G., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States; Kamruzzaman M., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, United States","As the demand for food surges and the agricultural sector undergoes a transformative shift towards sustainability and efficiency, the need for precise and proactive measures to ensure the health and welfare of livestock becomes paramount. In the egg and hatchery industry, hyperspectral imaging (HSI) has emerged as a cutting-edge, non-destructive technique for fast and accurate egg quality analysis, including detecting chick embryo mortality. However, the high cost and operational complexity compared to conventional RGB imaging are significant bottlenecks in the widespread adoption of HSI technology. To overcome these hurdles and unlock the full potential of HSI, a promising solution is hyperspectral image reconstruction from standard RGB images. This study aims to reconstruct hyperspectral images from RGB images for non-destructive early prediction of chick embryo mortality. Initially, the performance of different image reconstruction algorithms, such as HRNET, MST++, Restormer, and EDSR were compared to reconstruct the hyperspectral images of the eggs in the early incubation period. Later, the reconstructed spectra were used to differentiate live from dead embryos eggs using the XGBoost and Random Forest classification methods. Among the reconstruction methods, HRNET showed impressive reconstruction performance with MRAE of 0.0955, RMSE of 0.0159, and PSNR of 36.79 dB. This study motivated the idea that harnessing imaging technology integrated with smart sensors and data analytics has the potential to improve automation, enhance biosecurity, and optimize resource management towards sustainable agriculture 4.0. © 2024 The Authors","Agriculture 4.0; Classification; Deep learning; Embryo mortality; Hyperspectral imaging; Image reconstruction","","National Institute of Food and Agriculture, NIFA, (2023-67015-39154); National Institute of Food and Agriculture, NIFA","This work was supported by the USDA National Institute of Food and Agriculture , Award # 2023-67015-39154 . ","M. Kamruzzaman; Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, United States; email: mkamruz1@illinois.edu","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85201392052"
"Ide T.; Oguma A.; Arakawa T.","Ide, Tatsuki (59312759300); Oguma, Atsuko (59313070700); Arakawa, Toshiya (59312863000)","59312759300; 59313070700; 59312863000","Development of a Technology for Detecting Straining in Cows During Labor Using Deep Learning; [深層学習を用いた乳牛の分娩時「いきみ」検知技術の開発]","2024","IEEJ Transactions on Electronics, Information and Systems","144","9","","918","925","7","0","10.1541/ieejeiss.144.918","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203047148&doi=10.1541%2fieejeiss.144.918&partnerID=40&md5=1fdf71c6885c4ee7d39192914098fb45","Industrial Research Institute of Shizuoka Prefecture Fuji Technical Support Center, 2590-1, Obuchi, Shizuoka, Fuji, 417-0801, Japan; Nippon Institute of Technology, 4-1, Gakuendai, Miyashiro-machi, Minamisaitama-gun, Saitama, 345-8501, Japan; Seibu Livestock Disease Diagnostic Center of Shizuoka Prefecture, 392, Nakagori-chou, Chuo-ku, Shizuoka, Hamamatsu, 431-3111, Japan","Ide T., Industrial Research Institute of Shizuoka Prefecture Fuji Technical Support Center, 2590-1, Obuchi, Shizuoka, Fuji, 417-0801, Japan, Nippon Institute of Technology, 4-1, Gakuendai, Miyashiro-machi, Minamisaitama-gun, Saitama, 345-8501, Japan; Oguma A., Seibu Livestock Disease Diagnostic Center of Shizuoka Prefecture, 392, Nakagori-chou, Chuo-ku, Shizuoka, Hamamatsu, 431-3111, Japan; Arakawa T., Nippon Institute of Technology, 4-1, Gakuendai, Miyashiro-machi, Minamisaitama-gun, Saitama, 345-8501, Japan","In the dairy farming sector, the need for efficient individual cow management via ICT has intensified owing to a declining workforce in the industry and a simultaneous increase in livestock numbers. To address challenges such as reducing night-time cow-monitoring hours for workers and preventing calving accidents, we developed a system capable of automatically detecting straining during labor (a key indicator of impending cow calving). Our approach involved collecting waveform data on cow movements from an unrestrained cow positioned on a sensor sheet. The collected data were subsequently analyzed using deep learning techniques. Employing a sample of 40 cows, veterinarians correlated data collected using the sensor sheet with those collected using a video camera, classifying the data into straining and other movements unrelated to calving. This curated dataset was then used to train a staining detection model using a convolutional neural network, the accuracy of which was verified. Consequently, we successfully used the staining detection model to predict cow calving with a high accuracy rate of > 95%. © 2024 The Institute of Electrical Engineers of Japan.","convolutional neural network (CNN); cows; detection; non-restraint; straining","Fertilizers; Convolutional neural network; Cow; Cow management; Dairy farming; Detection; Detection models; Non-restraint; Sensor sheet; Straining; Convolutional neural networks","","","T. Ide; Industrial Research Institute of Shizuoka Prefecture Fuji Technical Support Center, Fuji, 2590-1, Obuchi, Shizuoka, 417-0801, Japan; email: tatsuki1_ide@pref.shizuoka.lg.jp","","Institute of Electrical Engineers of Japan","03854221","","","","Japanese","IEEJ Trans. Electron. Inf. Syst.","Article","Final","","Scopus","2-s2.0-85203047148"
"Kim D.; Kim H.; Hwang M.; Lee Y.; Min C.; Yoon S.; Seo S.","Kim, Dohyeong (7409759970); Kim, Heeseok (55812270800); Hwang, Minseon (59532335500); Lee, Yongchan (57222541004); Min, Choongki (57477504600); Yoon, Sungwon (55232199700); Seo, Sungchul (8688348600)","7409759970; 55812270800; 59532335500; 57222541004; 57477504600; 55232199700; 8688348600","Enhancing Particulate Matter Estimation in Livestock-Farming Areas with a Spatiotemporal Deep Learning Model","2025","Atmosphere","16","1","12","","","","0","10.3390/atmos16010012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216261560&doi=10.3390%2fatmos16010012&partnerID=40&md5=6b67b9341658327b45f23fa3e7942096","School of Economic, Political and Policy Sciences, University of Texas at Dallas, Dallas, 75080-3021, TX, United States; Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea; Waycen, Inc., Seoul, 06009, South Korea; Department of Environmental and Energy Engineering, Graduate School, Incheon National University, Inchoen, 22012, South Korea","Kim D., School of Economic, Political and Policy Sciences, University of Texas at Dallas, Dallas, 75080-3021, TX, United States; Kim H., Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea; Hwang M., Waycen, Inc., Seoul, 06009, South Korea; Lee Y., Department of Environmental and Energy Engineering, Graduate School, Incheon National University, Inchoen, 22012, South Korea; Min C., Waycen, Inc., Seoul, 06009, South Korea; Yoon S., Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea; Seo S., Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea","Livestock farms are recognized sources of ammonia emissions, impacting nearby regions’ fine dust particle concentrations, though the full extent of this impact remains uncertain. Air dispersion models, commonly employed to estimate particulate matter (PM) levels, are heavily reliant on data quality, resulting in varying levels of accuracy. This study compares the performance of both air dispersion models and spatiotemporal deep learning models in estimating PM concentrations in Republic of Korea’s livestock-farming areas. Hourly PM concentration data, alongside temperature, humidity, and air pressure, were collected from seven monitoring stations across the study area. Using a 200 m × 200 m prediction grid, forecasts were generated for both 1 h and 24 h intervals using the Graz Lagrangian model (GRAL) and a one-dimensional convolutional neural network combined with the long short-term memory algorithm (1DCNN-LSTM). Results highlight the potential of the deep learning model to enhance PM prediction, indicating its promise as an effective alternative or supplement to conventional air dispersion models, particularly in data-scarce areas such as those surrounding livestock farms. Gaining a comprehensive understanding and evaluating the advantages and disadvantages of each approach would offer valuable scientific insights for monitoring atmospheric pollution levels within a specific area. © 2024 by the authors.","air dispersion model; deep learning; livestock-farming areas; particulate matter; spatiotemporal prediction","South Korea; Air dispersion model; Ammonia emissions; Deep learning; Dust particle; Fine dusts; Learning models; Livestock farming; Livestock-farming area; Particulate Matter; Spatio-temporal prediction; algorithm; ammonia; artificial intelligence; artificial neural network; atmospheric pollution; data quality; livestock farming; particulate matter; pollution monitoring; spatiotemporal analysis; Atmospheric pressure","","","S. Yoon; Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea; email: ss5372@skuniv.ac.kr; S. Seo; Department of Nano, Chemical & Biological Engineering, College of Natural Science & Engineering, Seokyeong University, Seoul, 06115, South Korea; email: haha0694@gmail.com","","Multidisciplinary Digital Publishing Institute (MDPI)","20734433","","","","English","Atmosphere","Article","Final","","Scopus","2-s2.0-85216261560"
"Yang L.; Zhao J.; Ying X.; Lu C.; Zhou X.; Gao Y.; Wang L.; Liu H.; Song H.","Yang, Lingling (58631301000); Zhao, Jizheng (56144394200); Ying, Xiaoyi (59225219600); Lu, Cheng (57231688100); Zhou, Xinyi (59224625000); Gao, Yannian (59224477000); Wang, Lei (59140157500); Liu, Han (59224923800); Song, Huaibo (17342958900)","58631301000; 56144394200; 59225219600; 57231688100; 59224625000; 59224477000; 59140157500; 59224923800; 17342958900","Utilization of deep learning models to predict calving time in dairy cattle from tail acceleration data","2024","Computers and Electronics in Agriculture","225","","109253","","","","1","10.1016/j.compag.2024.109253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198978250&doi=10.1016%2fj.compag.2024.109253&partnerID=40&md5=8bc177dd311361f68d4bc12bcafe2c04","College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Yang L., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Zhao J., School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; Ying X., Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Lu C., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Zhou X., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Gao Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Wang L., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Liu H., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Song H., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China","Calving monitoring in dairy cattle is a crucial task for modern farms. Accurate prediction of calving time is a prerequisite for timely and effective calving management. Recently, deep learning models using behavioral data have been proposed to predict calving in dairy cattle. However, few studies have utilized deep learning models on tail acceleration data to directly predict calving in dairy cattle. This study assessed the performance of the convolutional neural network (CNN), long short-term memory (LSTM), CNN-LSTM, and CNN based bidirectional long short-term attention (CNN-BiLSTM-attention) models using tail acceleration data to predict dairy cattle calving. We collected tail acceleration data from 25 cows for model training and obtained tail acceleration data from 11 additional cows for model testing. Given that the tail acceleration data at the prenatal stage were less than the data at the nonprenatal stage, a time-series generative adversarial networks (TimeGAN) was used to augment the amount of data at the prenatal stage. The performance of each model during the training phase was evaluated, and the differences between the calving alert-generation times and the actual calving times were compared. The CNN, LSTM, CNN-LSTM, and CNN-BiLSTM-attention models achieved precision values of 0.8537, 0.8605, 0.8861, and 0.9147; recall values of 0.9085, 0.9427, 0.9455, and 0.9459; and F1 scores of 0.8803, 0.8997, 0.9148, and 0.9300, respectively. The four models generated the first alert within 12 h before calving for 64 %, 73 %, 91 %, and 100 % of the total 11 cows. When trained using one-dimensional tail acceleration data (parallel to the tail direction of the cow and pointed downward), the CNN–BiLSTM–attention model achieved a precision of 0.8427, a recall of 0.8219, and an F1 score of 0.8322. These preliminary results indicate the feasibility of utilizing deep learning models with tail acceleration data to predict dairy cattle calving time, providing methods to optimize calving management and improve animal monitoring. © 2024","Accelerometer; Calving prediction; Dairy cattle monitoring; Precision livestock farming; Sequential models","Convolutional neural networks; Farms; Generative adversarial networks; Learning systems; Long short-term memory; Acceleration data; Attention model; Calfing prediction; Cattle monitoring; Convolutional neural network; Dairy cattle monitoring; Dairy cattles; Learning models; Precision livestock farming; Sequential modeling; artificial neural network; cattle; data assimilation; livestock farming; machine learning; monitoring system; prediction; Forecasting","Shaanxi Provincial Technology Innovation Guidance Planned Program, (2022QFY11-02)","This work was supported by the Shaanxi Provincial Technology Innovation Guidance Planned Program (2022QFY11-02).","J. Zhao; School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, China; email: zhaojizheng@nwsuaf.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85198978250"
"Ma N.; Cao S.; Bai T.; Kong F.; Sun W.","Ma, Nan (59507161700); Cao, Shanshan (57217367100); Bai, Tao (57221446308); Kong, Fantao (59526586900); Sun, Wei (57208906031)","59507161700; 57217367100; 57221446308; 59526586900; 57208906031","Research Progress and Prospect of Multi-robot Collaborative SLAM in Complex Agricultural Scenarios; [农业复杂场景下多机器人协同 SLAM 研究进展与展望]","2024","Smart Agriculture","6","6","","23","43","20","0","10.12133/j.smartag.SA202406005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215942896&doi=10.12133%2fj.smartag.SA202406005&partnerID=40&md5=0af405d33dd1fef9d5cb32910dedd704","College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China; Agricultural Information Institute of CAAS, Beijing, 100081, China; National Agriculture Science Data Center, Beijing, 100081, China; Engineering Research Center of Intelligent Agriculture Ministry of Education, Urumqi, 830052, China; Xinjiang Agricultural Informatization Engineering Technology Research Center, Urumqi, 830052, China; Institute of Agricultural Economics and development, Beijing, 100081, China","Ma N., College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China, Agricultural Information Institute of CAAS, Beijing, 100081, China, Engineering Research Center of Intelligent Agriculture Ministry of Education, Urumqi, 830052, China, Xinjiang Agricultural Informatization Engineering Technology Research Center, Urumqi, 830052, China; Cao S., Agricultural Information Institute of CAAS, Beijing, 100081, China, National Agriculture Science Data Center, Beijing, 100081, China; Bai T., College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China, Engineering Research Center of Intelligent Agriculture Ministry of Education, Urumqi, 830052, China, Xinjiang Agricultural Informatization Engineering Technology Research Center, Urumqi, 830052, China; Kong F., Institute of Agricultural Economics and development, Beijing, 100081, China; Sun W., Agricultural Information Institute of CAAS, Beijing, 100081, China, National Agriculture Science Data Center, Beijing, 100081, China","[Significance] The rapid development of artificial intelligence and automation has greatly expanded the scope of agricultural automation, with applications such as precision farming using unmanned machinery, robotic grazing in outdoor environments, and automated harvesting by orchard-picking robots. Collaborative operations among multiple agricultural robots enhance production efficiency and reduce labor costs, driving the development of smart agriculture. Multi-robot simultaneous localization and mapping (SLAM) plays a pivotal role by ensuring accurate mapping and localization, which are essential for the effective management of unmanned farms. Compared to single-robot SLAM, multi-robot systems offer several advantages, including higher localization accuracy, larger sensing ranges, faster response times, and improved real-time performance. These capabilities are particularly valuable for completing complex tasks efficiently. However, deploying multi-robot SLAM in agricultural settings presents significant challenges. Dynamic environmental factors, such as crop growth, changing weather patterns, and livestock movement, increase system uncertainty. Additionally, agricultural terrains vary from open fields to irregular greenhouses, requiring robots to adjust their localization and path-planning strategies based on environmental conditions. Communication constraints, such as unstable signals or limited transmission range, further complicate coordination between robots. These combined challenges make it difficult to implement multi-robot SLAM effectively in agricultural environments. To unlock the full potential of multi-robot SLAM in agriculture, it is essential to develop optimized solutions that address the specific technical demands of these scenarios. [Progress] Existing review studies on multi-robot SLAM mainly focus on a general technological perspective, summarizing trends in the development of multi-robot SLAM, the advantages and limitations of algorithms, universally applicable conditions, and core issues of key technologies. However, there is a lack of analysis specifically addressing multi-robot SLAM under the characteristics of complex agricultural scenarios. This study focuses on the main features and applications of multi-robot SLAM in complex agricultural scenarios. The study analyzes the advantages and limitations of multi-robot SLAM, as well as its applicability and application scenarios in agriculture, focusing on four key components: multi-sensor data fusion, collaborative localization, collaborative map building, and loopback detection. From the perspective of collaborative operations in multi-robot SLAM, the study outlines the classification of SLAM frameworks, including three main collaborative types: centralized, distributed, and hybrid. Based on this, the study summarizes the advantages and limitations of mainstream multi-robot SLAM frameworks, along with typical scenarios in robotic agricultural operations where they are applicable. Additionally, it discusses key issues faced by multi-robot SLAM in complex agricultural scenarios, such as low accuracy in mapping and localization during multi-sensor fusion, restricted communication environments during multi-robot collaborative operations, and low accuracy in relative pose estimation between robots. [Conclusions and Prospects] To enhance the applicability and efficiency of multi-robot SLAM in complex agricultural scenarios, future research needs to focus on solving these critical technological issues. Firstly, the development of enhanced data fusion algorithms will facilitate improved integration of sensor information, leading to greater accuracy and robustness of the system. Secondly, the combination of deep learning and reinforcement learning techniques is expected to empower robots to better interpret environmental patterns, adapt to dynamic changes, and make more effective real-time decisions. Thirdly, large language models will enhance human-robot interaction by enabling natural language commands, improving collaborative operations. Finally, the integration of digital twin technology will support more intelligent path planning and decision-making processes, especially in unmanned farms and livestock management systems. The convergence of digital twin technology with SLAM is projected to yield innovative solutions for intelligent perception and is likely to play a transformative role in the realm of agricultural automation. This synergy is anticipated to revolutionize the approach to agricultural tasks, enhancing their efficiency and reducing the reliance on labor. © 2024 Agricultural Information Institute, Chinese Academy of Agricultural Sciences. All rights reserved.","agricultural complex scene; collaborative framework; collaborative mapping; collaborative positioning; multi-robot SLAM algorithm","","","","W. Sun; Agricultural Information Institute of CAAS, Beijing, 100081, China; email: sunwei02@caas.cn","","Agricultural Information Institute, Chinese Academy of Agricultural Sciences","20968094","","","","Chinese","Smart. Agric.","Article","Final","","Scopus","2-s2.0-85215942896"
"Yang X.; Bahadur Bist R.; Paneru B.; Liu T.; Applegate T.; Ritz C.; Kim W.; Regmi P.; Chai L.","Yang, Xiao (57743060600); Bahadur Bist, Ramesh (59128443800); Paneru, Bidur (57216784609); Liu, Tianming (58741130700); Applegate, Todd (7003993401); Ritz, Casey (7004847094); Kim, Woo (58734421000); Regmi, Prafulla (56849288800); Chai, Lilong (57222280822)","57743060600; 59128443800; 57216784609; 58741130700; 7003993401; 7004847094; 58734421000; 56849288800; 57222280822","Computer Vision-Based cybernetics systems for promoting modern poultry Farming: A critical review","2024","Computers and Electronics in Agriculture","225","","109339","","","","5","10.1016/j.compag.2024.109339","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201615527&doi=10.1016%2fj.compag.2024.109339&partnerID=40&md5=d476f3ece449b04ef8db874d1277053a","Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; School of Computing, University of Georgia, Athens, 30602, GA, United States","Yang X., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Bahadur Bist R., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Paneru B., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Liu T., School of Computing, University of Georgia, Athens, 30602, GA, United States; Applegate T., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Ritz C., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Kim W., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Regmi P., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Chai L., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States","As global demands on the poultry production and welfare both intensify, the precision poultry farming technologies such as computer vision-based cybernetics system is becoming important in addressing the current issues related to animal welfare and production efficiencies. The integration of computer vision technology has become a catalyst for transformative change in precision farming, particularly concerning productivity and welfare. This review paper delineates the central role of computer vision in precision poultry farming, focusing on its applications in non-contact monitoring methods that employ advanced sensors and cameras to enhance farm biosecurity and bird observation without disturbance. We delved into the multifaceted advancements such as the utilization of convolutional neural networks (CNNs) for behavior analysis and health monitoring, evidenced by the high accuracy sorting of eggs and identification of health concerns within target-dense farm environments. The review paper underscores advancements in precision agriculture, including accurate egg weight estimation and egg classification within cage-free systems, paralleling the poultry sector's evolution towards more ethical farming practices. Moreover, it addresses the progress in poultry growth monitoring and examines case studies of commercial farms, showcasing how these innovations are being practically applied to enhance productivity and animal welfare. Challenges remain, particularly in terms of environmental variability and data annotation for deep learning models. Nevertheless, the review emphasizes the scope for future innovations like voice-controlled robotics and virtual reality applications, which have the potential to enhance poultry farming to new levels of efficiency, humanity, and sustainability. The insights assert that the continued exploration and development in computer vision technologies are not only instrumental for the poultry sector but also offer a blueprint for agricultural enhancement at large. © 2024 Elsevier B.V.","Animal welfare; Computer vision; Deep learning; Poultry production","'current; Animal welfare; Computer vision technology; Critical review; Deep learning; Global demand; Poultry production; Poultry sector; Review papers; Vision based; animal welfare; catalyst; clonal growth; computer vision; evolution; exploration; growth rate; poultry; Livestock","UGA IIPA; Georgia Research Alliance, GRA; USDA-NIFA AFRI, (2023-68008-39853); Oracle America, (CPQ-2060433)","The study was sponsored by the USDA-NIFA AFRI (2023-68008-39853), Georgia Research Alliance (Venture Fund), Oracle America (Oracle for Research Grant, CPQ-2060433), and UGA IIPA Equipment grant.","L. Chai; Department of Poultry Science, University of Georgia, Athens, 30602, United States; email: chai@uga.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85201615527"
"Cotillard T.; Sécheresse X.; Aubin J.; Mikus M.-A.; Vergara V.; Gambs S.; Michaud R.; Martins C.C.A.; Turgeon S.; Chion C.; Roca I.","Cotillard, Tristan (59463424800); Sécheresse, Xavier (59463424900); Aubin, Jaclyn (57225325689); Mikus, Marie-Ana (55579789400); Vergara, Valeria (25639077100); Gambs, Sébastien (56025052200); Michaud, Robert (7004392242); Martins, Cristiane C. A. (42561521600); Turgeon, Samuel (24469307700); Chion, Clément (15041780800); Roca, Irene (56583952100)","59463424800; 59463424900; 57225325689; 55579789400; 25639077100; 56025052200; 7004392242; 42561521600; 24469307700; 15041780800; 56583952100","Automatic detection and classification of beluga whale calls in the St. Lawrence estuary","2024","Journal of the Acoustical Society of America","156","6","","3723","3740","17","0","10.1121/10.0030472","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211357543&doi=10.1121%2f10.0030472&partnerID=40&md5=56936e61a8c40ef24a57814f17dc2d56","Department of Natural Sciences, Université du Quebec en Outaouais, Gatineau, QC, Canada; Mines Paris, Paris Sciences et Lettres University, Paris, France; Raincoast Conservation Foundation, Victoria, BC, Canada; Université du Québec à Montréal, Montréal, QC, Canada; Groupe de Recherche et d'Éducation sur les Mammifères Marins, Tadoussac, QC, Canada; Parks Canada, Gatineau, QC, Canada","Cotillard T., Department of Natural Sciences, Université du Quebec en Outaouais, Gatineau, QC, Canada, Mines Paris, Paris Sciences et Lettres University, Paris, France; Sécheresse X., Department of Natural Sciences, Université du Quebec en Outaouais, Gatineau, QC, Canada, Mines Paris, Paris Sciences et Lettres University, Paris, France; Aubin J., Raincoast Conservation Foundation, Victoria, BC, Canada; Mikus M.-A., Raincoast Conservation Foundation, Victoria, BC, Canada; Vergara V., Raincoast Conservation Foundation, Victoria, BC, Canada; Gambs S., Université du Québec à Montréal, Montréal, QC, Canada; Michaud R., Groupe de Recherche et d'Éducation sur les Mammifères Marins, Tadoussac, QC, Canada; Martins C.C.A., Parks Canada, Gatineau, QC, Canada; Turgeon S., Parks Canada, Gatineau, QC, Canada; Chion C., Department of Natural Sciences, Université du Quebec en Outaouais, Gatineau, QC, Canada; Roca I., Department of Natural Sciences, Université du Quebec en Outaouais, Gatineau, QC, Canada","The endangered beluga whale (Delphinapterus leucas) of the St. Lawrence Estuary (SLEB) faces threats from a variety of anthropogenic factors. Since belugas are a highly social and vocal species, passive acoustic monitoring has the potential to deliver, in a non-invasive and continuous way, real-time information on SLEB spatiotemporal habitat use, which is crucial for their monitoring and conservation. In this study, we introduce an automatic pipeline to analyze continuous passive acoustic data and provide standard and accurate estimations of SLEB acoustic presence and vocal activity. An object detector extracted vocalizations of beluga whales from an acoustic recording of beluga vocal activity. Then, two deep learning classifiers discriminated between high-frequency call types (40-120 kHz) and the presence of low-frequency components (0-20 kHz), respectively. Different algorithms were tested for each step and their main combinations were compared in time and performance. We focused our work on a high residency area, Baie Sainte-Marguerite (BSM), used for socialization and feeding by SLEB. Overall, this project showed that accurate continuous analysis of SLEB vocal activity at BSM could provide valuable information to estimate habitat use, link beluga behavior and acoustic activity within and between herds, and quantify beluga presence and abundance. © 2024 Author(s).","","Acoustics; Algorithms; Animals; Beluga Whale; Deep Learning; Endangered Species; Estuaries; Signal Processing, Computer-Assisted; Sound Spectrography; Vocalization, Animal; Abiotic; Invertebrates; Livestock; Anthropogenic factors; Automatic classification; Automatic Detection; Beluga whales; Delphinapterus leucas; Habitat use; Lawrence estuaries; Passive acoustic monitoring; Real-time information; Whale calls; algorithm; article; beluga whale; classification; classifier; deep learning; estuary; feeding; habitat use; herd; human impact (environment); nonhuman; socialization; vocalization; acoustics; animal; classification; endangered species; physiology; signal processing; sound detection; vocalization; Anthropogenic","","","","","Acoustical Society of America","00014966","","JASMA","39636175","English","J. Acoust. Soc. Am.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85211357543"
"Peng Y.; Chen Y.; Yang Y.; Liu M.; Hu R.; Zou H.; Xiao J.; Jiang Y.; Wang Z.; Xu L.","Peng, Yingqi (57205304515); Chen, Yingxi (59337514500); Yang, Yuxiang (59231423000); Liu, Meiqi (57645782800); Hu, Rui (57169447400); Zou, Huawei (55272415000); Xiao, Jianxin (57189046242); Jiang, Yahui (55733627500); Wang, Zhisheng (24476765500); Xu, Lijia (55798972500)","57205304515; 59337514500; 59231423000; 57645782800; 57169447400; 55272415000; 57189046242; 55733627500; 24476765500; 55798972500","A multimodal classification method: Cow behavior pattern classification with improved EdgeNeXt using an inertial measurement unit","2024","Computers and Electronics in Agriculture","226","","109453","","","","1","10.1016/j.compag.2024.109453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204597858&doi=10.1016%2fj.compag.2024.109453&partnerID=40&md5=38e67eb136215a912c5c5701579116fd","College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China; Animal Nutrition Institute, Sichuan Agricultural University, China; College of Animal Science and Technology, Sichuan Agricultural University, China","Peng Y., College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China; Chen Y., College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China; Yang Y., College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China; Liu M., College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China; Hu R., Animal Nutrition Institute, Sichuan Agricultural University, China; Zou H., Animal Nutrition Institute, Sichuan Agricultural University, China; Xiao J., Animal Nutrition Institute, Sichuan Agricultural University, China; Jiang Y., College of Animal Science and Technology, Sichuan Agricultural University, China; Wang Z., Animal Nutrition Institute, Sichuan Agricultural University, China; Xu L., College of Mechanical and Electrical Engineering, Sichuan Agricultural University, China","In this study, a dynamic cow behavior pattern classification model using an improved EdgeNeXt network with inertial measurement unit (IMU) multimodal data was developed. This model was trained using three modalities of the IMU data: acceleration, angular velocity, and magnetic field. To augment the spatial connection among the three modalities, the nine-axis IMU data were extracted into matrices and then transformed into images. The improved EdgeNeXt model was trained to classify cow behavior patterns using two batch sizes: 128 and 256. Two EdgeNeXt models and the Swin Transformer, MobileNetV2, and ConvNeXt models were also trained for comparison. The results of the improved EdgeNeXt classification model were superior to those of the other five algorithms. The best overall classification accuracy of the improved EdgeNeXt model was 95.85 % with a batch size of 256. In this model, the classification accuracy of particular behavior patterns was 95.2 % (feeding), 95.6 % (lying), 96.8 % (ruminating–lying), 97.6 % (rub scratching (legs)), 95.5 % (social licking), and 94.4 % (rub scratching (neck)). The limitation of this study is that the EdgeNeXt model relies more on global feature extraction, resulting in some misclassifications between behavior patterns and similar movements. The continuous operation and data transformation of the IMU limit the battery life. In the future, the lightweight EdgeNeXt cow behavior classification model will be applied to edge computing devices in livestock farms to improve computational efficiency. Moreover, multimodal data, such as video and acoustics, will be added to train the cow behavior pattern classification model to extend the scope of livestock monitoring. © 2024 Elsevier B.V.","Cow behavior classification; Deep learning; Improved EdgeNeXt; IMU; Multimodal","Behaviour classification; Behaviour patterns; Classification models; Cow behavior; Cow behavior classification; Deep learning; Improved edgenext; Inertial measurements units; Multi-modal; Patterns classification; cattle; classification; data transmission; magnetic field; measurement method; model validation; pattern recognition","National Key Research and Development Program of China, NKRDPC, (2021YFD1600200); National Key Research and Development Program of China, NKRDPC; Sichuan Agricultural University, SICAU, (T202107, 20200057); Sichuan Agricultural University, SICAU","This study was supported by National Key R&D Program of China (Grant Number 2021YFD1600200) and subject double support program of Sichuan Agriculture University (Grant Number T202107). This work was supported by Sichuan Agricultural University Institutional Animal Care and Use Committee [Approval No. 20200057].","L. Xu; College of Mechanical and Electrical Engineering, Sichuan Agricultural University, Ya'an, No. 46 Xinkang Road, Yucheng District, 625014, China; email: xulijia@sicau.edu.cn; Z. Wang; Animal Nutrition Institute, Sichuan Agricultural University, Ya'an, No. 46 Xinkang Road, Yucheng District, 625014, China; email: wangzs@sicau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85204597858"
"Wang R.; Gao R.; Li Q.; Zhao C.; Ru L.; Ding L.; Yu L.; Ma W.","Wang, Rong (57221059245); Gao, Ronghua (24824729600); Li, Qifeng (57205964175); Zhao, Chunjiang (57303287700); Ru, Lin (57712915300); Ding, Luyu (55823588400); Yu, Ligen (36976801900); Ma, Weihong (55524058400)","57221059245; 24824729600; 57205964175; 57303287700; 57712915300; 55823588400; 36976801900; 55524058400","An ultra-lightweight method for individual identification of cow-back pattern images in an open image set","2024","Expert Systems with Applications","249","","123529","","","","7","10.1016/j.eswa.2024.123529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187203407&doi=10.1016%2fj.eswa.2024.123529&partnerID=40&md5=5387ef187a5816062baedcaf2bf4c2b4","Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; College of Civil Engineering and Water Conservancy, Heilongjiang Bayi Agricultural University, Daqing, 163319, China","Wang R., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Gao R., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Li Q., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Zhao C., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, College of Information Engineering, Northwest A&F University, Yangling, 712100, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Ru L., College of Civil Engineering and Water Conservancy, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; Ding L., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Yu L., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China; Ma W., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, 100097, China","Cow recognition forms the foundation of smart livestock management. Current closed-set cow recognition models can exclusively identify cows within their training set and are ill-equipped to recognize other subjects in the open world. Additionally, there is a dearth of comprehensive research on open-set cow recognition. Existing open-set recognition models not only exhibit suboptimal accuracy but also impose a substantial burden on hardware resources due to their extensive parameterization. These challenges hinder the practical deployment of cow recognition models. This study introduces an innovative open-set metric learning-based recognition framework for cow-back patterns, aiming to rectify a significant limitation of individual cow recognition models, which is their inability to identify cows not included in the training set. The proposed framework combines various loss functions, metric methods, and backbone networks. This integration enables the open-set recognition of cow back pattern images, thereby facilitating the identification of cows previously unrecognized by the model. To mitigate hardware resource consumption, we have devised a novel ultra-lightweight backbone network known as LightCowsNet. This network draws inspiration from existing lightweight backbone networks such as MobileFaceNet, MobileViT, and EfficientNetV2. Its purpose is to extract features from cow back pattern images efficiently. LightCowsNet employs attention mechanisms, inverted residual structures, and depthwise separable convolutions. LightCowsNet first uses attention mechanisms, inverted residual structures, and depthwise separable convolutions to design multiple new feature extraction modules. These modules, which have different structures, are subsequently placed to extract different levels of image information, which enables complete extraction of the textural information of the cows’ back patterns. The present study reorganized the Cows2021 dataset to render it suitable for open-set recognition of cow-back patterns, with the test set comprising image pairs. The results revealed that using A-SoftMax as the loss function and Euclidean distance as the metric method enabled LightCowsNet to achieve the highest accuracy of 94.26 %, with a model weight space occupation of only 4.06 MB. Compared to MobileFaceNet, MobileViT, and EfficientNetV2, LightCowsNet achieved 6.24 %, 8.82 %, and 2.3 % greater accuracy on the test set, respectively. Its weight also decreased by 0.71 MB, 0.24 MB, and 4.3 MB, respectively. These results demonstrate that the proposed model achieved both accuracy improvement and model lightweightness, and these findings could serve as reference solutions for intelligent farming on cow farms. © 2024","Cow recognition; Cow-back pattern; Deep learning; LightCowsNet; Open image set","Agriculture; Deep learning; Extraction; Image processing; Learning systems; Pattern recognition; Statistical tests; Back-bone network; Cow recognition; Cow-back pattern; Deep learning; Images sets; Lightcowsnet; Open image set; Pattern images; Recognition models; Ultra lightweights; Convolution","Beijing Academy of Agricultural and Forestry Sciences, BAAFS, (KJCX20220404, KJCX20230204); Beijing Academy of Agricultural and Forestry Sciences, BAAFS","This work was supported by the technological innovation capacity construction of Beijing Academy of agricultural and Forestry Sciences [grant numbers KJCX20220404] and [grant number KJCX20230204]. ","R. Gao; Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; email: gaorh@nercita.org.cn","","Elsevier Ltd","09574174","","ESAPE","","English","Expert Sys Appl","Article","Final","","Scopus","2-s2.0-85187203407"
"Deka A.; Das P.J.","Deka, Aniruddha (57210653090); Das, Parag Jyoti (59251124100)","57210653090; 59251124100","Enhancing Endangered Animal Conservation Through Deep Learning-Powered Monitoring","2025","Lecture Notes in Electrical Engineering","1243 LNEE","","","55","68","13","0","10.1007/978-981-97-6465-5_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215801770&doi=10.1007%2f978-981-97-6465-5_6&partnerID=40&md5=0b2877ff56799eac0d607885ae9627e7","Faculty of Engineering, Assam Down Town University, Sankar Madhab Path, Gandhi Nagar Panikhaiti, Assam, Guwahati, 781026, India","Deka A., Faculty of Engineering, Assam Down Town University, Sankar Madhab Path, Gandhi Nagar Panikhaiti, Assam, Guwahati, 781026, India; Das P.J., Faculty of Engineering, Assam Down Town University, Sankar Madhab Path, Gandhi Nagar Panikhaiti, Assam, Guwahati, 781026, India","With the growing advancement of artificial intelligence, its scope can also be expanded towards wildlife protection without harming the ecological balance that is caused with the use of invasive techniques. These invasive techniques include fitting of trackers, injecting biochemical transmitters, etc., which not only hinders the physical ecology of the animals but also requires manual searching and fitting the devices. Hence, non-invasive techniques have proved to be very useful in this scenario. The proposed work aims to use non-invasive techniques to identify endangered animals in the wild using YOLOv2 object detection algorithm. Since animal detection in the wild is a challenging task, therefore the captured videos for tracking are converted into a series of image sequences where the detection algorithm is applied. A surveillance system on a smaller scale is proposed for three specific endangered animals based on the detection model that detects and monitors the animals on a real-time basis. The work is carried out for three specific endangered animals, i.e. tigers, elephants, and one-horned rhinos. Since elephants and rhinos can appear similar in terms of skin colour depending on the environmental changes, blob analysis is performed as a support study to resolve this problem. However, it has been noticed that increasing the data also removes this error while using YOLOv2. The model is robust to distortions and unwanted point of references. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.","Blob analysis; Image sequence; Object detection; Surveillance system; YOLOv2","Deep learning; Livestock; Blob analysis; Ecological balance; Image sequence; Invasive techniques; Noninvasive technique; Object detection algorithms; Objects detection; Surveillance systems; Wildlife protection; YOLOv2; Invertebrates","","","A. Deka; Faculty of Engineering, Assam Down Town University, Guwahati, Sankar Madhab Path, Gandhi Nagar Panikhaiti, Assam, 781026, India; email: dekaaniruddha@gmail.com","Dhar S.; Mukhopadhyay S.; Do D.-T.; Sur S.N.; Imoize A.L.","Springer Science and Business Media Deutschland GmbH","18761100","978-981976464-8","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85215801770"
"Arablouei R.; Bishop-Hurley G.J.; Bagnall N.; Ingham A.","Arablouei, Reza (36975104100); Bishop-Hurley, Greg J. (6602321037); Bagnall, Neil (9436779900); Ingham, Aaron (6701747577)","36975104100; 6602321037; 9436779900; 6701747577","Cattle behavior recognition from accelerometer data: Leveraging in-situ cross-device model learning","2024","Computers and Electronics in Agriculture","227","","109546","","","","0","10.1016/j.compag.2024.109546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207695976&doi=10.1016%2fj.compag.2024.109546&partnerID=40&md5=2af867d19f9c808df2aced8925c80461","Data61, CSIRO, Pullenvale QLD, 4069, Australia; Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia","Arablouei R., Data61, CSIRO, Pullenvale QLD, 4069, Australia; Bishop-Hurley G.J., Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia; Bagnall N., Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia; Ingham A., Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia","Automating livestock behavior recognition using wearable sensors offers significant benefits for monitoring animal health, ensuring welfare, and enhancing farm productivity. While collar-mounted accelerometers provide useful data leading to accurate behavior recognition models, ear-tags offer greater practicality and scalability. However, ear-tag data is affected by independent ear movements (e.g., for flicking flies), necessitating extensive labeled data for accurate recognition, which is time-consuming and costly to obtain. To address this challenge, we propose a pioneering cross-device learning approach. By leveraging a pre-trained behavior recognition model from collar data to guide ear-tag model training, we significantly reduce the need for manual labeling of ear-tag data. This facilitates the development of efficient and scalable behavior recognition models suitable for wider deployment. Additionally, we introduce a novel deep neural network architecture that integrates linearly-constrained convolutional layers specifically designed to counteract gravity's impact on accelerometer data, along with a confidence penalty term to combat overfitting when training on limited labeled data. Evaluation on real-world cattle data demonstrates that our approach yields ear-tag model performance nearly on par with collar models. This breakthrough paves the way for personalized behavior recognition models using ear-tags, requiring only brief periods of collar-based labeling per animal. © 2024 The Authors","Accelerometer data; Animal behavior classification; Cross-device learning; Deep learning; Embedded systems; On-device inference","Accelerometer data; Animal behavior classification; Animal behaviour; Behaviour classification; Behaviour recognition; Cross-device learning; Deep learning; Embedded-system; On-device inference; Recognition models; accelerometer; cattle; environmental monitoring; livestock; machine learning; recognition; Accelerometers","Australian Brahman Breeders Association","We express our sincere appreciation to John Croaker for his project oversight and the financial support provided by the Australian Brahman Breeders Association. In addition, we acknowledge the contributions of the Queensland Animal Science Precinct (QASP) staff, including Milou Dekkers, Stacey Groves, Natalie Scott, Hugh Wells, and Brodie Dance, for their assistance with animal care, monitoring, and collection of live weights during the trials. Finally, we extend our gratitude to Jordan Yates for providing technical support with the embedded systems.","R. Arablouei; Data61, CSIRO, Pullenvale QLD, 4069, Australia; email: reza.arablouei@csiro.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85207695976"
"Lee J.-H.; Choi Y.H.; Lee H.-S.; Park H.J.; Hong J.S.; Lee J.H.; Sa S.J.; Kim Y.M.; Kim J.E.; Jeong Y.D.; Cho H.-C.","Lee, Ji-Hyeon (58290078700); Choi, Yo Han (35298577000); Lee, Han-Sung (57910006800); Park, Hyun Ju (58532273000); Hong, Jun Seon (57214450589); Lee, Ji Hwan (57203145183); Sa, Soo Jin (55695134700); Kim, Yong Min (57194380024); Kim, Jo Eun (57191097201); Jeong, Yong Dae (57192102384); Cho, Hyun-Chong (22233514800)","58290078700; 35298577000; 57910006800; 58532273000; 57214450589; 57203145183; 55695134700; 57194380024; 57191097201; 57192102384; 22233514800","Enhanced Swine Behavior Detection with YOLOs and a Mixed Efficient Layer Aggregation Network in Real Time","2024","Animals","14","23","3375","","","","0","10.3390/ani14233375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212674934&doi=10.3390%2fani14233375&partnerID=40&md5=52c5998d76d1dd4cc8d65726969a9941","Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Department of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Department of Data Science, Kangwon National University, Chuncheon, 24341, South Korea","Lee J.-H., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Choi Y.H., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Lee H.-S., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Park H.J., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Hong J.S., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Lee J.H., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Sa S.J., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Kim Y.M., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Kim J.E., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Jeong Y.D., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Cho H.-C., Department of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Department of Data Science, Kangwon National University, Chuncheon, 24341, South Korea","Effective livestock management has become essential owing to an aging workforce and the growth of large-scale farming operations in the agricultural industry. Conventional monitoring methods, primarily reliant on manual observation, are increasingly reaching their limits, necessitating the development of innovative automated solutions. This study developed a system, termed mixed-ELAN, for real-time sow and piglet behavior detection using an extended ELAN architecture with diverse kernel sizes. The standard convolution operations within the ELAN framework were replaced with MixConv using diverse kernel sizes to enhance feature learning capabilities. To ensure high reliability, a performance evaluation of all techniques was conducted using a k-fold cross-validation (k = 3). The proposed architecture was applied to YOLOv7 and YOLOv9, yielding improvements of 1.5% and 2%, with mean average precision scores of 0.805 and 0.796, respectively, compared with the original models. Both models demonstrated significant performance improvements in detecting behaviors critical for piglet growth and survival, such as crushing and lying down, highlighting the effectiveness of the proposed architecture. These advances highlight the potential of AI and computer vision in agriculture, as well as the system’s benefits for improving animal welfare and farm management efficiency. The proposed architecture enhances the real-time monitoring and understanding of livestock behavior, establishing improved benchmarks for smart farming technologies and enabling further innovation in livestock management. © 2024 by the authors.","deep learning; mixed-ELAN; precision agriculture; smart farming technologies; YOLOv7 and YOLOv9","animal behavior; animal welfare; Article; artificial intelligence; artificial neural network; computer vision; convolutional neural network; cross stage partial network; economic aspect; farming system; feeding; financial burden; generalized efficient layer aggregation network; information processing; k fold cross validation; livestock; machine learning; mathematical model; Mixed Efficient Layer Aggregation Network; nonhuman; piglet; sitting; sow (swine); standing; starvation; YOLOs","Rural Development Administration, RDA; National Institute of Animal Science, NIAS; Cooperative Research Program for Agriculture Science and Technology Development, (PJ01694303)","Funding text 1: This work was supported by the Cooperative Research Program for Agriculture Science and Technology Development (Project No. PJ01694303), Rural Development Administration, Republic of Korea.; Funding text 2: This study was supported by the 2024 RDA Fellowship Program of the National Institute of Animal Science, Rural Development Administration, Republic of Korea. ","H.-C. Cho; Department of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Department of Data Science, Kangwon National University, Chuncheon, 24341, South Korea; email: hyuncho@kangwon.ac.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","","Scopus","2-s2.0-85212674934"
"Zhao X.; Zhang S.; Shi R.; Yan W.; Pan X.","Zhao, Xuanhe (57219556034); Zhang, Shengwei (57204676930); Shi, Ruifeng (57222981974); Yan, Weihong (57188965969); Pan, Xin (57189594477)","57219556034; 57204676930; 57222981974; 57188965969; 57189594477","Classification of grassland conditions using a hyperspectral camera and deep learning","2025","International Journal of Remote Sensing","","","","","","","0","10.1080/01431161.2025.2452313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215278846&doi=10.1080%2f01431161.2025.2452313&partnerID=40&md5=4dc072fa716e4cfbcb247aec57a445f6","School of Computer, North China Institute of Science and Technology, Beijing, China; College of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot, China; Inner Mongolia Autonomous Region Key Laboratory of Big Data Research and Application of Agriculture and Animal Husbandry, Hohhot, China; College of Water Conservancy and Civil Engineering, Inner Mongolia Agricultural University, Hohhot, China; The Center of Information and Network Technology, Inner Mongolia Agricultural University, Hohhot, China; Institute of Grassland Research of CAAS, Hohhot, China","Zhao X., School of Computer, North China Institute of Science and Technology, Beijing, China; Zhang S., College of Water Conservancy and Civil Engineering, Inner Mongolia Agricultural University, Hohhot, China; Shi R., The Center of Information and Network Technology, Inner Mongolia Agricultural University, Hohhot, China; Yan W., Institute of Grassland Research of CAAS, Hohhot, China; Pan X., College of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot, China, Inner Mongolia Autonomous Region Key Laboratory of Big Data Research and Application of Agriculture and Animal Husbandry, Hohhot, China","Grassland is the most widespread vegetation type in terrestrial ecosystem, but it has been threatened by degradation in recent years. Developing an operational species detection model is necessary for achieving grassland monitoring and making management plans. Therefore, this study aims to quickly and accurately classify grassland species based on hyperspectral imaging (HSI) technology and deep learning algorithms. In the present study, 16,200 hyperspectral data are collected from grassland samples over a period of 3 years using a hyperspectral imager, with a wavelength range of 400–1000 nm. Second, the hyperspectral data are preprocessed by the multiple scatter correction and mean normalization, improving the quality of input data and thereby enhancing modelling capabilities. Finally, four models are established, including temporal convolutional neural network (TempCNN), recurrent neural network with long short-term memory (LSTM-RNN), Transformer, and support vector machines (SVM). The results show that the preprocessed data have stronger modelling ability than the original data, and the classification performance of the model is ranked in descending order as Transformer, LSTM-RNN, TempCNN, and SVM. Among them, the classification performance of Medicago sativa L. in the TempCNN is superior to other combinations. The LSTM-RNN achieved accuracy of 1 for Agropyron cristatum var. pectinatum and Leymus chinensis (Trin.) Tzvel. The accuracy of both the Transformer and SVM for Leymus chinensis (Trin.) Tzvel. and Lotus corniculatus L. is 1. The results indicate the effectiveness and robustness of the proposed hyperspectral imaging technology combined with deep learning model, which has well classification performance for specific forage varieties. © 2025 Informa UK Limited, trading as Taylor & Francis Group.","Deep learning; Grassland species; Hyperspectral classification; Transformer","Abiotic; Convolutional neural networks; Hyperspectral imaging; Livestock; Long short-term memory; Vegetation; Classification performance; Convolutional neural network; Deep learning; Grassland species; Hyper-spectral classification; Hyperspectral Data; Hyperspectral imaging technologies; Leymus chinensis; Support vectors machine; Transformer; Support vector machines","","","X. Pan; College of Computer and Information Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China; email: nndjsj10@163.com","","Taylor and Francis Ltd.","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Article in press","","Scopus","2-s2.0-85215278846"
"Sadrtdinova R.; Perez G.A.C.; Solomatine D.P.","Sadrtdinova, Renata (58912396700); Perez, Gerald Augusto Corzo (57211158562); Solomatine, Dimitri P. (6601958160)","58912396700; 57211158562; 6601958160","Improved drought forecasting in Kazakhstan using machine and deep learning: a non-contiguous drought analysis approach","2024","Hydrology Research","55","2","","237","261","24","1","10.2166/nh.2024.154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186369721&doi=10.2166%2fnh.2024.154&partnerID=40&md5=6d27cc0b064c1e4d4ec445202af792dc","Department of Hydroinformatics and Socio-Technical Innovation, IHE Delft Institute for Water Education, Delft, 2611 AX, Netherlands; Water Problems Institute of RAS, Gubkina 3, Moscow, 119333, Russian Federation; Water Resources Section, Delft University of Technology, Delft, 2628 CD, Netherlands","Sadrtdinova R., Department of Hydroinformatics and Socio-Technical Innovation, IHE Delft Institute for Water Education, Delft, 2611 AX, Netherlands; Perez G.A.C., Department of Hydroinformatics and Socio-Technical Innovation, IHE Delft Institute for Water Education, Delft, 2611 AX, Netherlands; Solomatine D.P., Department of Hydroinformatics and Socio-Technical Innovation, IHE Delft Institute for Water Education, Delft, 2611 AX, Netherlands, Water Problems Institute of RAS, Gubkina 3, Moscow, 119333, Russian Federation, Water Resources Section, Delft University of Technology, Delft, 2628 CD, Netherlands","Kazakhstan is recently experiencing an increase in drought trends. However, low-capacity probabilistic drought forecasts and poor dissemination have led to a drought crisis in 2021 that resulted in the loss of thousands of livestock. To improve drought forecasting accuracy, this study applies Machine Learning and Deep Learning (ML and DL) algorithms to capture the sequences of drought events using a non-contiguous drought analysis (NCDA). Precipitation, 2-m temperature, runoff, solar radiation, relative humidity, and evaporation were collected from the ERA5 database as input variables. Combinations of inputs were used to build ML models, including seven classifiers (Logistic, K-NN, Kernel SVM, Decision Tree, Random Forest, XGBoost, and GRU). The output events were defined by standardized precipitation index (SPI) and SPEI indicators as binary classes. Weekly time series from 1991 to 2021 for each cell were used to forecast a lead time from 1 week to 6 months. GRU provided 97–99% accuracy in more volatile regions while Random Forest and XGBoost showed 94–99% accuracy at a lead time of 6 months. The accuracy evaluation was based on the confusion matrix and F1 score to analyze the stage change capture. This study demonstrates the effectiveness of using ML and DL algorithms for drought forecasting, with potential applications for other regions. © 2024 The Authors.","deep learning; machine learning; NCDA; spatiotemporal drought forecasting; SPEI; SPI","Agriculture; Decision trees; Deep learning; Learning systems; Nearest neighbor search; Weather forecasting; Deep learning; Drought analysis; Kazakhstan; Leadtime; Machine-learning; Non-contiguous drought analyse; Random forests; Spatiotemporal drought forecasting; SPEI; Standardized precipitation index; accuracy assessment; air temperature; algorithm; drought; evaporation; forecasting method; machine learning; precipitation assessment; relative humidity; runoff; solar radiation; Drought","","","G.A.C. Perez; Department of Hydroinformatics and Socio-Technical Innovation, IHE Delft Institute for Water Education, Delft, 2611 AX, Netherlands; email: g.corzo@un-ihe.org","","IWA Publishing","19989563","","","","English","Hydrol. Res.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85186369721"
"Witte J.-H.; Gómez J.M.","Witte, Jan-Hendrik (57667657200); Gómez, Jorge Marx (7402100609)","57667657200; 7402100609","Image-based activity monitoring of pigs","2024","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-344","","","167","178","11","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216798491&partnerID=40&md5=2eca05d7978fae09a8edb476d190167f","Carl von Ossietzky Universität Oldenburg, Abteilung VLBA, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany","Witte J.-H., Carl von Ossietzky Universität Oldenburg, Abteilung VLBA, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany; Gómez J.M., Carl von Ossietzky Universität Oldenburg, Abteilung VLBA, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany","In modern pig livestock farming, animal well-being is of paramount importance. Monitoring activity is crucial for early detection of potential health or behavioral anomalies. Traditional object tracking methods such as DeepSort often falter due to the pigs' similar appearances, frequent overlaps, and close-proximity movements, making consistent long-term tracking challenging. To address this, our study presents a novel methodology that eliminates the need for conventional tracking to capture activity on pen-level. Instead, we segment video frames into predefined sectors, where pig postures are determined using YOLOv8 for pig detection and EfficientNetV2 for posture classification. Activity levels are then assessed by comparing sector counts between consecutive frames. Preliminary results indicate discernible variations in pig activity throughout the day, highlighting the efficacy of our method in capturing activity patterns. While promising, this approach remains a proof of concept, and its practical implications for real-world agricultural settings warrant further investigation. © 2024 Gesellschaft fur Informatik (GI). All rights reserved.","activity monitoring; computer vision; deep learning; precision livestock farming","Invertebrates; Mammals; Activity monitoring; Deep learning; Image-based; Livestock farming; Monitoring activities; Object Tracking; Potential health; Precision livestock farming; Tracking method; Well being; Livestock","","","","Hoffmann C.; Stein A.; Gallmann E.; Dorr J.; Krupitzer C.; Floto H.","Gesellschaft fur Informatik (GI)","16175468","978-388579738-8","","","English","Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)","Conference paper","Final","","Scopus","2-s2.0-85216798491"
"Mishra S.; Yadav C.S.S.; Raju A.; Balasundaram A.; Shaik A.","Mishra, Shivani (59420931200); Yadav, Chavali Sai Sreeram (59420019500); Raju, Akansha (58929433300); Balasundaram, A. (57204932205); Shaik, Ayesha (56906756700)","59420931200; 59420019500; 58929433300; 57204932205; 56906756700","EfficientNetB0-based Rice Disease Classification: A Strategic Fine-Tuning Approach","2024","Proceedings of the 5th International Conference on Smart Electronics and Communication, ICOSEC 2024","","","","1682","1687","5","0","10.1109/ICOSEC61587.2024.10722610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209934155&doi=10.1109%2fICOSEC61587.2024.10722610&partnerID=40&md5=66d98b98033a69d744c7412318810f0d","Vellore Institute of Technology, Chennai, Dept. of Computer Science and Engineering, Chennai, India; Vellore Institute of Technology, Chennai, Center for Cyber Physical Systems, Chennai, India","Mishra S., Vellore Institute of Technology, Chennai, Dept. of Computer Science and Engineering, Chennai, India; Yadav C.S.S., Vellore Institute of Technology, Chennai, Dept. of Computer Science and Engineering, Chennai, India; Raju A., Vellore Institute of Technology, Chennai, Dept. of Computer Science and Engineering, Chennai, India; Balasundaram A., Vellore Institute of Technology, Chennai, Center for Cyber Physical Systems, Chennai, India; Shaik A., Vellore Institute of Technology, Chennai, Center for Cyber Physical Systems, Chennai, India","Rice, a major stape food, is crucial for both sustenance and the living of millions of farmers. Detecting rice leaf diseases is vital to prevent widespread crop losses, safeguarding farmers' incomes and global food supply. While current detection systems face accuracy and robustness challenges, deep learning models, particularly the EfficientNetB0, show promise. Despite their sensitivity to noise and variations in leaf appearance, ongoing research addresses these issues. This study introduces the use of EfficientNetB0 for accurate classification, incorporating a fine-tuning strategy in a brief five-epoch training period. This approach optimally balances model adaptation and prevents overfitting, leveraging the deep neural network's ability to capture complex hierarchical features. The limited fine-tuning duration addresses potential risks while maximizing the model's robust features. This unique amalgamation of advanced architecture and tailored training strategy aims to achieve efficient and accurate disease classification. The research signifies a step toward revolutionizing rice disease detection, emphasizing the importance of striking a balance between cutting-edge methodologies and practical considerations in the pursuit of agricultural sustainability. Before Fine Tuning the Training Accuracy was 98.46%, Validation Accuracy was 99.15% and Test Accuracy was 98.44%.After Fine Tuning there was a significant increase in the accuracy. The Training Accuracy came to be 99.77%, Validation Accuracy is 99.43% and Test Accuracy: 99.12%  © 2024 IEEE.","Classification; Deep neural network; EfficientNetB0 model; Fine-tuning; Preprocessing; Rice leaf disease detection","Deep neural networks; Livestock; Disease classification; Efficientnetb0 model; Fine tuning; Leaf disease detections; Neural-networks; Preprocessing; Rice leaf disease detection; Rice leaves; Test accuracy; Training accuracy; Plant diseases","","","S. Mishra; Vellore Institute of Technology, Chennai, Dept. of Computer Science and Engineering, Chennai, India; email: shivani.mishra2021@vitstudent.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-833150440-3","","","English","Proc. Int. Conf. Smart Electron. Commun., ICOSEC","Conference paper","Final","","Scopus","2-s2.0-85209934155"
"El Moutaouakil K.; Falih N.","El Moutaouakil, Khalid (57721360300); Falih, Noureddine (57205694849)","57721360300; 57205694849","Deep learning-based classification of cattle behavior using accelerometer sensors","2024","IAES International Journal of Artificial Intelligence","13","1","","524","532","8","4","10.11591/ijai.v13.i1.pp524-532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178163596&doi=10.11591%2fijai.v13.i1.pp524-532&partnerID=40&md5=33be8e5b1b7050e74edeccf1ac8e33e1","LIMATI Laboratory, Computer Science Department, Polydisciplinary Faculty, University of Sultan Moulay Slimane, Beni Mellal, Morocco","El Moutaouakil K., LIMATI Laboratory, Computer Science Department, Polydisciplinary Faculty, University of Sultan Moulay Slimane, Beni Mellal, Morocco; Falih N., LIMATI Laboratory, Computer Science Department, Polydisciplinary Faculty, University of Sultan Moulay Slimane, Beni Mellal, Morocco","The increasing demand for food has led to the adoption of precision livestock, which relies on information and communication technology to promote the best practices in meat production. By automating various aspects of the industry, precision livestock allows for increased productivity, more effective management strategies, and decision-making. The paper proposes a methodology that uses deep learning techniques to automatically classify cattle behavior using accelerometer sensors embedded in collars. The work aims to enhance the efficiency and productivity of the industry by improving the classification of cattle behaviors, which is essential for farmers and barn managers to make informed decisions. We tested three different classification techniques to classify rumination, movement, resting, feeding, salting and other cattle behaviors and we achieved promising results that can contribute to a better understanding and management of cattle behavior in the livestock industry. © 2024 Institute of Advanced Engineering and Science. All rights reserved.","Agriculture 4.0; Deep learning; Livestock farming; Precision livestock; Smart farm","","","","K. El Moutaouakil; LIMATI Laboratory, Polydisciplinary Faculty, Sultan Moulay Slimane University Mghila, Beni Mellal, BP 592, Morocco; email: elmoutaouakil.kh@gmail.com","","Institute of Advanced Engineering and Science","20894872","","","","English","IAES Int. J. Artif. Intell.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85178163596"
"Afridi H.; Ullah M.; Nordbø Ø.; Hoff S.C.; Furre S.; Larsgard A.G.; Cheikh F.A.","Afridi, Hina (57213236792); Ullah, Mohib (7006278145); Nordbø, Øyvind (20436739300); Hoff, Solvei Cottis (58959450200); Furre, Siri (56433463600); Larsgard, Anne Guro (6507772420); Cheikh, Faouzi Alaya (42461035200)","57213236792; 7006278145; 20436739300; 58959450200; 56433463600; 6507772420; 42461035200","Analyzing Data Modalities for Cattle Weight Estimation Using Deep Learning Models","2024","Journal of Imaging","10","3","72","","","","4","10.3390/jimaging10030072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188862594&doi=10.3390%2fjimaging10030072&partnerID=40&md5=835bf5595cf42552e9ea7fad6730ae43","Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway; Geno SA, Storhamargata 44, Hamar, 2317, Norway; Norsvin SA, Storhamargata 44, Hamar, 2317, Norway; TYR SA, Storhamargata 44, Hamar, 2317, Norway","Afridi H., Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway, Geno SA, Storhamargata 44, Hamar, 2317, Norway; Ullah M., Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway; Nordbø Ø., Norsvin SA, Storhamargata 44, Hamar, 2317, Norway; Hoff S.C., TYR SA, Storhamargata 44, Hamar, 2317, Norway; Furre S., TYR SA, Storhamargata 44, Hamar, 2317, Norway; Larsgard A.G., Geno SA, Storhamargata 44, Hamar, 2317, Norway; Cheikh F.A., Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway","We investigate the impact of different data modalities for cattle weight estimation. For this purpose, we collect and present our own cattle dataset representing the data modalities: RGB, depth, combined RGB and depth, segmentation, and combined segmentation and depth information. We explore a recent vision-transformer-based zero-shot model proposed by Meta AI Research for producing the segmentation data modality and for extracting the cattle-only region from the images. For experimental analysis, we consider three baseline deep learning models. The objective is to assess how the integration of diverse data sources influences the accuracy and robustness of the deep learning models considering four different performance metrics: mean absolute error (MAE), root mean squared error (RMSE), mean absolute percentage error (MAPE), and R-squared ((Formula presented.)). We explore the synergies and challenges associated with each modality and their combined use in enhancing the precision of cattle weight prediction. Through comprehensive experimentation and evaluation, we aim to provide insights into the effectiveness of different data modalities in improving the performance of established deep learning models, facilitating informed decision-making for precision livestock management systems. © 2024 by the authors.","cattle weight estimation; data modalities; deep learning models; depth information; segmentation","Agriculture; Deep learning; Errors; Image segmentation; Information management; Learning algorithms; Learning systems; Mean square error; Cattle weight estimation; Combined segmentations; Data modality; Deep learning model; Depth information; Learning models; Segmentation; Segmentation data; Segmentation informations; Weights estimation; Decision making","Norges Teknisk-Naturvitenskapelige Universitet, NTNU; Norges Forskningsråd, (282252, 310239)","We would like to thank the Research Council of Norway for funding this study, within the BIONÆR program, project number 282252 and the Industrial PhD program, project number 310239. We would also like to thank the Norwegian University of Science and Technology for supporting the APC through the open-access journal publication fund.","H. Afridi; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway; email: hinaa@stud.ntnu.no; M. Ullah; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, 2815, Norway; email: mohib.ullah@ntnu.no","","Multidisciplinary Digital Publishing Institute (MDPI)","2313433X","","","","English","J. Imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85188862594"
"Agarwal M.; Dovdon E.; Hailu H.M.; Barge L.R.; Dajsuren Y.; de Vlieg J.","Agarwal, Manu (58834811300); Dovdon, Enkhzol (57223108547); Hailu, Haftom Meles (59523022400); Barge, Luis Roma (58836053500); Dajsuren, Yanja (55317374300); de Vlieg, Jakob (58112751500)","58834811300; 57223108547; 59523022400; 58836053500; 55317374300; 58112751500","IMAGEN Data Analytics Platform for Animal Phenotype Detection","2024","Lecture Notes in Networks and Systems","1220 LNNS","","","1","12","11","0","10.1007/978-3-031-78698-3_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215659623&doi=10.1007%2f978-3-031-78698-3_1&partnerID=40&md5=41443bb4168379e3ee1a98feb4158595","Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands","Agarwal M., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Dovdon E., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Hailu H.M., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Barge L.R., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Dajsuren Y., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; de Vlieg J., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands","In Precision Livestock Farming (PLF), deep learning-based approaches are increasingly employed to study animal behavior on farms. These behavioral studies enable animal phenotyping, which can be used for genetic selection and social network analysis in large animal groups. While considerable attention is often given to models in the deep learning field, the models themselves do not function in isolation. Efficient deep learning workflows require systems that bridge research and prototyping with production operations. In this chapter, we introduce the IMAGEN Data Analytics Platform. This platform supports the development of deep learning models for animal phenotyping and addresses various data challenges through a DataOps approach. Although initially designed for the animal phenotype detection domain, the platform is domain-neutral and can be applied to similar cases in other application domains. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Animal phenotype detection; Data analytics platform; DataOps; Deep learning; HPC; Pilot-job framework; Precision livestock farming","Adversarial machine learning; Contrastive Learning; Federated learning; Invertebrates; Animal phenotype detection; Data analytic platform; Data analytics; Dataops; Deep learning; HPC; Phenotyping; Pilot jobs; Pilot-job framework; Precision livestock farming; Livestock","","","M. Agarwal; Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; email: m.agarwal@tue.nl; E. Dovdon; Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; email: e.d.dovdon@tue.nl","Zbakh M.; Essaaidi M.; Tadonki C.; Touhafi A.; Panda D.K.","Springer Science and Business Media Deutschland GmbH","23673370","978-303178697-6","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85215659623"
"Sutovsky P.; Hamilton L.E.; Zigo M.; Ortiz D’Avila Assumpção M.E.; Jones A.; Tirpak F.; Agca Y.; Kerns K.; Sutovsky M.","Sutovsky, Peter (7005070026); Hamilton, Lauren E. (57201076288); Zigo, Michal (49965165700); Ortiz D’Avila Assumpção, Mayra E. (59180365000); Jones, Alexis (55475911900); Tirpak, Filip (57195960334); Agca, Yuksel (6603565021); Kerns, Karl (56583899900); Sutovsky, Miriam (6508052757)","7005070026; 57201076288; 49965165700; 59180365000; 55475911900; 57195960334; 6603565021; 56583899900; 6508052757","Biomarker-based human and animal sperm phenotyping: the good, the bad and the ugly","2024","Biology of Reproduction","110","6","","1135","1156","21","5","10.1093/biolre/ioae061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196326839&doi=10.1093%2fbiolre%2fioae061&partnerID=40&md5=d5e366e6d7f51ec5257879a6b10cb747","Division of Animal Sciences, University of Missouri, Columbia, MO, United States; Department of Obstetrics, Gynecology and Women’s Health, University of Missouri, Columbia, MO, United States; Department of Animal Reproduction, School of Veterinary Medicine and Animal Science, University of São Paulo, SP, São Paulo, Brazil; Department of Veterinary Pathobiology, College of Veterinary Medicine, University of Missouri, Columbia, MO, United States; Department of Animal Science, Iowa State University, Ames, IA, United States","Sutovsky P., Division of Animal Sciences, University of Missouri, Columbia, MO, United States, Department of Obstetrics, Gynecology and Women’s Health, University of Missouri, Columbia, MO, United States; Hamilton L.E., Division of Animal Sciences, University of Missouri, Columbia, MO, United States; Zigo M., Division of Animal Sciences, University of Missouri, Columbia, MO, United States; Ortiz D’Avila Assumpção M.E., Division of Animal Sciences, University of Missouri, Columbia, MO, United States, Department of Animal Reproduction, School of Veterinary Medicine and Animal Science, University of São Paulo, SP, São Paulo, Brazil; Jones A., Division of Animal Sciences, University of Missouri, Columbia, MO, United States; Tirpak F., Division of Animal Sciences, University of Missouri, Columbia, MO, United States; Agca Y., Department of Veterinary Pathobiology, College of Veterinary Medicine, University of Missouri, Columbia, MO, United States; Kerns K., Department of Animal Science, Iowa State University, Ames, IA, United States; Sutovsky M., Division of Animal Sciences, University of Missouri, Columbia, MO, United States","Conventional, brightfield-microscopic semen analysis provides important baseline information about sperm quality of an individual; however, it falls short of identifying subtle subcellular and molecular defects in cohorts of “bad,” defective human and animal spermatozoa with seemingly normal phenotypes. To bridge this gap, it is desirable to increase the precision of andrological evaluation in humans and livestock animals by pursuing advanced biomarker-based imaging methods. This review, spiced up with occasional classic movie references but seriously scholastic at the same time, focuses mainly on the biomarkers of altered male germ cell proteostasis resulting in post-testicular carryovers of proteins associated with ubiquitin-proteasome system. Also addressed are sperm redox homeostasis, epididymal sperm maturation, sperm–seminal plasma interactions, and sperm surface glycosylation. Zinc ion homeostasis-associated biomarkers and sperm-borne components, including the elements of neurodegenerative pathways such as Huntington and Alzheimer disease, are discussed. Such spectrum of biomarkers, imaged by highly specific vital fluorescent molecular probes, lectins, and antibodies, reveals both obvious and subtle defects of sperm chromatin, deoxyribonucleic acid, and accessory structures of the sperm head and tail. Introduction of next-generation image-based flow cytometry into research and clinical andrology will soon enable the incorporation of machine and deep learning algorithms with the end point of developing simple, label-free methods for clinical diagnostics and high-throughput phenotyping of spermatozoa in humans and economically important livestock animals. Summary Sentence Advanced imaging technologies enable high precision & throughput, biomarker-based semen analysis. © The Author(s) 2024.","","Animals; Biomarkers; Humans; Male; Phenotype; Semen Analysis; Spermatozoa; DNA; glycosylated protein; lectin; proteasome; seminal plasma protein; ubiquitin; zinc ion; biological marker; Alzheimer disease; apoptosis; Article; comparative proteomics; deep learning; epididymis; flow cytometry; fluorescence; germ cell; glycosylation; high throughput sequencing; human; Huntington chorea; learning algorithm; machine learning; microtubule; molecular probe; nonhuman; phenotype; protein homeostasis; proteomics; semen abnormality; semen analysis; seminal plasma; sperm; sperm quality; spermatozoon; spermatozoon head; spermatozoon maturation; sumoylation; surface property; testis; animal; male; metabolism; physiology; procedures; spermatozoon; veterinary medicine","MEODA; Lalor Foundation; USDA National Needs; University of Missouri, MU; Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP; National Institute of Food and Agriculture, NIFA, (2023-67015-39262, 2022-67015-36298, 2021-67015-33404, 2020-67015-31017); National Institute of Food and Agriculture, NIFA","Funding text 1: We would like to extend our sincere thanks to past members of our laboratory and to countless external collaborators who contributed to studies reviewed in this article. Several coauthors were supported by the following fellowships: George Washington Carver Fellowship from University of Missouri (AJ), USDA National Needs Graduate Fellowship (AJ), Lalor Foundation Research Postdoctoral Fellowship (LEH) and a visiting scholar fellowship from FAPESP Sao Paulo, Brazil (MEODA). ; Funding text 2: Grant Support: USDA NIFA Animal Breeding, Genetics & Genomics grant number 2023-67015-39262; USDA NIFA Animal Reproduction grant number 2022-67015-36298; USDA NIFA Animal Reproduction grant number 2021-67015-33404; USDA NIFA Animal Reproduction grant number 2020-67015-31017. ","P. Sutovsky; Division of Animal Sciences, University of Missouri, Columbia, S141 ASRC920 East Campus Drive, 65211-5300, United States; email: sutovskyp@missouri.edu","","Oxford University Press","00063363","","BIREB","38640912","English","Biol. Reprod.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85196326839"
"Scaillierez A.J.; García-Faria T.I.; Broers H.; van Nieuwamerongen-De Koning S.E.; Van Der Tol R.P.P.J.; Bokkers E.A.M.; Boumans I.J.M.M.","Scaillierez, Alice J. (58915195100); García-Faria, Tomás Izquierdo (58187115600); Broers, Harry (59472439200); van Nieuwamerongen-De Koning, Sofie E. (58914714000); Van Der Tol, Rik P.P.J. (6504318930); Bokkers, Eddie A.M. (6603234590); Boumans, Iris J.M.M. (55812191600)","58915195100; 58187115600; 59472439200; 58914714000; 6504318930; 6603234590; 55812191600","Determining the posture and location of pigs using an object detection model under different lighting conditions","2024","Translational Animal Science","8","","txae167","","","","0","10.1093/tas/txae167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212151153&doi=10.1093%2ftas%2ftxae167&partnerID=40&md5=45ca47c5e0cdf0414aa046e073f08a57","Animal Production Systems group, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; Wageningen Livestock Research, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; Signify Research, Signify, High Tech Campus 7, Eindhoven, 5656 AE, Netherlands; Agricultural Biosystems Engineering group, Wageningen University & Research, P.O. Box 16, Wageningen, 6700 AA, Netherlands","Scaillierez A.J., Animal Production Systems group, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; García-Faria T.I., Wageningen Livestock Research, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; Broers H., Signify Research, Signify, High Tech Campus 7, Eindhoven, 5656 AE, Netherlands; van Nieuwamerongen-De Koning S.E., Animal Production Systems group, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; Van Der Tol R.P.P.J., Agricultural Biosystems Engineering group, Wageningen University & Research, P.O. Box 16, Wageningen, 6700 AA, Netherlands; Bokkers E.A.M., Animal Production Systems group, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands; Boumans I.J.M.M., Animal Production Systems group, Wageningen University & Research, P.O. Box 338, Wageningen, 6700 AH, Netherlands","Computer vision techniques are becoming increasingly popular for monitoring pig behavior. For instance, object detection models allow us to detect the presence of pigs, their location, and their posture. The performance of object detection models can be affected by variations in lighting conditions (e.g., intensity, spectrum, and uniformity). Furthermore, lighting conditions can influence pigs' active and resting behavior. In the context of experiments testing different lighting conditions, a detection model was developed to detect the location and postures of group-housed growing-finishing pigs. The objective of this paper is to validate the model developed using YOLOv8 detecting standing, sitting, sternal lying, and lateral lying pigs. Training, validation, and test datasets included annotation of pigs from 10 to 24 wk of age in 10 different light settings; varying in intensity, spectrum, and uniformity. Pig detection was comparable for the different lighting conditions, despite a slightly lower posture agreement for warm light and uneven light distribution, likely due to a less clear contrast between pigs and their background and the presence of shadows. The detection reached a mean average precision (mAP) of 89.4%. Standing was the best-detected posture with the highest precision, sensitivity, and F1 score, while the sensitivity and F1 score of sitting was the lowest. This lower performance resulted from confusion of sitting with sternal lying and standing, as a consequence of the top camera view and a low occurrence of sitting pigs in the annotated dataset. This issue is inherent to pig behavior and could be tackled using data augmentation. Some confusion was reported between types of lying due to occlusion by pen mates or pigs' own bodies, and grouping both types of lying postures resulted in an improvement in the detection (mAP = 97.0%). Therefore, comparing resting postures (both lying types) to active postures could lead to a more reliable interpretation of pigs' behavior. Some detection errors were observed, e.g., two detections for the same pig were generated due to posture uncertainty, dirt on cameras detected as a pig, and undetected pigs due to occlusion. The localization accuracy measured by the intersection over union was higher than 95.5% for 75% of the dataset, meaning that the location of predicted pigs was very close to annotated pigs. Tracking individual pigs revealed challenges with ID changes and switches between pen mates, requiring further work.  © 2024 The Author(s).","behavior; computer vision; deep learning; precision livestock farming; single-stage detector; swine","","Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO","This research received funding from the Netherlands Organization for Scientific Research (NWO) in the framework of the ENW PPP Fund and private parties Signify and De Hoeve Innovatie (ENWSS.2018.005). ","","","Oxford University Press","25732102","","","","English","Transl. Anim. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85212151153"
"Li S.; Jin J.; Afrin M.; Zheng Q.; Fu J.; Tian Y.-C.","Li, Shi (58725336800); Jin, Jiong (24465681500); Afrin, Mahbuba (56592961900); Zheng, Qiushi (57213424734); Fu, Jing (55827315900); Tian, Yu-Chu (57203032002)","58725336800; 24465681500; 56592961900; 57213424734; 55827315900; 57203032002","UAV-as-a-Service for Robotic Edge System Resilience","2024","Proceedings of the IEEE International Conference on Web Services, ICWS","","","","142","148","6","0","10.1109/ICWS62655.2024.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210229299&doi=10.1109%2fICWS62655.2024.00034&partnerID=40&md5=e005beaee35b7b295ec6904bff412a26","Swinburne University of Technology, School of Science, Computing and Engineering Technologies, Melbourne, Australia; Curtin University, School of Electrical Engineering, Computing and Mathematical Sciences, Perth, Australia; Rmit University, School of Engineering, Stem College, Melbourne, Australia; Queensland University of Technology, School of Computer Science, Brisbane, Australia","Li S., Swinburne University of Technology, School of Science, Computing and Engineering Technologies, Melbourne, Australia; Jin J., Swinburne University of Technology, School of Science, Computing and Engineering Technologies, Melbourne, Australia; Afrin M., Curtin University, School of Electrical Engineering, Computing and Mathematical Sciences, Perth, Australia; Zheng Q., Swinburne University of Technology, School of Science, Computing and Engineering Technologies, Melbourne, Australia; Fu J., Rmit University, School of Engineering, Stem College, Melbourne, Australia; Tian Y.-C., Queensland University of Technology, School of Computer Science, Brisbane, Australia","By melding the capabilities of robotics with the agility of edge computing, Robotic Edge System (RES) exemplifies the next generation of Internet Intelligent Service Systems, delivering incredible efficiency and adaptability across diverse real world applications. Inevitably, RES is susceptible to mechanical disruptions of robots, particularly when some tasks are assigned to faulty ones, leading to uncertain failures and performance degradation. Due to communication and latency constraints, it is not always feasible to rely on edge/cloud computing infrastructure for system recovery. To address these issues, a UAV-as-a-Service (UAVaaS) approach is proposed that leverages the mobility of UAVs to enhance system resilience. Specifically, a Markov Decision Process (MDP) is utilized to assign tasks dynamically among active UAVs to achieve system recovery in a livestock monitoring scenario. Additionally, a Dual Noise Deep Deterministic Policy Gradient (DNDDPG)-based mechanism is proposed to minimize system recovery time and energy consumption. The proposed DNDDPG enhances exploration and decision-making during training by integrating parameter noise and behavioral noise into the classic Deep Deterministic Policy Gradient (DDPG) algorithm. The simulation results indicate that the proposed mechanism can achieve convergence within 100 episodes, thereby effectively minimizing the time and energy required for system recovery.  © 2024 IEEE.","deep reinforcement learning; Robotic Edge System; smart farming; task allocation; UAV-as-a-Service","Deep learning; Deep reinforcement learning; Edge computing; Infrastructure as a service (IaaS); Intelligent robots; Reinforcement learning; Deterministics; Edge computing; Policy gradient; Reinforcement learnings; Robotic edge system; Smart farming; System recovery; System resiliences; Task allocation; UAV-as-a-service; Markov processes","","","S. Li; Swinburne University of Technology, School of Science, Computing and Engineering Technologies, Melbourne, Australia; email: 103791828@student.swin.edu.au","Chang R.N.; Chang C.K.; Jiang Z.; Yang J.; Jin Z.; Sheng M.; Fan J.; Fletcher K.K.; He Q.; He Q.; Ardagna C.; Yang J.; Yin J.; Wang Z.; Beheshti A.; Russo S.; Atukorala N.; Wu J.; Yu P.S.; Ludwig H.; Reiff-Marganiec S.; Zhang E.; Sailer A.; Bena N.; Li K.; Watanabe Y.; Zhao T.; Wang S.; Tu Z.; Wang Y.; Wei K.","Institute of Electrical and Electronics Engineers Inc.","28363876","979-835036855-0","","","English","Proc. IEEE Int. Conf. Web Serv., ICWS","Conference paper","Final","","Scopus","2-s2.0-85210229299"
"Arshad M.F.; Burrai G.P.; Varcasia A.; Sini M.F.; Ahmed F.; Lai G.; Polinas M.; Antuofermo E.; Tamponi C.; Cocco R.; Corda A.; Parpaglia M.L.P.","Arshad, Muhammad Furqan (59033930900); Burrai, Giovanni Pietro (26767468900); Varcasia, Antonio (8541308500); Sini, Maria Francesca (57459489500); Ahmed, Fahad (57224769781); Lai, Giovanni (58902694500); Polinas, Marta (55444904500); Antuofermo, Elisabetta (23484355600); Tamponi, Claudia (55536905700); Cocco, Raffaella (21740747900); Corda, Andrea (35169032900); Parpaglia, Maria Luisa Pinna (8922242100)","59033930900; 26767468900; 8541308500; 57459489500; 57224769781; 58902694500; 55444904500; 23484355600; 55536905700; 21740747900; 35169032900; 8922242100","The groundbreaking impact of digitalization and artificial intelligence in sheep farming","2024","Research in Veterinary Science","170","","105197","","","","5","10.1016/j.rvsc.2024.105197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185792830&doi=10.1016%2fj.rvsc.2024.105197&partnerID=40&md5=80fa7c26716a3a9a33d5df60d9423aa1","Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Nutrition Innovation Centre for Food and Health (NICHE), School of Biomedical Sciences, Ulster University, Coleraine, BT52 1SA, United Kingdom","Arshad M.F., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Burrai G.P., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Varcasia A., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Sini M.F., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Ahmed F., Nutrition Innovation Centre for Food and Health (NICHE), School of Biomedical Sciences, Ulster University, Coleraine, BT52 1SA, United Kingdom; Lai G., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Polinas M., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Antuofermo E., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Tamponi C., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Cocco R., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Corda A., Department of Veterinary Medicine, University of Sassari, Sassari, Italy; Parpaglia M.L.P., Department of Veterinary Medicine, University of Sassari, Sassari, Italy","The integration of digitalization and Artificial Intelligence (AI) has marked the onset of a new era of efficient sheep farming in multiple aspects ranging from the general well-being of sheep to advanced web-based management applications. The resultant improvement in sheep health and consequently better farming yield has already started to benefit both farmers and veterinarians. The predictive analytical models embedded with machine learning (giving sense to machines) has helped better decision-making and has enabled farmers to derive most out of their farms. This is evident in the ability of farmers to remotely monitor livestock health by wearable devices that keep track of animal vital signs and behaviour. Additionally, veterinarians now employ advanced AI-based diagnostics for efficient parasite detection and control. Overall, digitalization and AI have completely transformed traditional farming practices in livestock animals. However, there is a pressing need to optimize digital sheep farming, allowing sheep farmers to appreciate and adopt these innovative systems. To fill this gap, this review aims to provide available digital and AI-based systems designed to aid precision farming of sheep, offering an up-to-date understanding on the subject. Various contemporary techniques, such as sky shepherding, virtual fencing, advanced parasite detection, automated counting and behaviour tracking, anomaly detection, precision nutrition, breeding support, and several mobile-based management applications are currently being utilized in sheep farms and appear to be promising. Although artificial intelligence and machine learning may represent key features in the sustainable development of sheep farming, they present numerous challenges in application. © 2024 The Authors","Artificial intelligence (AI); Digitalization; Precision livestock farming (PLF); Sheep farming","Animal Husbandry; Animals; Artificial Intelligence; Farmers; Farms; Humans; Livestock; Sheep; animal behavior; animal food; artificial intelligence; automation; bacterial transmission; body weight; breast tumor; breeding; computer vision; convolutional neural network; decision support system; deep learning; digital technology; early diagnosis; fascioliasis; heart beat; intervertebral disk degeneration; machine learning; mastitis; measurement accuracy; nonhuman; parasite identification; predictive model; predictive value; respiratory tract disease; Review; sheep farming; skin tumor; sky spheperding; Streptococcus uberis; virtual fencing; agricultural worker; animal; animal husbandry; human; livestock; procedures; sheep","Innovation for Next Generation Sardinia, (ECS 00000038); National Recovery and Resilience Plan; Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR","This work has been developed within the framework of the project e.INS- Ecosystem of Innovation for Next Generation Sardinia (cod. ECS 00000038) funded by the Italian Ministry for Research and Education (MUR) under the National Recovery and Resilience Plan (NRRP) - MISSION 4 COMPONENT 2, “From research to business” INVESTMENT 1.5, “Creation and strengthening of Ecosystems of innovation” and construction of “Territorial R&D Leaders”.","A. Varcasia; Department of Veterinary Medicine, University of Sassari, Sassari, Italy; email: varcasia@uniss.it","","Elsevier B.V.","00345288","","RVTSA","38395008","English","Res. Vet. Sci.","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85185792830"
"Ahmad M.; Zhang W.; Smith M.; Brilot B.; Bell M.","Ahmad, Misbah (57208081360); Zhang, Wenhao (57188662601); Smith, Melvyn (55495905800); Brilot, Ben (6506985377); Bell, Matt (15922235000)","57208081360; 57188662601; 55495905800; 6506985377; 15922235000","IYOLO-FAM: Improved YOLOv8 with Feature Attention Mechanism for Cow Behaviour Detection","2024","2024 IEEE 15th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2024","","","","210","219","9","1","10.1109/UEMCON62879.2024.10754666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212712545&doi=10.1109%2fUEMCON62879.2024.10754666&partnerID=40&md5=65329425c2b0f946b0f11b2629a839da","Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom","Ahmad M., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom, University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Zhang W., University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Smith M., University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Brilot B., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; Bell M., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom","We introduced IYOLO-FAM (Improved YOLOv8 with Feature Attention Mechanism) for detecting cow behaviours. By leveraging the robust YOLOv8 architecture improved with Feature Attention Mechanisms (FAM), Squeeze-and-Excitation (SE) blocks and data augmentation techniques, we enhanced the ability of the model to focus on salient features and generalize across a diverse farm environment. The experimental results demonstrated that IYOLO-FAM outperforms baseline YOLO models, achieving a mean Average Precision (m A P) of 88 % at an IoU threshold of 0.5 and 70 % across IoU thresholds from 0.5 to 0.95. These results highlighted substantial improvements over previous versions, particularly in detecting specific cow behaviours such as eating, lying, standing, and walking. The integration of SE blocks and FAM within the YOLOv8 framework proved effective in highlighting relevant features and enhancing detection accuracy, underscoring the significance of integrating advanced deep learning techniques with robust data augmentation techniques to tackle the challenges posed by a real-world farm environment. The proposed approach has the potential to benefit animal welfare in real-world applications, with future research focusing on integrating multimodal data. Additionally, real-world trials will validate the model's robustness and effectiveness in a practical farm environment. © 2024 IEEE.","Cow Behaviour Detection; Deep Learning; Machine Learning; Precision Livestock Farming","Adversarial machine learning; Contrastive Learning; Deep learning; Deep reinforcement learning; Attention mechanisms; Behavior detection; Cow behavior; Cow behavior detection; Data augmentation; Deep learning; Farm environments; Machine-learning; Precision livestock farming; Real-world; Livestock","John Oldacre Trust; Hartpury University","We gratefully acknowledge the support and funding provided by John Oldacre Trust and Hartpury University under project No.HK009687 for this research endeavour. Their commitment to advancing scientific knowledge and innovative solutions in livestock management has been instrumental in the successful implementation of this work.","M. Ahmad; Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; email: Misbah.Ahmad@hartpury.ac.uk; B. Brilot; Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; email: Ben.Brilot@hartpury.ac.uk; M. Bell; Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; email: Matt.Bell@hartpury.ac.uk","Paul R.; Kundu A.","Institute of Electrical and Electronics Engineers Inc.","","979-833154090-6","","","English","IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON","Conference paper","Final","","Scopus","2-s2.0-85212712545"
"Zhao L.; Hao R.; Chai Z.; Fu W.; Yang W.; Li C.; Liu Q.; Jiang Y.","Zhao, Liangwei (59007064600); Hao, Ran (58555713400); Chai, Ziyi (58891540600); Fu, Weiwei (56625479200); Yang, Wei (7407760172); Li, Chen (57221848087); Liu, Quanzhong (55869897600); Jiang, Yu (55733627100)","59007064600; 58555713400; 58891540600; 56625479200; 7407760172; 57221848087; 55869897600; 55733627100","DeepOCR: A multi-species deep-learning framework for accurate identification of open chromatin regions in livestock","2024","Computational Biology and Chemistry","110","","108077","","","","0","10.1016/j.compbiolchem.2024.108077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191663073&doi=10.1016%2fj.compbiolchem.2024.108077&partnerID=40&md5=09c6f8d80cf0825c73fb7d65ed1a6c2e","College of Information Engineering, Northwest A&F University, Yangling, 712100, China; College of Pastoral Agriculture Science and Technology, Lanzhou University, Gansu, Lanzhou, 730020, China; National Clinical Research Center for Infectious Diseases, Shenzhen Third People's Hospital, Shenzhen, 518112, China; Monash Biomedicine Discovery Institute and Department of Biochemistry and Molecular Biology, Monash University, Melbourne, 3800, VIC, Australia; Key Laboratory of Animal Genetics, Breeding and Reproduction of Shaanxi Province, College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China; Key Laboratory of Livestock Biology, Northwest A&F University, Shaanxi, Yangling, 712100, China","Zhao L., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Hao R., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Chai Z., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Fu W., College of Pastoral Agriculture Science and Technology, Lanzhou University, Gansu, Lanzhou, 730020, China; Yang W., National Clinical Research Center for Infectious Diseases, Shenzhen Third People's Hospital, Shenzhen, 518112, China; Li C., Monash Biomedicine Discovery Institute and Department of Biochemistry and Molecular Biology, Monash University, Melbourne, 3800, VIC, Australia; Liu Q., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Jiang Y., Key Laboratory of Animal Genetics, Breeding and Reproduction of Shaanxi Province, College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Livestock Biology, Northwest A&F University, Shaanxi, Yangling, 712100, China","A wealth of experimental evidence has suggested that open chromatin regions (OCRs) are involved in many critical biological activities, such as DNA replication, enhancer activity, and gene transcription. Accurately identifying OCRs in livestock species can provide critical insights into the distribution and characteristics of OCRs for disease treatment in livestock, thereby improving animal welfare. However, most current machine-learning methods for OCR prediction were originally designed for a limited number of model organisms, such as humans and some model organisms, and thus their performance on non-model organisms, specifically livestock, is often unsatisfactory. To bridge this gap, we propose DeepOCR, a lightweight depth-separable residual network model for predicting OCRs in livestock, including chicken, cattle, and sheep. DeepOCR integrates a single convolution layer and two improved residue structure blocks to extract and learn important features from the input DNA sequences. A fully connected layer was also employed to further process the extracted features and improve the robustness of the entire network. Our benchmarking experiments demonstrated superior prediction performance of DeepOCR compared to state-of-the-art approaches on testing datasets of the three species. The source code of DeepOCR is freely available for academic purposes at https://github.com/jasonzhao371/DeepOCR/. We anticipate DeepOCR servers as a practical and reliable computational tool for OCR-related studies in livestock species. © 2024 The Authors","Deep learning; Lightweight depth-separable residual network; Multi-species prediction; Open chromatin region prediction","Animals; Cattle; Chickens; Chromatin; Deep Learning; Livestock; Sheep; Agriculture; Animals; Benchmarking; Deep learning; DNA sequences; Learning systems; Transcription; Deep learning; DNA replications; Experimental evidence; Learning frameworks; Lightweight depth-separable residual network; Model organisms; Multi-species; Multi-species prediction; Open chromatin region prediction; Region predictions; animal; bovine; chemistry; chromatin; deep learning; Gallus gallus; genetics; livestock; metabolism; sheep; Forecasting","National Key Research and Development Program of China, NKRDPC, (2022YFF1000100); National Key Research and Development Program of China, NKRDPC","This work was supported by the National Key R&D Program of China (2022YFF1000100).","C. Li; Department of Biochemistry and Molecular Biology, Monash University, 3800, Australia; email: Chen.Li@monash.edu; Q. Liu; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: liuqzhong@nwsuaf.edu.cn; Y. Jiang; Key Laboratory of Animal Genetics, Breeding and Reproduction of Shaanxi Province, College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China; email: yu.jiang@nwafu.edu.cn","","Elsevier Ltd","14769271","","","38691895","English","Comput. Biol. Chem.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85191663073"
"Vargas J.; Mori D.; Ramon N.; Ticona W.","Vargas, Juan (59453470100); Mori, Dick (59453733700); Ramon, Naydu (59453470200); Ticona, Wilfredo (57200162823)","59453470100; 59453733700; 59453470200; 57200162823","Proposal of a Model for the Detection of Violence Against Animals Through Convolutional Neural Networks","2024","Lecture Notes in Networks and Systems","1120 LNNS","","","218","228","10","0","10.1007/978-3-031-70518-2_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210833622&doi=10.1007%2f978-3-031-70518-2_19&partnerID=40&md5=18296ea0334f5a3419be769a3ca27d25","Universidad Tecnológica del Perú, Lima, Peru; Universidad ESAN, Lima, Peru","Vargas J., Universidad Tecnológica del Perú, Lima, Peru; Mori D., Universidad Tecnológica del Perú, Lima, Peru; Ramon N., Universidad ESAN, Lima, Peru; Ticona W., Universidad Tecnológica del Perú, Lima, Peru, Universidad ESAN, Lima, Peru","Animal violence is a global problem that affects millions of animals every year. It can manifest itself in various forms, such as abandonment, physical abuse, and commercial exploitation. Animal violence has a negative impact on animal welfare, but it can also have consequences for society at large. Detecting animal violence is a major challenge. Traditional methods, such as bystander reporting, may be insufficient to identify all cases of abuse. Automated detection methods, such as machine vision, may be a valuable tool to improve the detection of animal violence. This study evaluated the effectiveness of convolutional neural networks, coupled with pose estimation, in detecting animal violence in videos. A dataset of animal violence videos was created and preprocessed to improve model performance. Several detection models, such as YOLOV7, VGG-16, VGG-19, GoogleNet and ResNet, were implemented, of which, they were evaluated based on their accuracy, precision, and sensitivity. Accordingly, the results showed that YOLOV7, in combination with pose estimation, obtained the best performance metrics. The model demonstrated an accuracy of 93.49%, precision of 98% and sensitivity of 88%. suggest that YOLOV7, together with pose estimation, is an effective method for detecting animal violence. The model provides a comprehensive approach to identify violent behaviors towards animals and can contribute to the automation and improvement of the identification of such behaviors. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Animals; Convolutional Neural Networks; Deep Learning; Violence","Convolutional neural networks; Deep neural networks; Livestock; Animal welfare; Automated detection; Commercial exploitation; Convolutional neural network; Deep learning; Detection methods; Global problems; Machine-vision; Pose-estimation; Violence; Invertebrates","","","W. Ticona; Universidad Tecnológica del Perú, Lima, Peru; email: wmamani@esan.edu.pe","Silhavy R.; Silhavy P.","Springer Science and Business Media Deutschland GmbH","23673370","978-303170517-5","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85210833622"
"Kiflie M.A.; Sharma D.P.; Haile M.A.; Srinivasagan R.","Kiflie, Mulugeta Adibaru (58820927100); Sharma, Durga Prasad (56134754400); Haile, Mesfin Abebe (56208636200); Srinivasagan, Ramasamy (57972081600)","58820927100; 56134754400; 56208636200; 57972081600","EfficientNet Ensemble Learning: Identifying Ethiopian Medicinal Plant Species and Traditional Uses by Integrating Modern Technology with Ethnobotanical Wisdom","2024","Computers","13","2","38","","","","0","10.3390/computers13020038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185961786&doi=10.3390%2fcomputers13020038&partnerID=40&md5=92759a14084e10d7cec20f3891812153","Department of Computer Science and Engineering, School of Electrical Engineering and Computing, Adama Science and Technology University, Adama, 1888, Ethiopia; AMUIT, MOEFDRE under UNDP, MAISM—RTU, Kota, 324010, India; Department of Computer Engineering, King Faisal University, Al Hofuf, 31982, Saudi Arabia","Kiflie M.A., Department of Computer Science and Engineering, School of Electrical Engineering and Computing, Adama Science and Technology University, Adama, 1888, Ethiopia; Sharma D.P., AMUIT, MOEFDRE under UNDP, MAISM—RTU, Kota, 324010, India; Haile M.A., Department of Computer Science and Engineering, School of Electrical Engineering and Computing, Adama Science and Technology University, Adama, 1888, Ethiopia; Srinivasagan R., Department of Computer Engineering, King Faisal University, Al Hofuf, 31982, Saudi Arabia","Ethiopia is renowned for its rich biodiversity, supporting a diverse variety of medicinal plants with significant potential for therapeutic applications. In regions where modern healthcare facilities are scarce, traditional medicine emerges as a cost-effective and culturally aligned primary healthcare solution in developing countries. In Ethiopia, the majority of the population, around 80%, and for a significant proportion of their livestock, approximately 90% continue to prefer traditional medicine as their primary healthcare option. Nevertheless, the precise identification of specific plant parts and their associated uses has posed a formidable challenge due to the intricate nature of traditional healing practices. To address this challenge, we employed a majority based ensemble deep learning approach to identify medicinal plant parts and uses of Ethiopian indigenous medicinal plant species. The primary objective of this research is to achieve the precise identification of the parts and uses of Ethiopian medicinal plant species. To design our proposed model, EfficientNetB0, EfficientNetB2, and EfficientNetB4 were used as benchmark models and applied as a majority vote-based ensemble technique. This research underscores the potential of ensemble deep learning and transfer learning methodologies to accurately identify the parts and uses of Ethiopian indigenous medicinal plant species. Notably, our proposed EfficientNet-based ensemble deep learning approach demonstrated remarkable accuracy, achieving a significant test and validation accuracy of 99.96%. Future endeavors will prioritize expanding the dataset, refining feature-extraction techniques, and creating user-friendly interfaces to overcome current dataset limitations. © 2024 by the authors.","deep learning; EfficientNet; ensemble learning; Ethiopian medicinal plants; identification","","Vice Presidency for Graduate Studies and Scientific Research at King Faisal University, (GRANT5643); Deanship of Scientific Research, King Saud University; Adama Science and Technology University, ASTU; King Faisal University, KFU","Funding text 1: The authors acknowledge the Deanship of Scientific Research, Vice Presidency for Graduate Studies and Scientific Research at King Faisal University, Saudi Arabia, for financial support under the annual funding track [GRANT5643]. Grammarly and QuillBot AI are acknowledged for their contributions in improving grammar and paraphrasing. The authors also acknowledge Adama Science and Technology University for financial support under Post Graduate Studies program. ; Funding text 2: This research got support under the annual funding track [GRANT5643] from Deanship of Scientific Research, King Faisal University, Saudi Arabia.","M.A. Kiflie; Department of Computer Science and Engineering, School of Electrical Engineering and Computing, Adama Science and Technology University, Adama, 1888, Ethiopia; email: muler.kmu@gmail.com; R. Srinivasagan; Department of Computer Engineering, King Faisal University, Al Hofuf, 31982, Saudi Arabia; email: rsamy@kfu.edu.sa","","Multidisciplinary Digital Publishing Institute (MDPI)","2073431X","","","","English","Comput.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185961786"
"Ojo M.O.; Viola I.; Miretti S.; Martignani E.; Giordano S.; Baratta M.","Ojo, Mike O. (57191574587); Viola, Irene (57219393836); Miretti, Silvia (13405590300); Martignani, Eugenio (22135236700); Giordano, Stefano (7101995308); Baratta, Mario (7003339497)","57191574587; 57219393836; 13405590300; 22135236700; 7101995308; 7003339497","A Deep Learning Approach for Accurate Path Loss Prediction in LoRaWAN Livestock Monitoring","2024","Sensors","24","10","2991","","","","1","10.3390/s24102991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194152120&doi=10.3390%2fs24102991&partnerID=40&md5=4f4cef357d3eb83d3e40618efece3f91","Department of Biological and Agricultural Engineering, Dallas, 75252, TX, United States; Department of Veterinary Sciences, University of Turin, 10095, Italy; Department of Information Engineering, University of Pisa, Pisa, 56126, Italy; Life Sciences and Environmental Sustainability, University of Parma, 43124, Italy","Ojo M.O., Department of Biological and Agricultural Engineering, Dallas, 75252, TX, United States, Department of Veterinary Sciences, University of Turin, 10095, Italy; Viola I., Department of Veterinary Sciences, University of Turin, 10095, Italy; Miretti S., Department of Veterinary Sciences, University of Turin, 10095, Italy; Martignani E., Department of Veterinary Sciences, University of Turin, 10095, Italy; Giordano S., Department of Information Engineering, University of Pisa, Pisa, 56126, Italy; Baratta M., Department of Veterinary Sciences, University of Turin, 10095, Italy, Life Sciences and Environmental Sustainability, University of Parma, 43124, Italy","The agricultural sector is amidst an industrial revolution driven by the integration of sensing, communication, and artificial intelligence (AI). Within this context, the internet of things (IoT) takes center stage, particularly in facilitating remote livestock monitoring. Challenges persist, particularly in effective field communication, adequate coverage, and long-range data transmission. This study focuses on employing LoRa communication for livestock monitoring in mountainous pastures in the north-western Alps in Italy. The empirical assessment tackles the complexity of predicting LoRa path loss attributed to diverse land-cover types, highlighting the subtle difficulty of gateway deployment to ensure reliable coverage in real-world scenarios. Moreover, the high expense of densely deploying end devices makes it difficult to fully analyze LoRa link behavior, hindering a complete understanding of networking coverage in mountainous environments. This study aims to elucidate the stability of LoRa link performance in spatial dimensions and ascertain the extent of reliable communication coverage achievable by gateways in mountainous environments. Additionally, an innovative deep learning approach was proposed to accurately estimate path loss across challenging terrains. Remote sensing contributes to land-cover recognition, while Bidirectional Long Short-Term Memory (Bi-LSTM) enhances the path loss model’s precision. Through rigorous implementation and comprehensive evaluation using collected experimental data, this deep learning approach significantly curtails estimation errors, outperforming established models. Our results demonstrate that our prediction model outperforms established models with a reduction in estimation error to less than 5 dB, marking a 2X improvement over state-of-the-art models. Overall, this study signifies a substantial advancement in IoT-driven livestock monitoring, presenting robust communication and precise path loss prediction in rugged landscapes. © 2024 by the authors.","internet of things; link quality; LoRa; LPWAN; propagation loss; remote sensing; smart agriculture","Agriculture; Forecasting; Gateways (computer networks); Learning systems; Long short-term memory; Remote sensing; Learning approach; Link quality; Lora; LPWAN; Mountainous environment; Path loss; Path loss prediction; Propagation loss; Remote-sensing; Smart agricultures; agriculture; Alps; article; artificial intelligence; controlled study; deep learning; internet of things; Italy; land use; livestock; pasture; prediction; remote sensing; short term memory; Internet of things","Italian Ministry for University and Research; CRC Foundation; Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR, (2023–2027); Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR","This work has benefited from the equipment and framework of the COMP-HUB and COMP-R Initiatives, funded by the \u2018Departments of Excellence\u2019 program of the Italian Ministry for University and Research (MIUR, 2018\u20132022 and MUR, 2023\u20132027) and SMARTSHEEP 4.0 funded by CRC Foundation.","M.O. Ojo; Department of Biological and Agricultural Engineering, Dallas, 75252, United States; email: mike.ojo@ag.tamu.edu; M. Baratta; Department of Veterinary Sciences, University of Turin, 10095, Italy; email: mario.baratta@unipr.it","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","38793846","English","Sensors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85194152120"
"Li S.; Yan P.; Tian X.; Zhai Y.","Li, Shuting (59255853900); Yan, Pengcheng (57215667065); Tian, Xiaolin (7202380154); Zhai, Yikui (57198524573)","59255853900; 57215667065; 7202380154; 57198524573","CassavaViM: MambaVision with Efficient Multi-Scale Attention for Cassava Leaf Disease Recognition","2024","2024 5th International Conference on Computer Engineering and Intelligent Control, ICCEIC 2024","","","","342","345","3","0","10.1109/ICCEIC64099.2024.10775923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215505308&doi=10.1109%2fICCEIC64099.2024.10775923&partnerID=40&md5=992d734a3fccf130dacc5970d49bcc66","Macau University of Science and Technology, Faculty of Innovation Engineering, Macao, Macao; Wuyi University, Department of Intelligent Manufacturing, Jiangmen, China","Li S., Macau University of Science and Technology, Faculty of Innovation Engineering, Macao, Macao; Yan P., Macau University of Science and Technology, Faculty of Innovation Engineering, Macao, Macao; Tian X., Macau University of Science and Technology, Faculty of Innovation Engineering, Macao, Macao; Zhai Y., Wuyi University, Department of Intelligent Manufacturing, Jiangmen, China","Cassava leaf diseases represent a critical challenge to crop yield and quality. Despite the growing need for accurate disease identification, existing solutions often fail to adequately capture the subtle and diverse symptoms of these diseases. To address this gap, we propose Cassava ViM, a novel model for cassava leaf disease recognition that builds upon the Mamba Vision architecture. Cassava ViM leverages a multi-scale fusion strategy and integrates an Efficient Multi-Scale Attention (EMA) module with Cross-Spatial Learning, enabling it to effectively capture disease patterns across varying scales while refining feature integration. Through comprehensive experiments on the Cassava Leaf Diseases Dataset, Cassava ViM achieves a remarkable accuracy of 87.590/0, showing a substantial improvement over known state-of-the-art methods. This technology enhances cassava's early disease detection capabilities, facilitating timely intervention and improving agricultural productivity in southern regions such as the Greater Bay Area.  © 2024 IEEE.","Cassava protection; Deep learning; EMA; MambaVision; Precision agriculture","Invertebrates; Livestock; Cassavum protection; Critical challenges; Crop quality; Crop yield; Deep learning; Efficient multi-scale attention; Leaf disease; Mambavision; Multi-scales; Precision Agriculture; Plant diseases","","","X. Tian; Macau University of Science and Technology, Faculty of Innovation Engineering, Macao, Macao; email: xltian@must.edu.mo","","Institute of Electrical and Electronics Engineers Inc.","","979-833150799-2","","","English","Int. Conf. Comput. Eng. Intell. Control, ICCEIC","Conference paper","Final","","Scopus","2-s2.0-85215505308"
"Pramila S.; Kumar A.S.; Kishore K.V.; Guruprasad; Chaithra K.N.","Pramila, S. (59410297300); Kumar, Arun S. (57191783499); Kishore, K.V. (59407489900); Guruprasad (59410483100); Chaithra, K.N. (57215537495)","59410297300; 57191783499; 59407489900; 59410483100; 57215537495","Deep Learning-based Automated Classification of Chicken Fecal Samples for Disease Detection","2024","15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024","2","","","5806","5812","6","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209149720&partnerID=40&md5=0acc8c08e0c781696a1667f6867d2dc9","New Horizon College of Engineering, Bengaluru, India; Nagarjuna College of Engineering and Technology, Bengaluru, India; CMR Institute of Technology, Bengaluru, India; NMAM Institute of Technology, Nitte (Deemed to be University), Udupi, India; Nitte Meenakshi Institute of Technology, Bengaluru, India","Pramila S., New Horizon College of Engineering, Bengaluru, India; Kumar A.S., Nagarjuna College of Engineering and Technology, Bengaluru, India; Kishore K.V., CMR Institute of Technology, Bengaluru, India; Guruprasad, NMAM Institute of Technology, Nitte (Deemed to be University), Udupi, India; Chaithra K.N., Nitte Meenakshi Institute of Technology, Bengaluru, India","Disease detection in poultry farming is crucial for ensuring the health and productivity of chickens.This paper presents a novel deep learning-based approach for automated disease classification using images of chicken fecal matter.The objective is to develop a model capable of accurately identifying diseases early, facilitating timely intervention and effective management in poultry farms.The methodology encompasses several key components.Firstly, a diverse and balanced dataset of chicken fecal samples is collected, comprising images representing various diseases.Data preprocessing techniques such as image normalization, resizing, and augmentation are applied to enhance the model's robustness and generalization ability.The neural network architecture utilizes transfer learning with MobileNetV3Small, a lightweight yet powerful convolutional neural network (CNN) architecture known for its efficiency in image classification tasks.Results indicate that the developed model achieves competitive test accuracy, demonstrating its effectiveness in classifying diseases in chicken fecal samples.Precision, recall, and F1-score metrics further illustrate the model's ability to correctly identify and differentiate between different disease classes.The use of transfer learning with MobileNetV3Small contributes to the model's success by leveraging pre-trained features and optimizing computational efficiency. © Grenze Scientific Society, 2024.","Automated Classification; Deep Learning; Disease Detection; Fecal Matter Analysis; Poultry Farming; Transfer Learning","Deep learning; Livestock; Transfer learning; Automated classification; Deep learning; Disease classification; Disease detection; Faecal samples; Fecal matter analyze; Learning-based approach; Neural network architecture; Poultry farming; Transfer learning; Convolutional neural networks","","","","Stephen J.; Sharma P.; Chaba Y.; Abraham K.U.; Anooj P.K.; Mohammad N.; Thomas G.; Srikiran S.","Grenze Scientific Society","","979-833130057-9","","","English","Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT","Conference paper","Final","","Scopus","2-s2.0-85209149720"
"Higuchi Y.; Iwai Y.; Aoki K.","Higuchi, Yuito (58763867600); Iwai, Yoshio (57201788506); Aoki, Kota (59358370800)","58763867600; 57201788506; 59358370800","Combining Digital Ultrasound Images and Individual Information for Beef Marbling Standard Estimation of Live Cattle","2024","GCCE 2024 - 2024 IEEE 13th Global Conference on Consumer Electronics","","","","469","472","3","0","10.1109/GCCE62371.2024.10760952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213311535&doi=10.1109%2fGCCE62371.2024.10760952&partnerID=40&md5=e91670f76b5255c5835fb640bc88ce35","Tottori University, Graduate School of Sustainability Science, Tottori, Japan; Tottori University, Graduate School of Engineering, Tottori, Japan","Higuchi Y., Tottori University, Graduate School of Sustainability Science, Tottori, Japan; Iwai Y., Tottori University, Graduate School of Engineering, Tottori, Japan; Aoki K., Tottori University, Graduate School of Engineering, Tottori, Japan","Livestock farmers are keen to estimate the value of live beef cattle accurately before slaughter. This study designs a neural network to estimate the Beef Marbling Standard (BMS) using digital ultrasound images and individual information such as producer and sire details. Our multi-input neural network demonstrated high accuracy in BMS estimation, providing a valuable tool for the livestock industry. © 2024 IEEE.","beef marbling standard estimation; deep learning; digital ultrasound images; individual information","Ultrasonic imaging; Beef cattle; Beef marbling standard estimation; Cattles; Deep learning; Digital ultrasound image; Individual information; Livestock farmers; Neural-networks; Standard estimation; Ultrasound images; Beef","","","Y. Higuchi; Tottori University, Graduate School of Sustainability Science, Tottori, Japan; email: m24j4051k@edu.tottori-u.ac.jp","","Institute of Electrical and Electronics Engineers Inc.","","979-835035507-9","","","English","GCCE - IEEE Glob. Conf. Consum. Electron.","Conference paper","Final","","Scopus","2-s2.0-85213311535"
"Jung J.-M.; Kim D.-H.; Cho H.; Lee M.; Jeong J.; Lee D.-H.; Seo S.; Lee W.-H.","Jung, Jae-Min (57189522293); Kim, Dong-Hyeon (58752819400); Cho, Hyunjin (57865735700); Lee, Mingyung (57205299978); Jeong, Jinhui (58861087500); Lee, Dae-Hyun (55541929900); Seo, Seongwon (13103703200); Lee, Wang-Hee (55767355500)","57189522293; 58752819400; 57865735700; 57205299978; 58861087500; 55541929900; 13103703200; 55767355500","Multi-algorithmic approach for detecting outliers in cattle intake data","2024","Journal of Agriculture and Food Research","15","","101021","","","","4","10.1016/j.jafr.2024.101021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183931381&doi=10.1016%2fj.jafr.2024.101021&partnerID=40&md5=7781b8e68ffc1b00b9d1745a7eaf4993","Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; Division of Animal and Dairy Sciences, Chungnam National University, Daejeon, 34134, South Korea; Department of Smart Agriculture Systems, Chungnam National University, Daejeon, 34134, South Korea","Jung J.-M., Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; Kim D.-H., Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; Cho H., Division of Animal and Dairy Sciences, Chungnam National University, Daejeon, 34134, South Korea; Lee M., Division of Animal and Dairy Sciences, Chungnam National University, Daejeon, 34134, South Korea; Jeong J., Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; Lee D.-H., Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; Seo S., Division of Animal and Dairy Sciences, Chungnam National University, Daejeon, 34134, South Korea; Lee W.-H., Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea, Department of Smart Agriculture Systems, Chungnam National University, Daejeon, 34134, South Korea","Monitoring cattle feed intake is crucial for evaluating animal health, productivity, and farm profitability. In particular, an abnormal intake is related to the cattle activity. Therefore, outlier detection forms the basis for intake monitoring. This study employed multiple algorithms ranging from statistics to deep learning to detect outliers in time-series data of cattle intake. We used five models implementing mean + standard deviation, moving average, box plot, time series decomposition, and autoencoder, and attempted to enhance the detection performance by a voting system to combine more than one model. Both box plot and time-series decomposition demonstrated high accuracy (over 95 %) and F1-score (harmonic mean of precision and recall). Thus, it reliably distinguished normal values from outliers. Moving average exhibited a high true-skill statistic (TSS), thereby rendering it suitable for outlier detection. The voting system gave F1 and TSS scores of 0.49 and 0.65, respectively. Thus, it enhanced the detection performance compared with the individual model. These results demonstrate that the performance metrics vary depending on the type of algorithm. This, in turn, highlights the need to select algorithms adapted to the monitoring objectives. The algorithmic selection can be complemented by a voting system. This demonstrates its potential for generating a reliable database with accurate outlier detection and aiding decision-making by livestock producers. © 2024","Anomaly detection; Cattle intake; Machine learning; Statistics; Voting process","","Korea Smart Farm R&D Foundation; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421022–04); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT ( MSIT ), Rural Development Administration (RDA) (Project No. 421022–04 ). ","W.-H. Lee; Department of Biosystems Machinery Engineering, Chungnam National University, Daejeon, 34134, South Korea; email: wanghee@cnu.ac.kr; S. Seo; Division of Animal and Dairy Sciences, Chungnam National University, Daejeon, 34134, South Korea; email: swseo@cnu.ac.kr","","Elsevier B.V.","26661543","","","","English","J. Agric. Food. Res.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183931381"
"Pei S.; Zhang X.; Tan L.; Dong S.; Guo J.","Pei, ShengLei (51665593400); Zhang, Xin (59477921400); Tan, Lin (59477584200); Dong, Shi (59478358600); Guo, Jianshuo (59477693200)","51665593400; 59477921400; 59477584200; 59478358600; 59477693200","Realizing Cattle and Sheep Target Detection in Ecological Livestock Farming Based on Improved YOLOv8 Model","2024","Proceedings - 2024 6th Asia Symposium on Image Processing, ASIP 2024","","","","21","28","7","0","10.1109/ASIP63198.2024.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212387416&doi=10.1109%2fASIP63198.2024.00012&partnerID=40&md5=f122afb35f8a1cbb85862aea965a723b","Qinghai Minzu University, Xining, Qinghai, China","Pei S., Qinghai Minzu University, Xining, Qinghai, China; Zhang X., Qinghai Minzu University, Xining, Qinghai, China; Tan L., Qinghai Minzu University, Xining, Qinghai, China; Dong S., Qinghai Minzu University, Xining, Qinghai, China; Guo J., Qinghai Minzu University, Xining, Qinghai, China","With the advancement of AI and big data technology, the livestock industry in Qinghai Province has encountered a significant opportunity for livestock monitoring and management. The abundance of image data has provided a new driving force for economic development, but variations in lighting, background, and animal posture present challenges for target detection. To adapt to the plateau environment, continuous innovation and optimization of algorithms are necessary.In order to address the challenges in livestock monitoring, a large number of image data of cattle and sheep were collected through widespread deployment of cameras in Qinghai Province. After thorough investigation of YOLO series models, it was determined that YOLOv8 outperformed YOLOv5, YOLOv6, and YOLOv7 in accuracy while also demonstrating superior stability compared to YOLOv9. As a result, YOLOv8 was selected as the base model for optimization leading to the development of a new object detection model - YOLOV8-ZX. During the optimization process, several innovative strategies were implemented including selecting ODC-FPN as the backbone network to enhance feature extraction capability; introducing c2f-cam-Neck structure to improve recognition ability in complex backgrounds; unifying object detection head and integrating attention mechanism (Dy-Head)[2] which not only improved generalization performance but also significantly reduced computational complexity. These optimizations enabled YOLOv8-ZX to maintain excellent detection accuracy and stability even in complex and variable environments. Furthermore, introduction of attention mechanism enhanced network's ability to identify key features thereby improving accuracy and robustness of detection. Compared with YOLOV8 model alone,YOLOV8-ZX demonstrated 4.4% increase in mAP50 , 5.4% increase MAP50-95, and 3.7% decrease box damage. Experimental results indicate that optimizedYolo v8 model exhibits exceptional generalization performance & robustness when addressing livestock detection tasks within Qinghai province thus providing an efficient & practical technical solution for livestock monitoring. These research findings not only promote application computer vision within animal husbandry but also provide effective reference solutions similar target detections problems within similar environments. © 2024 IEEE.","Attention mechanism; Big data; Cattle and sheep breeding; Deep learning; Single stage target detection; Target detection; Tibetan Plateau; YOLOv8","Deep learning; Macroinvertebrates; Attention mechanisms; Cattle and sheep breeding; Cattles; Deep learning; Single stage; Single stage target detection; Targets detection; Tibetan Plateau; YOLOv8; Livestock","Key Research and Development Commercialization Projects of Qinghai Province Research and Demonstration; National Natural Science Foundation of China, NSFC, (62266035); National Natural Science Foundation of China, NSFC","This work is supported by National Natural Science Foundation of China, Grant No. 62266035. the Key Research and Development Commercialization Projects of Qinghai Province Research and Demonstration of Early Warning and Prevention Technology for Human-Bear Conflicts in Qinghai Province , Grant No.2023-SF-148S-1.","S. Pei; Qinghai Minzu University, Qinghai, Xining, China; email: peishenglei@126.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835037615-9","","","English","Proc. - Asia Symp. Image Process., ASIP","Conference paper","Final","","Scopus","2-s2.0-85212387416"
"Ben Ameur H.; Boubaker S.; Ftiti Z.; Louhichi W.; Tissaoui K.","Ben Ameur, Hachmi (55361753800); Boubaker, Sahbi (36545771900); Ftiti, Zied (35174578500); Louhichi, Wael (34979936200); Tissaoui, Kais (56823463300)","55361753800; 36545771900; 35174578500; 34979936200; 56823463300","Forecasting commodity prices: empirical evidence using deep learning tools","2024","Annals of Operations Research","339","1-2","","349","367","18","17","10.1007/s10479-022-05076-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146576311&doi=10.1007%2fs10479-022-05076-6&partnerID=40&md5=66aea019966d5fd06633cb252ad887ca","INSEEC Grande Ecole, Omnes Education, Paris, France; Department of Computer and Network Engineering, College of Computer Science and Engineering, University of Jeddah, Jeddah, 21959, Saudi Arabia; OCRE-Lab, EDC Paris Business School, Paris, France; ESSCA School of Management, Paris, France; University of Ha’il, Applied College, PO Box 2440, Hail City, Saudi Arabia; Faculty of Economic Sciences and Management of Tunis, Universityof Tunis El Manar, the InternationalFinance Group, Tunis, Tunisia","Ben Ameur H., INSEEC Grande Ecole, Omnes Education, Paris, France; Boubaker S., Department of Computer and Network Engineering, College of Computer Science and Engineering, University of Jeddah, Jeddah, 21959, Saudi Arabia; Ftiti Z., OCRE-Lab, EDC Paris Business School, Paris, France; Louhichi W., ESSCA School of Management, Paris, France; Tissaoui K., University of Ha’il, Applied College, PO Box 2440, Hail City, Saudi Arabia, Faculty of Economic Sciences and Management of Tunis, Universityof Tunis El Manar, the InternationalFinance Group, Tunis, Tunisia","Since the last two decades, financial markets have exhibited several transformations owing to recurring crises episodes that has led to the development of alternative assets. Particularly, the commodity market has attracted attention from investors and hedgers. However, the operational research stream has also developed substantially based on the growth of the artificial intelligence field, which includes machine learning and deep learning. The choice of algorithms in both machine learning and deep learning is case-sensitive. Hence, AI practitioners should first attempt solutions related to machine learning algorithms, and if such solutions are unsatisfactory, they must apply deep learning algorithms. Using this perspective, this study aims to investigate the potential of various deep learning basic algorithms for forecasting selected commodity prices. Formally, we use the Bloomberg Commodity Index (noted by the Global Aggregate Index) and its five component indices: Bloomberg Agriculture Subindex, Bloomberg Precious Metals Subindex, Bloomberg Livestock Subindex, Bloomberg Industrial Metals Subindex, and Bloomberg Energy Subindex. Based on daily data from January 2002 (the beginning wave of commodity markets' financialization) to December 2020, results show the effectiveness of the Long Short-Term Memory method as a forecasting tool and the superiority of the Bloomberg Livestock Subindex and Bloomberg Industrial Metals Subindex for assessing other commodities' indices. These findings is important in term for investors in term of risk management as well as policymakers in adjusting public policy, especially during Russian-Ukrainian war. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.","Bloomberg Commodity Index; Commodity markets; Deep learning; Forecasting; Performance metrics","","","","H. Ben Ameur; INSEEC Grande Ecole, Omnes Education, Paris, France; email: hechba@yahoo.fr","","Springer","02545330","","","","English","Ann. Oper. Res.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85146576311"
"Singh N.; Devi I.; Dudi K.; Chouriya A.","Singh, Naseeb (57225962597); Devi, Indu (56728343200); Dudi, Kuldeep (57191854426); Chouriya, Arjun (58335842800)","57225962597; 56728343200; 57191854426; 58335842800","Development of Attention-Enabled Multi-Scale Pyramid Network-Based Models for Body Part Segmentation of Dairy Cows","2024","Journal of Biosystems Engineering","49","2","","186","201","15","1","10.1007/s42853-024-00226-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193058492&doi=10.1007%2fs42853-024-00226-z&partnerID=40&md5=987a6f06146ea92191c88a6980415949","Division of System Research and Engineering, ICAR RC for NEH Region, Meghalaya, Umiam, 793103, India; Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Krishi Vigyan Kendra (Under Haryana Agricultural University, Hisar), Haryana, Panipat, 132103, India; Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721302, India","Singh N., Division of System Research and Engineering, ICAR RC for NEH Region, Meghalaya, Umiam, 793103, India; Devi I., Livestock Production Management Division, ICAR-National Dairy Research Institute, Haryana, Karnal, 132001, India; Dudi K., Krishi Vigyan Kendra (Under Haryana Agricultural University, Hisar), Haryana, Panipat, 132103, India; Chouriya A., Indian Institute of Technology Kharagpur, West Bengal, Kharagpur, 721302, India","Purpose: Automated assessment of dairy cow traits, important for productivity evaluation, provides advantages by mitigating personal biases, measurement errors, and stress factors typically associated with manual assessment. To develop such a system, the initial step involves accurately segmenting cow body regions for subsequent trait measurement. Methods: Thus, the present study introduces a refined DeepLabV3 + CNN model with EfficientNetB2 as the backbone and enhanced with attention mechanisms, aiming for precise segmentation of cow body regions from lateral and posterior views. In the DeepLabV3 + model, various backbone models, including MobileNet, MobileNetV2, MobileNetV3, EfficientNetB0, EfficientNetB1, and EfficientNetB2, were evaluated. Among these, EfficientNetB2 exhibited superior performance in lateral view segmentation, achieving a mean Intersection-over-union (m-IoU) of 94.19%. To further enhance segmentation accuracy, attention mechanisms such as Squeeze and Excitation (SE), Residual connection-infused Squeeze and Excitation (SER), Convolutional Block Attention Module (CBAM), and Residual connection-infused Convolutional Block Attention Module (CBAMR) were incorporated into the DeepLabV3 + model. Results: The introduction of attention mechanisms in the EfficientNetB2 model led to enhanced m-IoU values: SE (94.27%), SER (94.25%), CBAM (94.59%), and CBAMR (94.66%). EfficientNetB2, integrated with CBAM and Residual connections (termed CBAMR), found to be top-performing model, achieving m-IoU values of 94.66% (lateral view), 93.77% (posterior view), and 99.61% (stature). The lateral view segmentation demonstrated high IoU for the body (98.73%) and rump (96.54%), with lowest IoU for teats (79.70%) due to their smaller spatial presence in input image. For posterior view regions, the CBAMR model achieved IoU scores above 79.0%, with the rear leg showing the highest (96.70%) and rump bones the lowest (79.52%). The segmentation accuracy for stature exceeded 90.0%, indicating less complexity in single-body region segmentation. Conclusions: Therefore, these developed models demonstrate considerable accuracy in segmenting cow regions, making a significant contribution to the advancement of computer vision–based systems for measuring linear-type traits, and hold promise for deployment in such an automatic system. © The Author(s), under exclusive licence to The Korean Society for Agricultural Machinery 2024.","Computer vision; Dairy cattle; Deep learning; Precision livestock farming; Semantic segmentation","Convolution; Deep learning; Farms; Image enhancement; Semantic Segmentation; Semantics; Attention mechanisms; Body regions; Dairy cattles; Dairy cow; Deep learning; Multi-Scale pyramids; Precision livestock farming; Pyramid network; Segmentation accuracy; Semantic segmentation; Computer vision","Science and Engineering Research Board, SERB, (SRG/2020/001804); Science and Engineering Research Board, SERB","This study received financial grant from the Science Engineering Research Board (SERB), New Delhi, India (SRG/2020/001804). ","I. Devi; Livestock Production Management Division, ICAR-National Dairy Research Institute, Karnal, Haryana, 132001, India; email: indulathwal@gmail.com","","Springer Nature","17381266","","","","English","J. Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85193058492"
"Lee J.G.; Lee S.S.; Alam M.; Lee S.M.; Seong H.-S.; Park M.N.; Han S.; Nguyen H.-P.; Baek M.K.; Phan A.T.; Dang C.G.; Nguyen D.T.","Lee, Jae Gu (57195478511); Lee, Seung Soo (57192515518); Alam, Mahboob (35510212600); Lee, Sang Min (58000597700); Seong, Ha-Seung (57203391848); Park, Mi Na (55322279700); Han, Seungkyu (57929025900); Nguyen, Hoang-Phong (58846034200); Baek, Min Ki (58782344700); Phan, Anh Tuan (58876643300); Dang, Chang Gwon (55895072300); Nguyen, Duc Toan (58001897200)","57195478511; 57192515518; 35510212600; 58000597700; 57203391848; 55322279700; 57929025900; 58846034200; 58782344700; 58876643300; 55895072300; 58001897200","Utilizing 3D Point Cloud Technology with Deep Learning for Automated Measurement and Analysis of Dairy Cows","2024","Sensors","24","3","987","","","","4","10.3390/s24030987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184665347&doi=10.3390%2fs24030987&partnerID=40&md5=0ab001055352ddbefcdec08c02b58ac4","National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea","Lee J.G., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Lee S.S., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Alam M., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Lee S.M., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Seong H.-S., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Park M.N., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Han S., ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea; Nguyen H.-P., ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea; Baek M.K., ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea; Phan A.T., ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea; Dang C.G., National Institute of Animal Science, Rural Development Administration, Chungcheongnam-do, Cheonan, 31000, South Korea; Nguyen D.T., ZOOTOS Co., Ltd., R&D Center, Gyeonggi-do, Anyang, 14118, South Korea","This paper introduces an approach to the automated measurement and analysis of dairy cows using 3D point cloud technology. The integration of advanced sensing techniques enables the collection of non-intrusive, precise data, facilitating comprehensive monitoring of key parameters related to the health, well-being, and productivity of dairy cows. The proposed system employs 3D imaging sensors to capture detailed information about various parts of dairy cows, generating accurate, high-resolution point clouds. A robust automated algorithm has been developed to process these point clouds and extract relevant metrics such as dairy cow stature height, rump width, rump angle, and front teat length. Based on the measured data combined with expert assessments of dairy cows, the quality indices of dairy cows are automatically evaluated and extracted. By leveraging this technology, dairy farmers can gain real-time insights into the health status of individual cows and the overall herd. Additionally, the automated analysis facilitates efficient management practices and optimizes feeding strategies and resource allocation. The results of field trials and validation studies demonstrate the effectiveness and reliability of the automated 3D point cloud approach in dairy farm environments. The errors between manually measured values of dairy cow height, rump angle, and front teat length, and those calculated by the auto-measurement algorithm were within 0.7 cm, with no observed exceedance of errors in comparison to manual measurements. This research contributes to the burgeoning field of precision livestock farming, offering a technological solution that not only enhances productivity but also aligns with contemporary standards for sustainable and ethical animal husbandry practices. © 2024 by the authors.","automatic measurement and analysis; dairy cow; deep learning; machine learning; point cloud registration","Automation; Farms; Learning systems; Quality control; 3D point cloud; Automated analysis; Automatic analysis; Automatic measurements; Cloud technologies; Dairy cow; Deep learning; Machine-learning; Measurement and analysis; Point cloud registration; Deep learning","Korea Smart Farm R&D Foundation; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421011-03); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (421011-03).","C.G. Dang; National Institute of Animal Science, Rural Development Administration, Cheonan, Chungcheongnam-do, 31000, South Korea; email: gkgkgki@korea.kr; D.T. Nguyen; ZOOTOS Co., Ltd., R&D Center, Anyang, Gyeonggi-do, 14118, South Korea; email: dolphin@zootos.com","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","38339704","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85184665347"
"Liu Z.; Du H.; Lao F.-D.; Shen Z.-C.; Lv Y.-H.; Zhou L.; Jiang L.; Liu J.-F.","Liu, Zhen (57575483000); Du, Heng (57201130648); Lao, Feng-Dan (54947579500); Shen, Zhen-Cai (23568561900); Lv, Yi-Hang (58786568700); Zhou, Lei (57281231500); Jiang, Li (56784727400); Liu, Jian-Feng (51665144000)","57575483000; 57201130648; 54947579500; 23568561900; 58786568700; 57281231500; 56784727400; 51665144000","PIMFP: An accurate tool for the prediction of intramuscular fat percentage in live pigs using ultrasound images based on deep learning","2024","Computers and Electronics in Agriculture","217","","108552","","","","7","10.1016/j.compag.2023.108552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180962849&doi=10.1016%2fj.compag.2023.108552&partnerID=40&md5=f4c2ede970583b6f54f63293eb13d728","State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; Network Technology Center, China Agricultural University, Beijing, 100083, China; College of Science, China Agricultural University, Beijing, 100083, China; Muyuan Foods Co., Ltd, Nanyang, 474361, China","Liu Z., State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; Du H., State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; Lao F.-D., Network Technology Center, China Agricultural University, Beijing, 100083, China; Shen Z.-C., College of Science, China Agricultural University, Beijing, 100083, China; Lv Y.-H., Muyuan Foods Co., Ltd, Nanyang, 474361, China; Zhou L., State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; Jiang L., State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; Liu J.-F., State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China","Intramuscular fat (IMF) percentage is the proportion of fat between the muscle fibers of the loin eye, which contributes significantly to the flavor and tenderness of the pork. Traditional IMF percentage is measured after slaughter, which requires a high cost, and the slaughtered pig with high IMF percentage can not be directly retained for breeding. Instead of slaughter, using ultrasound images to predict IMF percentage has been gradually proposed as an alternative measurement. However, due to the high cost of dataset construction and limited feature extraction capability of traditional image analysis methods, there is still a lack of an accurate and robust method based on the large-scale dataset for predicting IMF percentage. Here, to establish an accurate model for non-invasive prediction of IMF percentage in pigs, a total number of 4,552 ultrasound images and their corresponding IMF percentage of 945 pigs are collected in this study. We proposed a general-purpose model, PIMFP, to accurately predict IMF percentage based on the deep convolutional neural network. The PIMFP model comprises three primary modules: image preprocessing, feature extraction, and IMF percentage prediction. The preprocessing procedures for the raw ultrasound images include channel conversion, region of interest selection, and contrast enhancement. The feature extraction module characterizes the preprocessed images using the residual neural network as the backbone to extract features. The image features are then delivered to the IMF percentage prediction module depending on the fully connected layer. Our proposed model exhibits significant advantages over existing models in predictive performance, computational speed, classification ability, robustness, and interpretability. For easier use, we also provide a user-friendly web tool for predicting IMF percentage automatically in a single step. In conclusion, our study demonstrates the power of the PIMFP model for predicting IMF percentage, which has great potential in future breeding and highlights the application of artificial intelligence algorithms in livestock. © 2023 The Author(s)","Convolutional neural network; Deep learning; Intramuscular fat percentage; Non-invasive prediction; Ultrasound image","Agriculture; Convolution; Convolutional neural networks; Cost benefit analysis; Deep neural networks; Extraction; Forecasting; Image enhancement; Image segmentation; Large dataset; Mammals; Ultrasonics; Convolutional neural network; Deep learning; Features extraction; High costs; Image-based; Intramuscular fat percentage; Intramuscular fats; Muscle fiber; Non-invasive prediction; Ultrasound images; accuracy assessment; artificial intelligence; artificial neural network; detection method; image analysis; performance assessment; pig; Feature extraction","National Natural Science Foundation of China, NSFC, (31972563); China Agricultural University, CAU; Earmarked Fund for China Agriculture Research System, (CARS-36-05B); National Key Research and Development Program of China, NKRDPC, (2021YFD1200801, 2021YFD1300800)","This research was funded by the National Natural Science Foundations of China ( 31972563 ), the National Key Research and Development Program of China ( 2021YFD1300800 ), the National Key Research and Development Program of China ( 2021YFD1200801 ), the Earmarked Fund for China Agriculture Research System (No. CARS-36-05B ) and the 2115 Talent Development Program of China Agricultural University. The data analysis was supported by High-performance Computing Platform of China Agricultural University. We also would like to thank github@chouxianyu for sharing the visualization on ResNet50 and Freepik@pch.vector for sharing the pig anatomy image.","J.-F. Liu; State Key Laboratory of Animal Biotech Breeding, College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; email: liujf@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85180962849"
"Wafa M.R.A.; Toh-Arlim M.; Sukaridhoto S.; Budiarti R.P.N.; Prayudi A.","Wafa, M. Rosyad Ahsanul (59517467400); Toh-Arlim, Muhaimin (59517467500); Sukaridhoto, Sritrusta (35100882700); Budiarti, Rizqi Putri Nourma (57193812041); Prayudi, Agus (57220072007)","59517467400; 59517467500; 35100882700; 57193812041; 57220072007","Optimizing Optical Character Recognition Accuracy in Livestock Weighting Systems Through Distributed Intelligent Computer Vision","2024","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","","","","367","371","4","0","10.1109/ISCT62336.2024.10791290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215318745&doi=10.1109%2fISCT62336.2024.10791290&partnerID=40&md5=e7ba150f1f0f72fed6546e840c5a0a97","Politeknik Elektronika Negeri Surabaya, Dept. of Multimedia Creative Technology, Surabaya, Indonesia; Universitas Nahdlatul Ulama Surabaya, Faculty of Economics, Business, and Digital Technology, Surabaya, Indonesia","Wafa M.R.A., Politeknik Elektronika Negeri Surabaya, Dept. of Multimedia Creative Technology, Surabaya, Indonesia; Toh-Arlim M., Politeknik Elektronika Negeri Surabaya, Dept. of Multimedia Creative Technology, Surabaya, Indonesia; Sukaridhoto S., Politeknik Elektronika Negeri Surabaya, Dept. of Multimedia Creative Technology, Surabaya, Indonesia; Budiarti R.P.N., Universitas Nahdlatul Ulama Surabaya, Faculty of Economics, Business, and Digital Technology, Surabaya, Indonesia; Prayudi A., Politeknik Elektronika Negeri Surabaya, Dept. of Multimedia Creative Technology, Surabaya, Indonesia","This research develops and optimizes an optical character recognition (OCR) system for goat weighing scales using intelligent computer vision and distributed computing to enhance accuracy and efficiency in real-world operational conditions. Images of the scales are captured and processed to remove noise, then recognized using deep learning algorithms. The implementation of distributed computing frameworks like Apache Hadoop and TensorFlow enables efficient parallel processing. Results show a 20% increase in OCR accuracy and a 30% improvement in processing efficiency compared to conventional systems. This integration provides a scalable and reliable solution for applications in the livestock industry, with potential adaptation for various operational conditions and other livestock types. © 2024 IEEE.","Distributed Computing; Intelligent Computer Vision; Livestock Management; Optical Character Recognition (OCR)","Intelligent computer vision; Livestock management; Operational conditions; Optical character recognition; Optical character recognition system; Optical-; Real-world; Recognition accuracy; REmove noise; Weighting systems; Scales (weighing instruments)","","","","","Institute of Electrical and Electronics Engineers Inc.","0747668X","979-835036519-1","DTPEE","","English","Dig Tech Pap IEEE Int Conf Consum Electron","Conference paper","Final","","Scopus","2-s2.0-85215318745"
"Moonis A.; Singh A.","Moonis, Abdullah (58483923200); Singh, Ajeet (57226053402)","58483923200; 57226053402","Optimized Insect Classification on Farms using Tuned Convolutional Neural Networks","2024","2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024","","","","","","","0","10.1109/ICCCNT61001.2024.10726116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213046331&doi=10.1109%2fICCCNT61001.2024.10726116&partnerID=40&md5=88827e824b9f9cf443573669a3d40392","Vellore Institute of Technology, VIT Bhopal University Sehore, School of Computing Science and Engineering (SCSE), Madhya Pradesh, 466 114, India","Moonis A., Vellore Institute of Technology, VIT Bhopal University Sehore, School of Computing Science and Engineering (SCSE), Madhya Pradesh, 466 114, India; Singh A., Vellore Institute of Technology, VIT Bhopal University Sehore, School of Computing Science and Engineering (SCSE), Madhya Pradesh, 466 114, India","Effective classification of farm insects plays a critical role in agricultural pest management. This paper introduces a refined convolutional neural network (CNN) model that enhances the capability of pre-trained models for the task of insect classification. Leveraging advanced architectures such as ResNet-50V2 and Xception, we modified these models to better suit our specific classification needs. Our investigation not only includes a comparative analysis of these pre-trained models but also explores the influence of various hyper-parameters on model performance. Experimental outcomes demonstrated that while the pre-trained models achieved classification accuracies of 90% and 93% for ResNet-50V2 and Xception respectively, our tailored model significantly outperformed these by securing a 99% accuracy rate on a dataset comprising 15 different classes of farm insects. Moreover, our model exhibited a notable reduction in overfitting, illustrating its robustness and effectiveness in insect classification. This research work paves the way for more empirical and efficient computational intelligence approaches in the use of deep learning for agricultural applications. © 2024 IEEE.","CNN-based methods; farm insect classification; image segmentation; pest detection","Convolutional neural networks; Image segmentation; Invertebrates; Livestock; Macroinvertebrates; Agricultural pests; Convolutional neural network; Convolutional neural network-based method; Farm insect classification; Images segmentations; Insect classifications; Network-based; Neural network model; Pest detection; Pest management","","","A. Moonis; Vellore Institute of Technology, VIT Bhopal University Sehore, School of Computing Science and Engineering (SCSE), Madhya Pradesh, 466 114, India; email: abdullahmoonis567@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835037024-9","","","English","Int. Conf. Comput. Commun. Netw. Technol., ICCCNT","Conference paper","Final","","Scopus","2-s2.0-85213046331"
"Li J.; Xu M.; Xiang L.; Chen D.; Zhuang W.; Yin X.; Li Z.","Li, Jiajia (58317754400); Xu, Mingle (57226784602); Xiang, Lirong (57201118152); Chen, Dong (57190854977); Zhuang, Weichao (56338038400); Yin, Xunyuan (57109662800); Li, Zhaojian (57202421579)","58317754400; 57226784602; 57201118152; 57190854977; 56338038400; 57109662800; 57202421579","Foundation models in smart agriculture: Basics, opportunities, and challenges","2024","Computers and Electronics in Agriculture","222","","109032","","","","8","10.1016/j.compag.2024.109032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194411048&doi=10.1016%2fj.compag.2024.109032&partnerID=40&md5=1135f2ab2f381ce4ebb912f6fcf8b856","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, United States; Department of Electronic Engineering, Core Research Institute of Intelligent Robots, Jeonbuk National University, South Korea; Department of Biological and Agricultural Engineering, North Carolina State University, Raleigh, NC, United States; School of Mechanical Engineering, Southeast University, Nanjing, China; School of Chemistry, Chemical Engineering and Biotechnology, Nanyang Technological University, Singapore; Department of Mechanical Engineering, Michigan State University, East Lansing, MI, United States","Li J., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, United States; Xu M., Department of Electronic Engineering, Core Research Institute of Intelligent Robots, Jeonbuk National University, South Korea; Xiang L., Department of Biological and Agricultural Engineering, North Carolina State University, Raleigh, NC, United States; Chen D., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, United States; Zhuang W., School of Mechanical Engineering, Southeast University, Nanjing, China; Yin X., School of Chemistry, Chemical Engineering and Biotechnology, Nanyang Technological University, Singapore; Li Z., Department of Mechanical Engineering, Michigan State University, East Lansing, MI, United States","The past decade has witnessed the rapid development and adoption of machine and deep learning (ML & DL) methodologies in agricultural systems, showcased by great successes in applications such as smart crop management, smart plant breeding, smart livestock farming, precision aquaculture farming, and agricultural robotics. However, these conventional ML/DL models have certain limitations: they heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, large pre-trained models, also known as foundation models (FMs), have demonstrated remarkable successes in language, vision, and decision-making tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture artificial intelligence (AI). Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, conceptual tools and technical background are presented to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, recent FMs in the general computer science (CS) domain are reviewed, and the models are categorized into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, the process of developing agriculture FMs (AFMs) is outlined and their potential applications in smart agriculture are discussed. In addition, the unique challenges and risks associated with developing AFMs are discussed, including model training, validation, and deployment. Through this study, the advancement of AI in agriculture is explored by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems. To facilitate further research, a well-classified and actively updated list of papers on AFMs is organized and accessible at https://github.com/JiajiaLi04/Agriculture-Foundation-Models. © 2024","Foundation models; Language foundation models; Multimodal foundation models; Reinforcement learning foundation models; Smart agriculture; Vision foundation models","Crops; Decision making; Deep learning; Farms; Foundations; Large datasets; Learning systems; Foundation models; Language foundation model; Multi-modal; Multimodal foundation model; Reinforcement learning foundation model; Reinforcement learnings; Smart agricultures; Vision foundation model; agricultural technology; computer vision; data set; decision making; language; machine learning; precision; robotics; Reinforcement learning","","","X. Yin; School of Chemistry, Chemical Engineering and Biotechnology, Nanyang Technological University, Singapore; email: xunyuan.yin@ntu.edu.sg","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","","Scopus","2-s2.0-85194411048"
"Luo W.; Zhang G.; Yuan Q.; Zhao Y.; Chen H.; Zhou J.; Meng Z.; Wang F.; Li L.; Liu J.; Wang G.; Wang P.; Yu Z.","Luo, Wei (57677752400); Zhang, Guoqing (58073513000); Yuan, Quanbo (35779677800); Zhao, Yongxiang (57888749800); Chen, Hongce (59128620500); Zhou, Jingjie (57282469800); Meng, Zhaopeng (7201894900); Wang, Fulong (59047511300); Li, Lin (59058250800); Liu, Jiandong (58916858100); Wang, Guanwu (59045862400); Wang, Penggang (59046962700); Yu, Zhongde (58203674400)","57677752400; 58073513000; 35779677800; 57888749800; 59128620500; 57282469800; 7201894900; 59047511300; 59058250800; 58916858100; 59045862400; 59046962700; 58203674400","High-precision tracking and positioning for monitoring Holstein cattle","2024","PLoS ONE","19","5 May","e0302277","","","","2","10.1371/journal.pone.0302277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193205356&doi=10.1371%2fjournal.pone.0302277&partnerID=40&md5=53c27ea00b6bf482e2e0855ef032a85a","North China Institute of Aerospace Engineering, Langfang, China; Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, China; National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Tellyes Scientific Inc., Tianjin, China","Luo W., North China Institute of Aerospace Engineering, Langfang, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, China; Zhang G., North China Institute of Aerospace Engineering, Langfang, China; Yuan Q., North China Institute of Aerospace Engineering, Langfang, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Zhao Y., North China Institute of Aerospace Engineering, Langfang, China; Chen H., North China Institute of Aerospace Engineering, Langfang, China; Zhou J., College of Intelligence and Computing, Tianjin University, Tianjin, China, Tellyes Scientific Inc., Tianjin, China; Meng Z., College of Intelligence and Computing, Tianjin University, Tianjin, China; Wang F., North China Institute of Aerospace Engineering, Langfang, China; Li L., North China Institute of Aerospace Engineering, Langfang, China; Liu J., North China Institute of Aerospace Engineering, Langfang, China; Wang G., North China Institute of Aerospace Engineering, Langfang, China; Wang P., North China Institute of Aerospace Engineering, Langfang, China; Yu Z., North China Institute of Aerospace Engineering, Langfang, China","Enhanced animal welfare has emerged as a pivotal element in contemporary precision animal husbandry, with bovine monitoring constituting a significant facet of precision agriculture. The evolution of intelligent agriculture in recent years has significantly facilitated the integration of drone flight monitoring tools and innovative systems, leveraging deep learning to interpret bovine behavior. Smart drones, outfitted with monitoring systems, have evolved into viable solutions for wildlife protection and monitoring as well as animal husbandry. Nevertheless, challenges arise under actual and multifaceted ranch conditions, where scale alterations, unpredictable movements, and occlusions invariably influence the accurate tracking of unmanned aerial vehicles (UAVs). To address these challenges, this manuscript proposes a tracking algorithm based on deep learning, adhering to the Joint Detection Tracking (JDT) paradigm established by the CenterTrack algorithm. This algorithm is designed to satisfy the requirements of multi-objective tracking in intricate practical scenarios. In comparison with several preeminent tracking algorithms, the proposed Multi-Object Tracking (MOT) algorithm demonstrates superior performance in Multiple Object Tracking Accuracy (MOTA), Multiple Object Tracking Precision (MOTP), and IDF1. Additionally, it exhibits enhanced efficiency in managing Identity Switches (ID), False Positives (FP), and False Negatives (FN). This algorithm proficiently mitigates the inherent challenges of MOT in complex, livestock-dense scenarios.  © 2024 Luo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","","Algorithms; Animal Husbandry; Animal Welfare; Animals; Cattle; Deep Learning; Unmanned Aerial Devices; accuracy; aircraft; algorithm; animal husbandry; Article; artificial intelligence; computer aided design; computer model; deep learning; facial recognition; Holstein cattle; image reconstruction; livestock; mathematical model; mathematical phenomena; nonhuman; recognition; visual field; algorithm; animal; animal welfare; bovine; procedures; unmanned aerial vehicle","","","W. Luo; North China Institute of Aerospace Engineering, Langfang, China; email: luowei@radi.ac.cn","","Public Library of Science","19326203","","POLNC","38743665","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85193205356"
"Russel N.S.; Selvaraj A.","Russel, Newlin Shebiah (36710665700); Selvaraj, Arivazhagan (8351565000)","36710665700; 8351565000","Decoding cow behavior patterns from accelerometer data using deep learning","2024","Journal of Veterinary Behavior","74","","","68","78","10","2","10.1016/j.jveb.2024.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199864840&doi=10.1016%2fj.jveb.2024.06.005&partnerID=40&md5=17cc8b2dd736042ffaae0e88be5ff8ed","Department of Electronics and Communication Engineering, Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Tamilnadu, Sivakasi, 626005, India","Russel N.S., Department of Electronics and Communication Engineering, Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Tamilnadu, Sivakasi, 626005, India; Selvaraj A., Department of Electronics and Communication Engineering, Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Tamilnadu, Sivakasi, 626005, India","This article explores the novel application of deep learning methods in the analysis of complex cattle behavior patterns using accelerometer data. With the information provided by accelerometer data regarding the movements of cows, valuable insights into their health, behavior, and overall welfare can be understood. Manual deciphering of these patterns presents an overwhelming challenge owing to the intricate and fluctuating nature of cattle behavior. The principal objective of this research is to construct a deep learning framework that can precisely interpret complex cow behavior patterns and enable more precise and efficient surveillance. To achieve this objective, the input accelerometer data collected during various cattle behavioral instances, such as grazing, lying, walking, and other activities, undergo preprocessing and augmentation. The preprocessed data then undergo a deep learning framework comprised of 23 layers, incorporating convolution layers, batch normalization, rectified linear unit (ReLu), and MaxPooling layers. The model demonstrates promising performance in categorizing cow behaviors based on the unique movement signatures captured by the sensors. Through rigorous evaluation using three distinct datasets, each containing a different number of activities, we achieve high accuracy rates of 96.72%, 87.15%, and 98.7%, respectively. It enhances livestock management by automating behavior analysis, enabling real-time monitoring, and informed decision-making. Improved animal welfare is achieved through early detection of stress or illness, leading to prompt interventions. © 2024 Elsevier Inc.","Accelerometer data; Animal welfare; Classification; Cow behavior; Deep learning","accuracy; animal behavior; animal health; animal welfare; Article; classification algorithm; convolution algorithm; cow; decision making; deep learning; early diagnosis; grazing; livestock; nonhuman; physiological stress; walking","","","N.S. Russel; Department of Electronics and Communication Engineering, Centre for Image Processing and Pattern Recognition, Mepco Schlenk Engineering College, Sivakasi, Tamilnadu, 626005, India; email: newlinshebiah@mepcoeng.ac.in","","Elsevier Inc.","15587878","","","","English","J. Vet. Behav.","Article","Final","","Scopus","2-s2.0-85199864840"
"Dhamodhar G.; Krishna B.V.; Reddy D.S.; Reddy A.S.; Amutha S.; Ganesan K.","Dhamodhar, G. (59410465500); Krishna, B. Vamsi (59086903900); Reddy, D. Sampath (59033221400); Reddy, A. Sameer (59118464300); Amutha, S. (57883269400); Ganesan, Kothai (57224193901)","59410465500; 59086903900; 59033221400; 59118464300; 57883269400; 57224193901","Farmer's Crop Welfare System","2024","15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024","2","","","3248","3253","5","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209151016&partnerID=40&md5=918febf988380fb928a81d7293bde45f","Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Department of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore Dist., Tamil Nadu, India","Dhamodhar G., Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Krishna B.V., Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Reddy D.S., Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Reddy A.S., Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Amutha S., Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Tamilnadu, Virudhnagar, India; Ganesan K., Department of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore Dist., Tamil Nadu, India","The ""Farmer's Crop Welfare System"" is a technology that uses advanced data analytics and artificial intelligence to improve crop management and yield. It combines crop prediction, disease detection, and fertilizer recommendation. Crop prediction uses historical and real- time data, while disease detection uses image recognition and deep learning to identify early signs of diseases, promoting sustainable farming practices. © Grenze Scientific Society, 2024.","artificial intelligence; CNN; crops; deep learning; farmers; machine learning","Adversarial machine learning; Crop managements; Crop yield; Data analytics; Deep learning; Disease detection; Farmer; Machine-learning; Real-time data; Sustainable Farming; Welfare systems; Livestock","","","","Stephen J.; Sharma P.; Chaba Y.; Abraham K.U.; Anooj P.K.; Mohammad N.; Thomas G.; Srikiran S.","Grenze Scientific Society","","979-833130057-9","","","English","Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT","Conference paper","Final","","Scopus","2-s2.0-85209151016"
"Guo J.; Kong Y.; Lin L.; Xu L.; Feng D.; Cao L.; Chen J.; Ye J.; Ye S.; Yao Z.; Liu Y.; Liu T.; Liu S.","Guo, Jianjun (57212984631); Kong, Yiyou (59154857600); Lin, Lijun (57212082015); Xu, Longqin (35732678500); Feng, Dachun (26665444900); Cao, Liang (57210171133); Chen, Jiexin (59154191600); Ye, Junwei (59155195200); Ye, Shuqing (59154524600); Yao, Zhaozhong (59154696700); Liu, Yue (59154020900); Liu, Tonglai (35790250200); Liu, Shuangyin (35770966100)","57212984631; 59154857600; 57212082015; 35732678500; 26665444900; 57210171133; 59154191600; 59155195200; 59154524600; 59154696700; 59154020900; 35790250200; 35770966100","Lightweight network based on Fourth order Runge-Kutta scheme and Hybrid Attention Module for pig face recognition","2024","Computers and Electronics in Agriculture","223","","109099","","","","0","10.1016/j.compag.2024.109099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195079719&doi=10.1016%2fj.compag.2024.109099&partnerID=40&md5=0a53dce71f64bea0198eb35932dc1688","College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; College of Automation, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Guangzhou Shunsheng Biotechnology Co., LTD., Guangzhou, 511316, China","Guo J., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Kong Y., Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, College of Automation, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Lin L., Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Xu L., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Feng D., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Cao L., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Chen J., Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, College of Automation, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Ye J., Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, College of Automation, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Ye S., Guangzhou Shunsheng Biotechnology Co., LTD., Guangzhou, 511316, China; Yao Z., Guangzhou Shunsheng Biotechnology Co., LTD., Guangzhou, 511316, China; Liu Y., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Liu T., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; Liu S., College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Smart Agriculture Engineering Technology Research Center of Guangdong Higher Education Institutes, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Guangzhou Key Laboratory of Agricultural Products Quality & Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China, Academy of Smart Agricultural Engineering Innovations, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China","Pig face recognition plays a significant role in intelligent pig farming. Accurate and lightweight methods for recognizing pig faces are crucial for precision pig farming. Generally, under the condition of the same computing resources, the lower the image resolution, the faster the model inference speed. Based on the above idea, this paper proposes a lightweight pig face model called RKNet-HAM, based on the fourth-order Runge-Kutta and hybrid attention mechanism. This model exhibits the most prominent focus on the semantic information of low-resolution(64 × 64) pig face images and achieves high accuracy in recognizing similar pig faces with a low error classification rate. A dataset of 21,742 images of 32 pigs was constructed for individual pig recognition. The proposed RKNet-HAM model and several other deep learning models, e.g., RKNet-CBAM, RKNet, ShuffleNetV2.0, DesNet121, and ResNet50. RKNet-HAM has a size of 1.52 Megabytes. Experimental results show that among these models, the proposed model ofachieved the highest accuracy, precision, recall, and specificity, with an accuracy rate of 99.26 %. RKNet-HAM also exhibits good generalization ability. It provides experimental support for mobile and embedded applications. © 2024 Elsevier B.V.","Attention mechanism; CNN; Computer vision; Deep learning; Pig face recognition","Classification (of information); Deep learning; Face recognition; Image resolution; Mammals; Runge Kutta methods; Semantics; Attention mechanisms; Condition; Deep learning; Fourth-order runge-kutta; High-accuracy; Intelligent pig; Network-based; Pig face recognition; Pig farming; Runge-kutta schemes; accuracy assessment; artificial neural network; computer vision; hybrid; image classification; image resolution; livestock farming; pattern recognition; pig; precision; Computer vision","Rural Science and technology correspondent project of Zengcheng District, Guangzhou city, (2021B42121631, 2023020101); 2022 School level graduate education innovation plan project, (KA220160255); Guangdong Province enterprise Science and technology commissioner project, (GDKTP2021004400); Zhongkai Agricultural Engineering College, ([2023); Guangdong Provincial Ordinary University, (2023KTSCX048); Science and technology innovation strategy of Guangdong province, (pdjh2022a0246); Yunfu City Enterprise Technology Special Representative Project, (2023–0012109); 2024 Guangzhou Rural Science and Technology Commissioner Project, (2024GZKTP0201); Key Project of Guangdong Provincial Basic and Applied Basic Research Fund, (2022B1515120059); Guangzhou Rural Science and Technology Commissioner Special Project, (2024E04J0106); National Fund General Projects, (62373390)","Funding text 1: Funding: This work was supported in part by Guangdong Province enterprise Science and technology commissioner project under Grant GDKTP2021004400, Rural Science and technology correspondent project of Zengcheng District, Guangzhou city under Grant 2021B42121631, Yunfu City 2023 Provincial Science and Technology Innovation Strategy Special City/County Science and Technology Innovation Support Project under Grant 2023020101, 2024 Guangzhou Rural Science and Technology Commissioner Project under Grant 2024GZKTP0201, Guangdong Provincial Ordinary University Characteristic Innovation Project under Grant 2023KTSCX048, Yunfu City Enterprise Technology Special Representative Project under Grant 2023\u20130012109, National Fund General Projects under Grant 62373390\uFF0CKey Project of Guangdong Provincial Basic and Applied Basic Research Fund under Grant 2022B1515120059, Science and technology innovation strategy of Guangdong province (\u201CClimbing Plan\u201D special fund) under Grant pdjh2022a0246, 2022 School level graduate education innovation plan project under Grant KA220160255\uFF0CZhongkai Agricultural Engineering College Graduate Course Ideological and Political Construction Project under Grant Zhongyan Zi [2023] No.1 ; Funding text 2: This work was supported in part by Guangdong Provincial Ordinary University Characteristic Innovation Project under Grant 2023KTSCX048, Guangzhou Rural Science and Technology Commissioner Special Project under Grant 2024E04J0106, Yunfu City 2023 Provincial Science and Technology Innovation Strategy Special City/County Science and Technology Innovation Support Project under Grant 2023020101, Yunfu City Enterprise Technology Special Representative Project under Grant 2023-0012109, National Fund General Projects under Grant 62373390, Key Project of Guangdong Provincial Basic and Applied Basic Research Fund under Grant 2022B1515120059, Science and technology innovation strategy of Guangdong province (\u201CClimbing Plan\u201D special fund) under Grant pdjh2022a0246, 2022 School level graduate education innovation plan project under Grant KA220160255, Zhongkai Agricultural Engineering College Graduate Course Ideological and Political Construction Project under Grant Zhongyan Zi [2023] No.1. The authors thank the editor and anonymous reviewers for providing helpful suggestions for improving the quality of this manuscript.","Y. Liu; College of Information Science and Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, 510225, China; email: liuyuett@126.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85195079719"
"Pedrosa V.B.; Chen S.-Y.; Gloria L.S.; Doucette J.S.; Boerman J.P.; Rosa G.J.M.; Brito L.F.","Pedrosa, Victor B. (31367595200); Chen, Shi-Yi (57201129503); Gloria, Leonardo S. (56018249500); Doucette, Jarrod S. (34976481400); Boerman, Jacquelyn P. (55959044200); Rosa, Guilherme J.M. (57219659850); Brito, Luiz F. (36843676900)","31367595200; 57201129503; 56018249500; 34976481400; 55959044200; 57219659850; 36843676900","Machine learning methods for genomic prediction of cow behavioral traits measured by automatic milking systems in North American Holstein cattle","2024","Journal of Dairy Science","107","7","","4758","4771","13","6","10.3168/jds.2023-24082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194837522&doi=10.3168%2fjds.2023-24082&partnerID=40&md5=5b0dc9a63d85a7bd0cab674310de53b1","Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States; Farm Animal Genetic Resources Exploration and Innovation Key Laboratory of Sichuan Province, Sichuan Agricultural University, Sichuan, Chengdu, 611130, China; Agriculture Information Technology (AgIT), Purdue University, West Lafayette, 47907, IN, United States; Department of Animal and Dairy Sciences, University of Wisconsin–Madison, Madison, 53706, WI, United States","Pedrosa V.B., Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States; Chen S.-Y., Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States, Farm Animal Genetic Resources Exploration and Innovation Key Laboratory of Sichuan Province, Sichuan Agricultural University, Sichuan, Chengdu, 611130, China; Gloria L.S., Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States; Doucette J.S., Agriculture Information Technology (AgIT), Purdue University, West Lafayette, 47907, IN, United States; Boerman J.P., Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States; Rosa G.J.M., Department of Animal and Dairy Sciences, University of Wisconsin–Madison, Madison, 53706, WI, United States; Brito L.F., Department of Animal Sciences, Purdue University, West Lafayette, 47907, IN, United States","Identifying genome-enabled methods that provide more accurate genomic prediction is crucial when evaluating complex traits such as dairy cow behavior. In this study, we aimed to compare the predictive performance of traditional genomic prediction methods and deep learning algorithms for genomic prediction of milking refusals (MREF) and milking failures (MFAIL) in North American Holstein cows measured by automatic milking systems (milking robots). A total of 1,993,509 daily records from 4,511 genotyped Holstein cows were collected by 36 milking robot stations. After quality control, 57,600 SNPs were available for the analyses. Four genomic prediction methods were considered: Bayesian least absolute shrinkage and selection operator (LASSO), multiple layer perceptron (MLP), convolutional neural network (CNN), and GBLUP. We implemented the first 3 methods using the Keras and TensorFlow libraries in Python (v.3.9) but the GBLUP method was implemented using the BLUPF90+ family programs. The accuracy of genomic prediction (mean square error) for MREF and MFAIL was 0.34 (0.08) and 0.27 (0.08) based on LASSO, 0.36 (0.09) and 0.32 (0.09) for MLP, 0.37 (0.08) and 0.30 (0.09) for CNN, and 0.35 (0.09) and 0.31(0.09) based on GBLUP, respectively. Additionally, we observed a lower reranking of top selected individuals based on the MLP versus CNN methods compared with the other approaches for both MREF and MFAIL. Although the deep learning methods showed slightly higher accuracies than GBLUP, the results may not be sufficient to justify their use over traditional methods due to their higher computational demand and the difficulty of performing genomic prediction for nongenotyped individuals using deep learning procedures. Overall, this study provides insights into the potential feasibility of using deep learning methods to enhance genomic prediction accuracy for behavioral traits in livestock. Further research is needed to determine their practical applicability to large dairy cattle breeding programs. © 2024 American Dairy Science Association","accuracy of prediction; automatic milking systems; deep learning; sensor-based systems","Algorithms; Animals; Behavior, Animal; Cattle; Dairying; Female; Genomics; Genotype; Lactation; Machine Learning; Milk; Phenotype; algorithm; animal; animal behavior; bovine; dairying; female; genetics; genomics; genotype; lactation; machine learning; milk; phenotype; procedures","National Institute of Food and Agriculture, NIFA; Homestead Dairy LLC; USDA National Institute of Food and Agriculture (Washington, DC); Purdue Ag Data Services; Purdue College of Agriculture","Funding text 1: This research was funded by the Agriculture and Food Research Initiative Competitive Grant number 2022-67021-37022 from the USDA National Institute of Food and Agriculture. We are very grateful for the support received from the Purdue College of Agriculture and Purdue Ag Data Services for supporting the development of the Purdue Animal Sciences Data Ecosystem (PASDE), which is a platform for automatically gathering, integrating, and storing large-scale datasets from precision dairy farms to be used for this type of research. The authors would also like to thank Homestead Dairy, LLC and especially Brian and Jill Houin for inviting us on their farm and for allowing our research group to utilize their farm records for research purposes. We also thank Lely for giving us access to their system for extracting the datasets.; Funding text 2: This research was funded by the Agriculture and Food Research Initiative Competitive Grant number 2022-67021-37022 from the USDA National Institute of Food and Agriculture (Washington, DC). We are very grateful for the support received from the Purdue College of Agriculture and Purdue Ag Data Services (West Lafayette, IN) for supporting the development of the Purdue Animal Sciences Data Ecosystem (PASDE), which is a platform for automatically gathering, integrating, and storing large-scale datasets from precision dairy farms to be used for this type of research. The authors also thank Homestead Dairy LLC (Plymouth, IN) and especially Brian and Jill Houin for inviting us on their farm and for allowing our research group to use their farm records for research purposes. We also thank Lely (Maassluis, the Netherlands) for giving us access to their system for extracting the datasets. Animal Care Committee approval was not necessary for the purposes of this study because all the information required was obtained from pre-existing databases. The authors have not stated any conflicts of interest. Abbreviations used: AMS = automated milking systems; CNN = convolutional neural network; DL = deep learning; LASSO = Bayesian LASSO; MFAIL = milking failure; ML = machine learning; MLP = multiple layer perceptron; MREF = milking refusal; MSE = mean squared error of prediction; NN = neural network; NOT_S = nonselected for both methods; QC = quality control; S_BOTH = selected for both methods; S_CNN = selected only for CNN method; S_GBLUP = selected only for GBLUP method; S_MLP = selected only for MLP method; S_LASSO = selected only for LASSO method.","L.F. Brito; Department of Animal Sciences, Purdue University, West Lafayette, 47907, United States; email: britol@purdue.edu","","Elsevier Inc.","00220302","","","38395400","English","J. Dairy Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85194837522"
"Werner J.P.S.; Belgiu M.; Bueno I.T.; Dos Reis A.A.; Toro A.P.S.G.D.; Antunes J.F.G.; Stein A.; Lamparelli R.A.C.; Magalhães P.S.G.; Coutinho A.C.; Esquerdo J.C.D.M.; Figueiredo G.K.D.A.","Werner, João P. S. (57212420665); Belgiu, Mariana (55962329600); Bueno, Inacio T. (56603977800); Dos Reis, Aliny A. (55320336300); Toro, Ana P. S. G. D. (57741497200); Antunes, João F. G. (15020172200); Stein, Alfred (7401758587); Lamparelli, Rubens A. C. (6602713722); Magalhães, Paulo S. G. (7003731076); Coutinho, Alexandre C. (55513012600); Esquerdo, Júlio C. D. M. (15020462500); Figueiredo, Gleyce K. D. A. (57185295400)","57212420665; 55962329600; 56603977800; 55320336300; 57741497200; 15020172200; 7401758587; 6602713722; 7003731076; 55513012600; 15020462500; 57185295400","Mapping Integrated Crop–Livestock Systems Using Fused Sentinel-2 and PlanetScope Time Series and Deep Learning","2024","Remote Sensing","16","8","1421","","","","2","10.3390/rs16081421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191359287&doi=10.3390%2frs16081421&partnerID=40&md5=ebafe23a10a8a5761cb3906f6f0d3914","School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil; Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, 7500 AE, Netherlands; Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, 13083-896, Brazil; Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, Campinas, 13083-886, Brazil","Werner J.P.S., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil; Belgiu M., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, 7500 AE, Netherlands; Bueno I.T., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, 13083-896, Brazil; Dos Reis A.A., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, 13083-896, Brazil; Toro A.P.S.G.D., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil; Antunes J.F.G., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, Campinas, 13083-886, Brazil; Stein A., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, 7500 AE, Netherlands; Lamparelli R.A.C., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, 13083-896, Brazil; Magalhães P.S.G., Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, 13083-896, Brazil; Coutinho A.C., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, Campinas, 13083-886, Brazil; Esquerdo J.C.D.M., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil, Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, Campinas, 13083-886, Brazil; Figueiredo G.K.D.A., School of Agricultural Engineering (FEAGRI), University of Campinas, Campinas, 13083-875, Brazil","Integrated crop–livestock systems (ICLS) are among the main viable strategies for sustainable agricultural production. Mapping these systems is crucial for monitoring land use changes in Brazil, playing a significant role in promoting sustainable agricultural production. Due to the highly dynamic nature of ICLS management, mapping them is a challenging task. The main objective of this research was to develop a method for mapping ICLS using deep learning algorithms applied on Satellite Image Time Series (SITS) data cubes, which consist of Sentinel-2 (S2) and PlanetScope (PS) satellite images, as well as data fused (DF) from both sensors. This study focused on two Brazilian states with varying landscapes and field sizes. Targeting ICLS, field data were combined with S2 and PS data to build land use and land cover classification models for three sequential agricultural years (2018/2019, 2019/2020, and 2020/2021). We tested three experimental settings to assess the classification performance using S2, PS, and DF data cubes. The test classification algorithms included Random Forest (RF), Temporal Convolutional Neural Network (TempCNN), Residual Network (ResNet), and a Lightweight Temporal Attention Encoder (L-TAE), with the latter incorporating an attention-based model, fusing S2 and PS within the temporal encoders. Experimental results did not show statistically significant differences between the three data sources for both study areas. Nevertheless, the TempCNN outperformed the other classifiers with an overall accuracy above 90% and an F1-Score of 86.6% for the ICLS class. By selecting the best models, we generated annual ICLS maps, including their surrounding landscapes. This study demonstrated the potential of deep learning algorithms and SITS to successfully map dynamic agricultural systems. © 2024 by the authors.","data fusion; ICLS; multi-sensor; regenerative agriculture; TempCNN; temporal encoder","Classification (of information); Convolutional neural networks; Deep learning; Forestry; Land use; Learning systems; Mapping; Sensor data fusion; Signal encoding; Time series; Agricultural productions; Convolutional neural network; Integrated crop–livestock system; Livestock systems; Multi sensor; Regenerative agriculture; Satellite images; Sustainable agricultural; Temporal convolutional neural network; Temporal encoder; Crops","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2017/50205-9, 2018/24985-0, 2021/15001-9); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (305271/2020-2)","This research was funded by the S\u00E3o Paulo Research Foundation (FAPESP), grant number: 2017/50205-9. The authors are thankful to the S\u00E3o Paulo Research Foundation (FAPESP) (Grants: 2018/24985-0 and 2021/15001-9), the National Council for Scientific and Technological Development (CNPq) (Grant: 305271/2020-2), and the Coordination for the Improvement of Higher Education Personnel (CAPES, Finance code 001).","M. Belgiu; Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, 7500 AE, Netherlands; email: m.belgiu@utwente.nl","","Multidisciplinary Digital Publishing Institute (MDPI)","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85191359287"
"Shinde S.; Himpalnerkar A.; Shendurkar S.; Deshmane S.; Jadhav S.","Shinde, Swapnil (58806704000); Himpalnerkar, Aditi (59459024700); Shendurkar, Sakshi (59459024800); Deshmane, Siddhi (59459767700); Jadhav, Sakshi (59460009500)","58806704000; 59459024700; 59459024800; 59459767700; 59460009500","Cattle Disease Detection using VGG16 CNN Architecture","2024","2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024","","","","","","","0","10.1109/ICCCNT61001.2024.10724717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211179859&doi=10.1109%2fICCCNT61001.2024.10724717&partnerID=40&md5=fef0a14989165ae99d5c57d6da42dd72","Vishwakarma Institute of Information Technology, Maharashtra, Pune, India","Shinde S., Vishwakarma Institute of Information Technology, Maharashtra, Pune, India; Himpalnerkar A., Vishwakarma Institute of Information Technology, Maharashtra, Pune, India; Shendurkar S., Vishwakarma Institute of Information Technology, Maharashtra, Pune, India; Deshmane S., Vishwakarma Institute of Information Technology, Maharashtra, Pune, India; Jadhav S., Vishwakarma Institute of Information Technology, Maharashtra, Pune, India","Cattle diseases pose major challenges to global agriculture, affecting productivity and animal welfare. To meet these challenges, efficient and accurate disease detection systems are needed. This paper presents a robust approach for automated cattle disease detection using Convolutional Neural Networks (CNN), specifically using the VGG16 architecture. Recognizing the need for timely and objective disease diagnosis, research focuses on developing a system that can accurately classify common cattle diseases, including foot and mouth disease (FMD), infectious bovine keratoconjunctivitis (IBK) and nodular dermatitis (LSD). Among others. Starting with a comprehensive review of related work highlighting the growing interest in CNNs for automated animal disease diagnosis, the methodology covers data collection from reliable sources, pre-processing steps to ensure consistency of datasets, and model architecture selection with a focus on learning using the VGG16 model. pretrained in ImageNet. Fine-tuning adapts the VGG16 model to the specific task of cattle disease detection, achieving competitive accuracy. Evaluation metrics, including precision, recall and F1 scores, demonstrate the effectiveness of the approach in different disease classes. In particular, the model achieves a total test accuracy of 88.14%, indicating its potential for practical use in livestock systems. Analysis of the learning curve explains the performance dynamics of the model during training. In conclusion, implications for livestock health monitoring research are discussed and opportunities for future research are outlined, highlighting the role of deep learning in agricultural practices.  © 2024 IEEE.","Automated Diagnosis; Cattle Disease Detection; Convolutional Neural Networks; Livestock Disease Classification; Livestock Health Monitoring; Transfer Learning; VGG16 CNN Architecture","","","","S. Shinde; Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; email: swapnil.shinde@viit.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835037024-9","","","English","Int. Conf. Comput. Commun. Netw. Technol., ICCCNT","Conference paper","Final","","Scopus","2-s2.0-85211179859"
"Xu Z.; Zhao Y.; Yin Z.; Yu Q.","Xu, Zhihao (58185714700); Zhao, Yaqin (9045137100); Yin, Zixuan (58186537200); Yu, Qiuping (56547855500)","58185714700; 9045137100; 58186537200; 56547855500","Optimized BottleNet Transformer model with Graph Sampling and Counterfactual Attention for cow individual identification","2024","Computers and Electronics in Agriculture","218","","108703","","","","1","10.1016/j.compag.2024.108703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184148133&doi=10.1016%2fj.compag.2024.108703&partnerID=40&md5=e230303da4fbd869255d20c0e84f0155","College of Mechanical and Electronic Engineering, Nanjing Forestry University, Longpan Road, Jiangsu, Nanjing, 210037, China","Xu Z., College of Mechanical and Electronic Engineering, Nanjing Forestry University, Longpan Road, Jiangsu, Nanjing, 210037, China; Zhao Y., College of Mechanical and Electronic Engineering, Nanjing Forestry University, Longpan Road, Jiangsu, Nanjing, 210037, China; Yin Z., College of Mechanical and Electronic Engineering, Nanjing Forestry University, Longpan Road, Jiangsu, Nanjing, 210037, China; Yu Q., College of Mechanical and Electronic Engineering, Nanjing Forestry University, Longpan Road, Jiangsu, Nanjing, 210037, China","In modern dairy farms, accurate and reliable identification of each individual cow is of great significance for precision livestock farming. Individual cow identification is the basis for applications such as disease detection, automatic behaviour analysis, intelligent milking, and individual counting and is crucial for improving the welfare and breeding efficiency of dairy cows. Computer vision-based method is a low-cost, non-contact, automatic, and efficient way. To improve the accuracy and efficiency of cow recognition in different large-scale dairy farms, we proposed a BottleNet Transformer (BoTNet) model based on Graph Sampling and Counterfactual Attention Learning for cow surveillance videos. First, we replace the 3 × 3 spatial convolution with Multi-Head Attention in the final three bottleneck blocks of the ResNet. The BoT block module combines attention mechanisms and residual connection to enhance the global representation of cow images, which in turn better captures the features of the cow's back pattern region and ignores the influence of irrelevant information, such as the background of the dairy barn. Subsequently, counterfactual learning measures the quality of attention by comparing the difference between the generated output and the true label. The difference can be used to enhance the causal relationship between prediction results and cow feature attention, allowing the model to obtain more comprehensive cow appearance features. Finally, we added a Graph Sampling module before the feature extraction phase to produce small batches of samples for training. The GS sampler improves the learning efficiency while reducing the memory and computation consumption compared with the usual adopted PK sampling. We conducted comparison experiments on the public dataset Dataset1, and the experimental results reveal that the Rank-1, Rank-5, and mAP values of this study's method are 4%, 3.2%, and 5.3% higher than the optimal results, respectively, when compared with the existing state-of-the-art methods for animal individual recognition. In particular, we construct a challenging dataset by intercepting individual cow images from videos in the public dataset of farms. Experimental results indicate that the proposed method has better generalization performance. © 2024 Elsevier B.V.","Counterfactual attention learning; Deep learning; Graph sampling; Individual cow identification; Precision livestock","Deep learning; Farms; Image enhancement; Learning systems; Security systems; Counterfactual attention learning; Counterfactuals; Dairy farms; Deep learning; Graph samplings; Individual cow identification; Individual identification; Precision livestock; Public dataset; Transformer modeling; livestock farming; machine learning; prediction; reproductive behavior; sampler; ungulate; Efficiency","National Natural Science Foundation of China, NSFC, (32371583); National Natural Science Foundation of China, NSFC","Supported by National Natural Science Foundation of China ( 32371583 ).","Y. Zhao; College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, Longpan Road, Jiangsu, 210037, China; email: zhaoyaqin@njfu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85184148133"
"Sutar V.; Kulhalli K.","Sutar, Vinayak (57339808000); Kulhalli, Kshama (57194656581)","57339808000; 57194656581","An Effective Stray Animal Detection and Crash Prevention System using Deep Learning and Internet of Things","2024","15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024","1","","","879","889","10","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208798739&partnerID=40&md5=f8dab91a48661603815ad33ec4d9ecf3","Shivaji University, Maharashtra, Kolhapur, India; Dr. D.Y. Patil College of Engineering & Technology, Kolhapur, India","Sutar V., Shivaji University, Maharashtra, Kolhapur, India; Kulhalli K., Dr. D.Y. Patil College of Engineering & Technology, Kolhapur, India","Road crossing or at road side stray animals like cows and buffalos are usually observed in India. Such animals creates critical situations on roadways due to the collision. According to data obtained from the veterinary department of Nagpur municipal corporation approximately 3,127 cattle were injured and at least 30% of them died immediately and after a few days of hospitalization. This paper showcase a solar powered deep learning and IoT enabled early warning system which could detect the stray animals and send an alerts to the vehicles on road. The IoT cloud platform and dashboard delivers the information of accident prone zone. The trained stray animal detection model in the present system could predict 86% to 90% true positive results with 91% accuracy on real world camera feed. The 433 Mhz RF transmitter in the system is capable to send alert up to 110m. © Grenze Scientific Society, 2024.","Animal Detection; Deep Learning; IoT; Road Safety; YOLO","Crashworthiness; Highway accidents; Livestock; Animal detection; Cattles; Deep learning; IoT; Prevention systems; Road crossing; Road safety; Road sides; Solar-powered; YOLO; Invertebrates","","","","Stephen J.; Sharma P.; Chaba Y.; Abraham K.U.; Anooj P.K.; Mohammad N.; Thomas G.; Srikiran S.","Grenze Scientific Society","","979-833130057-9","","","English","Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT","Conference paper","Final","","Scopus","2-s2.0-85208798739"
"Satpathy S.; Paikaray B.K.; Yang M.; Balakrishnan A.","Satpathy, Suneeta (57213835623); Paikaray, Bijay Kumar (56899181200); Yang, Ming (59472013000); Balakrishnan, Arun (59471983700)","57213835623; 56899181200; 59472013000; 59471983700","Sustainable Farming through Machine Learning: Enhancing Productivity and Efficiency","2024","Sustainable Farming through Machine Learning Enhancing: Productivity and Efficiency","","","","1","282","281","0","10.1201/9781003484608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212034581&doi=10.1201%2f9781003484608&partnerID=40&md5=f7fe36af850ce90e30777aea1126c311","AI & ML, Siksha ‘O’ Anusandhan (Deemed to be) University, India; Center for Data Science, Siksha ‘O’ Anusandhan (Deemed to be) University, India; Wright State University, Dayton, Ohio, United States; Computer Science and Engineering department, VIT-AP University, India","Satpathy S., AI & ML, Siksha ‘O’ Anusandhan (Deemed to be) University, India; Paikaray B.K., Center for Data Science, Siksha ‘O’ Anusandhan (Deemed to be) University, India; Yang M., Wright State University, Dayton, Ohio, United States; Balakrishnan A., Computer Science and Engineering department, VIT-AP University, India","This book explores the transformative potential of machine learning (ML) technologies in agriculture. It delves into specific applications, such as crop monitoring, disease detection, and livestock management, demonstrating how artificial intelligence/machine learning (AI/ML) can optimize resource management and improve overall productivity in farming practices. Sustainable Farming through Machine Learning: Enhancing Productivity and Efficiency provides an in-depth overview of AI and ML concepts relevant to the agricultural industry. It discusses the challenges faced by the agricultural sector and how AI/ML can address them. The authors highlight the use of AI/ML algorithms for plant disease and pest detection and examine the role of AI/ML in supply chain management and demand forecasting in agriculture. It includes an examination of the integration of AI/ML with agricultural robotics for automation and efficiency. The authors also cover applications in livestock management, including feed formulation and disease detection; they also explore the use of AI/ML for behavior analysis and welfare assessment in livestock. Finally, the authors also explore the ethical and social implications of using such technologies. This book can be used as a textbook for students in agricultural engineering, precision farming, and smart agriculture. It can also be a reference book for practicing professionals in machine learning, and deep learning working on sustainable agriculture applications. © 2025 selection and editorial matter, Suneeta Satpathy, Bijay Kumar Paikaray, Ming Yang, and Arun Balakrishnan; individual chapters, the contributors.","","","","","","","CRC Press","","978-104025478-3; 978-103277749-8","","","English","Sustainable Farming through Machine Learning Enhancing: Productivity and Efficiency","Book","Final","","Scopus","2-s2.0-85212034581"
"Sim H.-S.; Kim T.-K.; Lee C.-W.; Choi C.-S.; Kim J.S.; Cho H.-C.","Sim, Hyeon-Seok (58951794100); Kim, Tae-Kyeong (57444811500); Lee, Chang-Woo (58951957800); Choi, Chang-Sik (58951957900); Kim, Jin Soo (35754486700); Cho, Hyun-Chong (22233514800)","58951794100; 57444811500; 58951957800; 58951957900; 35754486700; 22233514800","Optimizing Cattle Behavior Analysis in Precision Livestock Farming: Integrating YOLOv7-E6E with AutoAugment and GridMask to Enhance Detection Accuracy","2024","Applied Sciences (Switzerland)","14","9","3667","","","","0","10.3390/app14093667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192837420&doi=10.3390%2fapp14093667&partnerID=40&md5=eb9c504389f1fdc9e68971d9d63a9a56","Department Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Department of Electronics Engineering, Kangwon National University, Chuncheon, 24341, South Korea","Sim H.-S., Department Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Kim T.-K., Department Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Lee C.-W., Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; Choi C.-S., Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; Kim J.S., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Cho H.-C., Department Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea, Department of Electronics Engineering, Kangwon National University, Chuncheon, 24341, South Korea","Recently, the growing demand for meat has increased interest in precision livestock farming (PLF), wherein monitoring livestock behavior is crucial for assessing animal health. We introduce a novel cattle behavior detection model that leverages data from 2D RGB cameras. It primarily employs you only look once (YOLO)v7-E6E, which is a real-time object detection framework renowned for its efficiency across various applications. Notably, the proposed model enhances network performance without incurring additional inference costs. We primarily focused on performance enhancement and evaluation of the model by integrating AutoAugment and GridMask to augment the original dataset. AutoAugment, a reinforcement learning algorithm, was employed to determine the most effective data augmentation policy. Concurrently, we applied GridMask, a novel data augmentation technique that systematically eliminates square regions in a grid pattern to improve model robustness. Our results revealed that when trained on the original dataset, the model achieved a mean average precision (mAP) of 88.2%, which increased by 2.9% after applying AutoAugment. The performance was further improved by combining AutoAugment and GridMask, resulting in a notable 4.8% increase in the mAP, thereby achieving a final mAP of 93.0%. This demonstrates the efficacy of these augmentation strategies in improving cattle behavior detection for PLF. © 2024 by the authors.","AutoAugment; cattle behavior; deep learning; GridMask; object detection; precision livestock farming; YOLOv7-E6E","","National Research Foundation of Korea, NRF; Ministry of Education, MOE, (2022R1I1A3053872, 2022RIS-005); Ministry of Education, MOE; Rural Development Administration, RDA, (00260110); Rural Development Administration, RDA","This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2022R1I1A3053872). This research was supported by the \u201CRegional Innovation Strategy (RIS)\u201D through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (MOE) (2022RIS-005). This research was supported by Rural Development Administration, Republic of Korea (No. 00260110).","H.-C. Cho; Department Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; email: hyuncho@kangwon.ac.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20763417","","","","English","Appl. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192837420"
"Kistanova E.; Yotov S.; Zaimova D.","Kistanova, Elena (6602329192); Yotov, Stanimir (8386614200); Zaimova, Darina (57204418685)","6602329192; 8386614200; 57204418685","Intelligent Animal Husbandry: Present and Future","2024","Animals","14","11","1645","","","","0","10.3390/ani14111645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195924712&doi=10.3390%2fani14111645&partnerID=40&md5=43bb651a0369aab9d49a34b2d4cee53c","Institute of Biology and Immunology of Reproduction, Bulgarian Academy of Sciences, Sofia, 1113, Bulgaria; Department of Obstetrics, Reproduction and Reproductive Disorders, Trakia University, Stara Zagora, 6000, Bulgaria; Department of Industrial Business and Entrepreneurship, Faculty of Economics, Trakia University, Stara Zagora, 6000, Bulgaria","Kistanova E., Institute of Biology and Immunology of Reproduction, Bulgarian Academy of Sciences, Sofia, 1113, Bulgaria; Yotov S., Department of Obstetrics, Reproduction and Reproductive Disorders, Trakia University, Stara Zagora, 6000, Bulgaria; Zaimova D., Department of Industrial Business and Entrepreneurship, Faculty of Economics, Trakia University, Stara Zagora, 6000, Bulgaria","[No abstract available]","","animal behavior; animal husbandry; breeding; broiler; deep learning; economic aspect; Editorial; estrus cycle; fertility; information and communication technology; information technology; livestock; machine learning; manure; methodology; microclimate; nonhuman; nuclear magnetic resonance imaging; poultry; reproductive health; sericulture; sex determination; waste management","","","E. Kistanova; Institute of Biology and Immunology of Reproduction, Bulgarian Academy of Sciences, Sofia, 1113, Bulgaria; email: kistanova@gmail.com","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Editorial","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85195924712"
"Liu S.; Hedström A.; Basavegowda D.H.; Weltzien C.; Höhne M.M.-C.","Liu, Shanghua (58775618900); Hedström, Anna (57339350700); Basavegowda, Deepak Hanike (57202612016); Weltzien, Cornelia (57189026254); Höhne, Marina M.-C. (57219765782)","58775618900; 57339350700; 57202612016; 57189026254; 57219765782","Explainable AI in grassland monitoring: Enhancing model performance and domain adaptability","2024","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-344","","","143","154","11","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216758940&partnerID=40&md5=6d2602fdce023f5862e8d228b7ec8904","Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany; Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany; Understandable Machine Intelligence Lab (UMI Lab), ATB, Max-Eyth-Allee 100, Potsdam, 14469, Germany; Universität Potsdam, An der Bahn 2, Potsdam, 14476, Germany","Liu S., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany; Hedström A., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany, Understandable Machine Intelligence Lab (UMI Lab), ATB, Max-Eyth-Allee 100, Potsdam, 14469, Germany; Basavegowda D.H., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany; Weltzien C., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany; Höhne M.M.-C., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Max-Eyth-Allee 100, Potsdam, 14469, Germany, Understandable Machine Intelligence Lab (UMI Lab), ATB, Max-Eyth-Allee 100, Potsdam, 14469, Germany, Universität Potsdam, An der Bahn 2, Potsdam, 14476, Germany","Grasslands are known for their high biodiversity and ability to provide multiple ecosystem services. Challenges in automating the identification of indicator plants are key obstacles to large-scale grassland monitoring. These challenges stem from the scarcity of extensive datasets, the distributional shifts between generic and grassland-specific datasets, and the inherent opacity of deep learning models. This paper delves into the latter two challenges, with a specific focus on transfer learning and eXplainable Artificial Intelligence (XAI) approaches to grassland monitoring, highlighting the novelty of XAI in this domain. We analyze various transfer learning methods to bridge the distributional gaps between generic and grassland-specific datasets. Additionally, we showcase how explainable AI techniques can unveil the model's domain adaptation capabilities, employing quantitative assessments to evaluate the model's proficiency in accurately centering relevant input features around the object of interest. This research contributes valuable insights for enhancing model performance through transfer learning and measuring domain adaptability with explainable AI, showing significant promise for broader applications within the agricultural community. © 2024 Gesellschaft fur Informatik (GI). All rights reserved.","deep learning; domain adaptation; grassland monitoring; indicator detection; XAI","Abiotic; Contrastive Learning; Federated learning; Livestock; Deep learning; Domain adaptation; Ecosystem services; Grassland monitoring; Indicator detection; Indicator plants; Model domains; Modeling performance; Transfer learning; XAI; Adversarial machine learning","","","","Hoffmann C.; Stein A.; Gallmann E.; Dorr J.; Krupitzer C.; Floto H.","Gesellschaft fur Informatik (GI)","16175468","978-388579738-8","","","English","Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)","Conference paper","Final","","Scopus","2-s2.0-85216758940"
"Ariza-Sentís M.; Vélez S.; Martínez-Peña R.; Baja H.; Valente J.","Ariza-Sentís, Mar (56964491300); Vélez, Sergio (57215185725); Martínez-Peña, Raquel (57697039300); Baja, Hilmy (57864451400); Valente, João (42062501000)","56964491300; 57215185725; 57697039300; 57864451400; 42062501000","Object detection and tracking in Precision Farming: a systematic review","2024","Computers and Electronics in Agriculture","219","","108757","","","","26","10.1016/j.compag.2024.108757","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185842634&doi=10.1016%2fj.compag.2024.108757&partnerID=40&md5=9576c1153c7562413a7af19304bced20","Information Technology Group, Wageningen University & Research, PB Wageningen, 6708, Netherlands; Regional Institute of Agri-Food and Forestry Research and Development of Castilla-La Mancha (IRIAF), CIAG-“El Chaparrillo”, Ctra. Porzuna km 4, Ciudad Real, 13071, Spain; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, PB Wageningen, 6708, Netherlands","Ariza-Sentís M., Information Technology Group, Wageningen University & Research, PB Wageningen, 6708, Netherlands; Vélez S., Information Technology Group, Wageningen University & Research, PB Wageningen, 6708, Netherlands; Martínez-Peña R., Regional Institute of Agri-Food and Forestry Research and Development of Castilla-La Mancha (IRIAF), CIAG-“El Chaparrillo”, Ctra. Porzuna km 4, Ciudad Real, 13071, Spain; Baja H., Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, PB Wageningen, 6708, Netherlands; Valente J., Information Technology Group, Wageningen University & Research, PB Wageningen, 6708, Netherlands","Object Detection and Tracking have gained importance in recent years because of the great advances in image and video analysis techniques and the accurate results these technologies are producing. Moreover, they have successfully been applied to multiple fields, including the agricultural domain since they offer real-time monitoring of the status of the crops and animals while counting how many are present within a field/barn. This review aims to review the current literature on Object Detection and Tracking within the field of Precision Farming. For that, over 300 research articles were explored, from which 150 articles from the last five years were systematically reviewed and analysed regarding the algorithms they implemented, the domain they belong to, the difficulties they faced, and which limitations should be tackled in the future. Lastly, it examines potential issues that this approach might have, for instance, the lack of open-source datasets with labelled data. The findings of this study indicate that Object Detection and Tracking are critical techniques to enhance Precision Farming and pave the way for robotization for the agricultural sector since they provide accurate results and insights on crop and animal management, and optimize resource allocation. Future work should focus on the optimal acquisition of the datasets prior to Object Detection and Tracking, along with the consideration of the biophysical environment of the farming scenarios. © 2024 The Author(s)","Computer vision; Deep Learning; Precision agriculture; Precision Livestock farming; Smart farming","Animals; Computer vision; Deep learning; Farms; Object detection; Object recognition; Precision agriculture; Analysis techniques; Deep learning; Image-analysis; Object detection and tracking; Precision Agriculture; Precision livestock farming; Precision-farming; Smart farming; Systematic Review; Video analysis; algorithm; detection method; literature review; precision agriculture; tracking; Crops","European Commission, EC; Horizon 2020 Framework Programme, H2020, (101017111)","This work has been carried out in the scope of the H2020 FlexiGroBots project, which has been funded by the European Commission in the scope of its H2020 programme (contract number 101017111, https://flexigrobots-h2020.eu/ ). The authors acknowledge valuable help and contributions from 'Bodegas Terras Gauda, S.A.' and all partners of the project.","M. Ariza-Sentís; Information Technology Group, Wageningen University & Research, PB Wageningen, 6708, Netherlands; email: mar.arizasentis@wur.nl","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85185842634"
"Martono N.P.; Sawada T.; Uchino T.; Ohwada H.","Martono, Niken Prasasti (56487908000); Sawada, Tomohide (58184074400); Uchino, Tom (57829039100); Ohwada, Hayato (7006250284)","56487908000; 58184074400; 57829039100; 7006250284","Deep Learning-Based Indoor Localization Using Wireless Sensor Network: An Efficient Approach for Livestock Monitoring","2024","Vietnam Journal of Computer Science","11","3","","447","463","16","0","10.1142/S2196888824500106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190833307&doi=10.1142%2fS2196888824500106&partnerID=40&md5=35f42e81da9614cd51f9ab483aa38675","Department of Industrial and Systems Engineering, Tokyo University of Science, Chiba, Noda, Japan","Martono N.P., Department of Industrial and Systems Engineering, Tokyo University of Science, Chiba, Noda, Japan; Sawada T., Department of Industrial and Systems Engineering, Tokyo University of Science, Chiba, Noda, Japan; Uchino T., Department of Industrial and Systems Engineering, Tokyo University of Science, Chiba, Noda, Japan; Ohwada H., Department of Industrial and Systems Engineering, Tokyo University of Science, Chiba, Noda, Japan","Indoor localization for livestock is important as it facilitates effective monitoring and management of animals within confined spaces, such as barns or stables. By accurately tracking the position of individual animals, farmers and livestock managers can gain valuable insights into their behavior, health, and welfare. This information enables the early detection of potential issues, such as diseases or injuries, allowing for prompt intervention and treatment. While GPS sensors offer global position estimation, they are limited to outdoor environments and inherently exhibit inaccuracies of several meters. In indoor spaces, alternative sensors like lasers and cameras can estimate positions, but they necessitate maps and substantial computational resources to process complex algorithms. Presently, Wireless Networks (WN) are extensively accessible in indoor environments, providing efficient global localization with relatively low cost and computing demands. This paper presents a novel approach to estimate the location of cows in a given area using Deep Neural Networks (DNNs) applied to LQI data. This method aims to improve the efficiency of livestock management, particularly in large-scale farming operations, by enabling precise tracking and monitoring of individual animals. Our proposed model leverages data from wireless sensor networks (WSNs) and demonstrates promising results in terms of accuracy and computational efficiency. This study contributes to the ongoing research in smart agriculture and the application of advanced technologies in the livestock industry.  © 2024 The Author(s).","deep learning; Indoor localization; livestock monitoring; neural network; wireless network","","","","N.P. Martono; Department of Industrial and Systems Engineering, Tokyo University of Science, Noda, Chiba, Japan; email: niken@rs.tus.ac.jp","","World Scientific","21968896","","","","English","Vietnam. J. Comput. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190833307"
"Dahiya N.K.; Bajaj S.B.; Ruhil A.P.; Jaglan V.","Dahiya, Naresh Kumar (57213234700); Bajaj, Shalini Bhaskar (55787034500); Ruhil, A.P. (14008061700); Jaglan, Vivek (57217603496)","57213234700; 55787034500; 14008061700; 57217603496","A deep learning-based framework for identification of Sahiwal cow","2024","AIP Conference Proceedings","3121","1","040004","","","","0","10.1063/5.0221772","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199163679&doi=10.1063%2f5.0221772&partnerID=40&md5=dfec9f8785436f1fac469c8145011159","AMITY University, Haryana, Gurugram, 122413, India; NDRI, Haryana, Karnal, 132001, India; AMITY University, Madhya Pradesh, Gwalior, 474005, India","Dahiya N.K., AMITY University, Haryana, Gurugram, 122413, India; Bajaj S.B., AMITY University, Haryana, Gurugram, 122413, India; Ruhil A.P., NDRI, Haryana, Karnal, 132001, India; Jaglan V., AMITY University, Madhya Pradesh, Gwalior, 474005, India","The discernible anatomical elements of cattle, commonly referred to as morphological characteristics or linear-type traits, play a pivotal role in the recognition of breed as well as the production estimation of cattle. This paper introduces a deep learning-based framework for the identification of Sahiwal cattle within herds by observing the morphological characteristics i.e., color and facial structure analysis. The proposed framework involves three distinct convolutional neural network (CNN) models: GoogleNet, InceptionV3, and ResNet50 for the identification of breeds out of Gir, Sahiwal and Tharparkar. The implementation of this framework is executed through MATLAB-2021, a robust computational tool. Furthermore, we curate an authentic dataset containing Sahiwal cow images sourced from the NCR North region of India, facilitating the comprehensive evaluation of our approach. Performance assessment encompasses diverse metrics, encompassing accuracy and confusion matrix analysis. This study contributes to the domain of livestock recognition, presenting a robust methodology forprecise Sahiwal cow identification, leveraging advanced deep learning techniques. © 2024 Author(s).","Convolutional Neural Network (CNN); Dairy Farming; Deep Learning; Sahiwal cow","","","","N.K. Dahiya; AMITY University, Gurugram, Haryana, 122413, India; email: naresh.dahiya28@gmail.com","Singh S.; Kaur S.; Jindal P.K.","American Institute of Physics","0094243X","","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85199163679"
"Shwetabh K.; Ambhaikar A.","Shwetabh, Kumar (58985398500); Ambhaikar, Asha (37013228600)","58985398500; 37013228600","Development of a Low-Cost Livestock Sorting Information Management System Leveraging Deep Learning, AI, and IoT Technologies","2024","BIO Web of Conferences","82","","05019","","","","0","10.1051/bioconf/20248205019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183847560&doi=10.1051%2fbioconf%2f20248205019&partnerID=40&md5=7b2d4c8ea3d4662bdc2572f428b201d0","Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India","Shwetabh K., Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India; Ambhaikar A., Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India","The implementation of effective livestock management methods is crucial to optimize agricultural operations. However, conventional livestock sorting and data management approaches encounter several obstacles regarding precision, labor requirements, and financial implications. The process exhibits inefficiency, increased labor costs, and an elevated risk of zoonotic infections. Housing livestock in extensive groups might intensify the transmission of diseases and complicate the surveillance and management of diseased animals. This study attempted to develop a Low-Cost Livestock Sorting Information Management System (LC-LSIMS) using a dataset enriched with crucial metrics and curated images collected over 24 months with the Internet of Things (IoT) and Artificial Intelligence (AI). The design of edge-cloud computing facilitates the redistribution of computational resources, leading to enhanced computational speed. The LC-LSIMS would have a predictive module to assist agricultural practitioners in safeguarding their crops during flood occurrences. This module will empower farmers to proactively anticipate natural phenomena, including floods, during intense rainfall. LC-LSIMS presents a multi-level design plan that facilitates attaining the specified goals. The findings obtained from the execution of the implemented system demonstrate a sorting accuracy of 91.47%, computational speed of 27.42 frames per second (fps), labor cost reduction of 50.84%, production efficiency improvement of 29.59%, and an average reduction in data input errors of 37.59%. © The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0.","","","","","K. Shwetabh; Faculty of CS & IT, Kalinga University, Naya Raipur, Chhattisgarh, India; email: ku.kumarshwetabh@kalingauniversity.ac.in","Trukhachev V.; Khavinson V.; Zhuravlev A.; Belopukhov S.; Migunov R.; Kukhar V.","EDP Sciences","22731709","","","","English","BIO. Web. Conf.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183847560"
"Al-Ahmed S.A.; Ahamed T.","Al-Ahmed, Shahriar Abdullah (57215317255); Ahamed, Tofael (9269729600)","57215317255; 9269729600","AI × IoT: Increasing Agricultural Productivity of Crops, Orchards, and Livestock Management Using Smart Agricultural Space for Achieving SDGs","2024","IoT and AI in Agriculture: Smart Automation Systems for Increasing Agricultural Productivity to Achieve SDGs and Society 5.0","","","","481","490","9","0","10.1007/978-981-97-1263-2_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210886091&doi=10.1007%2f978-981-97-1263-2_29&partnerID=40&md5=42d2488fbdaa9baffb83757b8d5ab0a4","University of the West of Scotland, Renfrewshire, United Kingdom; University of Tsukuba, Tsukuba, Japan","Al-Ahmed S.A., University of the West of Scotland, Renfrewshire, United Kingdom; Ahamed T., University of Tsukuba, Tsukuba, Japan","This concluding chapter discusses the development of sensors and sens­ing technologies within the advancements of the Internet of Things (IoT) and artifi­cial intelligence (AI) to achieve the aims of the sustainable development goals (SDGs) resulting from the outcomes detailed in the previous chapters. Concerning the SDGs, several chapters have also reported on increasing productivity, address­ing labor shortages in farming, and optimizing land use planning and water usage in arid lands for climate-smart agriculture. Furthermore, AI × IoT domains in agricul­ture include robotic research for ultra-labor savings and outdoor and indoor smart farm operations. These four domains can increase agricultural productivity to achieve the SDGs and Society 5.0. Finally, the book’s contents concisely highlight the application of machine learning and deep learning from preharvest to posthar­vest levels for infield and in-house agricultural production systems, intelligent auto­mation in fruit crops, and evaluation of postharvest efficiency for crops, orchards, livestock, aquaculture, and poultry production. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","","","","","","","Springer Nature","","978-981971263-2; 978-981971262-5","","","English","IoT and AI in Agriculture: Smart Automation Systems for Increasing Agricultural Productivity to Achieve SDGs and Society 5.0","Book chapter","Final","","Scopus","2-s2.0-85210886091"
"Saeliw A.; Hualkasin W.; Puttinaovarat S.","Saeliw, Aekarat (57204815370); Hualkasin, Watcharasuda (6503867254); Puttinaovarat, Supattra (56499350500)","57204815370; 6503867254; 56499350500","Real-Time Threat Prevention System for Mitigating Intrusions by Dogs in Livestock Farming using IoT and Machine Learning","2024","TEM Journal","13","2","","966","975","9","0","10.18421/TEM132-12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196078071&doi=10.18421%2fTEM132-12&partnerID=40&md5=b2cd14d72a4efbea14e3a42d8d4e3b52","Faculty of Science and Industrial Technology, Prince of Songkla University, Surat Thani Campus, Surat Thani, Thailand","Saeliw A., Faculty of Science and Industrial Technology, Prince of Songkla University, Surat Thani Campus, Surat Thani, Thailand; Hualkasin W., Faculty of Science and Industrial Technology, Prince of Songkla University, Surat Thani Campus, Surat Thani, Thailand; Puttinaovarat S., Faculty of Science and Industrial Technology, Prince of Songkla University, Surat Thani Campus, Surat Thani, Thailand","– One of the challenges encountered by farmers engaged in livestock farming is the menace posed by stray or ownerless dogs, causing harm to the animals being raised on the farm. This not only adversely affects the health of the animals but also impacts the overall cost associated with their upbringing. Consequently, this research introduces the development of a sophisticated system aimed at preventing threats and intrusions by dogs that pose harm to farm animals. The system leverages Internet of Things (IoT) technology and employs Machine Learning algorithms, specifically Convolutional Neural Network, for real-time tracking and monitoring. The research findings reveal that the developed system demonstrates a high level of efficiency, swiftly and accurately classifying animals entering areas equipped with cameras, achieving an impressive accuracy rate of 92.54%. Furthermore, the system is equipped to promptly notify users and emit deterrent sounds to repel dogs entering the monitored area, enhancing its effectiveness in safeguarding livestock and optimizing farm management practices. © 2024 Aekarat Saeliw, Watcharasuda Hualkasin & Supattra Puttinaovarat; published by UIKTEN. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License.","deep learning; Dog classification; IoT; mobile application","","","","S. Puttinaovarat; Prince of Songkla University, Surat Thani, Surat Thani Campus, Thailand; email: supattra.p@psu.ac.th","","UIKTEN - Association for Information Communication Technology Education and Science","22178309","","","","English","TEM J.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85196078071"
"Naren D.; Deepak S.; Abhisheik R.; Subhashini R.","Naren, D. (59411426400); Deepak, S. (59410860500); Abhisheik, R. (59410674500); Subhashini, R. (59410113900)","59411426400; 59410860500; 59410674500; 59410113900","Eco-Watch Guardian AI-Enhanced Drone Patrols against Poaching","2024","15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024","2","","","3854","3858","4","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209066816&partnerID=40&md5=1c3e0cc50a306cb77df7aa48fcd9ae7d","Sathyabama Institute of Science and Technology, Chennai, India","Naren D., Sathyabama Institute of Science and Technology, Chennai, India; Deepak S., Sathyabama Institute of Science and Technology, Chennai, India; Abhisheik R., Sathyabama Institute of Science and Technology, Chennai, India; Subhashini R., Sathyabama Institute of Science and Technology, Chennai, India","Conservationists are searching for cutting-edge technical solutions in response to the growing problem of poaching and its catastrophic effects on animal populations. Drones and other names for unmanned aerial vehicles, have shown promise as a tool for wildlife monitoring and anti-poaching operations in recent years. Drone data can be analyzed to provide important insights into animal behavior, migration patterns, and habitat conditions, assisting in the development of more informed conservation strategies. Since the device does not hurt the species being attacked but rather causes discomfort that results in spontaneous pull it is technologically more sophisticated. In order to ensure that the volume patterns of successive frames remain coherent across time, the graph regularized is applied to them. Equipped with various navigation systems such as the GPS and optical flow, they are able to practically navigate itself thanks to today's fly-by-wire technique. Among the many possible uses for drones in tandem with other technology include soil inspections and satellite surveillance. Additionally, the integration of artificial intelligence and machine learning algorithms improves the drone's ability to identify and differentiate between poachers and legitimate visitors or researchers. © Grenze Scientific Society, 2024.","Animal poacher detection; deep learning; MobileNet SSD; real-time recognition; TensorFlow; wildlife conservation","Aircraft detection; Global positioning system; Invertebrates; Livestock; Animal poacher detection; Animal populations; Catastrophic effects; Cutting edges; Deep learning; Mobilenet SSD; Real time recognition; Technical solutions; Tensorflow; Wildlife conservation; Drones","","","","Stephen J.; Sharma P.; Chaba Y.; Abraham K.U.; Anooj P.K.; Mohammad N.; Thomas G.; Srikiran S.","Grenze Scientific Society","","979-833130057-9","","","English","Int. Conf. Adv. Comput., Control, Telecommun. Technol., ACT","Conference paper","Final","","Scopus","2-s2.0-85209066816"
"Siachos N.; Neary J.M.; Smith R.F.; Oikonomou G.","Siachos, Nektarios (57194551523); Neary, Joseph M. (36673570500); Smith, Robert F. (35508911100); Oikonomou, Georgios (24466777400)","57194551523; 36673570500; 35508911100; 24466777400","Automated dairy cattle lameness detection utilizing the power of artificial intelligence; current status quo and future research opportunities","2024","Veterinary Journal","304","","106091","","","","4","10.1016/j.tvjl.2024.106091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186569889&doi=10.1016%2fj.tvjl.2024.106091&partnerID=40&md5=7920f53d9061126a8ac039f75d5d0667","Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom","Siachos N., Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom; Neary J.M., Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom; Smith R.F., Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom; Oikonomou G., Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom","Lameness represents a major welfare and health problem for the dairy industry across all farming systems. Visual mobility scoring, although very useful, is labour-intensive and physically demanding, especially in large dairies, often leading to inconsistencies and inadequate uptake of the practice. Technological and computational advancements of artificial intelligence (AI) have led to the development of numerous automated solutions for livestock monitoring. The objective of this study was to review the automated systems using AI algorithms for lameness detection developed to-date. These systems rely on gait analysis using accelerometers, weighing platforms, acoustic analysis, radar sensors and computer vision technology. The lameness features of interest, the AI techniques used to process the data as well as the ground truth of lameness selected in each case are described. Measures of accuracy regarding correct classification of cows as lame or non-lame varied with most systems being able to classify cows with adequate reliability. Most studies used visual mobility scoring as the ground truth for comparison with only a few studies using the presence of specific foot pathologies. Given the capabilities of AI, and the benefits of early treatment of lameness, longitudinal studies to identify gait abnormalities using automated scores related to the early developmental stages of different foot pathologies are required. Farm-specific optimal thresholds for early intervention should then be identified to ameliorate cow health and welfare but also minimise unnecessary inspections. © 2024 The Authors","Cattle; Locomotion; Machine learning; Mobility","Animals; Artificial Intelligence; Cattle; Cattle Diseases; Dairying; Female; Gait; Lactation; Lameness, Animal; Reproducibility of Results; acoustic analysis; animal lameness; animal welfare; artificial intelligence; artificial neural network; bovine; CatBoost classification; classification algorithm; comparative study; computer vision; convolutional neural network; Convolutional Siamese Neural Network with attention mechanism; cost benefit analysis; dairy cattle; decision tree; deep learning; DenseNet classification; developmental stage; foot disease; gait; gait disorder; Gradient Boosted Decision Tree Learning; ground reaction force; k nearest neighbor; livestock; locomotion; long short term memory network; machine learning; musculoskeletal disease assessment; Naive Bayesian; nonhuman; object detection; peer review; Pose Estimation model; probabilistic neural network; radar sensor; random forest; Receptive Field block Net single Shot Detector; recurrent neural network; reliability; residual neural network; Review; scoring system; signal transduction; support vector machine; telecommunication; Temporal Aggregation network; visual mobility scoring system; Willingness To Pay; animal; animal lameness; cattle disease; dairying; female; lactation; procedures; reproducibility","","","N. Siachos; Department of Livestock and One Health, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Leahurst Campus, Chester High Road, CH64 7TE, United Kingdom; email: Nektarios.siachos@liverpool.ac.uk","","Bailliere Tindall Ltd","10900233","","VTJRF","38431128","English","Vet. J.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85186569889"
"Farjon G.; Edan Y.","Farjon, Guy (57210638288); Edan, Yael (7004434501)","57210638288; 7004434501","AgroCounters—A repository for counting objects in images in the agricultural domain by using deep-learning algorithms: Framework and evaluation","2024","Computers and Electronics in Agriculture","222","","108988","","","","0","10.1016/j.compag.2024.108988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192862569&doi=10.1016%2fj.compag.2024.108988&partnerID=40&md5=a0da3d362207db85bfa546522216386e","Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","Farjon G., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; Edan Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","AgroCounters is an open-source repository for counting objects in images in the agricultural domain by utilizing deep-learning algorithms. In this paper, we present the framework of AgroCounters, which integrates state-of-the-art deep learning models, including regression-based counting, detection-based counting, and density-estimation-based counting, to accurately count various agricultural objects, such as fruits, vegetables, and livestock, in single images. The framework utilizes transfer learning techniques to optimize model performance on the limited labeled data available in the agricultural domain. We provide an open-source implementation of AgroCounters, which includes a multitude of algorithms for counting applications and a toolbox that includes metrics, training data tools, visualizations, and a simple installation guide for several open-source implementations of counting methods. We evaluated the performance of AgroCounters on multiple agricultural datasets acquired from RGB sensors, including plant leaves, melons, wheat grains, cherry tomatoes, grapes, apple flowers, bananas (fruit and leaves), pears, and chickens. We compared the results of the various implemented methods over these datasets and showcased the most suitable solution for each. YOLOv5, the most recent of the compared object detectors, provided the best results on all the examined datasets, and there was no clear ’winner’ between Faster-RCNN and RetinaNet. Based on the analyzed datasets, when higher accuracy is required, the direct regression network (DRN) should be used; for small datasets, multiple scale regression (MSR) gives superior results. Based on the developments, we proposed guidelines for developing deep-learning-based counting solutions for agricultural applications, focusing on solutions and best practices for the agricultural domain. Overall, AgroCounters presents a promising solution for automated counting in the agricultural domain, offering significant potential for reducing manual labor, improving crop management, and increasing productivity. © 2024 Elsevier B.V.","Agriculture; Computer vision; Counting framework; Deep learning; Guidelines; Precision agriculture; Visual counting","Crops; Deep learning; Fruits; Learning algorithms; Learning systems; Object detection; Plants (botany); Precision agriculture; Algorithm evaluation; Algorithm framework; Counting framework; Deep learning; Guideline; Open source implementation; Open source repositories; Precision Agriculture; State of the art; Visual counting; algorithm; crop plant; guideline; precision agriculture; wheat; Computer vision","Ben-Gurion University of the Negev, BGU; Cognitive Robotics Initiative; Marcus Endowment Fund; Ministry of Science and Technology, Israel, (3-16143); Ministry of Science and Technology, Israel; Ministry of Science, ICT and Future Planning, MSIP, (20187); Ministry of Science, ICT and Future Planning, MSIP","This work was partially supported by the Ministry of Science and Technology Fund No. 3-16143 , the Phenomics Consortium, Research Innovation Authority, and from the Ministry of Science Grant Number 20187 , and Ben-Gurion University of the Negev through the Agricultural, Biological, and Cognitive Robotics Initiative, the Marcus Endowment Fund , and the W. Gunther Plaut Chair in Manufacturing Engineering. ","G. Farjon; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; email: guyfar@post.bgu.ac.il","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85192862569"
"Vandana; Yogi K.K.; Yadav S.P.","Vandana (45961583200); Yogi, Kuldeep Kumar (56499071100); Yadav, Satya Prakash (57209717559)","45961583200; 56499071100; 57209717559","Chicken Diseases Detection and Classification Based on Fecal Images Using EfficientNetB7 Model","2024","Evergreen","11","1","","314","330","16","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189101782&partnerID=40&md5=cb44b3116327d47f9697d0d8c2d42877","Department of Computer Science and Engineering, Banas thali Vidyapith, Rajasthan, Banasthali, India; JIMS Engineering Management Technical Campus, Greater Noida, India; Department of Computer Science and Engineering, G L Bajaj Institute of Technology and Management, Greater Noida, India","Vandana, Department of Computer Science and Engineering, Banas thali Vidyapith, Rajasthan, Banasthali, India, JIMS Engineering Management Technical Campus, Greater Noida, India; Yogi K.K., Department of Computer Science and Engineering, Banas thali Vidyapith, Rajasthan, Banasthali, India; Yadav S.P., Department of Computer Science and Engineering, G L Bajaj Institute of Technology and Management, Greater Noida, India","The agriculture sector, particularly the chicken and poultry industries, is under pressure to produce more due to the increased demand for livestock products among consumers. Increased poultry production can lead to the enhanced spread of many infectious diseases in chickens, which can result in high bird fatality rates and significant financial losses. A shortage of trustworthy professionals or delayed diagnosis cause farmers to be losing an extensive number of domestic chickens. Deep learning algorithms can help the early detection of illnesses. This paper proposes a system based on convolutional neural networks to categorize chicken illnesses by identifying healthy and harmful fecal images. Unhealthy images may indicate a poultry illness. Through the use of deep learning algorithms and image analysis of chicken feces, the most common illnesses that affect chickens may be rapidly identified. With the use of a convolutional neural network (CNN) architecture, this research developed a model to identify different chicken ailments by classifying fecal images into two groups: those representing symptoms associated with healthy conditions and those representing symptoms associated with potentially dangerous conditions like Newcastle diseases, Coccidiosis, or Salmonella. To determine if chicken feces fell into one of four categories with the least amount of loss utilized the EfficientNetB7 model with additional layers that extracted the most appropriate features from the fecal images and achieved the highest accuracy. With an accuracy of 97.07%, the new proposed model generated the greatest results when compared to the aforementioned models. © 2024 Joint Journal of Novel Carbon Resource Sciences and Green Asia Strategy. All rights reserved.","Chicken Disease; CNN; EfficientNetB7; Fecal Images; Image Classification; Transfer Learning","","","","Vandana; Department of Computer Science and Engineering, Banas thali Vidyapith, Banasthali, Rajasthan, India; email: vandanabharti233@gmail.com","","Joint Journal of Novel Carbon Resource Sciences and Green Asia Strategy","21890420","","","","English","Evergreen","Article","Final","","Scopus","2-s2.0-85189101782"
"Jain E.; Kapoor S.; Singh M.","Jain, Eshika (59313491400); Kapoor, Sanyam (57219586075); Singh, Manpreet (58073601600)","59313491400; 57219586075; 58073601600","Advanced Chicken Disease Detection with Keras and TensorFlow Deep Learning Models","2024","2nd International Conference on Self Sustainable Artificial Intelligence Systems, ICSSAS 2024 - Proceedings","","","","577","581","4","0","10.1109/ICSSAS64001.2024.10760413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213347251&doi=10.1109%2fICSSAS64001.2024.10760413&partnerID=40&md5=eb265a46d21919a50d1c7b9edaccb94f","Chitkara University Institute of Engineering and Technology, Chitkara University, Centre for Research Impact & Outcome, Punjab, Rajpura, 140401, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, Rajpura, 140401, India; Chitkara University, Chitkara Centre for Research and Development, Himachal Pradesh, 174103, India","Jain E., Chitkara University Institute of Engineering and Technology, Chitkara University, Centre for Research Impact & Outcome, Punjab, Rajpura, 140401, India; Kapoor S., Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, Rajpura, 140401, India; Singh M., Chitkara University, Chitkara Centre for Research and Development, Himachal Pradesh, 174103, India","This study investigates the effectiveness of Convolutional Neural Networks in spotting diseases in images of chickens. The model, examining a set of chicken pictures, achieved an overall 98.27% accuracy. The CNN model perfected using TensorFlow Achieved Keras had excellent performance measures against types of diseases in terms of Precision, Recall, and F1-Score. In other words, the model is a resource that can assist a poultry farmer in determining diseases like coccidiosis, Newcastle Disease, and Salmonella. Early detection and management of chicken diseases are vital to maintaining healthy poultry. These old methods are cumbersome, time-inefficient, and require unique and valuable expertise. The research problem is conceived in the development of a CNN model that can automatically detect diseases with the best elevation in the accuracy and effectiveness of the disease identification process amid poultry. Diagnosing diseases using these methods in chickens can be highly difficult and precise, thereby challenging the farmers in this industry. © 2024 IEEE.","Chicken Disease Detection; Convolutional Neural Networks (CNNs); Image Classification; Keras; TensorFlow","Deep neural networks; Livestock; Chicken disease detection; Convolutional neural network; Disease detection; Images classification; Keras; Learning models; Neural network model; Performance measure; Tensorflow; Convolutional neural networks","","","E. Jain; Chitkara University Institute of Engineering and Technology, Chitkara University, Centre for Research Impact & Outcome, Rajpura, Punjab, 140401, India; email: eshika.jain@chitkara.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835036841-3","","","English","Int. Conf. Self Sustain. Artif. Intell. Syst., ICSSAS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85213347251"
"Tun S.C.; Onizuka T.; Tin P.; Aikawa M.; Kobayashi I.; Zin T.T.","Tun, San Chain (58087605300); Onizuka, Tsubasa (58544593700); Tin, Pyke (24923729700); Aikawa, Masaru (36663478200); Kobayashi, Ikuo (24174822700); Zin, Thi Thi (6506258245)","58087605300; 58544593700; 24923729700; 36663478200; 24174822700; 6506258245","Revolutionizing Cow Welfare Monitoring: A Novel Top-View Perspective with Depth Camera-Based Lameness Classification","2024","Journal of Imaging","10","3","67","","","","2","10.3390/jimaging10030067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188858740&doi=10.3390%2fjimaging10030067&partnerID=40&md5=8c77394855b4873a09bc87b9a0c90d7e","Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Organization for Learning and Student Development, University of Miyazaki, Miyazaki, 889-2192, Japan; Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyzaki, 889-2192, Japan","Tun S.C., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Onizuka T., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Tin P., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Aikawa M., Organization for Learning and Student Development, University of Miyazaki, Miyazaki, 889-2192, Japan; Kobayashi I., Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyzaki, 889-2192, Japan; Zin T.T., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan","This study innovates livestock health management, utilizing a top-view depth camera for accurate cow lameness detection, classification, and precise segmentation through integration with a 3D depth camera and deep learning, distinguishing it from 2D systems. It underscores the importance of early lameness detection in cattle and focuses on extracting depth data from the cow’s body, with a specific emphasis on the back region’s maximum value. Precise cow detection and tracking are achieved through the Detectron2 framework and Intersection Over Union (IOU) techniques. Across a three-day testing period, with observations conducted twice daily with varying cow populations (ranging from 56 to 64 cows per day), the study consistently achieves an impressive average detection accuracy of 99.94%. Tracking accuracy remains at 99.92% over the same observation period. Subsequently, the research extracts the cow’s depth region using binary mask images derived from detection results and original depth images. Feature extraction generates a feature vector based on maximum height measurements from the cow’s backbone area. This feature vector is utilized for classification, evaluating three classifiers: Random Forest (RF), K-Nearest Neighbor (KNN), and Decision Tree (DT). The study highlights the potential of top-view depth video cameras for accurate cow lameness detection and classification, with significant implications for livestock health management. © 2024 by the authors.","decision tree (DT); depth sensing camera; detection and tracking; k-nearest neighbor (KNN); lameness; random forest (RF)","Agriculture; Binary images; Data mining; Deep learning; Feature extraction; Motion compensation; Nearest neighbor search; Video cameras; Decision tree; Depth camera; Depth sensing; Depth sensing camera; Detection and tracking; K-near neighbor; Lameness; Random forest; Random forests; Top views; Decision trees","JKA, (2023M-425)","This publication is subsidized by JKA (Grant Number: 2023M-425) through its promotion funds from KEIRIN RACE.","T.T. Zin; Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; email: thithi@cc.miyazaki-u.ac.jp","","Multidisciplinary Digital Publishing Institute (MDPI)","2313433X","","","","English","J. Imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85188858740"
"Karki R.; Rana S.; Jain C.; Rawat D.; Vats S.; Sharma V.","Karki, Rahul (59423193800); Rana, Shubham (59423606900); Jain, Chahak (59423193900); Rawat, Daksh (59250755000); Vats, Satvik (57193131437); Sharma, Vikrant (56785043200)","59423193800; 59423606900; 59423193900; 59250755000; 57193131437; 56785043200","Illness and Gender Classification of Cattle","2024","2024 1st International Conference on Advanced Computing and Emerging Technologies, ACET 2024","","","","","","","0","10.1109/ACET61898.2024.10730546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210090946&doi=10.1109%2fACET61898.2024.10730546&partnerID=40&md5=58edea14a1145caef3ac1ce4ceb3e53d","Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; Graphic Era Hill University, Computer Science and Engineering, Dehradun, 248002, India","Karki R., Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; Rana S., Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; Jain C., Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; Rawat D., Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; Vats S., Graphic Era Hill University, Computer Science and Engineering, Dehradun, 248002, India; Sharma V., Graphic Era Hill University, Computer Science and Engineering, Dehradun, India","Cattles are one of the most important components of the global agriculture industry as they provide essential resources like meat and dairy products. This is the reason cattle health is very crucial to ensure animal welfare. This paper presents a deep learning model designed to classify cattles based on gender and diagnose illness in an efficient manner. In this paper two different datasets have been used, one for gender classification and the other for illness detection. Through an extensive dataset of cattle images, the model designed using deep learning techniques learns to distinguish between male and female cattle with a high degree of accuracy. Additionally, the model designed here has the capability of detecting signs of illness or distress in cattles based on visual cures, such as changes in posture, skin condition and facial expressions. The model can be proved helpful in timely intervention, therefore improving overall animal welfare economic. Moreover, this research opens doors for the development of similar models in the broader context of livestock management and animal health.  © 2024 IEEE.","cattle; deep learning neural network; machine learning; pretrained model","Agriculture industries; Animal welfare; Cattles; Deep learning neural network; Gender classification; Learning models; Learning neural networks; Learning techniques; Machine-learning; Pretrained model; Dairy products","","","R. Karki; Graphic Era Hill University, Computer Science and Engineering, Dehradun, India; email: rahulkarki226@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835036772-0","","","English","Int. Conf. Adv. Comput. Emerg. Technol., ACET","Conference paper","Final","","Scopus","2-s2.0-85210090946"
"Almadani I.; Ramos B.; Abuhussein M.; Robinson A.L.","Almadani, Iyad (57553426800); Ramos, Brandon (59196169300); Abuhussein, Mohammed (57208663376); Robinson, Aaron L. (21233931100)","57553426800; 59196169300; 57208663376; 21233931100","Advanced Swine Management: Infrared Imaging for Precise Localization of Reproductive Organs in Livestock Monitoring","2024","Digital","4","2","","446","460","14","1","10.3390/digital4020022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197189678&doi=10.3390%2fdigital4020022&partnerID=40&md5=eac0087d0ded4f1a7998f72c49fdfbde","Electrical and Computer Engineering, University of Memphis, Memphis, 38111, TN, United States; Sivananthan Laboratories, 586 Territorial Dr Suite A, Bolingbrook, 60440, IL, United States","Almadani I., Electrical and Computer Engineering, University of Memphis, Memphis, 38111, TN, United States; Ramos B., Sivananthan Laboratories, 586 Territorial Dr Suite A, Bolingbrook, 60440, IL, United States; Abuhussein M., Electrical and Computer Engineering, University of Memphis, Memphis, 38111, TN, United States; Robinson A.L., Electrical and Computer Engineering, University of Memphis, Memphis, 38111, TN, United States","Traditional methods for predicting sow reproductive cycles are not only costly but also demand a larger workforce, exposing workers to respiratory toxins, repetitive stress injuries, and chronic pain. This occupational hazard can even lead to mental health issues due to repeated exposure to violence. Managing health and welfare issues becomes pivotal in group-housed animal settings, where individual care is challenging on large farms with limited staff. The necessity for computer vision systems to analyze sow behavior and detect deviations indicative of health problems is apparent. Beyond observing changes in behavior and physical traits, computer vision can accurately detect estrus based on vulva characteristics and analyze thermal imagery for temperature changes, which are crucial indicators of estrus. By automating estrus detection, farms can significantly enhance breeding efficiency, ensuring optimal timing for insemination. These systems work continuously, promptly alerting staff to anomalies for early intervention. In this research, we propose part of the solution by utilizing an image segmentation model to localize the vulva. We created our technique to identify vulvae on pig farms using infrared imagery. To accomplish this, we initially isolate the vulva region by enclosing it within a red rectangle and then generate vulva masks by applying a threshold to the red area. The system is trained using U-Net semantic segmentation, where the input for the system consists of grayscale images and their corresponding masks. We utilize U-Net semantic segmentation to find the vulva in the input image, making it lightweight, simple, and robust enough to be tested on many images. To evaluate the performance of our model, we employ the intersection over union (IOU) metric, which is a suitable indicator for determining the model’s robustness. For the segmentation model, a prediction is generally considered ‘good’ when the intersection over union score surpasses 0.5. Our model achieved this criterion with a score of 0.58, surpassing the scores of alternative methods such as the SVM with Gabor (0.515) and YOLOv3 (0.52). © 2024 by the authors.","computer vision; deep learning; farming; image segmentation; localization; reproductive organ; U-Net","","Sivananthan Laboratories","This research was funded by Sivananthan Laboratories.","I. Almadani; Electrical and Computer Engineering, University of Memphis, Memphis, 38111, United States; email: mlmadani@memphis.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","26736470","","","","English","Digital.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85197189678"
"Senthil Pandi S.; Reshmy A.K.; Vinodh Kumar S.; Kumar P.","Senthil Pandi, S. (57215061080); Reshmy, A.K. (56938833600); Vinodh Kumar, S. (59184597100); Kumar, P. (58265901700)","57215061080; 56938833600; 59184597100; 58265901700","Effective Deep Learning Framework for Crop Pest Classification","2024","2nd IEEE International Conference on Advances in Information Technology, ICAIT 2024 - Proceedings","","","","","","","1","10.1109/ICAIT61638.2024.10690463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208826874&doi=10.1109%2fICAIT61638.2024.10690463&partnerID=40&md5=93a0aea58413f87f7a83e43129d54819","Department of CSE, Rajalakshmi Engineering College, Chennai, India; Department of CINTEL, School of Computing, SRM Institute of Science and Technology, Kattankulathur, Chennai, India","Senthil Pandi S., Department of CSE, Rajalakshmi Engineering College, Chennai, India; Reshmy A.K., Department of CINTEL, School of Computing, SRM Institute of Science and Technology, Kattankulathur, Chennai, India; Vinodh Kumar S., Department of CSE, Rajalakshmi Engineering College, Chennai, India; Kumar P., Department of CSE, Rajalakshmi Engineering College, Chennai, India","Pest infestations in agricultural ecosystems present considerable risks to vital crops including potatoes, rice, wheat, maize, soybeans, sugarcane and chickpeas, all of which result in considerable economic detriment. Determining the precise insect species accountable for the infestation is critical in order to efficiently mitigate these losses. Notwithstanding this, farmers frequently encounter difficulties in precisely differentiating among different species of crop insects on account of their restricted expertise and experience in the field of entomology. Advanced computer-based technologies, including as convolutional neural networks (CNNs), provide promising solutions to this critical problem. These technologies are becoming increasingly popular. CNNs are well-known for their capacity to automatically extract and learn detailed features from image data. As a result, they are ideally suited for jobs that include picture categorization. Through the utilisation of CNNs, researchers have successfully classified a large number of photos spanning a variety of fields. In our study, we offer a unique strategy for improving insect categorization reliability in agricultural contexts. Specifically, we present a hybrid BiLSTM (Bidirectional Long Short-Term Memory) network structure designed for this application. This design combines the features of a model that has been trained and a BiLSTM layer to effectively collect temporal data required for insect categorization. Our proposed model seeks to address the issues of insect identifying species in agricultural environments through integrating the strengths of CNNs with the temporal retention of memory capabilities of BiLSTM networks. Through the utilization of this hybrid architecture, we anticipate achieving superior performance in insect classification tasks compared to traditional CNN-based approaches. By leveraging temporal features alongside learned image characteristics, our model endeavors to provide farmers and agricultural stakeholders with a robust tool for accurate and timely identification of crop pests, thereby enabling proactive pest management strategies and minimizing economic losses in agricultural production. © 2024 IEEE.","Classification; CNN; Crop; Deep Learning; LSTM; Pests","Digital storage; Livestock; Long short-term memory; Multilayer neural networks; Agricultural ecosystems; Computer based technologies; Convolutional neural network; Deep learning; Learning frameworks; LSTM; Memory network; Pest; Pest infestation; Short term memory; Convolutional neural networks","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038386-7","","","English","IEEE Int. Conf. Adv. Inf. Technol., ICAIT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85208826874"
"","","","2024 IEEE International Symposium on Consumer Technology: Toward Innovation in Consumer Technology for A Sustainable Environment, ISCT 2024 - Proceeding","2024","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","","","","","","870","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215301780&partnerID=40&md5=df4dea54b0cf8f4b6ac65d479f202c50","","","The proceedings contain 127 papers. The topics discussed include: wireless edge controller for robot education using MQTT on dobot magician lite; a PID control algorithm for path tracking of a nonholonomic car-like robot; optimizing Latin to Balinese script transliteration: hybrid Jaro Winkler and Damerau Levenshtein methods; productivity improvement through energy efficiency training in manufacturing industry; validation of an academic resilience scale for Indonesian university students; assessing the relationship between regional economic advancement and digital activity; optimizing optical character recognition accuracy in livestock weighting systems through distributed intelligent computer vision; and early warning forecasting of large induction motor in the oil and gas industry from deep learning.","","","","","","","Institute of Electrical and Electronics Engineers Inc.","0747668X","979-835036519-1","DTPEE","","English","Dig Tech Pap IEEE Int Conf Consum Electron","Conference review","Final","","Scopus","2-s2.0-85215301780"
"Jin Z.; Shu H.; Hu T.; Jiang C.; Yan R.; Qi J.; Wang W.; Guo L.","Jin, Zhongming (57216124235); Shu, Hang (57222589919); Hu, Tianci (58295273200); Jiang, Chengxiang (58294943300); Yan, Ruirui (35263075300); Qi, Jingwei (36111666800); Wang, Wensheng (56937276700); Guo, Leifeng (56542160600)","57216124235; 57222589919; 58295273200; 58294943300; 35263075300; 36111666800; 56937276700; 56542160600","Behavior classification and spatiotemporal analysis of grazing sheep using deep learning","2024","Computers and Electronics in Agriculture","220","","108894","","","","6","10.1016/j.compag.2024.108894","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189658929&doi=10.1016%2fj.compag.2024.108894&partnerID=40&md5=1bc395afb5a73b26d9c12936e44a2688","Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China; Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; College of Animal Science, Inner Mongolia Agricultural University, Hohhot, 010018, China; Big Data Development Center, Ministry of Agriculture and Rural Affairs of the People's Republic of China, 100125, China; Xinjiang Wool Engineering Technology Research Center, Institute of Animal Husbandry Quality Standards, Xinjiang Academy of Animal Science, Urumqi, 830057, China","Jin Z., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; Shu H., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Hu T., College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China; Jiang C., College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, 830052, China; Yan R., Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; Qi J., College of Animal Science, Inner Mongolia Agricultural University, Hohhot, 010018, China; Wang W., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, Big Data Development Center, Ministry of Agriculture and Rural Affairs of the People's Republic of China, 100125, China; Guo L., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, Xinjiang Wool Engineering Technology Research Center, Institute of Animal Husbandry Quality Standards, Xinjiang Academy of Animal Science, Urumqi, 830057, China","Monitoring multiple behavioral patterns and grazing trajectories of individual sheep can provide valuable information for various aspects of livestock production. The emergence of low-cost miniature sensors, coupled with the continuous advancement of deep learning technologies, has ushered in a new generation of intelligent solutions for precision livestock farming. This study aims to explore the deployment methods of motion sensors, selection of data collection frequencies, and choice of deep learning algorithms to provide accurate classification of multiple behaviors in grazing sheep. Based on this, in conjunction with the acquired location information, the goal is to comprehend the spatiotemporal distribution of grazing sheep behaviors. Devices capable of collecting Inertial Measurement Unit (IMU) and location data were attached to the jaw, neck, and hind leg of sheep. Four datasets were created using IMU data with a frequency of 20 Hz and a 5 s time window from different positions (neck, neck & leg, jaw, jaw & leg). Two deep learning models, Convolutional Neural Network (CNN) - Long Short Term Memory (LSTM) and Temporal Convolutional Network (TCN)-Transformer, were employed to classify six grazing sheep behaviors: walking, standing, grazing, lying, standing-ruminating, and lying-ruminating. The results indicate that by fusing data from the neck- and leg-mounted devices and utilizing the CNN-LSTM model, the accuracy reached the highest at 99.3 %. Furthermore, a comparison was made regarding the behavior classification accuracy of this fused data at different IMU data frequencies (20, 10, 5, and 1 Hz). It was found that even when the data frequency was reduced to 1 Hz, the classification accuracy for the six sheep behaviors still exceeded 96 %. Additionally, the trained model with the highest accuracy was applied to monitoring grazing sheep behavior under two different management procedures. Further analysis of the time budgets of sheep behavior revealed differences in the durations of behaviors under different management procedures. For extensively grazing sheep, location information was also monitored and combined with behavior classification results to generate the spatiotemporal distribution of sheep behavior. The technologies presented are important for gaining further insights into the health status of grazing livestock and grassland, thereby enhancing grazing management practices by informing grazing choices and optimizing grassland utilization. © 2024 Elsevier B.V.","Behavior classification; Behavior spatiotemporal distribution; Deep learning; Grazing sheep","Agriculture; Budget control; Classification (of information); Convolution; Convolutional neural networks; Learning algorithms; Location; Behavior spatiotemporal distribution; Behaviour classification; Classification accuracy; Convolutional neural network; Deep learning; Grazing sheep; Inertial measurements units; Location information; Sheep behaviour; Spatiotemporal distributions; behavioral response; classification; grazing management; health status; livestock farming; machine learning; sheep; spatiotemporal analysis; Long short-term memory","National Key Research and Development Program of China, NKRDPC, (2021YFD1300500); Key Research and Development Program of Xinjiang Uygur Autonomous Region, (2023B02013); Science and technology innovation project of Chinese Academy of Agricultural Sciences, (CAAS-ASTIP-2023-AII); Chinese Academy of Agricultural Sciences, CAAS, (ASTIP - 2023-AII)","This research was supported by the National Key Research and Development Program of China (2021YFD1300500); the Key Research and Development Program of Xinjiang Uygur Autonomous Region (2023B02013); and the Science and technology innovation project of Chinese Academy of Agricultural Sciences (CAAS-ASTIP-2023-AII).","W. Wang; Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; email: wangwensheng@caas.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85189658929"
"Li J.; Yang Y.; Liu G.; Ning Y.; Song P.","Li, Jianquan (58485770800); Yang, Ying (57192546717); Liu, Gang (57374481500); Ning, Yuanlin (58080046100); Song, Ping (59235444000)","58485770800; 57192546717; 57374481500; 58080046100; 59235444000","Open-Set Sheep Face Recognition in Multi-View Based on Li-SheepFaceNet","2024","Agriculture (Switzerland)","14","7","1112","","","","0","10.3390/agriculture14071112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199663310&doi=10.3390%2fagriculture14071112&partnerID=40&md5=42b60deac95461ef0f54269ddc9c15ea","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Smart Agriculture System Integration, Ministry of Education, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, China Agricultural University, Beijing, 100083, China","Li J., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Yang Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Liu G., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Smart Agriculture System Integration, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs, China Agricultural University, Beijing, 100083, China; Ning Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Song P., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Deep learning-based sheep face recognition improves the efficiency and effectiveness of individual sheep recognition and provides technical support for the development of intelligent livestock farming. However, frequent changes within the flock and variations in facial features in different views significantly affect the practical application of sheep face recognition. In this study, we proposed the Li-SheepFaceNet, a method for open-set sheep face recognition in multi-view. Specifically, we employed the Seesaw block to construct a lightweight model called SheepFaceNet, which significantly improves both performance and efficiency. To enhance the convergence and performance of low-dimensional embedded feature learning, we used Li-ArcFace as the loss function. The Li-SheepFaceNet achieves an open-set recognition accuracy of 96.13% on a self-built dataset containing 3801 multi-view face images of 212 Ujumqin sheep, which surpasses other open-set sheep face recognition methods. To evaluate the robustness and generalization of our approach, we conducted performance testing on a publicly available dataset, achieving a recognition accuracy of 93.33%. Deploying Li-SheepFaceNet on an open-set sheep face recognition system enables the rapid and accurate identification of individual sheep, thereby accelerating the development of intelligent sheep farming. © 2024 by the authors.","computer vision; convolutional neural network; deep learning; intelligent livestock farming; sheep face recognition","","National Key Research and Development Program of China, NKRDPC, (2021YFD1300502); National Key Research and Development Program of China, NKRDPC","This research was funded by National Key R&D Program of China (Grant No. 2021YFD1300502).","Y. Yang; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; email: hbxtyy@126.com","","Multidisciplinary Digital Publishing Institute (MDPI)","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85199663310"
"Bárbulo Barrios D.; Valente J.; van Langevelde F.","Bárbulo Barrios, Diego (58876725100); Valente, João (42062501000); van Langevelde, Frank (6701790743)","58876725100; 42062501000; 6701790743","Monitoring mammalian herbivores via convolutional neural networks implemented on thermal UAV imagery","2024","Computers and Electronics in Agriculture","218","","108713","","","","4","10.1016/j.compag.2024.108713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184661750&doi=10.1016%2fj.compag.2024.108713&partnerID=40&md5=67ee5080e78f008d7aea00a531284a6c","Wildlife Ecology and Conservation Group, Wageningen University & Research, Droevendaalsesteeg 3a, Wageningen, 6708 PB, Netherlands; Information Technology Group, Wageningen University & Research, Hollandseweg 1, Wageningen, 6706 KN, Netherlands","Bárbulo Barrios D., Wildlife Ecology and Conservation Group, Wageningen University & Research, Droevendaalsesteeg 3a, Wageningen, 6708 PB, Netherlands, Information Technology Group, Wageningen University & Research, Hollandseweg 1, Wageningen, 6706 KN, Netherlands; Valente J., Information Technology Group, Wageningen University & Research, Hollandseweg 1, Wageningen, 6706 KN, Netherlands; van Langevelde F., Wildlife Ecology and Conservation Group, Wageningen University & Research, Droevendaalsesteeg 3a, Wageningen, 6708 PB, Netherlands","Lightweight Unmanned Aerial Vehicles (UAVs) are emerging as a remote sensing survey tool for animal monitoring in several fields, such as precision livestock farming. Together with state-of-the-art computer vision techniques, UAV. technology has drastically escalated our ability to acquire and analyse visual data in the field, lowering both costs and complications associated with collection and analysis. This paper addresses monitoring mammalian herbivores using the unexploited field of thermal Multi-Object Tracking and Segmentation (MOTS) in UAV imagery. In our research, a state-of-the-art MOTS algorithm (Track R-CNN) was trained and evaluated in the segmentation, detection and tracking of dairy cattle. Data collection was carried out in two farms with a UAV carrying a thermal camera at various angles and heights, and under different light (overcast/sunny) and thermal (16.5 °C range) conditions. Our findings suggest that dataset diversity and balance, especially regarding the range of conditions under which the data was collected, can significantly enhance tracking efficiency in specific scenarios. For training the algorithm, transfer learning was used as a knowledge migration method. The performance of our best model (68.5 sMOTSA, 79.6 MOTSA, 41 IDS, 100 % counting accuracy, and 87.2 MOTSP), which utilizes 3D convolutions and an association head, demonstrates the applicability and optimal performance of Track R-CNN in detecting, tracking, and counting herbivores in UAV thermal imagery under heterogenous conditions. Our findings demonstrate that 3D convolutions outperform Long-short Term Memory (LSTM) convolutions. However, LSTM convolutions also show optimal performance, offering a viable alternative. Furthermore, our results highlight the inability of Optical Flow to track motionless animals (-15 sMOTSA, −4.1 MOTSA and 2076 IDS) and the proficiency of the association head in differentiating static animals from the background. This research contributes to the growing body of knowledge in automated mammalian herbivore monitoring, with potential applications such as precision livestock farming and wildlife conservation. © 2024 The Author(s)","Animal monitoring; Deep learning; Instance segmentation; Livestock; Tracking","Aircraft detection; Antennas; Farms; Long short-term memory; Mammals; Optical remote sensing; Unmanned aerial vehicles (UAV); Aerial vehicle; Animal monitoring; Condition; Deep learning; Instance segmentation; Livestock; Mammalian herbivores; Precision livestock farming; Thermal; Tracking; algorithm; computer vision; herbivore; livestock; livestock farming; machine learning; monitoring system; precision; remote sensing; satellite imagery; segmentation; tracking; unmanned vehicle; Convolution","","","J. Valente; Information Technology Group, Wageningen University & Research, Wageningen, Hollandseweg 1, 6706 KN, Netherlands; email: joao.valente@wur.nl","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85184661750"
"Bao Y.; Llagostera P.; Plà-Aragonès L.M.","Bao, Yun (59152468300); Llagostera, Pol (57219754114); Plà-Aragonès, Lluís M. (57222251428)","59152468300; 57219754114; 57222251428","Is Deep Learning useful for decision making in pig production?","2024","Internet of Things (Netherlands)","26","","101229","","","","3","10.1016/j.iot.2024.101229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194909920&doi=10.1016%2fj.iot.2024.101229&partnerID=40&md5=5371721e5aabdff174c31e3f735384da","Departament of Mathematics, University of Lleida, c/ Jaume II, 73, Lleida, 25003, Spain; Agrotecnio CERCA Center, Av. Rovira Roure 191, Lleida, 25198, Spain","Bao Y., Departament of Mathematics, University of Lleida, c/ Jaume II, 73, Lleida, 25003, Spain, Agrotecnio CERCA Center, Av. Rovira Roure 191, Lleida, 25198, Spain; Llagostera P., Departament of Mathematics, University of Lleida, c/ Jaume II, 73, Lleida, 25003, Spain; Plà-Aragonès L.M., Departament of Mathematics, University of Lleida, c/ Jaume II, 73, Lleida, 25003, Spain, Agrotecnio CERCA Center, Av. Rovira Roure 191, Lleida, 25198, Spain","Numerous recent papers based on deep learning (DL) have been published covering a wide range of applications to pig production. These applications provide information susceptible of being used to make better decisions. However, the potential use as tools for supporting pig production decisions or the integration in existing or new decision models have not been explored yet. The goal of this systematic literature review (SLR) is to provide an overview of recent developments in cutting-edge DL methodologies proposed in pig production and how they can serve to improve decision making processes. The revised papers are analyzed under different dimensions: (1) authors and research institutions that have made the biggest contributions to DL for image processing, computer vision and other innovative applications in pig farms; (2) coverage of the echelons in the pig supply chain (3) technical aspects like data collection techniques, DL models, DL backbones, graphics processing units (GPUs), and evaluation metrics and (4) value of information. The review is briefly extended to DL applications in other livestock species not yet present in pig production to enrich the discussion. The revised applications suggest that DL is mostly applied to automatize data gathering and processing and to monitor animals or on farm activities. The current challenges and future research agenda are also identified envisioning the integration of DL and operational research(OR) methods as a way to produce more efficient decision-making support tools for the pig industry. © 2024 The Author(s)","Deep learning; Digitization; Operations research; Pig decision support; Pig industry","","CYTED Ciencia y Tecnología para el Desarrollo, CYTED, (524RT0158); CYTED Ciencia y Tecnología para el Desarrollo, CYTED; Ministerio de Ciencia e Innovación, MCIN, (TED2021-130829B-I00); Ministerio de Ciencia e Innovación, MCIN","This work was supported by CYTED program, grant number 524RT0158, and the Ministerio de Ciencia e Innovaci\u00F3n through project AI4PORK, project number TED2021-130829B-I00.","L.M. Plà-Aragonès; Departament of Mathematics, University of Lleida, Lleida, c/ Jaume II, 73, 25003, Spain; email: lluismiquel.pla@udl.cat","","Elsevier B.V.","25426605","","","","English","Internet. Thing.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85194909920"
"Liang X.; Pan D.; Yu J.","Liang, Xiaolong (59320873600); Pan, Derun (59320873700); Yu, Jiayi (58545899600)","59320873600; 59320873700; 58545899600","Intelligent identification system of wild animals image based on deep learning in biodiversity conservation law","2024","Journal of Computational Methods in Sciences and Engineering","24","3","","1523","1538","15","0","10.3233/JCM-247185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203547126&doi=10.3233%2fJCM-247185&partnerID=40&md5=b7b4771b8f339ee294d7f2a500dcc923","College of Intellectual Property, Hubei University of Automotive Technology, Hubei, Shiyan, China; College of Marxism, Xi’an Shiyou University, Shaanxi, Xi’an, China; College of Industrial Management, Gyeongsang National University, Jinju, South Korea","Liang X., College of Intellectual Property, Hubei University of Automotive Technology, Hubei, Shiyan, China; Pan D., College of Marxism, Xi’an Shiyou University, Shaanxi, Xi’an, China; Yu J., College of Industrial Management, Gyeongsang National University, Jinju, South Korea","This study aims to overcome the impact of complex environmental backgrounds on the recognition of wildlife in monitoring images, thereby exploring the role of a deep learning-based intelligent wildlife recognition system in biodiversity conservation. The automatic identification of wildlife images is conducted based on convolutional neural networks (CNNs). Target detection technology, based on regression algorithms, is initially employed to extract Regions of Interest (ROI) containing wildlife from images. The wildlife regions in monitoring images are detected, segmented, and converted into ROI images. A dual-channel network model based on Visual Geometry Group 16 (VGG16) is implemented to extract features from sample images. Finally, these features are input into a classifier to achieve wildlife recognition. The proposed optimized model demonstrates superior recognition performance for five wildlife species, caribou, lynx, mule deer, badger, and antelope, compared to the dual-channel network model based on VGG16. The optimized model achieves a Mean Average Precision (MAP) of 0.714, with a maximum difference of 0.145 compared to the other three network structures, affirming its effectiveness in enhancing the accuracy of automatic wildlife recognition. The model effectively addresses the issue of low recognition accuracy caused by the complexity of background information in monitoring images, achieving high-precision recognition and holding significant implications for the implementation of biodiversity conservation laws. © 2024 – IOS Press.","biodiversity conservation law; convolutional neural networks; Deep learning; dual-channel network; intelligent identification of images; wild animals","Livestock; Biodiversity conservation; Biodiversity conservation law; Channel network; Conservation law; Convolutional neural network; Deep learning; Dual channel; Dual-channel network; Intelligent identification; Intelligent identification of image; Wild animals; Invertebrates","SMEs; Hubei University of Automotive Technology, HUAT; Key Project of HuBei Province Philosophy and Social Science Foundation; Medium-sized Science and Technology Enterprises, (22D083)","Funding text 1: This work was supported by Doctoral Scientific Research Foundation of Hubei University of Automotive Technology \u201CElements, operational mechanism and construction path and strategy of the ecosystem that helps the high-quality development of SMEs through intellectual property financial services\u201D, (No.BK202341). This work was supported by Key Project of HuBei Province Philosophy and Social Science Foundation in 2022 \u201CResearch on the Ecological System and Policy Optimization of Intellectual Property Finance Supporting the Innovation and Development of Small and Medium-sized Science and Technology Enterprises\u201D, (No.22D083). The authors acknowledge the help from the university colleagues.; Funding text 2: This work was supported by Key Project of HuBei Province Philosophy and Social Science Foundation in 2022 \u201CResearch on the Ecological System and Policy Optimization of Intellectual Property Finance Supporting the Innovation and Development of Small and Medium-sized Science and Technology Enterprises\u201D, (No.22D083). ","J. Yu; College of Industrial Management, Gyeongsang National University, Jinju, 52828, South Korea; email: mangguo1218@126.com","","IOS Press BV","14727978","","","","English","J. Comput. Methods Sci. Eng.","Article","Final","","Scopus","2-s2.0-85203547126"
"Madasamy Raja G.; Rajesh S.; Kumar S.; Priya J.S.; Keerthi M.M.; Kumar B.S.","Madasamy Raja, G. (55792866100); Rajesh, S. (59520331200); Kumar, Santosh (57192413856); Priya, J. Shalini (58141685900); Keerthi, Murugesan Mohana (58573961900); Kumar, Bakkala Santha (57214816806)","55792866100; 59520331200; 57192413856; 58141685900; 58573961900; 57214816806","Poultry Diseases Diagnostics using Deep Learning Techniques","2024","4th International Conference on Power, Energy, Control and Transmission Systems: Harnessing Power and Energy for an Affordable Electrification of India, ICPECTS 2024","","","","","","","0","10.1109/ICPECTS62210.2024.10780092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215528000&doi=10.1109%2fICPECTS62210.2024.10780092&partnerID=40&md5=132121f544444296661d4c72c204c6c4","Paavai Engineering College, Department of It, Tamilnadu, India; Paavai Engineering College, Department of Cse, Tamil Nadu, India; Era University, Lucknow, India; Sri Sairam Engineering College, Department of Eee, Chennai, India; Sr University, School of Agriculture, Warangal, India; Koneru Lakshmaiah Education Foundation, Department of Cse, Andhra Pradesh, India","Madasamy Raja G., Paavai Engineering College, Department of It, Tamilnadu, India; Rajesh S., Paavai Engineering College, Department of Cse, Tamil Nadu, India; Kumar S., Era University, Lucknow, India; Priya J.S., Sri Sairam Engineering College, Department of Eee, Chennai, India; Keerthi M.M., Sr University, School of Agriculture, Warangal, India; Kumar B.S., Koneru Lakshmaiah Education Foundation, Department of Cse, Andhra Pradesh, India","Common poultry diseases including coccidiosis, salmonella, and newton can reduce chicken output if they are not caught in time. Due to poultry producers' restricted admittance to agricultural sustain services, many illnesses are not identified early in Tanzania. The probable for early identification of many poultry diseases exists with deep learning techniques. Through the classification of healthy and harmful faecal pictures, a deep convolutional-neural-network (CNN) was created in this study to identify poultry diseases. Unhealthy faecal pictures could be a sign of Newcastle, Salmonella, or Coccidiosis. In order to annotate the 1,255 laboratory-labeled faecal photos, we also collected faecal samples utilised in Polymerase Chain Reaction diagnosis. Using an Open Data Kit, we captured 6,812 images of chicken faces. The farm-labeled faecal pictures were annotated by specialists in agricultural support. Then, we applied the VGG16, InceptionV3, MobileNetV2, and Xception models, as well as a baseline CNN model. We developed models utilising tagged faecal pictures from farms and laboratories. Farm-labeled photos were utilised for the test set. The test accuracy scores for the baseline CNN, VGG16, InceptionV3, MobileNetV2, and Xception were 83.06%, 85.85%, 94.79%, 87.46%, and 88.27%, respectively. InceptionV3, MobileNetV2, and Xception all achieved model accuracy values of 95.01%, 95.45%, 98.02%, and 98.24%, respectively, after fine-tuning while glacial the batch normalization layer, with F1-scores for all classifiers over 76% in each of the four classes. We advise using this model to detect chicken diseases early on farms because to the trained MobileNetV2's smaller weight and higher ability to generalize. © 2024 IEEE.","agriculture; dataset; Deep-learning; image-classification; poultry-disease diagnostics","Deep learning; Fertilizers; Image annotation; Livestock; Macroinvertebrates; Chain reaction; Convolutional neural network; Dataset; Deep-learning; Disease diagnostics; Faecal samples; Images classification; Learning techniques; Poultry-disease diagnostic; Tanzania; Salmonella","","","G. Madasamy Raja; Paavai Engineering College, Department of It, Tamilnadu, India; email: drgmraja@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-833150884-5","","","English","Int. Conf. Power, Energy, Control Transm. Syst.: Harnessing Power Energy an Afford. Electrif. India, ICPECTS","Conference paper","Final","","Scopus","2-s2.0-85215528000"
"Li X.; Yin L.; Huang W.; Wu Z.; Cai G.; Tian X.","Li, Xiya (58984185300); Yin, Ling (36024801300); Huang, Wenjie (58984185400); Wu, Zhenfang (7501411790); Cai, Gengyuan (56498459200); Tian, Xuhong (11641329500)","58984185300; 36024801300; 58984185400; 7501411790; 56498459200; 11641329500","Counting and assessing piglet teats using object detection; [基于目标检测仔猪乳头计数及乳房形态评估方法]","2024","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","40","3","","156","164","8","0","10.11975/j.issn.1002-6819.202309180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190293759&doi=10.11975%2fj.issn.1002-6819.202309180&partnerID=40&md5=564fec89dcc361a8ac592486d3a794b3","College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; College of Animal Science, South China Agricultural University, Guangzhou, 510642, China; National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China; State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China","Li X., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; Yin L., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China, National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China; Huang W., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; Wu Z., College of Animal Science, South China Agricultural University, Guangzhou, 510642, China, National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China; Cai G., College of Animal Science, South China Agricultural University, Guangzhou, 510642, China, National Engineering Research Center for Swine Breeding Industry, Guangzhou, 510642, China, State Key Laboratory of Swine and Poultry Breeding Industry, Guangzhou, 510640, China; Tian X., College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China","The teat count in a sow can serve as a key reproductive phenotype, thus offering valuable insights for selective breeding in a vital component of PSY (pigs weaned per sow per year). There is a positive correlation between the number of teats in pigs and their litter size. Particularly, the teat count in piglets was closely aligned with the average of their parents. Consequently, it is very necessary to select the piglets using teat count. The symmetry and shape of a sow's teats are two of the most important indicators of nursing ability. The better lactation performance was represented by the more orderly and regular arrangements. However, it is still challenging to capture clear videos of an adult sow's abdomen, due to the potential interference from stains. Automatic teat counting is also required for the labor-saving, higher efficiency, and higher accuracy, compared with the manual. In this study, a deep learning-based approach was proposed for the teat counting and evaluation using videos of piglets' abdomens. Among them, there was consistency in the number of teats from birth to adulthood of female piglets. Specifically, a camera was first installed on the piglet management platform, in order to capture the videos of the abdomen of piglets (2-7 days old). Then, these videos were screened using clarity. A sequence of images was preprocessed to facilitate the automatic teat counting via an enhanced Pignip-YOLOv5s object detection network. A sliding window majority voting mechanism was applied to the teat count sequence to acquire the final tally for high counting accuracy. Experimental results show that the improved Pignip-YOLOv5s achieved a mean average precision (mAP) of 0.97. Better performance was also obtained in the challenging conditions, such as tightly spaced teats at the piglet's abdominal end, complex body textures, and obscure vision from the umbilical cord and the shadow. The higher robustness was observed, compared with the original. There was an accuracy rate of 90.26% for teat counting in the dataset of 113 piglet abdomen videos. Some parameters were selected to quantify the piglet teat morphology, such as the number of paired teats and the distance between teats. A teat classification was also established for the left and right teats, according to the teat positions obtained from the Pignip-YOLOv5s object detection network. The image was divided into the quartile regions. The teat midpoints in each region were calculated to classify the left and right teats on the piglet belly. Additionally, a teat pairing algorithm was introduced to identify the teats in pairs, in order to calculate the pairing rates for the other data of teat morphology. A practical value was offered for the piglet teat counting and morphology assessment using images of the piglet abdomen. In summary, the target detection-based piglet teat counting and mammary evaluation can serve as a novel and effective way to extract breeding indicators in the livestock breeding industry. The finding can also provide high accuracy, speed, and efficiency in the realm of boar breeding. © 2024 Chinese Society of Agricultural Engineering. All rights reserved.","deep learning; mammary morphology assessment; models; teat counting; video count; YOLOv5s","Complex networks; Deep learning; Image enhancement; Mammals; Object detection; Textures; Deep learning; Detection networks; High-accuracy; Higher efficiency; Mammary morphology assessment; Objects detection; Performance; Teat counting; Video count; YOLOv5; Morphology","","","X. Tian; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, 510642, China; email: tianxuhong@scau.edu.cn","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85190293759"
"Bakhshayeshi I.; Erfani E.; Taghikhah F.R.; Elbourn S.; Beheshti A.; Asadnia M.","Bakhshayeshi, Ivan (57552040100); Erfani, Eila (57217221571); Taghikhah, Firouzeh Rosa (57203461736); Elbourn, Stephen (57539834100); Beheshti, Amin (50361038600); Asadnia, Mohsen (55631835900)","57552040100; 57217221571; 57203461736; 57539834100; 50361038600; 55631835900","An Intelligence Cattle Reidentification System over Transport by Siamese Neural Networks and YOLO","2024","IEEE Internet of Things Journal","11","2","","2351","2363","12","9","10.1109/JIOT.2023.3294944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165385518&doi=10.1109%2fJIOT.2023.3294944&partnerID=40&md5=f35c0caf74331e08b44b09ed588578c6","University of New South Wales, Graduate School of Biomedical Engineering, Sydney, 2052, NSW, Australia; University of New South Wales, Business School, Sydney, 2052, NSW, Australia; University of Sydney, Business School, Sydney, 2052, NSW, Australia; Macquarie University, School of Computing, Faculty of Science and Engineering, Sydney, 2109, NSW, Australia; Macquarie University, School of Engineering, Sydney, 2109, NSW, Australia","Bakhshayeshi I., University of New South Wales, Graduate School of Biomedical Engineering, Sydney, 2052, NSW, Australia; Erfani E., University of New South Wales, Business School, Sydney, 2052, NSW, Australia; Taghikhah F.R., University of Sydney, Business School, Sydney, 2052, NSW, Australia; Elbourn S., Macquarie University, School of Computing, Faculty of Science and Engineering, Sydney, 2109, NSW, Australia; Beheshti A., Macquarie University, School of Computing, Faculty of Science and Engineering, Sydney, 2109, NSW, Australia; Asadnia M., Macquarie University, School of Engineering, Sydney, 2109, NSW, Australia","Livestock, throughout their lifespan, are transported to multiple destinations before being processed into consumable goods. The assurance of authentic product delivery hinges on the presence of a reliable, intelligent identification system. However, extant livestock identification methodologies, primarily relying on radio frequency identification (RFID) ear tags, are vulnerable to loss, failure, and cases of misidentification or improper substitution. This article introduces an artificial intelligence (AI)-enabled system to rectify these issues by leveraging deep learning facial recognition for cattle reidentification. It utilizes an integrated approach combining the Only Look Once version 5 (YOLOv5) algorithm for cattle face detection and the Siamese neural network (SNN) for subsequent recognition. The system was rigorously tested on a prepared data set consisting of 2500 cattle face images, demonstrating an impressive accuracy of 95.13% when supplied with a single query image and a 20-image sample per cow from our data set. This system can be deployed across diverse environments, including farms, cargo areas, and sale yards, without necessitating model retraining. Furthermore, it can be fine-tuned to identify other farm animals, indicating its broad applicability.  © 2014 IEEE.","Deep learning; livestock identification system; livestock transport; Siamese neural network (SNN); YOLO","Animals; Deep learning; Face recognition; Feature extraction; Radio frequency identification (RFID); Search engines; Cow; Deep learning; Ear; Features extraction; Livestock identification system; Livestock transport; Neural-networks; Radiofrequencies; Radiofrequency identification; Siamese neural network; YOLO; Agriculture","","","M. Asadnia; Macquarie University, School of Engineering, Sydney, 2109, Australia; email: Mohsen.Asadnia@mq.edu.au","","Institute of Electrical and Electronics Engineers Inc.","23274662","","","","English","IEEE Internet Things J.","Article","Final","","Scopus","2-s2.0-85165385518"
"Sudharshan Duth P.; Shibu M.; Nithyashree M.","Sudharshan Duth, P. (56737215100); Shibu, Maneesha (59443215800); Nithyashree, M. (59442155000)","56737215100; 59443215800; 59442155000","Animal Detection in Night Light Videos Using Deep Learning","2024","Proceedings - ICNEWS 2024: 2nd International Conference on Networking, Embedded and Wireless Systems: Wireless Technology - Building a Digital World","","","","","","","0","10.1109/ICNEWS60873.2024.10730881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210453123&doi=10.1109%2fICNEWS60873.2024.10730881&partnerID=40&md5=6dacb1f5c6ed2bbdaf4312d60042e1d2","Amrita School of Computing Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India","Sudharshan Duth P., Amrita School of Computing Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India; Shibu M., Amrita School of Computing Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India; Nithyashree M., Amrita School of Computing Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India","Nighttime surveillance and monitoring of wildlife are critical for various applications, including conservation efforts, ecological research, and security purposes. Understanding the behavior of animals makes it difficult to predict the presence of animals on roadsides, potentially resulting in human-animal conflicts. This research work provides a novel approach for detecting animals in nighttime videos using deep learning algorithms. The main aim is to effectively detect and recognize the nocturnal presence of the animal by which surveillance at night and animal wellbeing can be improved by contributing to SDG 3, 11 and 12. In this scenario, a set of 10 animals were taken as classes, and the experimental was carried out for animal detection and recognition utilizing the YOLO v8 model which enhances the efficiency and accuracy of animal detection at night when compared to other models. Overcoming the challenges of low-light images, blur images and occlusions. Experimental results showcase superior performance compared to existing models exhibit real-time processing capabilities, making it suitable for practical deployment in nocturnal wildlife monitoring systems. The animal elephant is identified more accurately in this study, with a 87.49% detection accuracy. © 2024 IEEE.","Animal Detection; Animal Recognition; Night Vision Videos; Sustainability; Yolov8","Livestock; Animal detection; Animal recognition; Blur images; Image blur; Low-light images; Night lights; Night vision; Night vision video; Wellbeing; Yolov8; Invertebrates","","","P. Sudharshan Duth; Amrita School of Computing Amrita Vishwa Vidyapeetham, Department of Computer Science, Mysuru, India; email: p_sudharshanduth@my.amrita.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835036746-1","","","English","Proc. - ICNEWS : Int. Conf. Netw., Embed. Wirel. Syst.: Wirel. Technol. - Build. a Digit. World","Conference paper","Final","","Scopus","2-s2.0-85210453123"
"Fruhner M.; Tapken H.","Fruhner, Maik (57205443100); Tapken, Heiko (56410804900)","57205443100; 56410804900","Towards Multi-Species Animal Re-Identification","2024","Computer Science Research Notes","3401","","","137","145","8","0","10.24132/CSRN.3401.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211184987&doi=10.24132%2fCSRN.3401.15&partnerID=40&md5=c20017a8575d0310851debf51231d8fc","University of Applied Sciences Osnabrueck, Albrechtstrasse 30, Osnabrueck, 49076, Germany","Fruhner M., University of Applied Sciences Osnabrueck, Albrechtstrasse 30, Osnabrueck, 49076, Germany; Tapken H., University of Applied Sciences Osnabrueck, Albrechtstrasse 30, Osnabrueck, 49076, Germany","Animal Re-Identification (ReID) is a computer vision task that aims to retrieve a query individual from a gallery of known identities across different camera perspectives. It is closely related to the well-researched topic of Person ReID, but offers a much broader spectrum of features due to the large number of animal species. This raises research questions regarding domain generalization from persons to animals and across multiple animal species. In this paper, we present research on the adaptation of popular deep learning-based person ReID algorithms to the animal domain as well as their ability to generalize across species. We introduce two novel datasets for animal ReID. The first one contains images of 376 different wild common toads. The second dataset consists of various species of zoo animals. Subsequently, we optimize various ReID models on these datasets, as well as on 20 datasets published by others, with the objective of evaluating the performance of the models in a non-person domain. Our findings indicate that the domain generalization capabilities of OSNet AIN extend beyond the person ReID task, despite its comparatively small size. This enables us to investigate real-time animal ReID on live video data. © 2024 University of West Bohemia. All rights reserved.","animals; computer vision; deep learning; re-identification","Adversarial machine learning; Livestock; Animal species; Broad spectrum; Camera perspectives; Deep learning; Generalisation; Identification algorithms; Multi-species; Person re identifications; Re identifications; Research questions; Invertebrates","","","","Skala V.; University of West Bohemia, Univerzitni 8, Plzen","Vaclav Skala Union Agency","24644617","","","","English","Comp. Sci. Res. Notes","Conference paper","Final","","Scopus","2-s2.0-85211184987"
"Pandey D.R.; Mishra N.","Pandey, Dev Ras (58838999100); Mishra, Nidhi (59398532800)","58838999100; 59398532800","An Integrated Approach to Dairy Farming: AI and IoT-Enabled Monitoring of Cows and Crops via a Mobile Application","2024","BIO Web of Conferences","82","","05020","","","","0","10.1051/bioconf/20248205020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183907138&doi=10.1051%2fbioconf%2f20248205020&partnerID=40&md5=fc5d5702b303b306af6f90b2bfa83271","Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India","Pandey D.R., Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India; Mishra N., Faculty of CS & IT, Kalinga University, Chhattisgarh, Naya Raipur, India","The globalized and fiercely competitive nature of the international market has expanded the range of demands across all sectors of the agri-food business. The dairy business needs to adjust to the prevailing market conditions by enhancing resource efficiency, adopting environmentally sustainable practices, promoting transparency, and ensuring security. The Internet of Things (IoT), Edge Computing (EC), and deep learning play pivotal roles in facilitating these advancements as they enable the digitization of various components within the value chain. Solutions that depend on human observation via visual inspections are susceptible to delayed detection and potential human mistakes and need more scalability. The growing herd numbers raise a significant worry due to the potential negative impact on cow health and welfare, particularly about extended or undiscovered lameness. This condition has severe consequences for cows, eventually leading to a decline in milk output on the farm. To address this issue, an Integrated Approach to Dairy Farming (IA-DF) has been developed, which utilizes sophisticated Artificial Intelligence (AI) and data analytics methodologies using mobile applications to continuously monitor livestock and promptly detect instances of lameness in cattle. Initially, the VGG16 model, pre-trained on the ImageNet dataset, was used as the underlying architecture to extract the sequence of feature vectors associated with each video. This approach was adopted to circumvent the limitations of conventional feature engineering methods, which tend to be both time-consuming and labor-intensive with deep learning-based classification algorithms. IA-DF can extract semantic details from historical data in both forward and backward directions, hence enabling precise identification of fundamental behaviors shown by dairy cows. © The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0.","","","","","D.R. Pandey; Faculty of CS & IT, Kalinga University, Naya Raipur, Chhattisgarh, India; email: ku.devraspandey@kalingauniversity.ac.in","Trukhachev V.; Khavinson V.; Zhuravlev A.; Belopukhov S.; Migunov R.; Kukhar V.","EDP Sciences","22731709","","","","English","BIO. Web. Conf.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183907138"
"Shu H.; Wang K.; Guo L.; Bindelle J.; Wang W.","Shu, Hang (57222589919); Wang, Kaiwen (57813839300); Guo, Leifeng (56542160600); Bindelle, Jérôme (16240687200); Wang, Wensheng (56937276700)","57222589919; 57813839300; 56542160600; 16240687200; 56937276700","Automated collection of facial temperatures in dairy cows via improved UNet","2024","Computers and Electronics in Agriculture","220","","108614","","","","2","10.1016/j.compag.2024.108614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189929319&doi=10.1016%2fj.compag.2024.108614&partnerID=40&md5=29229b745598048a535eab44ce7b687c","Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Information Technology Group, Wageningen University and Research, PB Wageningen, 6708, Netherlands","Shu H., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Wang K., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, Information Technology Group, Wageningen University and Research, PB Wageningen, 6708, Netherlands; Guo L., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; Bindelle J., AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Wang W., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China","In cattle, facial temperatures captured by infrared thermography provide useful information from physiological aspects for researchers and local practitioners. Traditional temperature collection requires massive manual operations on relevant software. Therefore, this paper aimed to propose a tool for automated temperature collection from cattle facial landmarks (i.e., eyes, muzzle, nostrils, ears, and horns). An improved UNet was designed by replacing the traditional convolutional layers in the decoder with Ghost modules and adding Efficient Channel Attention (ECA) modules. The improved model was trained on our open-source cattle infrared image dataset. The results show that Ghost modules reduced computational complexity and ECA modules further improved segmentation performance. The improved UNet outperformed other comparable models on the testing set, with the highest mean Intersection of Union of 80.76% and a slightly slower but still good inference speed of 32.7 frames per second. Further agreement analysis reveals small to negligible differences between the temperatures obtained automatically in the areas of eyes and ears and the ground truth. Collectively, this study demonstrates the capacity of the proposed method for automated facial temperature collection in cattle infrared images. Further modelling and correction with data collected in more complex conditions are required before it can be integrated into on-farm monitoring of animal health and welfare. © 2024 Elsevier B.V.","Animal welfare; Body surface temperature; Deep learning; Heat stress; Precision livestock farming","Animals; Automation; Deep learning; Farms; Image enhancement; Infrared imaging; Animal welfare; Automated collection; Body surface; Body surface temperature; Dairy cow; Deep learning; Efficient channels; Heat stress; Precision livestock farming; Surface temperatures; animal welfare; artificial neural network; body temperature; cattle; dairy farming; health monitoring; heat shock; machine learning; segmentation; Surface temperature","China Scholarship Council, CSC, (202203250080); China Scholarship Council, CSC; Hebei Provincial Key Research Projects, (22326609D); Hebei Provincial Key Research Projects; Chinese Academy of Agricultural Sciences, CAAS, (CAAS-ASTIP-2016-AII); Chinese Academy of Agricultural Sciences, CAAS","The first author (H. Shu) was funded by a PhD grant from China Scholarship Council [202203250080]. This work was supported by the Key Research and Development Program of Hebei Province [22326609D] and the Science and Technology Innovation Project of Chinese Academy of Agricultural Sciences [CAAS-ASTIP-2016-AII]. The authors are grateful to Yinxiang dairy farm for their assistance in data collection.","H. Shu; Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; email: shuhang@caas.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85189929319"
"Shwetha V.; Maddodi B.S.; Laxmi V.; Kumar A.; Shrivastava S.","Shwetha, V. (57208086549); Maddodi, B.S. (57210304986); Laxmi, Vijaya (59517755600); Kumar, Abhinav (59517852000); Shrivastava, Sakshi (58001669200)","57208086549; 57210304986; 59517755600; 59517852000; 58001669200","Latest Trend and Challenges in Machine Learning– and Deep Learning–Based Computational Techniques in Poultry Health and Disease Management: A Review","2024","Journal of Computer Networks and Communications","2024","","8674250","","","","0","10.1155/2024/8674250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215289341&doi=10.1155%2f2024%2f8674250&partnerID=40&md5=9c4f3fe78a46a7590b3b740446dee6e8","Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; Department of Civil Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India","Shwetha V., Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; Maddodi B.S., Department of Civil Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; Laxmi V., Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; Kumar A., Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; Shrivastava S., Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India","To determine the fock’s economic worth, free-range chicken growers must determine the gender, bird movement, behavior, disease detection, and lameness of the chickens. However, because of the complex environmental background and the fuctuating chicken population, it is difcult for farmers to efectively and properly measure those characteristics. Manual estimation is also inaccurate and time-consuming because probable identifcation occurs in their life cycle. Terefore, the industry benefts from automated systems that can produce fndings quickly and precisely in managing health and diseases. Te advancement of machine learning (ML)– and deep learning (DL)–based algorithms are boons for poultry health and disease management. Tis study reviews the literature using ML and DL techniques in prediction, classifcation, and disease detection in various metrics, namely, poultry health and disease management. We have considered the research article published from 2010 to 2023 in this study, which uses ML-and DL-based computation techniques in poultry welfare metrics such as gender identifcation, tracking of poultry, analysis of broiler chicken behavior, detection of poultry diseases, lameness and broiler weight, and stress monitoring. In addition, this review explores the most recent developments, difculties, strategies, and databases used in image preprocessing feature extraction and classifcation. Te review addresses these challenges and discusses the approaches and techniques researchers employ to tackle them in the feld of poultry management and disease detection. © 2024 Shwetha V. et al.","","Adversarial machine learning; Birds; Deep learning; Diseases; Automated systems; Bird movements; Classifcation; Computational technique; Disease detection; Disease management; Health management; Learning-based algorithms; Machine-learning; Movement behaviour; Livestock","","","V. Laxmi; Department of Electrical and Electronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, 576104, India; email: vijaya.laxmi@manipal.edu","","John Wiley and Sons Ltd","20907141","","","","English","J. Comput. Netw. Commun.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85215289341"
"Shankar P.; Thakur A.; Ansari H.; Bilal M.; Chaugule A.","Shankar, Prema (59538456300); Thakur, Ayush (57976628700); Ansari, Hamza (59538016400); Bilal, Mohammed (59537737100); Chaugule, Archana (56131197400)","59538456300; 57976628700; 59538016400; 59537737100; 56131197400","IoT and Machine Learning in Agriculture: A Comparative Review of Smart Farming Solutions","2024","Proceedings of the 5th International Conference on Data Intelligence and Cognitive Informatics, ICDICI 2024","","","","306","310","4","0","10.1109/ICDICI62993.2024.10810798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216676526&doi=10.1109%2fICDICI62993.2024.10810798&partnerID=40&md5=4eb943fedcfe250fef80b92bb61b966e","Sakec, Mumbai University, Department of Information Technology, Mumbai, India","Shankar P., Sakec, Mumbai University, Department of Information Technology, Mumbai, India; Thakur A., Sakec, Mumbai University, Department of Information Technology, Mumbai, India; Ansari H., Sakec, Mumbai University, Department of Information Technology, Mumbai, India; Bilal M., Sakec, Mumbai University, Department of Information Technology, Mumbai, India; Chaugule A., Sakec, Mumbai University, Department of Information Technology, Mumbai, India","Smart agriculture utilizes IoT and ML technologies to revolutionize traditional farming practices. IoT sensors collect real-time environmental data, which ML algorithms analyze for tasks like soil classification, crop yield prediction, and disease detection. Various ML techniques, including deep learning models, are employed to improve accuracy and efficiency of the system. Applications range from automated livestock monitoring to efficient water management. Hybrid models combining different ML approaches often achieve superior results. Smart agriculture shows great potential for enhancing crop yields, reducing waste, and promoting sustainable practices to address global food security concerns. © 2024 IEEE.","Agriculture; CNN; Image Processing; Internet of Things (IoT); Smart agriculture; Smart Farming","Algorithm analysis; Crop yield; Environmental data; Farming practices; Images processing; Internet of thing; Machine-learning; Real- time; Smart agricultures; Smart farming; Smart agriculture","","","P. Shankar; Sakec, Mumbai University, Department of Information Technology, Mumbai, India; email: prema16679@sakec.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835038960-9","","","English","Proc. Int. Conf. Data Intell. Cogn. Informatics, ICDICI","Conference paper","Final","","Scopus","2-s2.0-85216676526"
"Temenos A.; Voulodimos A.; Korelidou V.; Gelasakis A.; Kalogeras D.; Doulamis A.; Doulamis N.","Temenos, Anastasios (57225155462); Voulodimos, Athanasios (25032274400); Korelidou, Vera (58776347500); Gelasakis, Athanasios (36026613600); Kalogeras, Dimitrios (6603100945); Doulamis, Anastasios (35565008000); Doulamis, Nikolaos (7003749049)","57225155462; 25032274400; 58776347500; 36026613600; 6603100945; 35565008000; 7003749049","Goat-CNN: A lightweight convolutional neural network for pose-independent body condition score estimation in goats","2024","Journal of Agriculture and Food Research","16","","101174","","","","1","10.1016/j.jafr.2024.101174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192778629&doi=10.1016%2fj.jafr.2024.101174&partnerID=40&md5=3ed27183d3f1a7ffca9e17279c7fb205","School of Rural, Surveying and Geoinformatics Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece; Department of Animal Science, School of Animal Biosciences, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Institute of Communication and Computer Systems, National Technical University of Athens, Athens, 15780, Greece","Temenos A., School of Rural, Surveying and Geoinformatics Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece; Voulodimos A., School of Electrical and Computer Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece; Korelidou V., Department of Animal Science, School of Animal Biosciences, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Gelasakis A., Department of Animal Science, School of Animal Biosciences, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Kalogeras D., Institute of Communication and Computer Systems, National Technical University of Athens, Athens, 15780, Greece; Doulamis A., School of Rural, Surveying and Geoinformatics Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece; Doulamis N., School of Rural, Surveying and Geoinformatics Engineering, National Technical University of Athens, 9 Heroon Polytechneiou Str, Athens, 15773, Greece","Modern livestock farming systems face the challenge of meeting the growing demand for dairy and meat products while ensuring the well-being of animals. Body Condition Scoring serves as a vital process for assessing the body reserves in animals, impacting their health, welfare, and productivity. However, traditional body condition score (BCS) evaluation methods via observation and palpation of specific anatomical regions are labor-intensive and subjective, hindering their widespread adoption. To address this issue, Precision Livestock Farming (PLF) techniques, particularly those involving Internet of Things (IoT) devices and artificial intelligence (AI), have emerged as promising solutions. In this work, we explore the use of AI, specifically Convolutional Neural Networks (CNNs), to automate the assessment of BCS in goats utilizing imagery data. Our model was trained on 5000 images illustrating the dorsal view of the backside of goats achieving an overall accuracy of 97.94 % which was the highest compared to other popular deep learning architectures from literature (e.g. VGG16, ResNet34, ResNet50, DenseNet, GoogleNet). The proposed custom CNN model for goat-specific BCS estimation overcomes the limitations of manual sketching, providing automatic region identification for BCS assessment. Moreover, it is a lightweight model specifically designed for seamless integration with IoT devices, allowing for efficient on-board processing via cameras. The model's pose-independent nature and adaptability to environmental constraints make it a valuable tool for efficient and sustainable goat farming. This research advances the application of AI as a precision livestock farming tool, contributing to the reinforcement of the animal welfare and productivity, and supporting evidence-based decision-making processes to increase farms' resilience. © 2024 The Authors","Animal; Artificial intelligence; Body condition score; Computer vision; Convolutional neural network; Goat; Precision livestock farming; Signal processing","","HORIZON EUROPE Framework Programme; Horizon 2020 European Union, (101000216)","Funding text 1: Anastasios Temenos reports financial support was provided by Horizon Europe. This work is financially supported by the Horizon 2020 European Union project Code Re-farm \u201CConsumer-driven demands to reframe farming systems\u201D, funded under the call H2020-FNR-2020, with grant agreement No. 101000216. If there are other authors, they declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.This paper is financially supported by the Horizon 2020 European Union project Code Re-farm \u201CConsumer-driven demands to reframe farming systems\u201D, funded under the call H2020-FNR-2020, with grant agreement No. 101000216.; Funding text 2: This paper is financially supported by the Horizon 2020 European Union project Code Re-farm \u201CConsumer-driven demands to reframe farming systems\u201D, funded under the call H2020-FNR-2020, with grant agreement No. 101000216.","A. Temenos; School of Rural, Surveying and Geoinformatics Engineering, National Technical University of Athens, Athens, 9 Heroon Polytechneiou Str, 15773, Greece; email: tasostemenos@mail.ntua.gr","","Elsevier B.V.","26661543","","","","English","J. Agric. Food. Res.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192778629"
"Achour A.A.; Mourhir A.","Achour, Ayman Ait (59470809300); Mourhir, Asmaa (55204774200)","59470809300; 55204774200","Automated Recognition of Cattle's Breed Through Computer Vision: A Case Study on the Oulmes-Zaer Breed","2024","6th International Conference on Intelligent Computing in Data Sciences, ICDS 2024","","","","","","","0","10.1109/ICDS62089.2024.10756442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211930267&doi=10.1109%2fICDS62089.2024.10756442&partnerID=40&md5=60c07b70859598cdd8ddee8a0beb6d53","School of Science and Engineering Al Akhawayn University in Ifrane, Ifrane, Morocco","Achour A.A., School of Science and Engineering Al Akhawayn University in Ifrane, Ifrane, Morocco; Mourhir A., School of Science and Engineering Al Akhawayn University in Ifrane, Ifrane, Morocco","Recognizing different cattle breeds accurately is important for farmers to prevent accidental crossbreeding and to maintain the purity of the breeds. The goal of this research is to simplify cattle breed classification through computer vision, with an emphasis on models that are suitable for mobile devices. We collected and curated a dataset consisting of 3,048 images, including images of the Oulmes-Zaer cattle breed, and relied on transfer learning to fine-Tune various deep learning models on our dataset. The results show that the best-performing model is ResNet50, achieving an accuracy of 99.34% on the test set with a precision of 100%, recall of 97.41%, and F1-score of 98.70%. The quantized ResNet50 model requires 23.12 MB of ROM and 329.61 MB of RAM and has a latency of 108.33 ms, making it suitable for mobile and edge deployment. © 2024 IEEE.","cattle breed classification; computer vision; DenseNet; EfficientNet; MobileNet; ResNet","Automated recognition; Case-studies; Cattle breed classification; Cattles; Densenet; Efficientnet; Learning models; Mobilenet; Test sets; Transfer learning; Livestock","","","","Oubenaalla Y.; Nfaoui E.H.; Boumhidi J.; Loqman C.; Alippi C.","Institute of Electrical and Electronics Engineers Inc.","","979-835035120-0","","","English","Int. Conf. Intell. Comput. Data Sci., ICDS","Conference paper","Final","","Scopus","2-s2.0-85211930267"
"Zhang H.; Ma Y.; Shi Y.; Qiao Y.; Wang M.","Zhang, Haotian (57226862519); Ma, Yuan (57217164992); Shi, Yinghan (58087459100); Qiao, Yongliang (56486770900); Wang, Meili (55694491200)","57226862519; 57217164992; 58087459100; 56486770900; 55694491200","A framework for automatic analysis of sheep ruminating behavior based on keypoint detection","2024","M2VIP - Proceedings of the International Conference on Mechatronics and Machine Vision in Practice","","2024","","","","","0","10.1109/M2VIP62491.2024.10746109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214410338&doi=10.1109%2fM2VIP62491.2024.10746109&partnerID=40&md5=5a69e3670ed4ac19d5563b7ac8f5fc37","College of Information Engineering, Northwest A&f University, Yangling, 712100, China; Ministry of Agriculture, Key Laboratory of Agricultural Internet of Things, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; The University of Adelaide, Australian Institute for Machine Learning (AIML), 5005, Australia","Zhang H., College of Information Engineering, Northwest A&f University, Yangling, 712100, China, Ministry of Agriculture, Key Laboratory of Agricultural Internet of Things, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Ma Y., College of Information Engineering, Northwest A&f University, Yangling, 712100, China, Ministry of Agriculture, Key Laboratory of Agricultural Internet of Things, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Shi Y., College of Information Engineering, Northwest A&f University, Yangling, 712100, China, Ministry of Agriculture, Key Laboratory of Agricultural Internet of Things, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Qiao Y., The University of Adelaide, Australian Institute for Machine Learning (AIML), 5005, Australia; Wang M., College of Information Engineering, Northwest A&f University, Yangling, 712100, China, Ministry of Agriculture, Key Laboratory of Agricultural Internet of Things, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Ruminating behaviour is a critical indicator of sheep health and welfare, with abnormal rumination strongly associated with gastric ailments in sheep. Timely monitoring and analysing ruminating behaviour are imperative for assessing sheep health status and optimizing feed composition. However, deploying existing ruminant monitoring algorithms on sheep farms presents challenges due to the hypermobility of the sheep's head and the complexity of farm environments. This study proposes a real-time analysis framework based on the RTMPose keypoint detection method for sheep rumination behaviour. The framework employs keypoint detection to precisely analyze various aspects of sheep rumination, including the number of ruminating chews, ruminating time, chewing rate, ruminating rate, and ruminating time rate. Additionally, we constructed a dataset for sheep rumination behaviour to train the framework's object detection and keypoint detection algorithms. Experiments were conducted using fifteen challenging videos.Results demonstrate that the ruminating behaviour analysis framework based on key points achieves acceptable with 94.56% accuracy in the number of ruminating chews, 97.49% accuracy in ruminating time, and 97.99% accuracy in chewing rate, with 72.9M parameters. The proposed method can be flexibly deployed on handheld monitoring devices, presenting a viable approach for automatic identification and analysis of rumination behaviour using computer vision technology.  © 2024 IEEE.","Deep learning; Keypoint detection; Precision livestock.R; Precision livestock.uminating behaviour; Sheep welfare; uminating behaviour","Analysis frameworks; Automatic analysis; Deep learning; Keypoint detection; Precision livestock.; R; Sheep welfare; Uminating behavior; Deep learning","","","M. Wang; College of Information Engineering, Northwest A&f University, Yangling, 712100, China; email: wml@nwsuaf.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","29964156","979-835039191-6","","","English","M2VIP - Proc. Int. Conf. Mechatronics Mach. Vis. Pract.","Conference paper","Final","","Scopus","2-s2.0-85214410338"
"Choudhury S.B.; Kulat R.; Patil R.; Kumar A.; Sarangi S.; Rajpoot N.; Mittal A.; Singh D.; Pappula S.","Choudhury, Swagatam Bose (57206668565); Kulat, Rushikesh (57899671100); Patil, Ruturaj (58710008400); Kumar, Abhishek (58634437600); Sarangi, Sanat (55633353700); Rajpoot, Nandan (58736418700); Mittal, Ajay (57196602519); Singh, Dineshkumar (56046410400); Pappula, Srinivasu (6505567619)","57206668565; 57899671100; 58710008400; 58634437600; 55633353700; 58736418700; 57196602519; 56046410400; 6505567619","On Digital Twin for a High Yielding Soybean Variety Towards Optimal Field Recommendations","2024","2024 IEEE Global Humanitarian Technology Conference, GHTC 2024","","","","270","277","7","0","10.1109/GHTC62424.2024.10771522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215101492&doi=10.1109%2fGHTC62424.2024.10771522&partnerID=40&md5=67d95cbe8616e6941caf6afcedd018f3","TCS Research and Innovation, Mumbai, India","Choudhury S.B., TCS Research and Innovation, Mumbai, India; Kulat R., TCS Research and Innovation, Mumbai, India; Patil R., TCS Research and Innovation, Mumbai, India; Kumar A., TCS Research and Innovation, Mumbai, India; Sarangi S., TCS Research and Innovation, Mumbai, India; Rajpoot N., TCS Research and Innovation, Mumbai, India; Mittal A., TCS Research and Innovation, Mumbai, India; Singh D., TCS Research and Innovation, Mumbai, India; Pappula S., TCS Research and Innovation, Mumbai, India","Soybean in India is pivotal for its nutritional value and as livestock feed. Its cultivation faces challenges like pests, diseases, and adverse weather, impacting yields and farmer incomes. Sustainable soybean production can enhance farmer livelihoods and ensure long-term agricultural viability. Creating a digital twin of the crop which models how it matures under specific conditions and operations can be an effective way to understand the yield outputs and environmental impact. For a high-yielding soybean variety KDS 992 (Phule Durva), we present a digital twin based model for monitoring sustainability practices and crop health, validated with a field study over a Kharif (monsoon) season in India. Results showed that measured yield for the selected variety aligned well with the model's predicted yield. With organic practices, soil health improved with a net increase of 1.50 tCO2 e/ha in soil organic carbon (SOC). As a contrast, the model projected an SOC loss of 5.88 tCO2 e/ha with conventional practices. AI-driven models integrated with the framework detected and contained soybean stress timely, boosting crop health. We believe the integrated digital insights would assist in giving specific recommendations to farmers planting soybean in general and the variety in particular. © 2024 IEEE.","Carbon Sequestration; Deep Learning; Digital Twin; Emissions; Precision Agriculture; Sustainability","Adverse weather; Carbon sequestration; Deep learning; Emission; Farmer's incomes; Livestock feed; Nutritional value; Precision Agriculture; Soil organic carbon; Soybean production","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037734-7","","","English","IEEE Glob. Humanit. Technol. Conf., GHTC","Conference paper","Final","","Scopus","2-s2.0-85215101492"
"Xu Z.; Wang T.; Skidmore A.K.; Lamprey R.","Xu, Zeyu (58800052000); Wang, Tiejun (55709751800); Skidmore, Andrew K. (7006518879); Lamprey, Richard (8744656800)","58800052000; 55709751800; 7006518879; 8744656800","A review of deep learning techniques for detecting animals in aerial and satellite images","2024","International Journal of Applied Earth Observation and Geoinformation","128","","103732","","","","10","10.1016/j.jag.2024.103732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187647580&doi=10.1016%2fj.jag.2024.103732&partnerID=40&md5=a68965650e001f4b210405808de632fe","Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, P.O. Box 217, AE Enschede, 7500, Netherlands","Xu Z., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, P.O. Box 217, AE Enschede, 7500, Netherlands; Wang T., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, P.O. Box 217, AE Enschede, 7500, Netherlands; Skidmore A.K., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, P.O. Box 217, AE Enschede, 7500, Netherlands; Lamprey R., Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, P.O. Box 217, AE Enschede, 7500, Netherlands","Deep learning is an effective machine learning method that in recent years has been successfully applied to detect and monitor species population in remotely sensed data. This study aims to provide a systematic literature review of current applications of deep learning methods for animal detection in aerial and satellite images. We categorized methods in collated publications into image level, point level, bounding-box level, instance segmentation level, and specific information level. The statistical results show that YOLO, Faster R-CNN, U-Net and ResNet are the most used neural network structures. The main challenges associated with the use of these deep learning methods are imbalanced datasets, small samples, small objects, image annotation methods, image background, animal counting, model accuracy assessment, and uncertainty estimation. We explored possible solutions include the selection of sample annotation methods, optimizing positive or negative samples, using weakly and self-supervised learning methods, selecting or developing more suitable network structures. Future research trends we identified are video-based detection, very high-resolution satellite image-based detection, multiple species detection, new annotation methods, and the development of specialized network structures and large foundation models. We discussed existing research attempts as well as personal perspectives on these possible solutions and future trends. © 2024 The Author(s)","Artificial intelligence; Biodiversity; Livestock; Object detection; Remote sensing; Wildlife","aerial photograph; biodiversity; detection method; image resolution; livestock; machine learning; satellite imagery","China Scholarship Council, China; China Scholarship Council, CSC, (202104910129); China Scholarship Council, CSC","Funding text 1: This work was supported by the China Scholarship Council, China (grant number 202104910129).; Funding text 2: This work was supported by the China Scholarship Council under Grant 202104910129 and co-funded by the ITC Research Fund. ","Z. Xu; Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, AE Enschede, P.O. Box 217, 7500, Netherlands; email: z.xu-1@utwente.nl","","Elsevier B.V.","15698432","","","","English","Int. J. Appl. Earth Obs. Geoinformation","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85187647580"
"Basavegowda D.H.; Höhne M.M.-C.; Weltzien C.","Basavegowda, Deepak H. (57202612016); Höhne, Marina M.-C. (57219765782); Weltzien, Cornelia (57189026254)","57202612016; 57219765782; 57189026254","Deep Learning-based UAV-assisted grassland monitoring to facilitate Eco-scheme 5 realization","2024","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-344","","","197","202","5","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216815256&partnerID=40&md5=7fbaae8329b62562771a0bdd37e7e521","Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Agromechatronik, Data Science in der Bioökonomie, Max-Eyth-Allee 100, Potsdam, 14469, Germany; Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany","Basavegowda D.H., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Agromechatronik, Data Science in der Bioökonomie, Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany; Höhne M.M.-C., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Agromechatronik, Data Science in der Bioökonomie, Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany; Weltzien C., Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB), Agromechatronik, Data Science in der Bioökonomie, Max-Eyth-Allee 100, Potsdam, 14469, Germany, Technische Universität Berlin, Straße des 17. Juni 135, Berlin, 10623, Germany","Eco-scheme 5 has been introduced to promote biodiversity in permanent grasslands through sustainable land management. While this scheme motivates farmers through result-based remuneration, it also entails a significant monitoring cost in terms of time and money to identify indicators manually. To overcome this burden and facilitate the realization of Eco-scheme 5, we developed an object detection model based on Deep Learning (DL) to automate the indicator species identification. First, we trained and evaluated the model on high-resolution Unmanned Aerial Vehicle (UAV) data. The model achieved an Average Precision (AP) rate of 80.8 AP50, but limited training data and the class imbalance problem among indicators affected the model performance. To address these problems, we enriched training data with proximal images of indicators, resulting in a performance gain from 80.8 AP50 to 95.3 AP50. Our results demonstrate the potential of DL and UAV applications in assisting result-based agri-environmental schemes (AES) such as Eco-scheme 5. © 2024 Gesellschaft fur Informatik (GI). All rights reserved.","biodiversity monitoring; cross-domain knowledge transfer; object detection; result-based AES","Abiotic; Direct air capture; Livestock; Aerial vehicle; Biodiversity monitoring; Cross-domain; Cross-domain knowledge transfer; Domain knowledge; Knowledge transfer; Objects detection; Permanent grassland; Result-based agri-environmental scheme; Sustainable land managements; Environmental monitoring","","","","Hoffmann C.; Stein A.; Gallmann E.; Dorr J.; Krupitzer C.; Floto H.","Gesellschaft fur Informatik (GI)","16175468","978-388579738-8","","","English","Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)","Conference paper","Final","","Scopus","2-s2.0-85216815256"
"Ma W.; Sun Y.; Qi X.; Xue X.; Chang K.; Xu Z.; Li M.; Wang R.; Meng R.; Li Q.","Ma, Weihong (55524058400); Sun, Yi (58221797300); Qi, Xiangyu (58929837200); Xue, Xianglong (57891757700); Chang, Kaixuan (58933912200); Xu, Zhankang (58928795300); Li, Mingyu (58899450000); Wang, Rong (57221059245); Meng, Rui (58899449900); Li, Qifeng (57205964175)","55524058400; 58221797300; 58929837200; 57891757700; 58933912200; 58928795300; 58899450000; 57221059245; 58899449900; 57205964175","Computer-Vision-Based Sensing Technologies for Livestock Body Dimension Measurement: A Survey","2024","Sensors","24","5","1504","","","","3","10.3390/s24051504","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187529074&doi=10.3390%2fs24051504&partnerID=40&md5=681299e40affacb65acb26153e5d2e23","Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; College of Information Engineering, Northwest A&F University, Xianyang, 712199, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Ma W., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Sun Y., College of Information Engineering, Northwest A&F University, Xianyang, 712199, China; Qi X., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Xue X., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Chang K., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Xu Z., College of Information Engineering, Northwest A&F University, Xianyang, 712199, China; Li M., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Wang R., College of Information Engineering, Northwest A&F University, Xianyang, 712199, China; Meng R., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Li Q., Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China","Livestock’s live body dimensions are a pivotal indicator of economic output. Manual measurement is labor-intensive and time-consuming, often eliciting stress responses in the livestock. With the advancement of computer technology, the techniques for livestock live body dimension measurement have progressed rapidly, yielding significant research achievements. This paper presents a comprehensive review of the recent advancements in livestock live body dimension measurement, emphasizing the crucial role of computer-vision-based sensors. The discussion covers three main aspects: sensing data acquisition, sensing data processing, and sensing data analysis. The common techniques and measurement procedures in, and the current research status of, live body dimension measurement are introduced, along with a comparative analysis of their respective merits and drawbacks. Livestock data acquisition is the initial phase of live body dimension measurement, where sensors are employed as data collection equipment to obtain information conducive to precise measurements. Subsequently, the acquired data undergo processing, leveraging techniques such as 3D vision technology, computer graphics, image processing, and deep learning to calculate the measurements accurately. Lastly, this paper addresses the existing challenges within the domain of livestock live body dimension measurement in the livestock industry, highlighting the potential contributions of computer-vision-based sensors. Moreover, it predicts the potential development trends in the realm of high-throughput live body dimension measurement techniques for livestock. © 2024 by the authors.","3D point cloud; computer vision sensing; image processing; live body dimension measurement","Agriculture; Data acquisition; Data handling; Deep learning; Three dimensional computer graphics; 3D point cloud; Body dimensions; Computer vision sensing; Computer-vision-based sensing; Dimension measurements; Images processing; Live body dimension measurement; Sensing data; Vision sensing; Vision-based sensors; Computer vision","Ministry of Science and Technology of the People's Republic of China, MOST, (2021YFD1200900); Ministry of Science and Technology of the People's Republic of China, MOST; Beijing Agricultural and Rural Bureau, (QNJJ202309, BAIC10-2023); Beijing Academy of Agricultural and Forestry Sciences, BAAFS, (JKZX202214); Beijing Academy of Agricultural and Forestry Sciences, BAAFS; Science and Technology Department of Sichuan Province, SPDST, (2021ZDZX0011); Science and Technology Department of Sichuan Province, SPDST","Beijing Academy of Agriculture and Forestry Sciences: JKZX202214; Sichuan Provincial Department of Science and Technology: 2021ZDZX0011; Beijing Agricultural and Rural Bureau: BAIC10-2023; Beijing Academy of Agriculture and Forestry Sciences: QNJJ202309; Ministry of Science and Technology of the People’s Republic of China: 2021YFD1200900.","Q. Li; Information Technology Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; email: nercita1017@163.com","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","","English","Sensors","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85187529074"
"Kilic K.D.; Gökhan A.; Çavuşoğlu T.","Kilic, Kubilay Dogan (57195671816); Gökhan, Aylin (57219907255); Çavuşoğlu, Türker (55318368500)","57195671816; 57219907255; 55318368500","The transformative role of artificial intelligence in advancing bovine reproductive biology","2024","Future of AI in Biomedicine and Biotechnology","","","","64","83","19","0","10.4018/979-8-3693-3629-8.ch004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196588635&doi=10.4018%2f979-8-3693-3629-8.ch004&partnerID=40&md5=f6cc7060561cdc1770d07db9da5bd432","Faculty of Medicine, Ege University, Turkey; Izmir Bakircay University, Turkey","Kilic K.D., Faculty of Medicine, Ege University, Turkey; Gökhan A., Faculty of Medicine, Ege University, Turkey; Çavuşoğlu T., Izmir Bakircay University, Turkey","The integration of deep learning technologies into bovine reproductive biology heralds a significant paradigm shift that improves our approach to cattle breeding and reproductive health management. This chapter examines the versatile applications of deep learning, including image analysis, genomic information, and behavioral predictions, to advance the understanding and optimization of cattle reproduction. Adoption of these technologies facilitates a more detailed understanding of the genetic and physiological determinants of fertility and disease, contributing to the development of targeted breeding programs and improved herd health strategies. Despite the promise of deep learning to revolutionize greater efficiency and sustainability in livestock production, challenges around data privacy, security, and model interpretability remain. These issues require a concerted effort to develop ethical frameworks and transparent algorithms to ensure the responsible deployment of deep learning tools. This review highlights the transformative potential of deep learning in bovine reproductive biology and advocates for continued interdisciplinary collaboration to address the complexities of applying advanced computational techniques in agriculture. From this perspective, the future of livestock production is envisioned as a place where technological innovations and animal welfare converge, marking a new era in precision agriculture. © 2024, IGI Global. All rights reserved.","","","","","","","IGI Global","","979-836933630-4; 979-836933629-8","","","English","Future of AI in biomed. and biotechnol.","Book chapter","Final","","Scopus","2-s2.0-85196588635"
"Bretas I.L.; Dubeux J.C.B., Jr.; Zhao C.; Queiroz L.M.D.; Flynn S.; Ingram S.; Oduor K.T.; Cruz P.J.R.; Ruiz-Moreno M.; Loures D.R.S.; Valente D.S.M.; Chizzotti F.H.M.","Bretas, Igor L. (57211271797); Dubeux, Jose C. B. (6506391463); Zhao, Chang (59441859100); Queiroz, Luana M. D. (57209772484); Flynn, Scott (57224136931); Ingram, Sam (58691568200); Oduor, Kenneth T. (57911193700); Cruz, Priscila J. R. (58286021500); Ruiz-Moreno, Martin (24768096200); Loures, Daniele R. S. (12041826700); Valente, Domingos S. M. (55081047300); Chizzotti, Fernanda H. M. (18233302100)","57211271797; 6506391463; 59441859100; 57209772484; 57224136931; 58691568200; 57911193700; 58286021500; 24768096200; 12041826700; 55081047300; 18233302100","Detection and mapping of Amaranthus spinosus L. in bermudagrass pastures using drone imagery and deep learning for a site-specific weed management","2024","Agronomy Journal","116","3","","990","1002","12","1","10.1002/agj2.21545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185453918&doi=10.1002%2fagj2.21545&partnerID=40&md5=8e16544a55acc3f29e8c68a6470e707c","North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Agronomy Department, University of Florida, Gainesville, FL, United States; Corteva Agriscience, Lee's Summit, MO, United States; Range Cattle Research and Education Center, University of Florida, Ona, FL, United States; Universidade Federal do Recôncavo da Bahia, Bahia, Cruz das Almas, Brazil; Department of Agricultural Engineering, Universidade Federal de Viçosa, Minas Gerais, Viçosa, Brazil; Department of Animal Science, Universidade Federal de Viçosa, Minas Gerais, Viçosa, Brazil","Bretas I.L., North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Dubeux J.C.B., Jr., North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Zhao C., Agronomy Department, University of Florida, Gainesville, FL, United States; Queiroz L.M.D., North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Flynn S., Corteva Agriscience, Lee's Summit, MO, United States; Ingram S., Corteva Agriscience, Lee's Summit, MO, United States; Oduor K.T., North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Cruz P.J.R., Range Cattle Research and Education Center, University of Florida, Ona, FL, United States; Ruiz-Moreno M., North Florida Research and Education Center, University of Florida, Marianna, FL, United States; Loures D.R.S., Universidade Federal do Recôncavo da Bahia, Bahia, Cruz das Almas, Brazil; Valente D.S.M., Department of Agricultural Engineering, Universidade Federal de Viçosa, Minas Gerais, Viçosa, Brazil; Chizzotti F.H.M., Department of Animal Science, Universidade Federal de Viçosa, Minas Gerais, Viçosa, Brazil","Weed encroachment negatively affects pasture productivity by reducing herbage allowance, stocking rates, and livestock performance. Amaranthus spinosus L. is a weed species widely found in pastures worldwide and is considered challenging for ranchers due to its great potential for invasion, making it difficult to control. The high costs of chemical application and the global concern about environmental impacts restrict indiscriminate herbicide spraying in pastures. Site-specific weed management (SSWM) is a weed management strategy based on weed spot-spraying that has the potential to overcome these issues. Images from unmanned aerial vehicles (UAVs) can provide valuable information for weed mapping to drive the herbicide application in pastures. Deep learning techniques have been highlighted in image classification tasks. We developed a deep convolutional neural network (CNN)-based image segmentation model based on the U-Net architecture to detect and map Amaranthus spinosus in bermudagrass pastures using red–green–blue images acquired through UAV flying in moderate-high altitude. The images were acquired from twelve paddocks under three treatments (weed-free, weed-strips, or weed-infested) during the summer (2021-2022). The CNN model was able to detect around 80% of the A. spinosus with an average prediction accuracy of 94%. Our weed mapping showed the potential of using the U-Net model to generate a herbicide application map to be inserted into the sprayer system, reducing up to 76% of the amount of herbicide applied. Further studies are encouraged to increase the robustness of the model across species and development stages and develop sprayer systems to implement the spot-spraying in field conditions. © 2024 The Authors. Agronomy Journal © 2024 American Society of Agronomy.","","","","","I.L. Bretas; North Florida Research and Education Center, University of Florida, Marianna, United States; email: igorlbretas@gmail.com","","John Wiley and Sons Inc","00021962","","AGJOA","","English","Agron. J.","Article","Final","","Scopus","2-s2.0-85185453918"
"Mattina M.; Benzinou A.; Nasreddine K.; Richard F.","Mattina, Morann (57984394600); Benzinou, Abdesslam (25928231000); Nasreddine, Kamal (26434359300); Richard, Francis (57985112000)","57984394600; 25928231000; 26434359300; 57985112000","An efficient center-based method for real-time pig posture recognition and tracking","2024","Applied Intelligence","54","6","","5183","5196","13","1","10.1007/s10489-024-05439-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190572860&doi=10.1007%2fs10489-024-05439-5&partnerID=40&md5=9054c21202092955e3f35319d696e02a","ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France; Cooperl Innovation S.A.S, 7 rue de la Jeannaie, Lamballe, 22400, France","Mattina M., ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France, Cooperl Innovation S.A.S, 7 rue de la Jeannaie, Lamballe, 22400, France; Benzinou A., ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France; Nasreddine K., ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France; Richard F., Cooperl Innovation S.A.S, 7 rue de la Jeannaie, Lamballe, 22400, France","Detecting, tracking, and recognizing the posture of individual pigs are the primary computer vision tasks of many camera-based decision support tools for precise livestock monitoring. Recently, the use of deep learning approaches in computer vision, particularly convolutional neural networks (CNNs), has led to performance never before possible. However, the heavy constraints of pig environments such as instinctive grouping and similar animal appearance limit the effectiveness of state-of-the-art approaches popular in other application fields. To tackle these problems, we propose a fully anchor-free center-based CNN framework that detects pigs, classifies their postures, and tracks them throughout an image sequence. Input images are first fed into two sub-networks that detect pig posture and generate a global appearance map from which local appearance vectors corresponding to the detected pig centers can be extracted. Next, an assignment strategy uses spatial and appearance metrics to associate each pig detected in the frame with one of the tracked pig trajectories in a video sequence. As shown by the experiments, our real-time method significantly outperforms state-of-the-art approaches for pig detection, posture recognition, and tracking. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Anchor-free; Data augmentation; Livestock monitoring; Object tracking; Re-identification","Agriculture; Convolutional neural networks; Decision support systems; Deep learning; Mammals; Tracking (position); Anchor-free; Center-based; Convolutional neural network; Data augmentation; Livestock monitoring; Object Tracking; Posture recognition; Posture tracking; Re identifications; State-of-the-art approach; Computer vision","French National Association of Research and Technology; Association Nationale de la Recherche et de la Technologie, ANRT, (CIFRE 2018/1906); Association Nationale de la Recherche et de la Technologie, ANRT","This work is supported by the French National Association of Research and Technology (ANRT) with the Grant CIFRE 2018/1906. ","A. Benzinou; ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France; email: benzinou@enib.fr","","Springer","0924669X","","APITE","","English","Appl Intell","Article","Final","","Scopus","2-s2.0-85190572860"
"Li Q.; Zhao J.; Bai D.; Lu J.","Li, Qiangqiang (59161528300); Zhao, Jianmin (57446644300); Bai, Derigen (59161371000); Lu, Jianwen (59160904000)","59161528300; 57446644300; 59161371000; 59160904000","Cattle Body Size Measurement System Based on Dual-Position Cameras","2024","ACM International Conference Proceeding Series","","","47","","","","0","10.1145/3653781.3653831","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195280928&doi=10.1145%2f3653781.3653831&partnerID=40&md5=0bdc17e1579ee55823ace8c57236da38","School of Information Engineering, Inner Mongolia, University of Science and Technology, Baotou, China; Agricultural and Livestock Technology Extension Wulagai Management Area of Xilin Gol League, Wulagai, China; Inner Mongolia Xinlian Information Industry Co. Ltd, Inner Mongolia, Baotou, China","Li Q., School of Information Engineering, Inner Mongolia, University of Science and Technology, Baotou, China; Zhao J., School of Information Engineering, Inner Mongolia, University of Science and Technology, Baotou, China; Bai D., Agricultural and Livestock Technology Extension Wulagai Management Area of Xilin Gol League, Wulagai, China; Lu J., Inner Mongolia Xinlian Information Industry Co. Ltd, Inner Mongolia, Baotou, China","In order to release the labor consumption caused by high stress reaction of cattle in manual measurement of cattle body size, a non-contact measurement system based on detection and segmentation algorithms is proposed to automatically measure the body height, body length, body oblique length, chest girth, and abdominal girth of the cattle in a naturally standing posture. Firstly, a side camera is assigned to acquire profile image of the cattle. The profile image is then fed into the detector YoLov5s to detect the body-size region, including head, body, hoofs, joints and tail and to extract profile key points. Secondly, a back camera is additionally set on the back-top to get back image. The segmentation algorithm is used to segment and to obtain the back contour of the cattle for the back key points extraction. Finally, we use the key points extracted from the profile and back image to fit and calculate the size curve of body height, body length, body oblique length, chest girth, and abdominal girth respectively. The experimental results show that the average errors of body height, body length, body oblique length, chest girth, and abdominal girth were 3.75%, 3.32%, 3.14%, 4.41%, and 4.30%, respectively. Our proposed system based on dual-position cameras can provide an effective way for animal body size measurement.  © 2024 ACM.","cattle body size measurement; computer vision; deep learning; precision livestock farming","Agriculture; Anthropometry; Computer vision; Deep learning; Image segmentation; Body sizes; Cattle body size measurement; Deep learning; High stress; Keypoints; Measurement system; Precision livestock farming; Segmentation algorithms; Size measurements; Stress reaction; Cameras","Basic Research Funds for Universities; Inner Mongolia Autonomous Region in China; Natural Science Foundation of Inner Mongolia Autonomous Region, (2023MS06014, 2019LH06006); Natural Science Foundation of Inner Mongolia Autonomous Region; Science and Technology Major Project of Inner Mongolia Autonomous Region of China, (2021GG0224); Science and Technology Major Project of Inner Mongolia Autonomous Region of China","This work was supported by the Natural Science Foundation of Inner Mongolia Autonomous Region in China [2023MS06014, 2019LH06006], by the Inner Mongolia Autonomous Region Science and Technology Plan Project in China [2021GG0224], and by the Basic Research Funds for Universities directly under Inner Mongolia Autonomous Region in China [088].","","","Association for Computing Machinery","","979-840071819-9","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85195280928"
"Shu H.; Bindelle J.; Gu X.","Shu, Hang (57222589919); Bindelle, Jérôme (16240687200); Gu, Xianhong (8976627000)","57222589919; 16240687200; 8976627000","Non-contact respiration rate measurement of multiple cows in a free-stall barn using computer vision methods","2024","Computers and Electronics in Agriculture","218","","108678","","","","8","10.1016/j.compag.2024.108678","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183990028&doi=10.1016%2fj.compag.2024.108678&partnerID=40&md5=e9e17def890ab8e38378fc000331d060","State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium","Shu H., State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China, Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Bindelle J., AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Gu X., State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","In cattle, respiration rate (RR) provides researchers and practitioners with valuable physiological information. However, traditional visual observation requires massive labour and is impractical for large-scale commercial farms. Recently developed vision-based RR measurement methods are highly limited to measuring a small number of animals in a controlled environment. Therefore, this paper aimed to propose a vision-based multi-object RR measurement method for dairy cows lying in free stalls. An RGB camera was aimed at the lying zone and was able to cover about 16 stalls. The proposed framework first utilised two YOLOv5-based networks to segment cow instances and detect cow flank objects. Next, an object tracker was used to link the predictions of each cow throughout the video clip. The Lucas-Kanade optical flow was then calculated specifically on the overlapped area of the cow mask and the flank bounding box. Finally, RR was extracted using Fast Fourier Transform. The results show that the proposed method had a precise RR measurement with a correlation coefficient of 0.944, a root mean square error of 5.35 breaths per minute, and an intraclass correlation coefficient of 0.974 when compared to visual observation. The piecewise regression models identified a change in RR when the ambient temperature reached 23.6 °C or the temperature-humidity index reached 72. The corresponding RR thresholds were 60.9 and 60.2 breaths per minute, respectively. Collectively, these results can be used to inform an automated local cooling system, e.g., fans in the lying area. However, more experiments and calibration with data collected using more cost-effective video recording systems are required before this technology can be applied on farms. © 2024 Elsevier B.V.","Animal welfare; Deep learning; Heat stress; Multi-object measurement; Precision livestock farming","Agriculture; Animals; Computer vision; Cost effectiveness; Deep learning; Fast Fourier transforms; Mean square error; Object detection; Regression analysis; Animal welfare; Deep learning; Free-stalls; Heat stress; Multi-object measurement; Multiobject; Object measurement; Precision livestock farming; Rate measurements; Respiration rate; algorithm; animal welfare; calibration; computer vision; heating; livestock farming; measurement method; regression analysis; vision; Video recording","Beijing Dairy Industry Innovation Team Project, (BAIC06-2016-2021); China Scholarship Council, CSC, (202203250080); Agricultural Science and Technology Innovation Program, ASTIP, (ASTIP-IAS07)","The first author (H. Shu) was funded by a PhD grant from China Scholarship Council [ 202203250080 ]. This work was also supported by the Beijing Dairy Industry Innovation Team Project [ BAIC06-2016-2021 ] and the Agricultural Science and Technology Innovation Program [ASTIP-IAS07]. The authors are grateful to Dr. Gan Li, Dr. Alan, and Kangyuan dairy farm for their assistance in data collection.","H. Shu; State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; email: shuhang@caas.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85183990028"
"Farahnakian F.; Farahnakian F.; Björkman S.; Bloch V.; Pastell M.; Heikkonen J.","Farahnakian, Fahimeh (26029283800); Farahnakian, Farshad (57346336200); Björkman, Stefan (57191585466); Bloch, Victor (56963742200); Pastell, Matti (11339950500); Heikkonen, Jukka (7003349785)","26029283800; 57346336200; 57191585466; 56963742200; 11339950500; 7003349785","Pose estimation of sow and piglets during free farrowing using deep learning","2024","Journal of Agriculture and Food Research","16","","101067","","","","1","10.1016/j.jafr.2024.101067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188544553&doi=10.1016%2fj.jafr.2024.101067&partnerID=40&md5=d40acf078ebcb9fbb91695557399d56a","Department of Computing, University of Turku, Turku, 20500, Finland; Department of Production Animal Medicine, University of Helsinki, 00014, Finland; Resources Institute Finland (Luke), Latokartanonkaari 9, Helsinki, 00790, Finland","Farahnakian F., Department of Computing, University of Turku, Turku, 20500, Finland; Farahnakian F., Department of Computing, University of Turku, Turku, 20500, Finland; Björkman S., Department of Production Animal Medicine, University of Helsinki, 00014, Finland; Bloch V., Resources Institute Finland (Luke), Latokartanonkaari 9, Helsinki, 00790, Finland; Pastell M., Resources Institute Finland (Luke), Latokartanonkaari 9, Helsinki, 00790, Finland; Heikkonen J., Department of Computing, University of Turku, Turku, 20500, Finland","Automatic and real-time pose estimation is important in monitoring animal behavior, health, and welfare. In this paper, we utilized pose estimation for monitoring the farrowing process to prevent piglet mortality and preserve the health and welfare of the sow. State-of-the-art Deep Learning (DL) methods have lately been used for animal pose estimation. This paper aims to probe the generalization ability of five common DL networks (ResNet50, ResNet101, MobileNet, EfficientNet, and DLCRNet) for sow and piglet pose estimation. These architectures predict the body parts of several piglets and the sow directly from input video sequences. Real farrowing data from a commercial farm was used for training and validation of the proposed networks. The experimental results demonstrated that MobileNet was able to detect seven body parts of the sow with a median test error of 0.61 pixels. © 2024 The Authors","Animal behavior; Convolutional neural networks; Deep learning; Livestock; Pose estimation","","Turun Yliopisto, UTU; Finnish Ministry of Agriculture, (529/03.01.02/2018)","This study was funded by the Finnish Ministry of Agriculture (grant decision 529/03.01.02/2018) and further financially supported by the Algorithmic Computational Intelligence research group at the University of Turku . ","F. Farahnakian; Department of Computing, University of Turku, Turku, 20500, Finland; email: farfar@utu.fi","","Elsevier B.V.","26661543","","","","English","J. Agric. Food. Res.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85188544553"
"Manoj R.; Girish S.; Pushpa B.R.; Rani N.S.; Sangamesha D.","Manoj, R. (57777427300); Girish, Sandra (59377575100); Pushpa, B.R. (56884536700); Rani, N. Shobha (59390286400); Sangamesha, D. (59407795600)","57777427300; 59377575100; 56884536700; 59390286400; 59407795600","Early-Stage Disease Prediction in Chilli Plant Using YOLO Models","2024","2nd IEEE International Conference on Advances in Information Technology, ICAIT 2024 - Proceedings","","","","","","","0","10.1109/ICAIT61638.2024.10690819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208829312&doi=10.1109%2fICAIT61638.2024.10690819&partnerID=40&md5=e1d1c7d0efe4dca1e9df211c870110e7","Department of Computer Science, School of Computing, Amrita Vishwa Vidyapeetham, Mysuru, India; Department of Computer Science and Engineering, Gitam School of Technology, Gitam University, Bengaluru, India; Department of Chemistry, The National Institute of Engineering, Mysore, India","Manoj R., Department of Computer Science, School of Computing, Amrita Vishwa Vidyapeetham, Mysuru, India; Girish S., Department of Computer Science, School of Computing, Amrita Vishwa Vidyapeetham, Mysuru, India; Pushpa B.R., Department of Computer Science, School of Computing, Amrita Vishwa Vidyapeetham, Mysuru, India; Rani N.S., Department of Computer Science and Engineering, Gitam School of Technology, Gitam University, Bengaluru, India; Sangamesha D., Department of Chemistry, The National Institute of Engineering, Mysore, India","Agriculture is an important component of every country's economy, supplying the necessary resources to farmers and their families. The livelihoods of farmers are greatly threatened by crop diseases, which highlights the importance of early identification of diseases in chilli plants at the right stage of growth is crucial for timely fertilizer recommendation. This paper presents an intelligent transfer learning technique to detect thrips and viruses in chilli plants in the early stages of development. Stage-by-stage datasets are collected from chilli plant orchards around Mysore district, Karnataka, to carry out the training using the YOLO model. The dataset consists of three growth stages of chilli plants (15, 25, and 35 days) that are captured in diverse environmental settings with multiple-resolution smart phones. The annotations are created for the diseases healthy, viruses, and thrips at leaf level from whole plants for all three stages. Hyperparameters such as learning rate, the learning optimization algorithm, batch size, epochs, and loss functions are employed to fine-tune the YOLO v5, v6, and v7 models. To achieve high generalisation and reduce overfitting, a weight decay (weight_decay) of 0.0005 is employed. From the evaluation, it was noticed that YOLOv7 outperformed other YOLO models. A mAP score of 0.80 is achieved in stage 1, 0.82 in stage 2, and 0.73 in stage 3 with minimal validation loss and remarkable inference speed. © 2024 IEEE.","deep learning; disease detection; early-stage disease identification; Sustainable agriculture","Fertilizers; Livestock; Smart agriculture; Crop disease; Deep learning; Disease detection; Early-stage disease identification; Karnataka; Learning techniques; Stage of growth; Sustainable agriculture; Transfer learning; Weight decay; Plant diseases","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038386-7","","","English","IEEE Int. Conf. Adv. Inf. Technol., ICAIT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85208829312"
"","","","Research anthology on bioinformatics, genomics, and computational biology","2024","Research Anthology on Bioinformatics, Genomics, and Computational Biology","","","","1","1509","1508","0","10.4018/979-8-3693-3026-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193526846&doi=10.4018%2f979-8-3693-3026-5&partnerID=40&md5=00e8c71e270f3d90d9b208e0359ca68e","","","In the evolving environment of bioinformatics, genomics, and computational biology, academic scholars are facing a challenging challenge - keeping informed about the latest research trends and findings. With unprecedented advancements in sequencing technologies, computational algorithms, and machine learning, these fields have become indispensable tools for drug discovery, disease research, genome sequencing, and more. As scholars strive to decode the language of DNA, predict protein structures, and navigate the complexities of biological data analysis, the need for a comprehensive and up-to-date resource becomes paramount. The Research Anthology on Bioinformatics, Genomics, and Computational Biology is a collection of a carefully curated selection of chapters that serves as the solution to the pressing challenge of keeping pace with the dynamic advancements in these critical disciplines. This anthology is designed to address the informational gap by providing scholars with a consolidated and authoritative source that sheds light on critical issues, innovative theories, and transformative developments in the field. It acts as a single reference point, offering insights into conceptual, methodological, technical, and managerial issues while also providing a glimpse into emerging trends and future opportunities. Within the anthology, scholars will find a wealth of knowledge spanning diverse topics, from gene selection using metaheuristics to the application of deep learning in biological big data analysis. It delves into cancer classification methodologies, cluster analysis with big data applications, and the impact of digitalization, robotics, and genomic research in livestock development. The book also explores precision drug screening for cystic fibrosis patients, presents a hybrid high-performance computing algorithm for gene regulatory network construction, and discusses the collaborative efforts between librarians and bioinformatics communities. The Research Anthology on Bioinformatics, Genomics, and Computational Biology offers the comprehensive solution needed to face the challenges presented to academic scholars. By providing a consolidated, authoritative, and up-to-date reference that empowers researchers, scientists, educators, and professionals to navigate and contribute to the forefront of transformative research in biological data analysis, this anthology provides a gateway to understanding and advancing the pivotal disciplines shaping the future of life sciences. © 2024 by IGI Global. All rights reserved.","","","","","","","IGI Global","","979-836933027-2; 979-836933026-5","","","English","Res. Anthol. on Bioinform., Genomics, and Comput. Biol.","Book","Final","","Scopus","2-s2.0-85193526846"
"Ma C.; Zhang T.; Zheng H.; Yang J.; Chen R.; Fang C.","Ma, Chuang (57668744200); Zhang, Tiemin (7404374375); Zheng, Haikun (57219905282); Yang, Jikang (57668744100); Chen, Ruitian (59002346800); Fang, Cheng (57214258707)","57668744200; 7404374375; 57219905282; 57668744100; 59002346800; 57214258707","Measurement method for live chicken shank length based on improved ResNet and fused multi-source information","2024","Computers and Electronics in Agriculture","221","","108965","","","","0","10.1016/j.compag.2024.108965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191305094&doi=10.1016%2fj.compag.2024.108965&partnerID=40&md5=9b13b0f813afc4c218e106880ac82c04","State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China; College of Engineering, South China Agricultural University, Guangzhou, 510642, China; National Engineering Research Center for Breeding Swine Industry, Guangzhou, 510642, China","Ma C., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China; Zhang T., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China, National Engineering Research Center for Breeding Swine Industry, Guangzhou, 510642, China; Zheng H., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China; Yang J., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China; Chen R., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China; Fang C., State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China, College of Engineering, South China Agricultural University, Guangzhou, 510642, China","Phenotypic parameters are crucial reference indicators in poultry breeding. However, the chickens shank length is still manually measured, which is time-consuming and labor-intensive. Additionally, the measurement results are difficult to unify due to the subjective factors of different individuals. To address this issue, this paper proposed a method for live chicken shank length measurement (SLM). It enriches chicken shank feature by fusing visible images and infrared images. The fusion images are then input into a deep regression model based on the improved ResNet. The measurement model used ResNet as its backbone and introduces Squeeze-and-Excitation (SE) blocks and a Spatial Pyramid Pooling (SPP) block, resulting in more precise and stable shank length measurements. The average coefficient of variation, average floating error, average standard deviation and Pearson correlation coefficient for shank length measurements using the fusion images are 0.21 %, 0.49 %, 0.181 mm and 0.996, respectively, compared with using single visible or infrared image, the accuracy and stability are obviously improved. That indicated combining deep learning model and fusion information, the SLM proposed in this paper can achieve a more precise, reliable and standardized measurement of live chicken shank length. © 2024 Elsevier B.V.","Body size measurement; Infrared image; Precision Livestock Farming; ResNet; Visible image","Agriculture; Animals; Correlation methods; Deep learning; Image fusion; Infrared imaging; Regression analysis; Body size measurement; Body sizes; Fusion image; Infrared image; Length measurement; Measurement methods; Precision livestock farming; Resnet; Size measurements; Visible image; correlation; livestock farming; measurement method; regression analysis; Image enhancement","South China Agricultural University, SCAU; National Key Research and Development Program of China, NKRDPC, (2021YFD1300101); State Key Laboratory of Swine and Poultry Breeding Industry, (ZQQZ-31); Guangdong Province Special Fund for Modern Agricultural Industry Common Key Technology R&D Innovation Team, (2023KJ129); Special Project for Research and Development in Key areas of Guangdong Province, (2022 B0202100002)","Funding: This work is supported by National Key Research and Development Plan [grant No. 2021YFD1300101 ], State Key Laboratory of Swine and Poultry Breeding Industry\uFF08PI\uFF09research project [grant No. ZQQZ-31 ], Guangdong Province Special Fund for Modern Agricultural Industry Common Key Technology R&D Innovation Team [grant No. 2023KJ129 ], Guangdong Province Key Research and Development Plan [grant no. 2022 B0202100002 ], The second phase of project of the Wenshi Science and Technology Innovation Center at South China Agricultural University, China.","T. Zhang; State Key Laboratory of Swine and Poultry Breeding Industry, South China Agricultural University, Guangzhou, 510642, China; email: tm-zhang@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85191305094"
"Chen L.; Zhang L.; Tang J.; Tang C.; An R.; Han R.; Zhang Y.","Chen, Ling (58741118400); Zhang, Lianyue (58667541400); Tang, Jinglei (35176589500); Tang, Chao (59055675200); An, Rui (58865279800); Han, Ruizi (58865208100); Zhang, Yiyang (58873300900)","58741118400; 58667541400; 35176589500; 59055675200; 58865279800; 58865208100; 58873300900","GRMPose: GCN-based real-time dairy goat pose estimation","2024","Computers and Electronics in Agriculture","218","","108662","","","","3","10.1016/j.compag.2024.108662","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184139987&doi=10.1016%2fj.compag.2024.108662&partnerID=40&md5=8d1d97ca9e3947ff51957be06d2848bf","College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Shaanxi, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Shaanxi, Yangling, 712100, China","Chen L., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Zhang L., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Tang J., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Shaanxi, Yangling, 712100, China; Tang C., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; An R., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Han R., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Zhang Y., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Pose estimation can be used to analyse the behaviour of livestock, hence offering valuable insights into the physiological state of the livestock. Despite impressive progress in academic benchmarks, existing pose estimation methods do not meet the speed and accuracy trade-off requirements of industrial applications. Following the top-down paradigm, we propose GRMPose, a GCN-based real-time pose estimation framework for dairy goats. The framework adopts CSPNext as its backbone, which is highly efficient and potent for extracting features. In addition, a GCN-based coordination classification module (GCNCC) is proposed to improve the capability of extracting the structural pose information of the keypoint features. To assess the effectiveness of GRMPose, we built the DairyGoat dataset, which contains 2,108 images and 2,576 instances with different lighting conditions, different types of behaviours, and partially occluded dairy goat instance objects. Experimental results demonstrate that GRMPose achieves an AP of 87.48% on the DairyGoat dataset. Additionally, it exhibits a model inference latency of 13.82 ms, GFLOPs of 5.57, and parameters totalling 27.56 million. These results establish GRMPose as superior to other models like HRNet, Resnext, and MobileNetv2 in terms of speed-accuracy trade-off. © 2024 Elsevier B.V.","Dairy goat pose estimation; Deep learning; Graph convolution network","Classification (of information); Deep learning; Economic and social effects; Dairy goat pose estimation; Deep learning; Estimation methods; Extracting features; Graph convolution network; Physiological state; Pose-estimation; Real- time; Topdown; Trade off; accuracy assessment; artificial neural network; data set; estimation method; goat; machine learning; real time; satellite imagery; trade-off; Benchmarking","Xi'an Agricultural Technology Research and Development Project; Xianyang Science and Technology Plan Project, (L2022ZDYFSF050); Xi’an Agricultural Technology Research and Development Project, (22NYYF013); National Key Research and Development Program of China, NKRDPC, (2021YFD1600704); National Key Research and Development Program of China, NKRDPC; Key Research and Development Projects of Shaanxi Province, (2023-YBNY-121); Key Research and Development Projects of Shaanxi Province","Funding text 1: This work was supported in part by the Key Research and Development Project of Shaanxi province, China (No. 2023-YBNY-121 ), Xi’an Agricultural Technology Research and Development Project, China (No. 22NYYF013 ), Xianyang Science and Technology Plan Project, China (No. L2022ZDYFSF050 ), and National Key Research and Development Plan Project, China (No. 2021YFD1600704 ). The authors appreciate the funding organizations for their financial support.; Funding text 2: This work was supported in part by the Key Research and Development Project of Shaanxi province, China (No. 2023-YBNY-121), Xi'an Agricultural Technology Research and Development Project, China (No. 22NYYF013), Xianyang Science and Technology Plan Project, China (No. L2022ZDYFSF050), and National Key Research and Development Plan Project, China (No. 2021YFD1600704). The authors appreciate the funding organizations for their financial support.","J. Tang; College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: tangjinglei@nwsuaf.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85184139987"
"Gadagin N.; Sajjan S.; Hiremath S.; Saunshi G.; Naveen N.M.; Rajesh Y.","Gadagin, Nagaraj (57189239553); Sajjan, Swati (57960164900); Hiremath, Shobha (57215537556); Saunshi, Girish (57223677208); Naveen, N.M. (59517997500); Rajesh, Y. (59518094800)","57189239553; 57960164900; 57215537556; 57223677208; 59517997500; 59518094800","Harnessing Data Science for Healthy Herds: Deep Learning for Cattle Disease Detection","2024","Proceedings of NKCon 2024 - 3rd Edition of IEEE NKSS's Flagship International Conference: Digital Transformation: Unleashing the Power of Information","","","","","","","0","10.1109/NKCon62728.2024.10774770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215291484&doi=10.1109%2fNKCon62728.2024.10774770&partnerID=40&md5=ada2399be90786a3d1ec42df756e47ff","Visveswaraya Technological University, Cec, Dept. of Cse Bldeacet, Vijayapur and Dept. of Aiml, Mangaluru, Belagavi, 590018, India; Hubballi Visveswaraya Technological University, K. L. E. Institute of Technology, Dept. of Cse, Belagavi, 590018, India; Presidency University, SoCSE & Is, Bengaluru, India","Gadagin N., Visveswaraya Technological University, Cec, Dept. of Cse Bldeacet, Vijayapur and Dept. of Aiml, Mangaluru, Belagavi, 590018, India; Sajjan S., Hubballi Visveswaraya Technological University, K. L. E. Institute of Technology, Dept. of Cse, Belagavi, 590018, India; Hiremath S., Hubballi Visveswaraya Technological University, K. L. E. Institute of Technology, Dept. of Cse, Belagavi, 590018, India; Saunshi G., Hubballi Visveswaraya Technological University, K. L. E. Institute of Technology, Dept. of Cse, Belagavi, 590018, India; Naveen N.M., Presidency University, SoCSE & Is, Bengaluru, India; Rajesh Y., Hubballi Visveswaraya Technological University, K. L. E. Institute of Technology, Dept. of Cse, Belagavi, 590018, India","Precision livestock management has shown promise in maintaining the well-being and productivity of cow herds. Lumpy skin disease (LSD) is a global problem for cow populations that affects food security and results in large financial losses. It is imperative to detect LSD in herds early and take prompt action to stop its spread. In this research work, a thorough method for the automatic identification of LSD in cattle that uses deep learning techniques is presented. Here, the performance of five listed convolutional neural network (CNN) models has already been trained: DenseNet, ResNet, InceptionV3, VGG, and Xception, which are investigated in detecting LSD in cow images. These models are refined using a large-scale dataset collected from a veterinary clinic and Kaggle, which includes pictures of calves that show signs of LSD infection. Work exploits the pre-trained representations obtained by these models on general image recognition tasks and adapts them to the LSD detection task, specifically using transfer learning. A comparison study is carried out to evaluate these models' accuracy and computational effectiveness. This work also looks into how resilient the models are to changes in lighting, environmental factors, and cow breeds.  © 2024 IEEE.","CNN; Deep Learning; DenseNet; Lumpy Skin Disease (LSD)","Convolutional neural networks; Deep learning; Economic and social effects; Cattle disease; Convolutional neural network; Deep learning; Densenet; Disease detection; Global problems; Lumpy skin disease; Skin disease; Well being; Well productivity; Livestock","","","N. Gadagin; Visveswaraya Technological University, Cec, Dept. of Cse Bldeacet, Vijayapur and Dept. of Aiml, Belagavi, Mangaluru, 590018, India; email: nagaraj.gadagin@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835036456-9","","","English","Proc. NKCon - Ed. IEEE NKSS's Flagship Int. Conf.: Digit. Transform.: Unleashing Power Inf.","Conference paper","Final","","Scopus","2-s2.0-85215291484"
"Giagnoni G.; Lassen J.; Lund P.; Foldager L.; Johansen M.; Weisbjerg M.R.","Giagnoni, G. (57222489230); Lassen, J. (35578112500); Lund, P. (7201878212); Foldager, L. (9743558600); Johansen, M. (57117317300); Weisbjerg, M.R. (6701620596)","57222489230; 35578112500; 7201878212; 9743558600; 57117317300; 6701620596","Feed intake in housed dairy cows: validation of a three-dimensional camera−based feed intake measurement system","2024","Animal","18","6","101178","","","","1","10.1016/j.animal.2024.101178","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194920723&doi=10.1016%2fj.animal.2024.101178&partnerID=40&md5=7479cce452c399c6f4da6c64ff3980f2","Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark; Center for Quantitative Genetics and Genomics, Aarhus University, C. F. Møllers Allé 3, Aarhus, 8000, Denmark; Viking Genetics, Ebeltoftvej 16, Randers, 8960, Denmark; Bioinformatics Research Centre, Aarhus University, Universitetsbyen 81, Aarhus, 8000, Denmark","Giagnoni G., Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark; Lassen J., Center for Quantitative Genetics and Genomics, Aarhus University, C. F. Møllers Allé 3, Aarhus, 8000, Denmark, Viking Genetics, Ebeltoftvej 16, Randers, 8960, Denmark; Lund P., Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark; Foldager L., Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark, Bioinformatics Research Centre, Aarhus University, Universitetsbyen 81, Aarhus, 8000, Denmark; Johansen M., Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark; Weisbjerg M.R., Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark","Measuring feed intake accurately is crucial to determine feed efficiency and for genetic selection. A system using three-dimensional (3D) cameras and deep learning algorithms can measure the volume of feed intake in dairy cows, but for now, the system has not been validated for feed intake expressed as weight of feed. The aim of this study was to validate the weight of feed intake predicted from the 3D cameras with the actual measured weight. It was hypothesised that diet−specific coefficients are necessary for predicting changes in weight, that the relationship between weight and volume is curvilinear throughout the day, and that manually pushing the feed affects this relationship. Twenty-four lactating Danish Holstein cows were used in a cross-over design with four dietary treatments, 2 × 2 factorial arranged with either grass-clover silage or maize silage as silage factor, and barley or dried beet pulp as concentrate factor. Cows were adapted to the diets for 11 d, and for 3 d to tie-stall housing before camera measurements. Six cameras were used for recording, each mounted over an individual feeding platform equipped with a weight scale. When building the predictive models, four cameras were used for training, and the remaining two for testing the prediction of the models. The most accurate predictions were found for the average feed intake over a period when using the starting density of the feed pile, which resulted in the lowest errors, 6% when expressed as RMSE and 5% expressed as mean absolute error. A model including curvilinear effects of feed volume and the impact of manual feed pushing was used on a dataset including daily time points. When cross-validating, the inclusion of a curvilinear effect and a feed push effect did not improve the accuracy of the model for neither the feed pile nor the feed removed by the cow between consecutive time points. In conclusion, measuring daily feed intake from this 3D camera system in the present experimental setup could be accomplished with an acceptable error (below 8%), but the system should be improved for individual meal intake measurements if these measures were to be implemented. © 2024 The Author(s)","Dry matter intake; Feed efficiency; Feeding behaviour; Precision livestock farming; Total mixed ration density","Animal Feed; Animals; Body Weight; Cattle; Cross-Over Studies; Dairying; Deep Learning; Diet; Eating; Feeding Behavior; Female; Housing, Animal; Imaging, Three-Dimensional; Lactation; Silage; animal; animal food; animal housing; body weight; bovine; crossover procedure; dairying; deep learning; diet; eating; feeding behavior; female; lactation; physiology; procedures; silage; three-dimensional imaging; veterinary medicine","Danish Meat Research Institute; Innovationsfonden, IFD, (9090-00083B); Innovationsfonden, IFD","Funding text 1: Thanks to Ester Bjerregaard, Torkild Nyholm Jakobsen, and to the barn staff at the Danish Cattle Research Centre for assistance, and caring, moving, feeding, and milking the cows during the trial. Thanks to Peter Trier and his team (Svaneg\u00E5rden ApS) for fitting the 3D cameras, and to Bjerringbro V\u00E6gte ApS for assistance with the scales. Thanks to Glenn Gunner Brink Nielsen and Rikke Hjort Hansen from the Danish Meat Research Institute (Danish Technologic Institute) for help with the data. A preliminary draft of this manuscript is presented in the PhD thesis from Giagnoni (2023b). This study was financed from the Innovation Fund Denmark (project number: 9090-00083B); Funding text 2: This study was financed from the Innovation Fund Denmark (project number: 9090-00083B) ","G. Giagnoni; Department of Animal and Veterinary Sciences, AU Viborg – Research Centre Foulum, Aarhus University, Tjele, Blichers Allé 20, 8830, Denmark; email: gigi@anivet.au.dk","","Elsevier B.V.","17517311","","","38823283","English","Animal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85194920723"
"de Oliveira F.M.; Ferraz G.A.E.S.; André A.L.G.; Santana L.S.; Norton T.; Ferraz P.F.P.","de Oliveira, Franck Morais (57275191700); Ferraz, Gabriel Araújo e Silva (55322659100); André, Ana Luíza Guimarães (59197036300); Santana, Lucas Santos (57208742161); Norton, Tomas (35273348100); Ferraz, Patrícia Ferreira Ponciano (56421977300)","57275191700; 55322659100; 59197036300; 57208742161; 35273348100; 56421977300","Digital and Precision Technologies in Dairy Cattle Farming: A Bibliometric Analysis","2024","Animals","14","12","1832","","","","2","10.3390/ani14121832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197125619&doi=10.3390%2fani14121832&partnerID=40&md5=ce24beb5f704c7a3493df087fdbd140e","Department of Agricultural Engineering, School of Engineering, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; Department of Animal Science, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; Department of Agricultural and Environmental Engineering (EEA), Institute of Agricultural Sciences (ICA), Federal University of Vales Jequitinhonha and Mucuri—Campus Unaí, Avenida Universitária, nº 1.000, B. Universitários, Unai, 38610-000, Brazil; M3-BIORES-Measure, Model & Manage Bioresponses, KU Leuven, Kasteelpark Arenberg 30, Leuven, B-3001, Belgium","de Oliveira F.M., Department of Agricultural Engineering, School of Engineering, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; Ferraz G.A.E.S., Department of Agricultural Engineering, School of Engineering, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; André A.L.G., Department of Animal Science, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; Santana L.S., Department of Agricultural and Environmental Engineering (EEA), Institute of Agricultural Sciences (ICA), Federal University of Vales Jequitinhonha and Mucuri—Campus Unaí, Avenida Universitária, nº 1.000, B. Universitários, Unai, 38610-000, Brazil; Norton T., M3-BIORES-Measure, Model & Manage Bioresponses, KU Leuven, Kasteelpark Arenberg 30, Leuven, B-3001, Belgium; Ferraz P.F.P., Department of Agricultural Engineering, School of Engineering, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil","The advancement of technology has significantly transformed the livestock landscape, particularly in the management of dairy cattle, through the incorporation of digital and precision approaches. This study presents a bibliometric analysis focused on these technologies involving dairy farming to explore and map the extent of research in the scientific literature. Through this review, it was possible to investigate academic production related to digital and precision livestock farming and identify emerging patterns, main research themes, and author collaborations. To carry out this investigation in the literature, the entire timeline was considered, finding works from 2008 to November 2023 in the scientific databases Scopus and Web of Science. Next, the Bibliometrix (version 4.1.3) package in R (version 4.3.1) and its Biblioshiny software extension (version 4.1.3) were used as a graphical interface, in addition to the VOSviewer (version 1.6.19) software, focusing on filtering and creating graphs and thematic maps to analyze the temporal evolution of 198 works identified and classified for this research. The results indicate that the main journals of interest for publications with identified affiliations are “Computers and Electronics in Agriculture” and “Journal of Dairy Science”. It has been observed that the authors focus on emerging technologies such as machine learning, deep learning, and computer vision for behavioral monitoring, dairy cattle identification, and management of thermal stress in these animals. These technologies are crucial for making decisions that enhance health and efficiency in milk production, contributing to more sustainable practices. This work highlights the evolution of precision livestock farming and introduces the concept of digital livestock farming, demonstrating how the adoption of advanced digital tools can transform dairy herd management. Digital livestock farming not only boosts productivity but also redefines cattle management through technological innovations, emphasizing the significant impact of these trends on the sustainability and efficiency of dairy production. © 2024 by the authors.","bibliometric review; dairy cows; precision livestock farming; technologies","agriculture; Article; bibliometrics; cattle farming; computer vision; dairy cattle; deep learning; livestock; machine learning; milk production; nonhuman; temperature stress","Minas Gerais Research Funding Foundation; KU Leuven; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Universidade Federal de Lavras, UFLA; Fundação de Amparo à Pesquisa do Estado de Minas Gerais, FAPEMIG, (APQ-01082-21, BPD-00034-22); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (404420/2021-4, 310186/2023-4)","Funding text 1: This research was funded by Minas Gerais Research Funding Foundation (FAPEMIG) projects APQ-01082-21 and BPD-00034-22, the National Council for Scientific and Technological Development (CNPq) projects 404420/2021-4 and 310186/2023-4.; Funding text 2: The authors would like to thank the Minas Gerais Research Funding Foundation, the National Council for Scientific and Technological Development (CNPq), Coordination for the Improvement of Higher Education Personnel (CAPES), the Federal University of Lavras, the Coimbra Group and KU Leuven. ","G.A.E.S. Ferraz; Department of Agricultural Engineering, School of Engineering, Federal University of Lavras (UFLA), Lavras, 37200-900, Brazil; email: gabriel.ferraz@ufla.br","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85197125619"
"Pretto A.; Savio G.; Gottardo F.; Uccheddu F.; Concheri G.","Pretto, Andrea (57940292800); Savio, Gianpaolo (14048903400); Gottardo, Flaviana (8625094600); Uccheddu, Francesca (24169884700); Concheri, Gianmaria (25421014800)","57940292800; 14048903400; 8625094600; 24169884700; 25421014800","A novel low-cost visual ear tag based identification system for precision beef cattle livestock farming","2024","Information Processing in Agriculture","11","1","","117","126","9","7","10.1016/j.inpa.2022.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149851456&doi=10.1016%2fj.inpa.2022.10.003&partnerID=40&md5=369c84e424e40d6831926dfe0a357b4f","Department of Civil, Environmental and Architectural Engineering, University of Padua, Italy; Department of Animal Medicine, Production and Health, University of Padova, Italy; Department of Industrial Engineering, University of Padova, Italy","Pretto A., Department of Civil, Environmental and Architectural Engineering, University of Padua, Italy; Savio G., Department of Civil, Environmental and Architectural Engineering, University of Padua, Italy; Gottardo F., Department of Animal Medicine, Production and Health, University of Padova, Italy; Uccheddu F., Department of Industrial Engineering, University of Padova, Italy; Concheri G., Department of Civil, Environmental and Architectural Engineering, University of Padua, Italy","The precision livestock farming (PLF) has the objective to maximize each animal's performance while reducing the environmental impact and maintaining the quality and safety of meat production. Among the PLF techniques, the personalised management of each individual animal based on sensors systems, represents a viable option. It is worth noting that the implementation of an effective PLF approach can be still expensive, especially for small and medium-sized farms; for this reason, to guarantee the sustainability of a customized livestock management system and encourage its use, plug and play and cost-effective systems are needed. Within this context, we present a novel low-cost method for identifying beef cattle and recognizing their basic activities by a single surveillance camera. By leveraging the current state-of-the-art methods for real-time object detection, (i.e., YOLOv3) cattle's face areas, we propose a novel mechanism able to detect the ear tag as well as the water ingestion state when the cattle is close to the drinker. The cow IDs are read by an Optical Character Recognition (OCR) algorithm for which, an ad hoc error correction algorithm is here presented to avoid numbers misreading and correctly match the IDs to only actually present IDs. Thanks to the detection of the tag position, the OCR algorithm can be applied only to a specific region of interest reducing the computational cost and the time needed. Activity times for the areas are outputted as cattle activity recognition results. Evaluation results demonstrate the effectiveness of our proposed method, showing a mAP@0.50 of 89%. © 2022 China Agricultural University","Cattle identification; Computer vision; Deep learning; Low-cost sensors; Precision livestock farming","Animals; Cost effectiveness; Deep learning; Environmental impact; Error correction; Farms; Image segmentation; Object detection; Optical character recognition; Security systems; Sustainable development; Animal performance; Beef cattle; Cattle identification; Deep learning; Livestock farming; Low-cost sensors; Low-costs; Precision livestock farming; Recognition algorithm; Tag-based; algorithm; cattle; cost analysis; livestock farming; machine learning; small and medium-sized enterprise; Computer vision","European fund for rural development; PSR; Regional Rural Development Programmes","This work has been carried out within the project LOWeMEAT (LOW Emission MEAT), thanks to the decisive contribution from the Regional Rural Development Programmes (PSR), which are co-financed by the European fund for rural development (FEASR) – Bando Regione Veneto PSR 2014-2020 DGR 1203/2016 misura 16.1.","A. Pretto; Department of Civil, Environmental and Architectural Engineering, University of Padua, Italy; email: andrea.pretto@unipd.it","","China Agricultural University","22143173","","","","English","Inf. Process. Agric.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85149851456"
"Roca A.; Torre G.; Giribet J.I.; Castro G.; Colombo L.; Mas I.; Pereira J.","Roca, Agustín (59459428400); Torre, Gabriel (58504859300); Giribet, Juan I. (13411185400); Castro, Gastón (59460396000); Colombo, Leonardo (36609790200); Mas, Ignacio (15623522200); Pereira, Javier (15763538200)","59459428400; 58504859300; 13411185400; 59460396000; 36609790200; 15623522200; 15763538200","Efficient Endangered Deer Species Monitoring with UAV Aerial Imagery and Deep Learning","2024","2024 7th IEEE Biennial Congress of Argentina, ARGENCON 2024","","","","","","","1","10.1109/ARGENCON62399.2024.10735858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211063548&doi=10.1109%2fARGENCON62399.2024.10735858&partnerID=40&md5=2608cdc455e5951d8ec213177ab05b1e","Universidad de San Andrés, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Linar - Instituto de Ingenierí a Biomé dica, Universidad de San Andrés - Uba, Buenos Aires, Argentina; Universidad de San Andrés - Conicet, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Centro de Automá tica y Robótica, Csic, Madrid, Spain; Museo Argentino de Ciencias Naturales 'Bernardino Rivadavia', Conicet, Caba, Argentina","Roca A., Universidad de San Andrés, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Torre G., Linar - Instituto de Ingenierí a Biomé dica, Universidad de San Andrés - Uba, Buenos Aires, Argentina; Giribet J.I., Universidad de San Andrés - Conicet, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Castro G., Universidad de San Andrés - Conicet, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Colombo L., Centro de Automá tica y Robótica, Csic, Madrid, Spain; Mas I., Universidad de San Andrés - Conicet, Laboratorio de Inteligencia Artificial y Robótica, Buenos Aires, Argentina; Pereira J., Museo Argentino de Ciencias Naturales 'Bernardino Rivadavia', Conicet, Caba, Argentina","This paper examines the use of Unmanned Aerial Vehicles (UAVs) and deep learning for detecting endangered deer species in their natural habitats. As traditional identification processes require trained manual labor that can be costly in resources and time, there is a need for more efficient solutions. Leveraging high-resolution aerial imagery, advanced computer vision techniques are applied to automate the identification process of deer across two distinct projects in Buenos Aires, Argentina. The first project, Pantano Project, involves the marsh deer in the Paraná Delta, while the second, WiMoBo, focuses on the Pampas deer in Campos del Tuyú National Park. A tailored algorithm was developed using the YOLO framework, trained on extensive datasets compiled from UAV-captured images. The findings demonstrate that the algorithm effectively identifies marsh deer with a high degree of accuracy and provides initial insights into its applicability to Pampas deer, albeit with noted limitations. This study not only supports ongoing conservation efforts but also highlights the potential of integrating AI with UAV technology to enhance wildlife monitoring and management practices. © 2024 IEEE.","Computer Vision; Deep Learning; UAV; Wildlife detection; YOLO","Deep learning; Invertebrates; Livestock; Unmanned aerial vehicles (UAV); Aerial imagery; Aerial vehicle; Deep learning; High resolution aerial imagery; Identification process; Manual labors; Natural habitat; Unmanned aerial vehicle; Wildlife detection; YOLO; Aerial photography","Consejo Superior de Investigaciones Científicas, CSIC; Agencia Nacional de Promoción Científica y Tecnológica, ANPCyT; National Parks, (PICT-2019-0373, PICT-2019-2371)","We thank the National Parks for all the support received to carry out this research. This work has been partially funded by the project PICT-2019-0373 and PICT-2019-2371 of the ANPCyT, Argentina, and by the project LINCGLOBAL 2022, of the Spanish National Research Council.","","","Institute of Electrical and Electronics Engineers Inc.","","979-835036593-1","","","English","IEEE Bienn. Congr. Argentina, ARGENCON","Conference paper","Final","","Scopus","2-s2.0-85211063548"
"Teja A.R.; Stephen S.; Subathra M.S.P.; Ewards S.E.V.; Mahmood Md.R.; Mohan A.; George S.T.","Teja, Ashwala Ravi (59540037700); Stephen, S. (59539373700); Subathra, M.S.P. (39862021400); Ewards, S.E. Vinodh (19933742200); Mahmood, Md. Rashid (57207912438); Mohan, Ashwala (59539707500); George, S. Thomas (59277673600)","59540037700; 59539373700; 39862021400; 19933742200; 57207912438; 59539707500; 59277673600","Multi-class Classification and Pretrained Sustainable Agriculture Model for Plant Leaf Disease Detection Using Resnet-101","2024","IET Conference Proceedings","2024","23","","295","300","5","0","10.1049/icp.2024.4438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216751834&doi=10.1049%2ficp.2024.4438&partnerID=40&md5=8cb4623675a2b4535c294386ea73cc44","Department of CSE, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Department of Robotics Engineering, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Department of Biomedical Engineering, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Department of ECE, Guru Nanak Institutions Technical Campus, Hyderabad, India","Teja A.R., Department of CSE, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Stephen S., Department of CSE, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Subathra M.S.P., Department of Robotics Engineering, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Ewards S.E.V., Department of CSE, Karunya Institute of Technology and Sciences, Tamil Nadu, India; Mahmood Md.R., Department of ECE, Guru Nanak Institutions Technical Campus, Hyderabad, India; Mohan A., Department of ECE, Guru Nanak Institutions Technical Campus, Hyderabad, India; George S.T., Department of Biomedical Engineering, Karunya Institute of Technology and Sciences, Tamil Nadu, India","In recent years, advancements in technology have greatly benefited the agricultural sector. One critical aspect of ensuring healthy plant growth is promptly identifying infected leaves. For sustainable agriculture, automatically detecting diseased on plant leaves before the infection spreads is highly advantageous. This paper introduces deep learning models like CNN, along with various optimization techniques and hyperparameter tuning being used. The study utilizes machine learning frameworks and a deep learning library. It encompasses 10 different datasets comprising 151,007 leaf images depicting 270 different diseases, including healthy leaves from 44 plant species. Additionally, pretrained models such as AlexNet, ResNet50, GoogLeNet, DenseNet-121, and ResNet101 are employed, with the ResNet model achieving the highest accuracy rate of 94.45%. © The Institution of Engineering & Technology 2024.","","Adversarial machine learning; Livestock; Agricultural sector; Hyper-parameter; Leaf disease detections; Learning models; Machine-learning; Multi-class classification; Optimization techniques; Plant growth; Plant leaves; Sustainable agriculture; Plant diseases","","","M.S.P. Subathra; Department of Robotics Engineering, Karunya Institute of Technology and Sciences, Tamil Nadu, India; email: subathra@karunya.edu","","Institution of Engineering and Technology","27324494","","","","English","IET. Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85216751834"
"Gao J.; Bambrah C.K.; Parihar N.; Kshirsagar S.; Mallarapu S.; Yu H.; Wu J.; Yang Y.","Gao, Jerry (7404475003); Bambrah, Charanjit Kaur (58296217900); Parihar, Nidhi (58295067600); Kshirsagar, Sharvaree (58296218000); Mallarapu, Sruthi (58295555300); Yu, Hailong (58295719800); Wu, Jane (58295590200); Yang, Yunyun (56303640800)","7404475003; 58296217900; 58295067600; 58296218000; 58295555300; 58295719800; 58295590200; 56303640800","Analysis of Various Machine Learning Algorithms for Using Drone Images in Livestock Farms","2024","Agriculture (Switzerland)","14","4","522","","","","1","10.3390/agriculture14040522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191489906&doi=10.3390%2fagriculture14040522&partnerID=40&md5=faaba1da31009fa662c4aa4c4c462ecc","Department of Computer Engineering, San Jose State University, San Jose, 95192, CA, United States; Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China; BRI, Santa Clara, 95050, CA, United States","Gao J., Department of Computer Engineering, San Jose State University, San Jose, 95192, CA, United States, Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; Bambrah C.K., Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; Parihar N., Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; Kshirsagar S., Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; Mallarapu S., Department of Applied Data Science, San Jose State University, San Jose, 95192, CA, United States; Yu H., College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China; Wu J., BRI, Santa Clara, 95050, CA, United States; Yang Y., College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China","With the development of artificial intelligence, the intelligence of agriculture has become a trend. Intelligent monitoring of agricultural activities is an important part of it. However, due to difficulties in achieving a balance between quality and cost, the goal of improving the economic benefits of agricultural activities has not reached the expected level. Farm supervision requires intensive human effort and may not produce satisfactory results. In order to achieve intelligent monitoring of agricultural activities and improve economic benefits, this paper proposes a solution that combines unmanned aerial vehicles (UAVs) with deep learning models. The proposed solution aims to detect and classify objects using UAVs in the agricultural industry, thereby achieving independent agriculture without human intervention. To achieve this, a highly reliable target detection and tracking system is developed using Unmanned Aerial Vehicles. The use of deep learning methods allows the system to effectively solve the target detection and tracking problem. The model utilizes data collected from DJI Mirage 4 unmanned aerial vehicles to detect, track, and classify different types of targets. The performance evaluation of the proposed method shows promising results. By combining UAV technology and deep learning models, this paper provides a cost-effective solution for intelligent monitoring of agricultural activities. The proposed method offers the potential to improve the economic benefits of farming while reducing the need for intensive hum. © 2024 by the authors.","deep learning; DeepSort algorithm; livestock classification; object detection; object tracking","","Shanxi Province Research Foundation for Base Research, China, (202303021221002)","Project supported by the Shanxi Province Research Foundation for Base Research, China (Grant No. 202303021221002).","Y. Yang; College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China; email: yangyunyun@tyut.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85191489906"
"Nakrošis A.; Paulauskaite-Tarasevičiene A.; Gružauskas R.","Nakrošis, Arnas (57960548700); Paulauskaite-Tarasevičiene, Agne (45561472400); Gružauskas, Romas (25226830000)","57960548700; 45561472400; 25226830000","Advancements in AI for Poultry Farming To Ensure Early Detection to Tackle Fallen Bird Incidents","2024","CEUR Workshop Proceedings","3885","","","39","48","9","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214810366&partnerID=40&md5=1e788ab3bddd0c0e73dd6feb5e4d101d","Faculty of Informatics, Kaunas University of Technology, Studentu 50, Kaunas, 51368, Lithuania; Artificial Intelligence Centre, Kaunas University of Technology, K. Barsausko 59, Kaunas, 51423, Lithuania","Nakrošis A., Faculty of Informatics, Kaunas University of Technology, Studentu 50, Kaunas, 51368, Lithuania; Paulauskaite-Tarasevičiene A., Artificial Intelligence Centre, Kaunas University of Technology, K. Barsausko 59, Kaunas, 51423, Lithuania; Gružauskas R., Artificial Intelligence Centre, Kaunas University of Technology, K. Barsausko 59, Kaunas, 51423, Lithuania","This study explores the application of deep learning architectures for image classification and segmentation in poultry farms with overlapping objects. Early detection of fallen birds is crucial for preventing disease outbreaks and maintaining animal welfare. We investigate the efficacy of various architectures, including U-Net, mU-Net, SegNet, and O-Net, for segmenting live and dead birds within poultry farm real time images. Our experiments, conducted on a dataset of 1805 images with varying lighting, distances, and object numbers, reveal that U-Net achieves the highest Dice coefficient (0.95128) for segmentation accuracy. We further demonstrate the potential of these models for classifying individual birds as alive or dead, with U-Net reaching a classification accuracy of 88.938%. The findings suggest that AI-powered image segmentation holds promise for enhancing poultry farm management by enabling early detection of deceased birds and fostering improved animal health and welfare. © 2024 CEUR-WS. All rights reserved.","Computer vision; deep learning; overlapping images; poultry; segmentation","Birds; Animal welfare; Deep learning; Disease outbreaks; Images classification; Images segmentations; Learning architectures; Overlapping images; Poultry farms; Real time images; Segmentation; Livestock","","","A. Nakrošis; Faculty of Informatics, Kaunas University of Technology, Kaunas, Studentu 50, 51368, Lithuania; email: arnas.nakrosis@ktu.lt","Veitaite I.; Lopata A.; Krilavicius T.; Wozniak M.","CEUR-WS","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-85214810366"
"Chauhan S.","Chauhan, Shanvi (59377979900)","59377979900","Leveraging MobileNetV3 for Advanced Grapevine Leaf Classification in Viticulture","2024","Proceedings of the 5th International Conference on Data Intelligence and Cognitive Informatics, ICDICI 2024","","","","627","632","5","0","10.1109/ICDICI62993.2024.10810797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216634740&doi=10.1109%2fICDICI62993.2024.10810797&partnerID=40&md5=94ab9b0308ea49c4ef137b3633f8b8b4","Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India","Chauhan S., Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India","Using artificial vision approaches, we address the significant challenge of grapevine species recognition from leaf photos in this study; this is a necessary task in getting sustainable viticulture by enhancing crop management, disease detection, and production optimisation. Accurate species identification of grapevine ensures food security and supports responsible consumption and production techniques, by means of which it guarantees sustainable agriculture. Leveraging the knowledge acquired from previous tasks, we approach this difficulty using pre-trained MobileNetV3Large architecture and transfer learning to increase grapevine leaf categorisation accuracy. Five grapevine species - ""Buzgulu,""""Ala-Idris,""""Nazli,""""Dimnit,""and ""Ak""- represent 500 photos in our collection. We split the data in 80% for training (400 images) and 20% for validation (100 images) to evaluate the model strictly. This approach produced amazing 96% accuracy in identifying the grapevine species, therefore highlighting the prospects of sophisticated artificial vision technology in support of environmentally friendly farming methods. Particularly in relation to zero hunger, ethical consumption and production, and life on land, this study advances more general objectives of sustainable development. © 2024 IEEE.","Artificial Intelligence; Biodiversity preservation; Deep Learning; Grapevine Leaf Classification; MobileNetV3-Large","Deep learning; Livestock; Mammals; Plant diseases; Transfer learning; Biodiversity preservation; Crop managements; Deep learning; Disease detection; Grapevine leaf classification; Leaf classification; Mobilenetv3-large; Production optimization; Species recognition; Sustainable viticulture; Knowledge acquisition","","","S. Chauhan; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; email: shanvi@chitkara.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835038960-9","","","English","Proc. Int. Conf. Data Intell. Cogn. Informatics, ICDICI","Conference paper","Final","","Scopus","2-s2.0-85216634740"
"Gerdan Koc D.; Koc C.; Polat H.E.; Koc A.","Gerdan Koc, Dilara (57202851355); Koc, Caner (16642988900); Polat, Havva Eylem (55928558800); Koc, Atakan (23008938600)","57202851355; 16642988900; 55928558800; 23008938600","Artificial intelligence-based camel face identification system for sustainable livestock farming","2024","Neural Computing and Applications","36","6","","3107","3124","17","5","10.1007/s00521-023-09238-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177892814&doi=10.1007%2fs00521-023-09238-w&partnerID=40&md5=21b4fb4a46f16230ff47b229fb58fefb","Department of Agricultural Machinery and Technologies Engineering, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; Department of Farm Structure and Irrigation, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; Department of Animal Science, Faculty of Agriculture, Aydın Adnan Menderes University, Aydın, 09100, Turkey","Gerdan Koc D., Department of Agricultural Machinery and Technologies Engineering, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; Koc C., Department of Agricultural Machinery and Technologies Engineering, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; Polat H.E., Department of Farm Structure and Irrigation, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; Koc A., Department of Animal Science, Faculty of Agriculture, Aydın Adnan Menderes University, Aydın, 09100, Turkey","Artificial intelligence and machine learning have recently been applied to improve agricultural and livestock applications. The precise estimation, recommendations, and performances are the main justifications for using technology. The knowledge that can be gained from animal detection and tracking in videos is useful for monitoring body condition, calving processes, behavior analysis, and individual identification. Accurate animal identification is necessary for monitoring animal welfare, disease prevention, vaccination administration, production supply, and ownership management. In this study, a deep learning-based camera tracking system has been built for businesses where animal welfare is a priority. For this purpose, images of camels in their natural habitat were taken in order to create a dataset. The dataset was split into three categories: training, validation, and testing. It contains 19,081 records from 18 different camels. To identify specific camel faces, this study used deep learning algorithms. The EfficientNetV2B0 algorithm had the highest test accuracy, scoring 98.85% with a validation accuracy of 98.53%. The AI for the camel face recognition task has been validated. The usability of AI on the camel face recognition task was successful in terms of recognition accuracy, and it can be used in place of conventional methods. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Camel identification; Deep learning; Face detection; Smart farming; Sustainable livestock farming","Animals; Deep learning; Farms; Learning algorithms; Learning systems; Statistical tests; Animal welfare; Artificial intelligence learning; Camel identification; Deep learning; Face identification; Faces detection; Livestock farming; Machine-learning; Smart farming; Sustainable livestock farming; Face recognition","","","C. Koc; Department of Agricultural Machinery and Technologies Engineering, Faculty of Agriculture, Ankara University, Ankara, 06110, Turkey; email: ckoc@ankara.edu.tr","","Springer Science and Business Media Deutschland GmbH","09410643","","","","English","Neural Comput. Appl.","Article","Final","","Scopus","2-s2.0-85177892814"
"Pann V.; Kwon K.-S.; Kim B.; Jang D.-H.; Kim J.-B.","Pann, Vandet (57736088500); Kwon, Kyeong-Seok (22993228900); Kim, Byeonghyeon (57216963861); Jang, Dong-Hwa (57214314779); Kim, Jong-Bok (57211615603)","57736088500; 22993228900; 57216963861; 57214314779; 57211615603","DCNN for Pig Vocalization and Non-Vocalization Classification: Evaluate Model Robustness with New Data","2024","Animals","14","14","2029","","","","1","10.3390/ani14142029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199476300&doi=10.3390%2fani14142029&partnerID=40&md5=61f731b8f6d042199884471334e77521","Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea","Pann V., Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea; Kwon K.-S., Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea; Kim B., Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea; Jang D.-H., Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea; Kim J.-B., Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea","Since pig vocalization is an important indicator of monitoring pig conditions, pig vocalization detection and recognition using deep learning play a crucial role in the management and welfare of modern pig livestock farming. However, collecting pig sound data for deep learning model training takes time and effort. Acknowledging the challenges of collecting pig sound data for model training, this study introduces a deep convolutional neural network (DCNN) architecture for pig vocalization and non-vocalization classification with a real pig farm dataset. Various audio feature extraction methods were evaluated individually to compare the performance differences, including Mel-frequency cepstral coefficients (MFCC), Mel-spectrogram, Chroma, and Tonnetz. This study proposes a novel feature extraction method called Mixed-MMCT to improve the classification accuracy by integrating MFCC, Mel-spectrogram, Chroma, and Tonnetz features. These feature extraction methods were applied to extract relevant features from the pig sound dataset for input into a deep learning network. For the experiment, three datasets were collected from three actual pig farms: Nias, Gimje, and Jeongeup. Each dataset consists of 4000 WAV files (2000 pig vocalization and 2000 pig non-vocalization) with a duration of three seconds. Various audio data augmentation techniques are utilized in the training set to improve the model performance and generalization, including pitch-shifting, time-shifting, time-stretching, and background-noising. In this study, the performance of the predictive deep learning model was assessed using the k-fold cross-validation (k = 5) technique on each dataset. By conducting rigorous experiments, Mixed-MMCT showed superior accuracy on Nias, Gimje, and Jeongeup, with rates of 99.50%, 99.56%, and 99.67%, respectively. Robustness experiments were performed to prove the effectiveness of the model by using two farm datasets as a training set and a farm as a testing set. The average performance of the Mixed-MMCT in terms of accuracy, precision, recall, and F1-score reached rates of 95.67%, 96.25%, 95.68%, and 95.96%, respectively. All results demonstrate that the proposed Mixed-MMCT feature extraction method outperforms other methods regarding pig vocalization and non-vocalization classification in real pig livestock farming. © 2024 by the authors.","audio classification; audio data augmentation; audio feature extraction; convolutional neural networks (CNNs); deep learning model; environmental animal; machine learning; pig vocalization; smart farming; smart livestock farming","accuracy; amplitude modulation; animal behavior; animal husbandry; Article; auditory network; background noise; chroma feature; convolutional neural network; data base; deep learning; discrete cosine transform; feature extraction; Fourier transform; health care quality; k fold cross validation; livestock; mel frequency cepstral coefficient; mel spectrogram; nonhuman; pig; pitch; predictive model; receiver operating characteristic; recognition; robustness; sensitivity and specificity; sound; support vector machine; tonnetz; training; vocalization; weaning","Rural Development Administration, RDA; National Institute of Animal Science, NIAS","Funding text 1: This work was carried out with the support of \u201CCooperative Research Program for Agriculture Science & Technology Development (Project No. PJ01681003)\u201D Rural Development Administration, Republic of Korea.; Funding text 2: This research was supported by the \u201CRDA Research Associate Fellowship Program\u201D of the National Institute of Animal Science, Rural Development Administration, Republic of Korea. ","J.-B. Kim; Animal Environment Division, National Institute of Animal Science, Rural Development Administration, Wanju, 55365, South Korea; email: jbkimj@korea.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85199476300"
"Akhmad F.K.; Sudianto S.; Aldo D.","Akhmad, Fajar Kamaludin (59505899400); Sudianto, Sudianto (57220103375); Aldo, Dasril (57222627286)","59505899400; 57220103375; 57222627286","Deep Learning for Predicting Food Commodity Prices in Traditional Markets","2024","Proceedings - 2024 2nd International Conference on Technology Innovation and Its Applications, ICTIIA 2024","","","","","","","0","10.1109/ICTIIA61827.2024.10761250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214512822&doi=10.1109%2fICTIIA61827.2024.10761250&partnerID=40&md5=a704a163026964a5b4e785f48ae2d552","Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia","Akhmad F.K., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia; Sudianto S., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia; Aldo D., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia","Banyumas is one region with a history of fluctuating food prices, especially the price of shallots and chicken meat. Several factors affect fluctuating food prices, such as harvests, seasons, market demand, and so on. Farmers or ranchers often experience losses in selling crops and livestock products. This phenomenon occurs due to intermediaries monopolizing prices, and farmers need more knowledge about market prices, so traditional transactions are detrimental. Price forecasting can be one of the practical solutions to help farmers and ranchers understand the prices of their crops and livestock yields. Therefore, Deep Learning is applied to the experience of food commodities with LSTM and GRU. In this study, the LSTM model achieved the highest performance by producing an MSE value of 0.00035, while the GRU model produced an MSE value of 0.00036. LSTM models perform better with layer LSTM, epoch 100, batch size 64, optimization Adam, and neuron 128. © 2024 IEEE.","Commodity; Food; Forecasting; GRU; LSTM; Price","Chicken meat; Commodity; Commodity prices; Food commodity; Food prices; GRU; Harvest season; LSTM; Price; Traditional markets","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835035161-3","","","English","Proc. - Int. Conf. Technol. Innov. Its Appl., ICTIIA","Conference paper","Final","","Scopus","2-s2.0-85214512822"
"Yang X.; Bist R.B.; Paneru B.; Chai L.","Yang, Xiao (57743060600); Bist, Ramesh Bahadur (57866456200); Paneru, Bidur (57216784609); Chai, Lilong (57222280822)","57743060600; 57866456200; 57216784609; 57222280822","Deep Learning Methods for Tracking the Locomotion of Individual Chickens","2024","Animals","14","6","911","","","","6","10.3390/ani14060911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188699824&doi=10.3390%2fani14060911&partnerID=40&md5=f55024ef208bb81b64ed4273f4f212d3","Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, GA, United States","Yang X., Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, GA, United States; Bist R.B., Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, GA, United States; Paneru B., Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, GA, United States; Chai L., Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, GA, United States","Poultry locomotion is an important indicator of animal health, welfare, and productivity. Traditional methodologies such as manual observation or the use of wearable devices encounter significant challenges, including potential stress induction and behavioral alteration in animals. This research introduced an innovative approach that employs an enhanced track anything model (TAM) to track chickens in various experimental settings for locomotion analysis. Utilizing a dataset comprising both dyed and undyed broilers and layers, the TAM model was adapted and rigorously evaluated for its capability in non-intrusively tracking and analyzing poultry movement by intersection over union (mIoU) and the root mean square error (RMSE). The findings underscore TAM’s superior segmentation and tracking capabilities, particularly its exemplary performance against other state-of-the-art models, such as YOLO (you only look once) models of YOLOv5 and YOLOv8, and its high mIoU values (93.12%) across diverse chicken categories. Moreover, the model demonstrated notable accuracy in speed detection, as evidenced by an RMSE value of 0.02 m/s, offering a technologically advanced, consistent, and non-intrusive method for tracking and estimating the locomotion speed of chickens. This research not only substantiates TAM as a potent tool for detailed poultry behavior analysis and monitoring but also illuminates its potential applicability in broader livestock monitoring scenarios, thereby contributing to the enhancement of animal welfare and management in poultry farming through automated, non-intrusive monitoring and analysis. © 2024 by the authors.","animal welfare; deep learning; non-intrusive tracking; poultry locomotion; track anything model","algorithm; animal welfare; Article; behavioral science; bioinformatics; broiler; deep learning; Gallus gallus; human; intersection over union; livestock; locomotion; nonhuman; nonintrusive monitoring; physiological stress; poultry; poultry farming; prediction; reliability; root mean squared error; signal noise ratio; track anything model; training","National Institute of Food and Agriculture, NIFA; Egg Industry Center, EIC; Georgia Research Alliance, GRA; College of Pharmacy, University of Georgia, UGA; USDA-NIFA AFRI CARE, (2023-68008-39853); Fostering Technologies, Metrics, and Behaviors for Sustainable Advances in Animal Agriculture, (S1074)","USDA-NIFA AFRI CARE (2023-68008-39853); Egg Industry Center; Georgia Research Alliance (Venture Fund); UGA COVID Recovery Research Fund; and USDA-NIFA Hatch Multistate projects: Fostering Technologies, Metrics, and Behaviors for Sustainable Advances in Animal Agriculture (S1074).","L. Chai; Department of Poultry Science, College of Agricultural & Environmental Sciences, University of Georgia, Athens, 30602, United States; email: lchai@uga.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85188699824"
"Chemme K.S.; Alitappeh R.J.","Chemme, Komeil Sadeghi (58993442000); Alitappeh, Reza Javanmard (57193091369)","58993442000; 57193091369","An End-to-End Model for Chicken Detection in a Cluttered Environment","2024","Iranian Conference on Machine Vision and Image Processing, MVIP","","","","","","","1","10.1109/MVIP62238.2024.10491186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190798005&doi=10.1109%2fMVIP62238.2024.10491186&partnerID=40&md5=fd8cec6b72d0ac9260e941f65f77ce05","University of Science and Technology of Mazandaran, Electrical and Computer Engineering Department, Behshahr, Iran","Chemme K.S., University of Science and Technology of Mazandaran, Electrical and Computer Engineering Department, Behshahr, Iran; Alitappeh R.J., University of Science and Technology of Mazandaran, Electrical and Computer Engineering Department, Behshahr, Iran","Animals in the livestock business need to be seen in real time and online in order for them to grow and develop. The growing global demand for meat and poultry, the development of the livestock and poultry business, and the expansion of their region have made it more difficult for humans to handle the task due to human mistake and the growing amount of work. But thanks to technological breakthroughs like artificial intelligence, learning models, and closed-circuit cameras, most tasks may now be left to robots, preventing human error. This system may be installed and connected worldwide, allowing it to count all animals at any one time and give us information on each animal's number and location.In this research, we created a model consisting of a segmentation module that readies input data for a detector model based on YOLOv8. In order to study chicken behavior, an integrated end-to-end approach is utilized to track and count the birds. The experimental results provide insight into the new proposal's accuracy performance compared to earlier methods, demonstrating an 8% improvement. The goal of this article is to teach a YOLOv8-based learning model that may be applied to the development of an automated, integrated system for counting and assessing animal welfare in the future.  © 2024 IEEE.","Chicken detection; Deep learning; Object detection; Segmentation","Agriculture; Animals; Deep learning; Intelligent robots; Learning systems; Business needs; Chicken detection; Cluttered environments; Deep learning; End-to-end models; Global demand; Learning models; Objects detection; Real- time; Segmentation; Object detection","","","K.S. Chemme; University of Science and Technology of Mazandaran, Electrical and Computer Engineering Department, Behshahr, Iran; email: komilsadeghicemme@Gmail.com","","IEEE Computer Society","21666776","979-835035049-4","","","English","Iran. Conf. mach. Vis. Image Process., MVIP","Conference paper","Final","","Scopus","2-s2.0-85190798005"
"Eckhardt R.; Arablouei R.; McCosker K.; Bernhardt H.","Eckhardt, Regina (59363289600); Arablouei, Reza (36975104100); McCosker, Kieren (37034436500); Bernhardt, Heinz (55427931000)","59363289600; 36975104100; 37034436500; 55427931000","Modeling Climate Change Impacts on Cattle Behavior Using Generative Artificial Intelligence: A Pathway to Adaptive Livestock Management","2024","2024 ASABE Annual International Meeting","","","","","","","0","10.13031/aim.202400377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206111621&doi=10.13031%2faim.202400377&partnerID=40&md5=f51304177074e5b184750892dcac45ec","Agricultural Systems Engineering, Technical University of Munich, Freising, Germany; CSIRO’s Data61, Pullenvale, QLD, Australia; Queensland Alliance for Agriculture and Food Innovation, University of Queensland, St Lucia, QLD, Australia","Eckhardt R., Agricultural Systems Engineering, Technical University of Munich, Freising, Germany; Arablouei R., CSIRO’s Data61, Pullenvale, QLD, Australia; McCosker K., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, St Lucia, QLD, Australia; Bernhardt H., Agricultural Systems Engineering, Technical University of Munich, Freising, Germany","Amid the increasing challenges of climate change, including rising temperatures, extreme weather events, and erratic precipitation, it is imperative to devise adaptive strategies for livestock exposed to these environmental shifts. This study delves into the innovative use of generative artificial intelligence (genAI) to predict the effects of future climatic scenarios on cattle behavior using wearable sensor data. The pronounced impact of climate-induced heat stress on cattle significantly affects their behavior, health, welfare, and key factors such as performance and reproduction, ultimately influencing the quality and quantity of livestock products. Unlike conventional machine learning models, genAI's distinctive capability to generate synthetic data for anticipated environmental conditions offers insights into cattle behavior under extreme future climates. Our study begins with a comprehensive review of existing research on the impact of climate change on cattle behavior and the use of wearable sensor data, particularly accelerometer data, to predict cattle behavior. We then explore recent weather projections to set the context for our predictive modeling. Building on previous work utilizing genAI for synthetic sensor data generation, we introduce a two-stage approach for modeling the effects of climate change on cattle behavior. First, we create varied climatic scenarios and generate synthetic accelerometer data using genAI. Subsequently, utilizing the synthesized sensor data, we employ an appropriate AI model to predict cattle behavior under forecasted future environmental conditions. This innovative methodology underscores the potential of genAI in advancing predictive livestock management and climate adaptation strategies. By enabling farmers to proactively adapt to and mitigate the effects of climate change, our research represents a significant step forward in agricultural systems engineering. © 2024 ASABE Annual International Meeting. All rights reserved.","accelerometer data; cattle behavior; climate change; deep learning; generative AI; precision agriculture","Accelerometer data; Cattle behavior; Cattles; Climate change impact; Climatic scenarios; Deep learning; Environmental conditions; Generative AI; Precision Agriculture; Sensors data; Accelerometers","","","","","American Society of Agricultural and Biological Engineers","","979-833130221-4","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85206111621"
"Lyu L.; Nidhi M.H.; Guo Z.; He Z.; Flay K.J.; Liu K.","Lyu, Li (58805389800); Nidhi, Mahejabeen Hossain (59344311100); Guo, Zhaojin (58804462600); He, Zheng (58444762800); Flay, Kate Jade (57222560692); Liu, Kai (55823366100)","58805389800; 59344311100; 58804462600; 58444762800; 57222560692; 55823366100","Real-time facial identification of Jersey cows using an edge computing device","2024","11th European Conference on Precision Livestock Farming","","","","720","727","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204961998&partnerID=40&md5=0af11480729aa0e27e8c094167db449d","Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Department of Veterinary Clinical Sciences, City University of HongKong, Hong Kong","Lyu L., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Nidhi M.H., Department of Veterinary Clinical Sciences, City University of HongKong, Hong Kong; Guo Z., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; He Z., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Flay K.J., Department of Veterinary Clinical Sciences, City University of HongKong, Hong Kong; Liu K., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong","Accurate individual identification of cattle is crucial for monitoring their productivity, health, and welfare, particular in the context of precision livestock farming. However, most existing cattle identification methods, such as ear tags and collars, are contact-based and can restrict the animals' movement, causing discomfort or pain. A promising alternative is machine vision, which offers a non-invasive and reliable approach to identify cattle, such as Holstein dairy cows. Nevertheless, in commercial farm settings, identifying a group of cows, particularly those without distinct coat pattern like Jersey cows, remains challenging. In this study, we propose and implement an improved YOLOv5-based method for real-time facial identification of multiple Jersey cows in the feeding area using cameras. Our approach achieves a remarkable accuracy of 95.1% when tested on the NVIDIA Jetson platform, running at 5 FPS. Furthermore, we present an application pipeline designed for the on-farm environment, enabling the monitoring of feed intake of individual cows along with facial identification. Overall, our machine vision and deep learning-based approach provides a non-invasive, non-contact, real-time, and accurate tool for identifying and monitoring each cow. This study lays the foundation for the development of an affordable smart cattle management system on farms, which has significant implications for precision livestock farming and animal welfare. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","Animal Welfare; Cattle; Individual Identification; Machine Vision; Precision Livestock Farming; Remote Monitoring","Animal welfare; Cattles; Edge computing; Facial identification; Individual identification; Jersey cows; Machine-vision; Precision livestock farming; Real- time; Remote monitoring; Machine vision","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204961998"
"Velugoti T.; Dasari S.B.; Sultana S.S.","Velugoti, Tejaswani (59324436500); Dasari, Suresh Babu (57209510018); Sultana, Shaik Sheeba (59324436600)","59324436500; 57209510018; 59324436600","Lumpy Skin Disease Detection Using Deep Learning","2024","2024 IEEE Students Conference on Engineering and Systems: Interdisciplinary Technologies for Sustainable Future, SCES 2024","","","","","","","1","10.1109/SCES61914.2024.10652331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203708779&doi=10.1109%2fSCES61914.2024.10652331&partnerID=40&md5=d89e433f3a338df5a862ed04a7f86d59","Velagapudi Ramakrishna Siddhartha Engineering College, Computer Science and Engineering, Vijayawada, India","Velugoti T., Velagapudi Ramakrishna Siddhartha Engineering College, Computer Science and Engineering, Vijayawada, India; Dasari S.B., Velagapudi Ramakrishna Siddhartha Engineering College, Computer Science and Engineering, Vijayawada, India; Sultana S.S., Velagapudi Ramakrishna Siddhartha Engineering College, Computer Science and Engineering, Vijayawada, India","A viral infection, Lumpy skin disease that mainly impacts cows, represents a substantial risk to both animal well-being and the agricultural industry. A virus belonging to the proxviridae family-the lumpy skin virus, is the cause of it. The Neethling strain induces distinct skin nodules and, in some cases, mild manifestations in internal organs. The consequences of this disease are extensive, ranging from permanent skin damage to adverse effects on essential aspects like milk production, fertility, growth, and, in severe cases, even leading to abortion or fatality. Timely identification is crucial for implementing effective intervention strategies and curbing the spread of the disease within livestock populations. Addressing this pressing challenge, our research introduces a cutting-edge solution employing DenseNet169,to detect lumpy skin illness in cows early and accurately, a resilient convolutional neural network was used. Leveraging deep learning techniques, our approach achieves a remarkable 95.10% accuracy in distinguishing affected animals. This not only offers a proactive avenue for disease management in agriculture but also underscores the potential of advanced technologies in safeguarding the animal welfare and ensuring the sustainability of the industry. © 2024 IEEE.","binary classification; convolutional neural network; densenet169; disease detection; lumpy skin disease","Diseases; Invertebrates; Agricultural industries; Binary classification; Convolutional neural network; Densenet169; Disease detection; Internal organs; Lumpy skin disease; Skin disease; Viral infections; Well being; Livestock","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037471-1","","","English","IEEE Stud. Conf. Eng. Syst.: Interdiscip. Technol. Sustain. Future, SCES","Conference paper","Final","","Scopus","2-s2.0-85203708779"
"Dede S.; Vrochidou E.; Kanakaris V.; Papakostas G.A.","Dede, Sinan (59306347700); Vrochidou, Eleni (53867522300); Kanakaris, Venetis (55014283800); Papakostas, George A. (14060879200)","59306347700; 53867522300; 55014283800; 14060879200","Deep Learning for Cattle Face Identification","2024","Communications in Computer and Information Science","2172 CCIS","","","316","335","19","0","10.1007/978-3-031-66705-3_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202604598&doi=10.1007%2f978-3-031-66705-3_21&partnerID=40&md5=e10b402e1726464b1764779e3402f18d","MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, 65404, Greece; Department of Economics, Democritus University of Thrace, Komotini, 69100, Greece","Dede S., MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, 65404, Greece; Vrochidou E., MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, 65404, Greece; Kanakaris V., Department of Economics, Democritus University of Thrace, Komotini, 69100, Greece; Papakostas G.A., MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, 65404, Greece","Cattle farming plays a crucial role in meeting the increasing nutritional demands of growing populations. Therefore, it is important to have efficient methods of identifying individual cattle for effective livestock management, maintaining the herd’s health, and ensuring farms’ financial sustainability. Traditional identification methods, such as ear tags and branding, are not ideal due to theft issues and discomfort for the animals, while microchips, although accurate, present logistical and ethical challenges. Therefore, novel identification methods are required. This work aims to review all current innovative cattle identification systems, specifically utilizing facial recognition technology based on deep learning. The conducted research identifies and summarizes state-of-the-art approaches for data preprocessing, feature extraction, model training, and testing. Furthermore, a comparative study on the performance metrics of the identified works takes place. Challenges associated with lighting conditions, dataset quality, processing speed, and practical implementation in dynamic farm environments, such as the motion of cattle, are also reported, indicating the need for a unified framework to confront implementation issues. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Animal facial recognition; Cattle face; cattle face identification; Computer vision; Deep learning; Precision livestock farming","Contrastive Learning; Deep learning; Animal facial recognition; Cattle face; Cattle face identification; Cattles; Deep learning; Face identification; Facial recognition; Identification method; Precision livestock farming; Livestock","Department of Informatics, Democritus University of Thrace","This work was supported by the MPhil program \u201CAdvanced Technologies in Informatics and Computers\u201D, which was hosted by the Department of Informatics, Democritus University of Thrace, Kavala, Greece.","G.A. Papakostas; MLV Research Group, Department of Informatics, Democritus University of Thrace, Kavala, 65404, Greece; email: gpapak@cs.duth.gr","Fred A.; Hadjali A.; Gusikhin O.; Sansone C.","Springer Science and Business Media Deutschland GmbH","18650929","978-303166704-6","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85202604598"
"Geetha Rani E.; Chalasani R.; Anusha D.; Dhanalakshmi M.; Vadlamudi S.","Geetha Rani, E. (58153367300); Chalasani, Ramadevi (57211321721); Anusha, D. (59545073500); Dhanalakshmi, M. (59392945100); Vadlamudi, Sandyarani (59393490700)","58153367300; 57211321721; 59545073500; 59392945100; 59393490700","Organic Farming Automation to Revolutionize the Agricultural Industry Than Traditional Farming Practices Using IOT and Technological Development","2024","Lecture Notes in Electrical Engineering","1220 LNEE","","","333","350","17","0","10.1007/978-981-97-4650-7_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208055611&doi=10.1007%2f978-981-97-4650-7_25&partnerID=40&md5=3d674145a6c7cbe7739737d060b64703","Department of Computer Science Engineering, Alliance University, Karnataka, Bengaluru, India; Department of Computer Science Engineering, Sir C R Reddy College of Engineering, Andhra Pradesh, Eluru, India; Department of CSE-Artificial Intelligence and Machine Learning, SRKIT, Andhra Pradesh, Vijayawada, India; Department of Computer Science Engineering, NHCE, Karnataka, Bengaluru, India; Department of Aritficial Intelligence and Machine Learning, New Horizon College of Engineeing, Karnataka, Bengaluru, India","Geetha Rani E., Department of Computer Science Engineering, Alliance University, Karnataka, Bengaluru, India; Chalasani R., Department of Computer Science Engineering, Sir C R Reddy College of Engineering, Andhra Pradesh, Eluru, India; Anusha D., Department of CSE-Artificial Intelligence and Machine Learning, SRKIT, Andhra Pradesh, Vijayawada, India; Dhanalakshmi M., Department of Computer Science Engineering, NHCE, Karnataka, Bengaluru, India; Vadlamudi S., Department of Aritficial Intelligence and Machine Learning, New Horizon College of Engineeing, Karnataka, Bengaluru, India","Agriculture has remained the spine of human sustenance on earth. Since times unknown, humans have learned and developed methods to produce food and other resources for their livelihood. Agriculture has seen large developments, especially in the last century. Scientific methods have accelerated the process of agriculture to a large extent. These developments have improved productivity by leaps and bounds. Though these developments come with a lot of benefits, they also include several demerits that adversely affect the soil, humans, and livestock. To retain and multiply the benefits contributed by this scientific development and to overcome the demerits and challenges faced in today’s agricultural practices, this paper aims at discussing methodologies that can be used to build an agricultural model that is much simpler and more efficient. The model aims at automation using IoT that could lead to minimized labor efforts by almost refraining human intervention in agricultural practices. Machine learning algorithms can be used for weather and soil-based crop detection for better productivity. The downside caused due to the use of synthetic chemicals can be overcome using organic methods. A web portal to facilitate direct producer-to-consumer retail can be deployed to curb the exploitation of farmers by the elimination of middlemen. Hence, the paper aims at developing an agricultural model based on IoT, machine learning, organic farming, and a web portal for direct producer-to-consumer retail. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Automation; Convolutional neural network; Deep learning neural network; Internet of Things (IoT); Machine learning; Organic farming; Producer-to-Consumer retail; Random forest; Soil infertility; Technological development","Deep neural networks; Fertilizers; Portals; Convolutional neural network; Deep learning neural network; Internet of thing; Learning neural networks; Machine-learning; Organic farming; Producer-to-consumer retail; Random forests; Soil infertility; Technological development; Convolutional neural networks","","","E. Geetha Rani; Department of Computer Science Engineering, Alliance University, Bengaluru, Karnataka, India; email: geetha.edupuganti@alliance.edu.in","Suresh S.; Lal S.; Kiran M.S.","Springer Science and Business Media Deutschland GmbH","18761100","978-981974649-1","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85208055611"
"Ray C.; Bakshi S.; Kumar Sa P.; Panda G.","Ray, Camellia (57210290575); Bakshi, Sambit (37048401000); Kumar Sa, Pankaj (57200213261); Panda, Ganapati (7005294702)","57210290575; 37048401000; 57200213261; 7005294702","A Resource-Efficient Deep Learning Approach to Visual-Based Cattle Geographic Origin Prediction","2024","Mobile Networks and Applications","","","","","","","0","10.1007/s11036-024-02350-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196431116&doi=10.1007%2fs11036-024-02350-8&partnerID=40&md5=bbf4759c10321e109fb65f61fe7b9bc2","Department of Computer Science and Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, 769008, India; Retired Professor, Department of Computer Science and Engineering, Indian Institute of Technology Bhubaneswar, Odisha, Bhubaneswar, 752050, India","Ray C., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, 769008, India; Bakshi S., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, 769008, India; Kumar Sa P., Department of Computer Science and Engineering, National Institute of Technology Rourkela, Odisha, Rourkela, 769008, India; Panda G., Retired Professor, Department of Computer Science and Engineering, Indian Institute of Technology Bhubaneswar, Odisha, Bhubaneswar, 752050, India","Customized healthcare for cattle health monitoring is essential, which aims to optimize individual animal health, thereby enhancing productivity, minimizing illness-related risks, and improving overall welfare. Tailoring healthcare practices to individual requirements guarantees that individual animals receive proper attention and intervention, resulting in better health outcomes and sustainable cattle farming practices. In this regard, the manuscript proposes a visual cues-based region prediction methodology to design a customized cattle healthcare system. The proposed automated AI healthcare system uses resource-efficient deep learning-inspired architecture for computer vision applications like performing region-wise classification. The classification mechanism can be used further to identify a cattle and the regions it belongs. Extensive experimentation has been conducted on a redesigned image dataset to identify the best-suited deep-learning framework to perform region classification for livestock, such as cattle. MobileNetV2 outperforms the considered state-of-the-art frameworks by achieving an accuracy of 93% in identifying the regions of the cattle. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Cattle health; Image classification; MobileNetV2; Precision livestock management; Region-wise prediction; Resource-efficient deep learning model","Agriculture; Animals; Classification (of information); Deep learning; Health care; Health risks; Image classification; Learning systems; Cattle health; Healthcare systems; Images classification; Learning approach; Learning models; Mobilenetv2; Precision livestock management; Region-wise prediction; Resource-efficient; Resource-efficient deep learning model; Forecasting","NVIDIA Titan V and Quadro RTX 8000 GPU; NITROAA, (P920)","This research is supported by the project titled \""Deep learning applications for computer vision task\"" funded by NITROAA with support of Lenovo P920 and Dell Inception 7820 workstation and NVIDIA Corporation with support of NVIDIA Titan V and Quadro RTX 8000 GPU. ","S. Bakshi; Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela, Odisha, 769008, India; email: bakshisambit@ieee.org","","Springer","1383469X","","","","English","Mobile Networks Appl","Article","Article in press","","Scopus","2-s2.0-85196431116"
"Witte J.-H.; Heseker P.; Probst J.; Traulsen I.; Kemper N.; Gómez J.M.","Witte, Jan-Hendrik (57667657200); Heseker, Philipp (58906329900); Probst, Jeanette (58906330000); Traulsen, Imke (35410826800); Kemper, Nicole (55941369100); Gómez, Jorge Marx (7402100609)","57667657200; 58906329900; 58906330000; 35410826800; 55941369100; 7402100609","Image-based Tail Posture Monitoring of Pigs","2024","Proceedings of the Annual Hawaii International Conference on System Sciences","","","","7831","7839","8","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199809459&partnerID=40&md5=1ecc65be3e149601f6a0b1c3975cb525","University of Oldenburg, Germany; University of Göttingen, Germany; Hanover Veterinary School Foundation, Germany","Witte J.-H., University of Oldenburg, Germany; Heseker P., University of Göttingen, Germany; Probst J., Hanover Veterinary School Foundation, Germany; Traulsen I., University of Göttingen, Germany; Kemper N., Hanover Veterinary School Foundation, Germany; Gómez J.M., University of Oldenburg, Germany","Tail biting presents a significant challenge in conventional pig farming, impacting animal welfare and farmers' economic viability. This paper introduces a novel approach for image-based tail posture monitoring, a potential early indicator of tail biting outbreaks. Our two-step tail posture detection approach, consisting of an initial pig detection and a subsequent tail posture detection step, shows significant improvements compared to previous methods. To mitigate ambiguity, our pipeline incorporates an EfficientNetV2 image classification model, filtering out lying pigs in the tail posture monitoring process. When applied to video sequences containing tail biting incidents, our method effectively captures the shift in tail posture from predominantly upright to hanging preceding outbreaks. Our findings offer a promising foundation for an early warning system to aid undocked pig husbandry, improve animal welfare, and provide targeted insights for farmers. The proposed approach demonstrates the potential for real-world applications, fostering proactive interventions to mitigate tail biting. © 2024 IEEE Computer Society. All rights reserved.","computer vision; deep learning; pig; precision livestock farming; tail posture detection","Deep learning; Farms; Mammals; Animal welfare; Deep learning; Economic viability; Image-based; Pig; Pig farming; Posture detection; Precision livestock farming; Tail posture detection; Tail-biting; Computer vision","Federal Ministry of Food and Agriculture; BMEL; Federal Office for Agriculture and Food, (28DE109A18)","The project is supported (was supported) by funds of the Federal Ministry of Food and Agriculture (BMEL)based on a decision of the Parliament of the Federal Republic of Germany. The Federal Office for Agriculture and Food (BLE) provides (provided) coordinating support for digitalization in agriculture as funding organization, grantnumber 28DE109A18.","","Bui T.X.","IEEE Computer Society","15301605","978-099813317-1","","","English","Proc. Annu. Hawaii Int. Conf. Syst. Sci.","Conference paper","Final","","Scopus","2-s2.0-85199809459"
"Du X.; Qi Y.; Zhu J.; Li Y.; Liu L.","Du, Xiaoxu (58249424900); Qi, Yongsheng (24403028100); Zhu, Junfeng (58621599700); Li, Yongting (56693212200); Liu, Liqiang (57199083517)","58249424900; 24403028100; 58621599700; 56693212200; 57199083517","Enhanced lightweight deep network for efficient livestock detection in grazing areas","2024","International Journal of Advanced Robotic Systems","21","1","","","","","3","10.1177/17298806231218865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182183648&doi=10.1177%2f17298806231218865&partnerID=40&md5=55c04014b1c217fc3aac203c37a3f346","Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China; Ordos Vocational College, Ordos, China; Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Hohhot, China; Pastoral water conservancy Research Institute of the Ministry of water resources, Hohhot, China","Du X., Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China, Ordos Vocational College, Ordos, China; Qi Y., Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China, Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Hohhot, China; Zhu J., Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China, Pastoral water conservancy Research Institute of the Ministry of water resources, Hohhot, China; Li Y., Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China, Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Hohhot, China; Liu L., Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China, Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Hohhot, China","There are problems in the special pastoral environment, including large changes in target size and serious interference from light and environmental factors. To solve the above problems, an enhanced YOLOv4-tiny target detection network is proposed in this study. This network first solves the problem of livestock size fluctuation in pastoral areas, uses a pyramid network with multiscale feature fusion, and considers shallow local detail features and deep semantic information. Subsequently, a novel compound multichannel attention mechanism is proposed to increase the accuracy of the target detection network for the pastoral environment. The problem of poor accuracy of target detection network is solved. The algorithm is ported to Jetson AGX embedded platform for validation to examine the real-time performance of the algorithm. As revealed by the experimental results, enhanced YOLOv4-tiny achieves 89.77% detection accuracy and 30 frames/second detection speed, which increases the average detection accuracy by 11.67% compared with the conventional YOLOv4-tiny while maintaining almost the same detection rate. © The Author(s) 2024.","attention mechanism; computer vision; deep learning; Object recognition and classification; YOLOv4-tiny","Agriculture; Computer vision; Deep learning; Object detection; Semantics; Attention mechanisms; Deep learning; Detection accuracy; Detection networks; Grazing areas; IS problems; Object classification; Objects recognition; Targets detection; YOLOv4-tiny; Object recognition","Inner Mongolia Key Laboratory of Electromechanical Control, (IMMEC2020001); Inner Mongolia Science and Technology Program, (2021GG164); National Natural Science Foundation of China, NSFC, (61763037); Natural Science Foundation of Inner Mongolia Autonomous Region, (2021MS06018)","The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the National Natural Science Foundation of China (61763037); Inner Mongolia Key Laboratory of Electromechanical Control (IMMEC2020001); Inner Mongolia Science and Technology Program (2021GG164); Inner Mongolia Natural Science Foundation (2021MS06018). ","Y. Qi; Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, China; email: 1136683950@qq.com","","SAGE Publications Inc.","17298806","","","","English","Int. J. Adv. Rob. Syst.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85182183648"
"Michelli L.M.; Sudianto S.; Aldo D.","Michelli, Laureta Mauren (59378147600); Sudianto, Sudianto (57220103375); Aldo, Dasril (57222627286)","59378147600; 57220103375; 57222627286","Prediction of Egg Price in Traditional Markets using Deep Learning","2024","Proceeding - 2024 International Conference on Information Technology Research and Innovation, ICITRI 2024","","","","293","298","5","0","10.1109/ICITRI62858.2024.10698950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207069817&doi=10.1109%2fICITRI62858.2024.10698950&partnerID=40&md5=b110df1e460e6004ae5f3a50886b631e","Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia","Michelli L.M., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia; Sudianto S., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia; Aldo D., Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia","The agricultural sector, especially livestock, contributes significantly to the Indonesian economy. One of the many livestock products produced is chicken eggs. The problem for farmers is the uncertainty of selling prices, which exploitation by intermediaries causes. So, in solving this problem, information in the form of a prediction of the price of chicken eggs is needed to help farmers know the price before they sell them to the market. Predictions are made using deep learning algorithms, namely Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which are evaluated using Root Mean Squared Error (RMSE), Mean Squared Error (MSE), Mean Absolute Error (MAE), and R Square to determine the performance of the two algorithms in predicting prices. This study uses data from April 2017 to December 2023 from the National Strategic Food Price Information Center (PIHPSN). Based on the experiments that have been carried out, the algorithm that is more dominant in making predictions is the GRU algorithm with the evaluation metric values of RMSE, MSE, MAE, and R Square in the Manis market, respectively of 0.034621,0.001199,0.022617 and 0.952266. At the same time, the metric values in the market are 0.036574,0.001338,0.023991, and 0.944691. With this research, the GRU algorithm is more dominant than LSTM.  © 2024 IEEE.","Chicken Egg; GRU; LSTM; Prediction; Price","Agricultural economics; Deep learning; Mean square error; Chicken eggs; Gated recurrent unit; Mean absolute error; Mean squared error; Metric values; Price; R square; Root mean squared errors; Short term memory; Traditional markets","","","L.M. Michelli; Institut Teknologi Telkom Purwokerto, Department of Informatics, Purwokerto, Indonesia; email: 20102002@ittelkom-pwt.ac.id","","Institute of Electrical and Electronics Engineers Inc.","","979-835037621-0","","","English","Proceeding - Int. Conf. Inf. Technol. Res. Innov., ICITRI","Conference paper","Final","","Scopus","2-s2.0-85207069817"
"Penn M.J.; Miles V.; Astley K.L.; Ham C.; Woodroffe R.; Rowcliffe M.; Donnelly C.A.","Penn, Matthew J. (57453729000); Miles, Verity (57222285345); Astley, Kelly L. (57211447872); Ham, Cally (57191518090); Woodroffe, Rosie (36894338600); Rowcliffe, Marcus (6701682562); Donnelly, Christl A. (57216108885)","57453729000; 57222285345; 57211447872; 57191518090; 36894338600; 6701682562; 57216108885","Sherlock—A flexible, low-resource tool for processing camera-trapping images","2024","Methods in Ecology and Evolution","15","1","","91","102","11","3","10.1111/2041-210X.14254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178345499&doi=10.1111%2f2041-210X.14254&partnerID=40&md5=fa2879e6ce60212d6fd82bb23bce75a8","Department of Statistics, University of Oxford, Oxford, United Kingdom; Institute of Zoology, London, United Kingdom; Pandemic Sciences Institute, University of Oxford, Oxford, United Kingdom","Penn M.J., Department of Statistics, University of Oxford, Oxford, United Kingdom; Miles V., Institute of Zoology, London, United Kingdom; Astley K.L., Institute of Zoology, London, United Kingdom; Ham C., Institute of Zoology, London, United Kingdom; Woodroffe R., Institute of Zoology, London, United Kingdom; Rowcliffe M., Institute of Zoology, London, United Kingdom; Donnelly C.A., Department of Statistics, University of Oxford, Oxford, United Kingdom, Pandemic Sciences Institute, University of Oxford, Oxford, United Kingdom","The use of camera traps to study wildlife has increased markedly in the last two decades. Camera surveys typically produce large data sets which require processing to isolate images containing the species of interest. This is time consuming and costly, particularly if there are many empty images that can result from false triggers. Computer vision technology can assist with data processing, but existing artificial intelligence algorithms are limited by the requirement of a training data set, which itself can be challenging to acquire. Furthermore, deep-learning methods often require powerful hardware and proficient coding skills. We present Sherlock, a novel algorithm that can reduce the time required to process camera trap data by removing a large number of unwanted images. The code is adaptable, simple to use and requires minimal processing power. We tested Sherlock on 240,596 camera trap images collected from 46 cameras placed in a range of habitats on farms in Cornwall, United Kingdom, and set the parameters to find European badgers (Meles meles). The algorithm correctly classified 91.9% of badger images and removed 49.3% of the unwanted ‘empty’ images. When testing model parameters, we found that faster processing times were achieved by reducing both the number of sampled pixels and ‘bouncing’ attempts (the number of paths explored to identify a disturbance), with minimal implications for model sensitivity and specificity. When Sherlock was tested on two sites which contained no livestock in their images, its performance greatly improved and it removed 92.3% of the empty images. Although further refinements may improve its performance, Sherlock is currently an accessible, simple and useful tool for processing camera trap data. © 2023 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society.","camera-trapping; image classification","","Cornwall Wildlife Trust, (NE/L002515/1); Engineering and Physical Sciences Research Council, EPSRC; Natural Environment Research Council, NERC; Imperial College London, ICL, (NE/S007415/1); Imperial College London, ICL; University of Oxford","Matthew J. Penn's work on this paper was funded by a EPSRC DTP studentship, awarded by the University of Oxford to fund his DPhil in Statistics. Verity Miles' work on this paper was funded by the NERC Science and Solutions for a Changing Planet Doctoral Training Programme, Imperial College London, grant number NE/S007415/1. Cally Ham's work was funded by the Cornwall Wildlife Trust and the NERC Science and Solutions for a Changing Planet Doctoral Training Programme, Imperial College London, grant number NE/L002515/1. The funders had no role in the design of this study, its execution, analyses, interpretation of the data or the decision to submit results. ","M.J. Penn; Department of Statistics, University of Oxford, Oxford, United Kingdom; email: matthew.penn@st-annes.ox.ac.uk","","British Ecological Society","2041210X","","","","English","Methods Ecol. Evol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85178345499"
"Xu X.","Xu, Xiaoqing (57402249900)","57402249900","Research on Remote Monitoring Model of Livestock Health Based on Computer Deep Learning","2024","2024 IEEE 2nd International Conference on Image Processing and Computer Applications, ICIPCA 2024","","","","565","569","4","0","10.1109/ICIPCA61593.2024.10709243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207913827&doi=10.1109%2fICIPCA61593.2024.10709243&partnerID=40&md5=fa235cbe76a30ac05942579ba3fd542b","School of Information Engineering, Eurasia University, Shaanxi, Xi'an, China","Xu X., School of Information Engineering, Eurasia University, Shaanxi, Xi'an, China","A livestock individual status information acquisition device is established and integrated with a visual virtual simulation application. The serial port transmission completes the work of transmitting to the MT7688 system. In this paper, a wireless LAN based on 4 GEC204G is proposed, which realizes real-time transmission to 4G network by Socket. The identification of animals is obtained through personal identification of animal images on the cloud platform. The animal ID was obtained from the image data through the individual recognition model, and the health indicator information was evaluated to obtain the health status of the livestock. VGGNet-16 and ResNet-50 are used to learn ImageNet respectively, and then they are adjusted according to the selected samples, and finally the model is fused by means method. Remote access to the database was obtained with the authorization of the user account and password. The data collected by the sensor is viewed and analyzed and processed on the client page. This study provides a useful basis for regulating the feeding environment, monitoring the movement state and health status of livestock.  © 2024 IEEE.","intelligent farming; Internet of Things; STM32; virtual simulation","Virtual environments; Acquisition device; Health status; Information acquisitions; Intelligent farming; Monitoring models; Remote monitoring; Simulation applications; Status informations; STM32; Virtual simulations; Electronic health record","","","X. Xu; School of Information Engineering, Eurasia University, Xi'an, Shaanxi, China; email: xuxiaoqing@eurasia.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835036024-0","","","English","IEEE Int. Conf. Image Process. Comput. Appl., ICIPCA","Conference paper","Final","","Scopus","2-s2.0-85207913827"
"Hu W.-C.; Chen L.-B.; Yu P.-J.; Wu M.-Y.; Chen K.-H.; Huang X.-R.","Hu, Wu-Chih (7404359750); Chen, Liang-Bi (8266743400); Yu, Po-Ju (58565567600); Wu, Ming-Yuan (58566086200); Chen, Kai-Hung (58566086300); Huang, Xiang-Rui (57417385600)","7404359750; 8266743400; 58565567600; 58566086200; 58566086300; 57417385600","A Deep Learning-Based Underwater Image Enhancement Scheme for Turbid Underwater Aquaculture Environments","2024","Proceedings of the IEEE International Conference on Computer Communication and the Internet, ICCCI","","2024","","107","112","5","1","10.1109/ICCCI62159.2024.10674548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208457420&doi=10.1109%2fICCCI62159.2024.10674548&partnerID=40&md5=b9f6c1b5032e6bcc7f675da586b5f24d","National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan","Hu W.-C., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; Chen L.-B., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; Yu P.-J., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; Wu M.-Y., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; Chen K.-H., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; Huang X.-R., National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan","In outdoor aquaculture farms, farmers are often unable to observe the situation in aquaculture tanks due to poor water quality, which may cause them to fail to detect problems and take timely measures, especially for typical benthic aquatics such as white shrimp. For example, when the harmful substances in the water exceed the normal range, the farmer may not treat them quickly, and the white shrimp will grow slowly or die. Therefore, for underwater water quality image enhancement, this paper proposes an underwater image enhancement scheme for turbid underwater aquaculture environments based on deep learning. This scheme can effectively improve underwater images' clarity, thereby improving the effect of underwater monitoring. Through this technology, farmers can more clearly observe the conditions in aquaculture ponds, improving aquaculture efficiency and promoting the development of the aquaculture industry.  © 2024 IEEE.","deep learning; intelligent aquaculture; turbid underwater; underwater; underwater image enhancement; white shrimp","Livestock; Shellfish; Underwater imaging; Underwater photography; Deep learning; Harmful substances; Image clarity; Intelligent aquaculture; Quality image; Turbid underwater; Underwater; Underwater image enhancements; Underwater monitoring; White shrimps; Aquaculture","Ministry of Science and Technology, Taiwan, MOST, (110-2221-E-346-003-MY3, 109-2221-E-346-003); Ministry of Science and Technology, Taiwan, MOST","This work was supported in part by the Ministry of Science and Technology (MOST), Taiwan, under the following grants: MOST 110-2221-E-346-003-MY3 and MOST 109-2221-E-346-003.","L.-B. Chen; National Penghu University of Science and Technology, Department of Computer Science and Information Engineering, Penghu, Taiwan; email: liangbichen@gms.npu.edu.tw","","Institute of Electrical and Electronics Engineers Inc.","28332342","","","","English","Proc. IEEE Int. Conf. Comput. Commun. Internet, ICCCI","Conference paper","Final","","Scopus","2-s2.0-85208457420"
"Gomes A.L.B.; Fernandes A.M.R.; Horta B.A.C.; de Oliveira M.F.","Gomes, Ana L. B. (59000371900); Fernandes, Anita M. R. (58562869000); Horta, Bruno A. C. (8605965900); de Oliveira, Maurílio F. (7202819850)","59000371900; 58562869000; 8605965900; 7202819850","Machine learning algorithms applied to weed management in integrated crop-livestock systems: a systematic literature review","2024","Advances in Weed Science","42","","e020240047","","","","0","10.51694/AdvWeedSci/2024;42:00004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191255783&doi=10.51694%2fAdvWeedSci%2f2024%3b42%3a00004&partnerID=40&md5=93040fa9b490c351d60a98f74e4b5b78","University of Vale do Itajaí, SC, Itajaí, Brazil; Embrapa Maize and Sorghum, MG, Sete Lagoas, Brazil","Gomes A.L.B., University of Vale do Itajaí, SC, Itajaí, Brazil; Fernandes A.M.R., University of Vale do Itajaí, SC, Itajaí, Brazil; Horta B.A.C., University of Vale do Itajaí, SC, Itajaí, Brazil; de Oliveira M.F., Embrapa Maize and Sorghum, MG, Sete Lagoas, Brazil","In recent times, there has been an environmental pressure to reduce the amount of pesticides applied to crops and, consequently, the crop production costs. Therefore, investments have been made in technologies that could potentially reduce the usage of herbicides on weeds. Among such technologies, Machine Learning approaches are rising in number of applications and potential impact. Therefore, this article aims to identify the main machine learning algorithms used in integrated crop-livestock systems for weed management. Based on a systematic literature review, it was possible to determine where the selected studies were performed and which crop types were mostly used. The main research terms in this study were: “machine learning algorithms” + “weed management” + “integrated crop-livestock system”. Although no results were found for the three terms altogether, the combinations involving “weed management” + “integrated crop-livestock system” and “machine learning algorithms” + “weed management” returned a significant number of studies which were subjected to a second layer of refinement by applying an eligibility criteria. The achieved results show that most of the studies were from the United States and from nations in Asia. Machine vision and deep learning were the most used machine learning models, representing 28% and 19% of all cases, respectively. These systems were applied to different practical solutions, the most prevalent being smart sprayers, which allow for a sitespecific herbicide application. © 2024, Sociedade Brasileira da Ciencia das Plantas Daninha. All rights reserved.","Artificial Intelligence; Image processing; Weed control; Weed prevention","","Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq; Universidade do Vale do Itajaí, UNIVALI, (200/2022, 54/2022); Universidade do Vale do Itajaí, UNIVALI; Ministério da Ciência, Tecnologia e Inovação, MCTI, (Nº 10/2023, 404755/2023-2); Ministério da Ciência, Tecnologia e Inovação, MCTI","This research was funded by Univali (University of Vale do Itaja\u00ED), notice 200/2022 and the article processing charge was funded by Fapesc (Foundation for Support to Research and Innovation of the State of Santa Catarina), projects from public call Fapesc No 54/2022, and CNPq (National Council for Scientific and Technological Development), project from call CNPq/MCTI N\u00BA 10/2023, track A, universal notice, process N\u00B0 404755/2023-2.","A.L.B. Gomes; University of Vale do Itajaí, Itajaí, SC, Brazil; email: gomes.ana@edu.univali.br","","Sociedade Brasileira da Ciencia das Plantas Daninha","26759462","","","","English","Adv. Weed Sci.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85191255783"
"El Moutaouakil K.; Falih N.","El Moutaouakil, Khalid (57721360300); Falih, Noureddine (57205694849)","57721360300; 57205694849","A comparative study on time series data-based artificial intelligence approaches for classifying cattle feeding behavior","2024","Indonesian Journal of Electrical Engineering and Computer Science","33","1","","324","332","8","3","10.11591/ijeecs.v33.i1.pp324-332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184587275&doi=10.11591%2fijeecs.v33.i1.pp324-332&partnerID=40&md5=e332433c365dd28d49ddc4d33b1b2999","LIMATI Laboratory, Department of Computer Science, Faculty of Polydisciplinary, University of Sultan Moulay Slimane, Beni Mellal, Morocco","El Moutaouakil K., LIMATI Laboratory, Department of Computer Science, Faculty of Polydisciplinary, University of Sultan Moulay Slimane, Beni Mellal, Morocco; Falih N., LIMATI Laboratory, Department of Computer Science, Faculty of Polydisciplinary, University of Sultan Moulay Slimane, Beni Mellal, Morocco","Cattle feeding behavior analysis is crucial for optimizing livestock management practices and ensuring animal well-being. This study presents a comparative analysis of three models: two machine learning algorithms including random forest and support vector machine (SVM), in addition to a deep learning convolutional neural networks (CNN) model, for classifying cattle feeding behaviors (eating, ruminating, and other) using time series data generated from a 3-axis accelerometer. The results of this study highlight the performance of these methods in accurately categorizing cattle feeding behaviors and demonstrate the importance of precise and efficient livestock monitoring and contributing to the improvement of animal well-being and enhancing the overall effectiveness of livestock operations. © 2024 Institute of Advanced Engineering and Science. All rights reserved.","Cattle behavior; Machine learning; Precision agriculture; Smart farming; Time series data","","","","K. El Moutaouakil; LIMATI Laboratory, Department of Computer Science, Faculty of Polydisciplinary, University of Sultan Moulay Slimane, Beni Mellal, Morocco; email: elmoutaouakil.kh@gmail.com","","Institute of Advanced Engineering and Science","25024752","","","","English","Indones. J. Electrical Eng. Comput. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85184587275"
"Arıkan İ.; Ayav T.; Seçkin A.Ç.; Soygazi F.","Arıkan, İbrahim (58781072000); Ayav, Tolga (13408184500); Seçkin, Ahmet Çağdaş (57103461800); Soygazi, Fatih (57220960947)","58781072000; 13408184500; 57103461800; 57220960947","Estrus Detection and Dairy Cow Identification with Cascade Deep Learning for Augmented Reality-Ready Livestock Farming","2023","Sensors","23","24","9795","","","","6","10.3390/s23249795","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180616159&doi=10.3390%2fs23249795&partnerID=40&md5=86c8306405d4b5806a6ff46deb83089e","Computer Engineering Department, İzmir Institute of Technology, Izmir, 35430, Turkey; Computer Engineering Department, Aydın Adnan Menderes University, Aydın, 09100, Turkey","Arıkan İ., Computer Engineering Department, İzmir Institute of Technology, Izmir, 35430, Turkey; Ayav T., Computer Engineering Department, İzmir Institute of Technology, Izmir, 35430, Turkey; Seçkin A.Ç., Computer Engineering Department, Aydın Adnan Menderes University, Aydın, 09100, Turkey; Soygazi F., Computer Engineering Department, Aydın Adnan Menderes University, Aydın, 09100, Turkey","Accurate prediction of the estrus period is crucial for optimizing insemination efficiency and reducing costs in animal husbandry, a vital sector for global food production. Precise estrus period determination is essential to avoid economic losses, such as milk production reductions, delayed calf births, and disqualification from government support. The proposed method integrates estrus period detection with cow identification using augmented reality (AR). It initiates deep learning-based mounting detection, followed by identifying the mounting region of interest (ROI) using YOLOv5. The ROI is then cropped with padding, and cow ID detection is executed using YOLOv5 on the cropped ROI. The system subsequently records the identified cow IDs. The proposed system accurately detects mounting behavior with 99% accuracy, identifies the ROI where mounting occurs with 98% accuracy, and detects the mounting couple with 94% accuracy. The high success of all operations with the proposed system demonstrates its potential contribution to AR and artificial intelligence applications in livestock farming. © 2023 by the authors.","artificial intelligence; augmented reality; dairy cow identification; deep learning; estrus detection; image processing; livestock; precision livestock farming; transfer learning","Animals; Artificial Intelligence; Augmented Reality; Cattle; Dairying; Deep Learning; Estrus Detection; Female; Livestock; Milk; Augmented reality; Dairies; Deep learning; Farms; Image segmentation; Losses; Dairy cow; Dairy cow identification; Deep learning; Estrus detection; Images processing; Livestock; Precision livestock farming; Region-of-interest; Regions of interest; Transfer learning; animal; artificial intelligence; augmented reality; bovine; dairying; deep learning; estrus; female; livestock; milk; procedures; Mountings","","","F. Soygazi; Computer Engineering Department, Aydın Adnan Menderes University, Aydın, 09100, Turkey; email: fatih.soygazi@adu.edu.tr","","Multidisciplinary Digital Publishing Institute (MDPI)","14248220","","","38139641","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180616159"
"King K.; Thornton C.; Roopaei M.","King, Kyle (59223347900); Thornton, Caleb (58743720300); Roopaei, Mehdi (14026018100)","59223347900; 58743720300; 14026018100","Smart Feeding: Integrating Deep Learning into Dairy Farm Practices","2024","2024 IEEE 5th World AI IoT Congress, AIIoT 2024","","","","151","156","5","0","10.1109/AIIoT61789.2024.10579035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198855303&doi=10.1109%2fAIIoT61789.2024.10579035&partnerID=40&md5=baa1c6a4d2482bf48e1e70e89da95712","University of Wisconsin-Platteville, Electrical and Computer Engineering Department, Platteville, United States","King K., University of Wisconsin-Platteville, Electrical and Computer Engineering Department, Platteville, United States; Thornton C., University of Wisconsin-Platteville, Electrical and Computer Engineering Department, Platteville, United States; Roopaei M., University of Wisconsin-Platteville, Electrical and Computer Engineering Department, Platteville, United States","This paper introduces a novel approach to precision cow feeding forecasting through the application of machine vision and deep learning technologies. The research addresses the critical need for efficient and accurate monitoring of dairy cow feeding behaviors, essential for optimizing milk production and ensuring animal welfare. Leveraging advanced image processing algorithms and the YOLOv8 model, the system achieves real-time, non-invasive tracking of feeding times, providing farmers with actionable insights to adjust feed schedules and compositions. By accurately quantifying feed intake and detecting deviations in feeding patterns, the proposed solution aids in early detection of health issues, potentially saving significant costs associated with livestock diseases. Furthermore, this work contributes to sustainable dairy farming practices by enhancing feed utilization efficiency. The implementation showcases the scalable application of computer vision in livestock management, demonstrating significant promise for improving the productivity and sustainability of dairy operations. © 2024 IEEE.","Computer Vision for Animal Monitoring; Dairy Cow Feeding Behavior; Machine Vision in Agriculture; Precision Livestock Farming; Sustainable Dairy Farming Practices","Computer vision; Dairies; Deep learning; Farms; Feeding; Computer vision for animal monitoring; Cow feeding; Dairy cow; Dairy cow feeding behavior; Dairy farming; Farming practices; Feeding behavior; Machine vision in agriculture; Machine-vision; Precision livestock farming; Sustainable dairy farming practice; Animals","","","K. King; University of Wisconsin-Platteville, Electrical and Computer Engineering Department, Platteville, United States; email: kingky@uwplatt.edu","Paul R.; Kundu A.; Bhattacharyya R.","Institute of Electrical and Electronics Engineers Inc.","","979-835038780-3","","","English","IEEE World AI IoT Congr., AIIoT","Conference paper","Final","","Scopus","2-s2.0-85198855303"
"Chae J.-W.; Sim H.-S.; Lee C.-W.; Choi C.-S.; Cho H.-C.","Chae, Jung-Woo (57215821871); Sim, Hyeon-Seok (58951794100); Lee, Chang-Woo (58951957800); Choi, Chang-Sik (58951957900); Cho, Hyun-Chong (22233514800)","57215821871; 58951794100; 58951957800; 58951957900; 22233514800","Video-Based Analysis of Cattle Behaviors: Improved Classification Using FlowEQ Transform","2024","IEEE Access","12","","","42860","42867","7","1","10.1109/ACCESS.2024.3379277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188416270&doi=10.1109%2fACCESS.2024.3379277&partnerID=40&md5=98c9e5f6ccb263f847847b759d529170","Kangwon National University, Department Graduate Program for Bit Medical Convergence, Chuncheon-si, 24341, South Korea; Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; Kangwon National University, Department of Electronics Engineering, Chuncheon-si, 24341, South Korea","Chae J.-W., Kangwon National University, Department Graduate Program for Bit Medical Convergence, Chuncheon-si, 24341, South Korea; Sim H.-S., Kangwon National University, Department Graduate Program for Bit Medical Convergence, Chuncheon-si, 24341, South Korea; Lee C.-W., Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; Choi C.-S., Gangwon State Livestock Research Institute, Hoengseong-gun, 25266, South Korea; Cho H.-C., Kangwon National University, Department Graduate Program for Bit Medical Convergence, Chuncheon-si, 24341, South Korea, Kangwon National University, Department of Electronics Engineering, Chuncheon-si, 24341, South Korea","Cattle management plays a crucial role in determining the productivity of livestock farms. With the expansion of large-scale livestock operations, it has become increasingly impractical for livestock managers to rely on traditional visual observations for comprehensive monitoring of cattle behaviors, encompassing health and overall welfare. Consequently, the incorporation of automation technology in livestock management is emphasized. The objective of this study is the video-based identification of cattle behavior that can be utilized in automated cattle management systems. With a specific focus on behaviors closely associated with their management, the study employs deep learning-based action classification methods over the commonly used object detection. This approach enables the classification of intricate, repetitive, and slow behaviors that were challenging to detect. Furthermore, a novel method named FlowEQ transform was introduced, incorporating temporal information into the input data. This enhancement proved instrumental in providing valuable insights for inferring cattle behavior, resulting in an impressive 8% improvement in classification performance and achieving a high accuracy rate of 91.5%. The utilization of action classification and the introduction of the innovative FlowEQ transform mark a significant advancement in automated cattle management. This approach is poised to enhance the efficiency of behavior monitoring on livestock farms.  © 2013 IEEE.","Action classification; automation technology; cattle behavior; cattle management; deep learning; FlowEQ transform","Automation; Deep learning; Farms; Learning algorithms; Object detection; Action classifications; Automation technology; Cattle behavior; Cattle management; Classification algorithm; Cow; Deep learning; Floweq transform; Large-scales; Objects recognition; Object recognition","","","H.-C. Cho; Kangwon National University, Department Graduate Program for Bit Medical Convergence, Chuncheon-si, 24341, South Korea; email: hyuncho@kangwon.ac.kr","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188416270"
"Retta S.; Srinivasan R.; Tan S.","Retta, Sivaji (58775430400); Srinivasan, Ramarajulu (58775461300); Tan, Shawn (58775494000)","58775430400; 58775461300; 58775494000","CattleDeSegNet: A Joint Approach to Cattle Denoising and Interpretable Segmentation","2024","Proceedings of SPIE - The International Society for Optical Engineering","13072","","130720A","","","","1","10.1117/12.3023338","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191662809&doi=10.1117%2f12.3023338&partnerID=40&md5=3563fb413f5d5d3f29a60f51e2adbdd4","AnimalEYEQ, Australia","Retta S., AnimalEYEQ, Australia; Srinivasan R., AnimalEYEQ, Australia; Tan S., AnimalEYEQ, Australia","In modern agriculture, livestock monitoring plays a vital role in ensuring animal health, welfare, and production efficiency. Leveraging computer vision and deep learning, this paper presents an innovative framework aimed at enhancing livestock monitoring. Specifically, we address two crucial challenges: denoising and segmentation of cattle in livestock images. The denoising task is fundamental in preprocessing noisy images affected by adverse environmental conditions and equipment limitations. To tackle this, we introduce an encoder-decoder model that effectively denoises cattle images while preserving critical anatomical details. Our framework incorporates a segmentation module inspired by the U-Net architecture. Notably, both denoising and segmentation tasks share a common encoder, optimizing computational efficiency. The segmentation model employs hybrid loss functions and leverages the Grad-CAM technique to provide interpretable insights into the decision-making process. Our approach stands as one of the pioneering joint solutions for cattle denoising and segmentation, particularly focusing on top-view cattle images. © 2024 SPIE. All rights reserved.","Boundary loss; Cattle; Denoising; Grad-CAM; Segmentation; SSIM","Cams; Computational efficiency; Computer vision; Decision making; Deep learning; Image denoising; Image enhancement; Image segmentation; Signal encoding; Animal health; Animal production; Animal welfare; Boundary loss; Cattle; De-noising; Grad-CAM; Modern agricultures; Segmentation; SSIM; Production efficiency","","","","Osten W.","SPIE","0277786X","978-151067462-2","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85191662809"
"Pillewan M.; Rachalwar O.; Wyawahare N.; Wazalwar S.; Kapse S.","Pillewan, Mayur (58182119300); Rachalwar, Om (59374971500); Wyawahare, Nikhil (56538589000); Wazalwar, Sampada (57215426317); Kapse, Smita (56958429900)","58182119300; 59374971500; 56538589000; 57215426317; 56958429900","Real Time Detection of Predators near the Domestic Animal's Shelter Based on the Artificial Intelligence (AI) and Internet of Things (IoT)","2024","2024 OPJU International Technology Conference on Smart Computing for Innovation and Advancement in Industry 4.0, OTCON 2024","","","","","","","0","10.1109/OTCON60325.2024.10688160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206936052&doi=10.1109%2fOTCON60325.2024.10688160&partnerID=40&md5=33b305ea899671e457f45b3437f6579f","G H Rasioni University Amravati, Department of Cse, Nagpur, India; G H Rasioni Institute of Engineering and Technology, CoE-IoT, Department of Artificial Intelligence, Nagpur, India; Embedded IoT, G H Raisoni College of Engineering, Department of Dic, Nagpur, India; Information Security, G H Raisoni College of Engineering, Nagpur, India; Yeshwantrao Chavan College of Engineering, Computer Technology Deperment, Nagpur, India","Pillewan M., G H Rasioni University Amravati, Department of Cse, Nagpur, India; Rachalwar O., G H Rasioni Institute of Engineering and Technology, CoE-IoT, Department of Artificial Intelligence, Nagpur, India; Wyawahare N., Embedded IoT, G H Raisoni College of Engineering, Department of Dic, Nagpur, India; Wazalwar S., Information Security, G H Raisoni College of Engineering, Nagpur, India; Kapse S., Yeshwantrao Chavan College of Engineering, Computer Technology Deperment, Nagpur, India","India is an agricultural country where farmers grow various types of crops on their agricultural lands using organic fertilizers. These fertilizers are provided by farm animals such as cows, bulls, goats, sheep, and many more. These animals also provide daily-need products like milk, fertilizer, and raw materials to industries, and help farmers improve their financial conditions. Due to these reasons, farmers raise these animals and take care of them. However, these animals face various problems in animal shelters, including wild animal attacks that kill them. To protect domestic animals from wild animal attacks, farmers use different methods like fencing, guard animals, and safe shelters, as well as eliminating food sources and making the property less attractive to predators. But despite these efforts, wild animal attacks on domestic animals still happen. Approximately 1000 domestic animals are killed by wild animal attacks every year. Therefore, a proper solution is required to address this problem by using artificial intelligence and IoT technology. This paper proposes a model to solve this problem using AI and IoT. The model detects wild animals near animal shelters and sends this information to farmers' smartphones. The model is deployed on Raspberry Pi and IoT sensors. Deep learning algorithms are used for wild animal detection and were developed using TensorFlow Lite. To alert the farmer in case any wild animals are detected near the animal shelters, Twilio API messages are sent to their mobile phones.  © 2024 IEEE.","Deep Learning; EfficientDet lite model; Internet of Things (IOT); Raspberry pi 4 module; TensorFlow Lite; Twilio","Dairy products; Deep learning; Fertilizers; Invertebrates; Mammals; Shelters (from attack); Smart agriculture; Smartphones; Agricultural land; Deep learning; Domestic animals; Efficientdet lite model; Internet of thing; Raspberry pi 4 module; Real-time detection; Tensorflow lite; Twilio; Wild animals; Livestock","","","M. Pillewan; G H Rasioni University Amravati, Department of Cse, Nagpur, India; email: mayur.pillewan@ghru.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835037378-3","","","English","OPJU Int. Technol. Conf. Smart Comput. Innov. Adv. Ind. 4.0, OTCON","Conference paper","Final","","Scopus","2-s2.0-85206936052"
"Giannone C.; Bovo M.; Ceccarelli M.; Benni S.; Tassinari P.; Torreggiani D.","Giannone, C. (57768432800); Bovo, M. (55504159800); Ceccarelli, M. (57224689530); Benni, S. (24334072400); Tassinari, P. (13609849700); Torreggiani, D. (24336908000)","57768432800; 55504159800; 57224689530; 24334072400; 13609849700; 24336908000","Real time identification of individual dairy cows through computer vision","2024","11th European Conference on Precision Livestock Farming","","","","452","458","6","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205015696&partnerID=40&md5=55fea53756010134980384be3ac98bca","Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy","Giannone C., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Bovo M., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Ceccarelli M., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Benni S., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Tassinari P., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Torreggiani D., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy","In livestock management, computer vision enables automated systems to interpret and understand visual information in animal environments. By drawing on methodologies from image processing and machine learning, computer vision facilitates the analysis, interpretation, and extraction of valuable information from visual data. This paper explores the application of a deep learning-based computer vision system for the identification of individual dairy cows. The initial phase involves training a YOLO neural network, focusing on recognizing cows based on unique features like coat patterns. Subsequently, this trained network is applied to a dataset collected in a different position of the barn to verify the capability of network to recognize the individual cow in different contexts. Performance evaluation of the network involves the use of precision-recall curves, coupled with the application of data augmentation techniques to enrich the dataset and optimize detection efficiency. This process provides insights into the system's generalizability and robustness across various contexts within the agricultural landscape. The findings not only contribute to advancing precision livestock farming but also illuminate the adaptability of the developed model across different farm locations and scenarios, particularly in recognizing cows collected in various positions. The application of the technology tested here can provide real-time recognition and the results of the paper shows the valuable insights into the monitoring of behaviours and movements of individual cows in a barn. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","computer vision; dairy cow; herd management; PLF; real-time recognition","Farm buildings; Machine vision; Automated systems; Dairy cow; Herd management; Identification of individuals; Images processing; Machine-learning; PLF; Real time recognition; Real-time identification; Visual information; Information management","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85205015696"
"Bhargavi T.; Sumathi D.","Bhargavi, T. (58153470400); Sumathi, D. (57216289324)","58153470400; 57216289324","Early detection of abiotic stress in plants through SNARE proteins using hybrid feature fusion model","2024","PeerJ Computer Science","10","","e2149","","","","0","10.7717/PEERJ-CS.2149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201905310&doi=10.7717%2fPEERJ-CS.2149&partnerID=40&md5=1e43fd93bfe2b6397a4276d0b51d31c9","School of Computer Science and Engineering, VIT-AP University, Andhra Pradesh, Amaravati, India","Bhargavi T., School of Computer Science and Engineering, VIT-AP University, Andhra Pradesh, Amaravati, India; Sumathi D., School of Computer Science and Engineering, VIT-AP University, Andhra Pradesh, Amaravati, India","Agriculture is the main source of livelihood for most of the population across the globe. Plants are often considered life savers for humanity, having evolved complex adaptations to cope with adverse environmental conditions. Protecting agricultural produce from devastating conditions such as stress is essential for the sustainable development of the nation. Plants respond to various environmental stressors such as drought, salinity, heat, cold, etc. Abiotic stress can significantly impact crop yield and development posing a major threat to agriculture. SNARE proteins play a major role in pathological processes as they are vital proteins in the life sciences. These proteins act as key players in stress responses. Feature extraction is essential for visualizing the underlying structure of the SNARE proteins in analyzing the root cause of abiotic stress in plants. To address this issue, we developed a hybrid model to capture the hidden structures of the SNAREs. A feature fusion technique has been devised by combining the potential strengths of convolutional neural networks (CNN) with a high dimensional radial basis function (RBF) network. Additionally, we employ a bi-directional long short-term memory (Bi-LSTM) network to classify the presence of SNARE proteins. Our feature fusion model successfully identified abiotic stress in plants with an accuracy of 74.6%. When compared with various existing frameworks, our model demonstrates superior classification results. © 2024 T and D","Abiotic stress; Agriculture; Bi-LSTM; CNN; Deep learning; Feature fusion; SNARE proteins","Livestock; Plant diseases; Radial basis function networks; Abiotic stress; Bi-directional; Bi-directional long short-term memory; Convolutional neural network; Deep learning; Features fusions; Fusion model; Hybrid features; Short term memory; SNARE proteins; Abiotic","","","D. Sumathi; School of Computer Science and Engineering, VIT-AP University, Amaravati, Andhra Pradesh, India; email: sumathi.d@vitap.ac.in","","PeerJ Inc.","23765992","","","","English","PeerJ Comput. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85201905310"
"Ozella L.; Magliola A.; Vernengo S.; Ghigo M.; Bartoli F.; Grangetto M.; Forte C.; Montrucchio G.; Giacobini M.","Ozella, Laura (35786395000); Magliola, Alessandro (59393202300); Vernengo, Simone (59393202400); Ghigo, Marco (59393617600); Bartoli, Francesco (57878513800); Grangetto, Marco (6701519103); Forte, Claudio (56432548200); Montrucchio, Gianluca (59393202500); Giacobini, Mario (57202734442)","35786395000; 59393202300; 59393202400; 59393617600; 57878513800; 6701519103; 56432548200; 59393202500; 57202734442","A computer vision approach for the automatic detection of social interactions of dairy cows in automatic milking systems","2024","Acta IMEKO","13","3","","","","","1","10.21014/actaimeko.v13i3.1628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206187602&doi=10.21014%2factaimeko.v13i3.1628&partnerID=40&md5=ecac6db8e3366ff19ab2979f58ef3419","Department of Veterinary Sciences, University of Turin, Largo Paolo Braccini 2, TO, Grugliasco, 10095, Italy; ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Department of Computer Science, University of Turin, Corso Svizzera 185, Turin, 10149, Italy","Ozella L., Department of Veterinary Sciences, University of Turin, Largo Paolo Braccini 2, TO, Grugliasco, 10095, Italy; Magliola A., ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Vernengo S., ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Ghigo M., ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Bartoli F., ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Grangetto M., Department of Computer Science, University of Turin, Corso Svizzera 185, Turin, 10149, Italy; Forte C., Department of Veterinary Sciences, University of Turin, Largo Paolo Braccini 2, TO, Grugliasco, 10095, Italy; Montrucchio G., ALTEN Italia, Via Pio VII 127, Turin, 10127, Italy; Giacobini M., Department of Veterinary Sciences, University of Turin, Largo Paolo Braccini 2, TO, Grugliasco, 10095, Italy","The integration of digital technologies and Artificial Intelligence (DT&AI) in veterinary practice is one of the key topics to improve Herd Health Management (HHM). The HHM includes the prevention of diseases, the assessment of the welfare, and the sustainability production of farm animals. In dairy cattle farming, particular attention is paid to automatic cow detection and tracking, as such information is closely related to animal welfare and thus to possible health issues. Cows are highly social animals; therefore, a better comprehension of social context can help improve their management and welfare. In the field of Precision Livestock Farming, computer vision represents a suitable and non-invasive method for automatic cow detection and tracking. In this study, we developed and tested the reliability of a deep learning-based computer vision system for the automatic recognition of dairy cows in a barn equipped with Automatic Milking System. We aimed to build the social network of 240 dairy cows (primiparous and multiparous) to understand how social interactions can influence their welfare and productivity. © 2024 International Measurement Confederation (IMEKO). All rights reserved.","Animal Social Networks; Automatic Milking System; Computer Vision System; dairy cows; social interactions","","Circular Health for Industry; Compagnia di San Paolo, CSP","This work is part of the research project Circular Health for Industry (CH4I) funded by Fondazione Compagnia di San Paolo. This work is part of the research project \u201CCircular Health for Industry\u201D funded by Fondazione Compagnia di San Paolo. The authors wish to thank Davide Vanzetti and colleagues for the precious collaboration.","L. Ozella; Department of Veterinary Sciences, University of Turin, Grugliasco, Largo Paolo Braccini 2, TO, 10095, Italy; email: laura.ozella@unito.it","","International Measurement Confederation (IMEKO)","0237028X","","","","English","Acta IMEKO","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85206187602"
"Devi S.J.; Doley J.; Bharati J.; Mohan N.H.; Gupta V.K.","Devi, Salam Jayachitra (56530223100); Doley, Juwar (54398435000); Bharati, Jaya (56922278200); Mohan, N.H. (36725086600); Gupta, Vivek Kumar (58957197100)","56530223100; 54398435000; 56922278200; 36725086600; 58957197100","Analysis of pig posture detection in group-housed pigs using deep learning-based mask scoring instance segmentation","2024","Animal Science Journal","95","1","e13975","","","","0","10.1111/asj.13975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198525968&doi=10.1111%2fasj.13975&partnerID=40&md5=010c57ba6ef1fcaa7ad438486e052544","ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India","Devi S.J., ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India; Doley J., ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India; Bharati J., ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India; Mohan N.H., ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India; Gupta V.K., ICAR-National Research Centre on Pig, Rani, Assam, Guwahati, India","Pig posture is closely linked with livestock health and welfare. There has been significant interest among researchers in using deep learning techniques for pig posture detection. However, this task is challenging due to variations in image angles and times, as well as the presence of multiple pigs in a single image. In this study, we explore an object detection and segmentation algorithm based on instance segmentation scoring to detect different pig postures (sternal lying, lateral lying, walking, and sitting) and segment pig areas in group images, thereby enabling the identification of individual pig postures within a group. The algorithm combines a residual network with 50 layers and a feature pyramid network to extract feature maps from input images. These feature maps are then used to generate regions of interest (RoI) using a region candidate network. For each RoI, the algorithm performs regression to determine the location, classification, and segmentation of each pig posture. To address challenges such as missing targets and error detections among overlapping pigs in group housing, non-maximum suppression (NMS) is used with a threshold of 0.7. Through extensive hyperparameter analysis, a learning rate of 0.01, a batch size of 512, and 4 images per batch offer superior performance, with accuracy surpassing 96%. Similarly, the mean average precision (mAP) exceeds 83% for object detection and instance segmentation under these settings. Additionally, we compare the method with the faster R-CNN object detection model. Further, execution times on different processing units considering various hyperparameters and iterations have been analyzed. © 2024 Japanese Society of Animal Science.","artificial intelligence; group-housed pig; instance segmentation; pig posture","Algorithms; Animals; Deep Learning; Housing, Animal; Image Processing, Computer-Assisted; Posture; Swine; algorithm; animal; animal housing; body position; deep learning; image processing; pig; procedures","","","S.J. Devi; ICAR-National Research Centre on Pig, Guwahati, Rani, Assam, 781131, India; email: salamjayachitradevi@gmail.com","","John Wiley and Sons Inc","13443941","","","39005155","English","Anim. Sci. J.","Article","Final","","Scopus","2-s2.0-85198525968"
"Sun H.; Palaoag T.D.; Quan Q.","Sun, Hanqing (57506731800); Palaoag, Thelma D. (56904198800); Quan, Qingle (57497383800)","57506731800; 56904198800; 57497383800","Design of Pig Inventory and Abnormality Monitoring System Based on Livestock Internet of Things Orbital Inspection Robot","2024","Proceedings - 2024 5th International Conference on Machine Learning and Human-Computer Interaction, MLHMI 2024","","","","17","21","4","0","10.1109/MLHMI63000.2024.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200587292&doi=10.1109%2fMLHMI63000.2024.00012&partnerID=40&md5=3d13060356b5d5a87c00e6ad9790117e","College of Information Technology and Computer Science, University of the Cordilleras, Baguio City, Philippines","Sun H., College of Information Technology and Computer Science, University of the Cordilleras, Baguio City, Philippines; Palaoag T.D., College of Information Technology and Computer Science, University of the Cordilleras, Baguio City, Philippines; Quan Q., College of Information Technology and Computer Science, University of the Cordilleras, Baguio City, Philippines","Pig inventory is an important task in intensive asset management of pig farms. Accurate quantity can realize precise feeding, help reduce farming cost and improve farming efficiency. The realization of pig inventory through computer vision algorithms has become the main development trend of intelligent pig inventory methods [1]. At present, pig inventory mostly adopts manual inventory, which is costly, inefficient and prone to errors. Some of the existing large pig farms use camera-based pig inventory, but they are based on fixed cameras, and cannot complete the inventory of the entire pig farm. Meanwhile, sometimes pigs will jump out of the pen and can't eat, which is very dangerous especially for piglets. Design a track inspection robot carrying a camera to complete the inspection of pig farms, according to the pen information on the pig data collection, this project collected 2000 valid pictures to form a dataset, through the YOLOv5 target detection algorithm to train it, learn a training model, in order to improve the accuracy of inventory, enhancement processing of the photos, through the deep learning model training to obtain the optimal model and deployed, and finally Data correction is performed by Kalman filtering method to improve the inventory accuracy, and this project is carried out on-site experiments in pig farms of Zhengzhou Xinrong Agricultural and Animal Husbandry Information Co. The experimental results show that the overall accuracy of pig inventory is 95.6%. Compared with manual inventory, this method has low cost and good application prospect. When pigs are found in the channel or non-pen labeling, it is an abnormal situation, and the relevant personnel will be notified in time to deal with it, so as to ensure the safety of farming. © 2024 IEEE.","abnormality monitoring; livestock inspection robot; pig inventory; YOLOV5","Agricultural robots; Cameras; Costs; Deep learning; Farms; Image enhancement; Information filtering; Internet of things; Machine design; Mammals; Abnormality monitoring; Assets management; Computer vision algorithms; Inspection robots; Livestock inspection robot; Monitoring system; Orbitals; Pig farms; Pig inventory; YOLOV5; Inspection","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037576-3","","","English","Proc. - Int. Conf. Mach. Learn. Hum.-Comput. Interact., MLHMI","Conference paper","Final","","Scopus","2-s2.0-85200587292"
"Witte J.-H.; Heseker P.; Probst J.; Kemper N.; Traulsen I.; Gómez J.M.","Witte, J.-H. (57667657200); Heseker, P. (58906329900); Probst, J. (58906330000); Kemper, N. (55941369100); Traulsen, I. (35410826800); Gómez, J. Marx (7402100609)","57667657200; 58906329900; 58906330000; 55941369100; 35410826800; 7402100609","Tail Posture as a Predictor of Tail Biting in Pigs: A Camera-Based Monitoring System","2024","11th European Conference on Precision Livestock Farming","","","","341","348","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205012194&partnerID=40&md5=9e14b54446039d00925114e33d221445","Department Very Large Business Applications, Carl von Ossietzky University Oldenburg, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany; Institute of Animal Hygiene, Animal Welfare and Farm Animal Behaviour, University of Veterinary Medicine Hanover, Bischofsholer Damm 15, Hannover, 30173, Germany; Institute of Animal Breeding and Husbandry, Christian-Albrechts University Kiel, Hermann-Rodewald-Straße 6, Kiel, 24118, Germany","Witte J.-H., Department Very Large Business Applications, Carl von Ossietzky University Oldenburg, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany; Heseker P., Institute of Animal Hygiene, Animal Welfare and Farm Animal Behaviour, University of Veterinary Medicine Hanover, Bischofsholer Damm 15, Hannover, 30173, Germany; Probst J., Institute of Animal Hygiene, Animal Welfare and Farm Animal Behaviour, University of Veterinary Medicine Hanover, Bischofsholer Damm 15, Hannover, 30173, Germany; Kemper N., Institute of Animal Hygiene, Animal Welfare and Farm Animal Behaviour, University of Veterinary Medicine Hanover, Bischofsholer Damm 15, Hannover, 30173, Germany; Traulsen I., Institute of Animal Breeding and Husbandry, Christian-Albrechts University Kiel, Hermann-Rodewald-Straße 6, Kiel, 24118, Germany; Gómez J.M., Department Very Large Business Applications, Carl von Ossietzky University Oldenburg, Ammerländer Heerstraße 114-118, Oldenburg, 26129, Germany","This study presents a novel camera-based monitoring pipeline for analyzing pig tail posture, a crucial indicator of tail biting risk in pig livestock farming. Utilizing a multi-step approach, the system first employs a YOLOv8 model for pig detection, followed by an EfficientNetV2 model to classify pigs into 'lying' and 'not lying' postures. This classification aids in focusing on relevant pigs for tail posture analysis by filtering out ambiguous tail postures of 'lying' pigs from the monitoring process. Subsequently, another YOLOv8 model detects upright or hanging tail postures on the pig detections classified as 'not lying', enhancing the accuracy of the monitoring process. The effectiveness of this pipeline is evaluated based on video recordings of seven piglet rearing batches across three different pens, including 14 pens with observed tail biting incidents and 7 control groups. Retrospectively, the system could have provided early alerts for impending tail biting outbreaks at least one day in advance in 61.5% of cases and indicated a 41.5% reduction in upright tail postures within seven days leading up to an outbreak. This study demonstrates significant promise in improving animal welfare and operational efficiency in farms. By providing a proactive tool for assessing tail biting risks, this system could reduce the reliance on traditional practices like tail docking, paving the way for more ethical and sustainable farming methods. With exception for the video data used for evaluating, all of the applied models, datasets, as well as the monitoring pipeline outputs and its implementation code are publicly available here. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","computer vision; deep learning; precision livestock farming; tail biting","Pipeline codes; Video recording; Camera-based; Classifieds; Deep learning; Livestock farming; Monitoring process; Monitoring system; Multi-step approaches; Posture analysis; Precision livestock farming; Tail-biting; Deep learning","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85205012194"
"Saini A.; Guleria K.; Sharma S.","Saini, Archana (58259865900); Guleria, Kalpna (57201090704); Sharma, Shagun (56727001500)","58259865900; 57201090704; 56727001500","VGG16-Based MaizeLeafNet Model for an Efficient Multiclass Classification of Maize Leaf Diseases","2024","2nd International Conference on Intelligent Data Communication Technologies and Internet of Things, IDCIoT 2024","","","","1068","1073","5","4","10.1109/IDCIoT59759.2024.10467965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190144046&doi=10.1109%2fIDCIoT59759.2024.10467965&partnerID=40&md5=c5c75cb36f751847aa17bbe0d936d973","Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India","Saini A., Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Guleria K., Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India; Sharma S., Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India","Maize holds substantial importance as a cereal crop in India, serving both as a staple food for human consumption and as fodder for livestock. The presence of diseases that affect maize leaves poses a significant threat, capable of causing substantial reductions in crop yield and quality. The timely detection and effective management of these diseases are imperative for ensuring the preservation of agricultural productivity. India is a country with a sizable and expanding population, and a major portion of its food comes from maize. A steady and safe supply of food for the populace is ensured by keeping maize crops free from diseases. Crops that are resistant to disease are crucial for providing for the dietary needs of an expanding population. Hence, in the proposed work, a deep learning-based VGG16 model called MaizeLeafNet has been proposed, for the early prediction of the maize leaf diseases. It is well known for having powerful feature-learning capabilities. The collection of a large dataset featuring a range of images of diseased maize leaves ensures a representation of the challenges faced in real-world farming scenarios. Measures like accuracy and loss are used to carefully evaluate the model's performance in multiclass classification. Through the use of visualization techniques, the model's interpretability is enhanced, improving understanding of disease patterns and providing new perspectives on the features that have been learned. Developing crop disease classification frameworks that use state-of-the-art neural network designs, such as VGG16 is a significant step towards creating resilient and sustainable farming methods. This article compares the loss and accuracy at different epochs in the training and validation stages. At the epoch value of 10, the accuracy of 90.34% and the loss of 0.4226 have been identified during validation, whereas the accuracy of 98.79% and the loss of 0.0380 resulted during training. These outcomes show that as the epoch increases to 20, the training accuracy and testing accuracy has been increased to 99.68% and 91.55%, respectively, whereas, the training and validation loss also decreased to 0.0039 and 0.3727, respectively.  © 2024 IEEE.","classification; Convolutional Neural Network; Corn; deep learning; diseases; image processing; leaf; Maize; Visual Geometry Group16","Convolutional neural networks; Crops; Deep learning; Farms; Image classification; Large datasets; Cereal crop; Convolutional neural network; Corn; Deep learning; Images processing; Leaf; Leaf disease; Maize; Multi-class classification; Visual geometry group16; Classification (of information)","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835032753-3","","","English","Int. Conf. Intell. Data Commun. Technol. Internet Things, IDCIoT","Conference paper","Final","","Scopus","2-s2.0-85190144046"
"Yan Q.; Fruzangohar M.; Taylor J.; Gong D.; Walter J.; Norman A.; Shi J.Q.; Coram T.","Yan, Qingsen (56603603600); Fruzangohar, Mario (55620642800); Taylor, Julian (7405404566); Gong, Dong (56014009700); Walter, James (57197834277); Norman, Adam (57195591645); Shi, Javen Qinfeng (24829507300); Coram, Tristan (58876787900)","56603603600; 55620642800; 7405404566; 56014009700; 57197834277; 57195591645; 24829507300; 58876787900","Improved genomic prediction using machine learning with Variational Bayesian sparsity","2023","Plant Methods","19","1","96","","","","2","10.1186/s13007-023-01073-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169686182&doi=10.1186%2fs13007-023-01073-3&partnerID=40&md5=718633116bdd38f1f100c8b686f00493","Australian Institute for Machine Learning, University of Adelaide, Adelaide, Australia; School of Food, Agriculture and Wine, University of Adelaide, Adelaide, Australia; School of Computer Science and Engineering, The University of New South Wales, Sydney, Australia; Australian Grains Technologies, Roseworthy, Australia; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","Yan Q., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Fruzangohar M., School of Food, Agriculture and Wine, University of Adelaide, Adelaide, Australia; Taylor J., School of Food, Agriculture and Wine, University of Adelaide, Adelaide, Australia; Gong D., School of Computer Science and Engineering, The University of New South Wales, Sydney, Australia; Walter J., Australian Grains Technologies, Roseworthy, Australia; Norman A., Australian Grains Technologies, Roseworthy, Australia; Shi J.Q., Australian Institute for Machine Learning, University of Adelaide, Adelaide, Australia; Coram T., Australian Grains Technologies, Roseworthy, Australia","Background: Genomic prediction has become a powerful modelling tool for assessing line performance in plant and livestock breeding programmes. Among the genomic prediction modelling approaches, linear based models have proven to provide accurate predictions even when the number of genetic markers exceeds the number of data samples. However, breeding programmes are now compiling data from large numbers of lines and test environments for analyses, rendering these approaches computationally prohibitive. Machine learning (ML) now offers a solution to this problem through the construction of fully connected deep learning architectures and high parallelisation of the predictive task. However, the fully connected nature of these architectures immediately generates an over-parameterisation of the network that needs addressing for efficient and accurate predictions. Results: In this research we explore the use of an ML architecture governed by variational Bayesian sparsity in its initial layers that we have called VBS-ML. The use of VBS-ML provides a mechanism for feature selection of important markers linked to the trait, immediately reducing the network over-parameterisation. Selected markers then propagate to the remaining fully connected feed-forward components of the ML network to form the final genomic prediction. We illustrated the approach with four large Australian wheat breeding data sets that range from 2665 lines to 10375 lines genotyped across a large set of markers. For all data sets, the use of the VBS-ML architecture improved genomic prediction accuracy over legacy linear based modelling approaches. Conclusions: An ML architecture governed under a variational Bayesian paradigm was shown to improve genomic prediction accuracy over legacy modelling approaches. This VBS-ML approach can be used to dramatically decrease the parameter burden on the network and provide a computationally feasible approach for improving genomic prediction conducted with large breeding population numbers and genetic markers. © 2023, BioMed Central Ltd., part of Springer Nature.","Bayesian; Feature selection; Genomic prediction; Linear mixed models; Machine learning; Variational inference","","Australian Machine Learning Institute; Grains Research Development Council, (AGT9177537); School of Agriculture, Food and Wine, University of Adelaide","Funding text 1: The authors gratefully acknowledge the Grains Research Development Council (GRDC) for their funding and support of this research through the grant AGT9177537. ; Funding text 2: The authors gratefully acknowledge members of the Biometry Hub in the School of Agriculture, Food and Wine for various minor contributions to the manuscript. We also would like to thank members of the Australian Machine Learning Institute for the initial engagements in this research.","J. Taylor; School of Food, Agriculture and Wine, University of Adelaide, Adelaide, Australia; email: julian.taylor@adelaide.edu.au","","BioMed Central Ltd","17464811","","","","English","Plant Methods","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85169686182"
"El Moutaouakil K.; Falih N.","El Moutaouakil, Khalid (57721360300); Falih, Noureddine (57205694849)","57721360300; 57205694849","A New Approach to Animal Behavior Classification using Recurrent Neural Networks","2024","2024 4th International Conference on Innovative Research in Applied Science, Engineering and Technology, IRASET 2024","","","","","","","0","10.1109/IRASET60544.2024.10549544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197109243&doi=10.1109%2fIRASET60544.2024.10549544&partnerID=40&md5=d12d34500daf3d7afa10091323b19acb","Sultan Moulay Slimane University, Limati Laboratory, Polydisciplinary Faculty, Beni Mellal, Morocco","El Moutaouakil K., Sultan Moulay Slimane University, Limati Laboratory, Polydisciplinary Faculty, Beni Mellal, Morocco; Falih N., Sultan Moulay Slimane University, Limati Laboratory, Polydisciplinary Faculty, Beni Mellal, Morocco","The behavior of cattle holds valuable insights into their health, well-being, and productivity. Accurately classifying their diverse actions has become increasingly important in modern precision livestock farming. This study explores the efficiency of recurrent neural networks (RNNs) in classifying cow behavior using time series data from tri-axial accelerometer sensors. The goal is to detect the complex patterns embedded in the cattle motion data, providing an accurate understanding of their behavior. The proposed methodology uses the inherent sequential nature of time series data to distinguish and categorize diverse behaviors of cows. The performance of the proposed model is evaluated on a comprehensive dataset capturing various behaviors, including feeding, resting, ruminating, moving, salting, and other behaviors. The results demonstrate the model's ability to accurately classify cow behavior with high precision and recall, highlighting the potential of RNNs for automated behavior monitoring in cattle farms. This opens doors for improved animal welfare, better productivity, optimized management practices, and enhanced decision-making in the livestock industry.  © 2024 IEEE.","Artificial intelligence; deep learning; livestock farming; precision agriculture; smart farming","Animals; Classification (of information); Crops; Decision making; Farms; Precision agriculture; Time series; Animal behaviour; Behaviour classification; Cow behavior; Deep learning; Livestock farming; New approaches; Precision Agriculture; Smart farming; Time-series data; Well being; Recurrent neural networks","","","","Benhala B.; Raihani A.; Qbadou M.","Institute of Electrical and Electronics Engineers Inc.","","979-835030950-8","","","English","Int. Conf. Innov. Res. Appl. Sci., Eng. Technol., IRASET","Conference paper","Final","","Scopus","2-s2.0-85197109243"
"Hensel S.; Marinov M.B.; Panter R.","Hensel, Stefan (25654786800); Marinov, Marin B. (57201575387); Panter, Raphael (58783053200)","25654786800; 57201575387; 58783053200","Design and Implementation of a Camera-Based Tracking System for MAV Using Deep Learning Algorithms","2023","Computation","11","12","244","","","","2","10.3390/computation11120244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180653676&doi=10.3390%2fcomputation11120244&partnerID=40&md5=5e47831e59336a790de896068271599a","Department for Electrical Engineering, University of Applied Sciences Offenburg, Offenburg, 77652, Germany; Department of Electronics, Technical University of Sofia, Sofia, 1756, Bulgaria","Hensel S., Department for Electrical Engineering, University of Applied Sciences Offenburg, Offenburg, 77652, Germany; Marinov M.B., Department of Electronics, Technical University of Sofia, Sofia, 1756, Bulgaria; Panter R., Department for Electrical Engineering, University of Applied Sciences Offenburg, Offenburg, 77652, Germany","In recent years, the advancement of micro-aerial vehicles has been rapid, leading to their widespread utilization across various domains due to their adaptability and efficiency. This research paper focuses on the development of a camera-based tracking system specifically designed for low-cost drones. The primary objective of this study is to build up a system capable of detecting objects and locating them on a map in real time. Detection and positioning are achieved solely through the utilization of the drone’s camera and sensors. To accomplish this goal, several deep learning algorithms are assessed and adopted because of their suitability with the system. Object detection is based upon a single-shot detector architecture chosen for maximum computation speed, and the tracking is based upon the combination of deep neural-network-based features combined with an efficient sorting strategy. Subsequently, the developed system is evaluated using diverse metrics to determine its performance for detection and tracking. To further validate the approach, the system is employed in the real world to show its possible deployment. For this, two distinct scenarios were chosen to adjust the algorithms and system setup: a search and rescue scenario with user interaction and precise geolocalization of missing objects, and a livestock control scenario, showing the capability of surveying individual members and keeping track of number and area. The results demonstrate that the system is capable of operating in real time, and the evaluation verifies that the implemented system enables precise and reliable determination of detected object positions. The ablation studies prove that object identification through small variations in phenotypes is feasible with our approach. © 2023 by the authors.","convolutional neural networks; deep learning algorithms; MAV; object identification; tracking systems","","Bulgarian National Science Fund, BNSF, (KΠ-06-H42/1); Technical University of Sofia, TU - Sofia","Funding text 1: The authors would like to thank the Research and Development Sector of the Technical University of Sofia for its financial support. ; Funding text 2: This research is supported by the Bulgarian National Science Fund in the scope of the project “Exploration of the application of statistics and machine learning in electronics” under contract number KΠ-06-H42/1.","M.B. Marinov; Department of Electronics, Technical University of Sofia, Sofia, 1756, Bulgaria; email: mbm@tu-sofia.bg","","Multidisciplinary Digital Publishing Institute (MDPI)","20793197","","","","English","Computation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180653676"
"Pacheco V.M.; Brown-Brandl T.; Martello L.S.; de Sousa R.V.; Sharma S.; Rohrer G.","Pacheco, V. Madeira (57218882298); Brown-Brandl, T. (57207606316); Martello, L. Silva (6602090108); de Sousa, R. Vieira (57197806074); Sharma, S. (58030507200); Rohrer, G. (7102792700)","57218882298; 57207606316; 6602090108; 57197806074; 58030507200; 7102792700","Evaluation of alternative farrowing pen layouts","2024","11th European Conference on Precision Livestock Farming","","","","1142","1149","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204998972&partnerID=40&md5=67db0057b5700e74c25ebb25ca3d0534","Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street., Lincoln, NE, United States; Department of Biosystems Engineering, University of Sao Paulo, Av. Duque de Caxias Norte, 225. Pirassununga, São Paulo, Brazil; United States Department of Agriculture (USDA), Agricultural Research Service US Meat Animal Research Center, Clay Center, Nebraska, United States","Pacheco V.M., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street., Lincoln, NE, United States; Brown-Brandl T., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street., Lincoln, NE, United States; Martello L.S., Department of Biosystems Engineering, University of Sao Paulo, Av. Duque de Caxias Norte, 225. Pirassununga, São Paulo, Brazil; de Sousa R.V., Department of Biosystems Engineering, University of Sao Paulo, Av. Duque de Caxias Norte, 225. Pirassununga, São Paulo, Brazil; Sharma S., Department of Biosystems Engineering, University of Nebraska-Lincoln, 1400 R Street., Lincoln, NE, United States; Rohrer G., United States Department of Agriculture (USDA), Agricultural Research Service US Meat Animal Research Center, Clay Center, Nebraska, United States","Preweaning mortality in swine production is a worldwide concern, with mortality ranging from 5-35%. There are different types of farrowing pen; however, there is little research to compare the layouts. The study evaluated 3 farrowing pen layouts, with the same overall dimensions (2.7 x 1.8 m and 2.1 x 0.6 m for the sow area). The objectives were to assess changes in preweaning mortality and sow posture. Treatments involved three different positions of the sow crate (standard, diagonal, and offset). Piglet performance traits (stillborns, mortality, overlays, and average daily weight gain) were monitored; depth images were collected every 3 seconds (3 days before to 5 days after farrowing). A total of 651 sows (approximately 20 sows/period), were monitored (Jan-June/2021). Sows (parities 1-4) were randomly distributed into treatments. No difference in preweaning mortality was observed across treatments for the entire population. However, high-mortality sows (> 2 piglets) had fewer overlays in the offset than in the traditional farrowing pen. Sow postures were determined on days -3, 0, and 5 of lactation using a deep learning model (accuracy = 98.3%). Pen layout only affected the percentage overlays when evaluating high-mortality sows (losses > 2 piglets), with offset pens presenting lower values than the standard. When investigating pen layouts' impact on sow behavior, all postures were affected. Sows in offset and diagonal treatments spent more time in lateral recumbency than in the standard. It was concluded that pen layout may become more important in sow populations with higher mortality and pen layout can improve sow welfare by positioning crates further from heat lamps. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","animal welfare; Image analysis; piglet performance; pre-weaning mortality; sow behaviour","Mammals; Animal welfare; Depth image; Image analyze; Image-analysis; Performance; Piglet performance; Pre-weaning mortality; Sow behavior; Swine production; Weight gain; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204998972"
"Zhang X.; Xuan C.; Ma Y.; Liu H.; Xue J.","Zhang, Xiwen (57222511917); Xuan, Chuanzhong (36618394100); Ma, Yanhua (55570493100); Liu, Haiyang (57201367944); Xue, Jing (57194387128)","57222511917; 36618394100; 55570493100; 57201367944; 57194387128","Lightweight model-based sheep face recognition via face image recording channel","2024","Journal of Animal Science","102","","skae066","","","","3","10.1093/jas/skae066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189285893&doi=10.1093%2fjas%2fskae066&partnerID=40&md5=35d908d046fe3183092a01de7787afa8","College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China; Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China","Zhang X., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China, Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China; Xuan C., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China, Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China; Ma Y., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China; Liu H., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China; Xue J., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China","The accurate identification of individual sheep is a crucial prerequisite for establishing digital sheep farms and precision livestock farming. Currently, deep learning technology provides an efficient and non-contact method for sheep identity recognition. In particular, convolutional neural networks can be used to learn features of sheep faces to determine their corresponding identities. However, the existing sheep face recognition models face problems such as large model size, and high computational costs, making it difficult to meet the requirements of practical applications. In response to these issues, we introduce a lightweight sheep face recognition model called YOLOv7-Sheep Face Recognition (YOLOv7-SFR). Considering the labor-intensive nature associated with manually capturing sheep face images, we developed a face image recording channel to streamline the process and improve efficiency. This study collected facial images of 50 Small-tailed Han sheep through a recording channel. The experimental sheep ranged in age from 1 to 3 yr, with an average weight of 63.1 kg. Employing data augmentation methods further enhanced the original images, resulting in a total of 22,000 sheep face images. Ultimately, a sheep face dataset was established. To achieve lightweight improvement and improve the performance of the recognition model, a variety of improvement strategies were adopted. Specifically, we introduced the shuffle attention module into the backbone and fused the Dyhead module with the model's detection head. By combining multiple attention mechanisms, we improved the model's ability to learn target features. Additionally, the traditional convolutions in the backbone and neck were replaced with depthwise separable convolutions. Finally, leveraging knowledge distillation, we enhanced its performance further by employing You Only Look Once version 7 (YOLOv7) as the teacher model and YOLOv7-SFR as the student model. The training results indicate that our proposed approach achieved the best performance on the sheep face dataset, with a mean average precision@0.5 of 96.9%. The model size and average recognition time were 11.3 MB and 3.6 ms, respectively. Compared to YOLOv7-tiny, YOLOv7-SFR showed a 2.1% improvement in mean average precision@0.5, along with a 5.8% reduction in model size and a 42.9% reduction in average recognition time. The research results are expected to drive the practical applications of sheep face recognition technology. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Society of Animal Science. All rights reserved.","deep learning; face image recording channel; lightweight recognition model; sheep face recognition; YOLOv7-tiny","Agriculture; Animals; Facial Recognition; Farms; Female; Labor, Obstetric; Livestock; Pregnancy; Sheep; agricultural worker; agriculture; animal; facial recognition; female; labor; livestock; pregnancy; sheep","Inner Mongolia Beiqi Technology Co.; Fundamental Research Funds of Inner Mongolia Agricultural University, (BR221314, BR221032); Science and Technology Major Project of Inner Mongolia, (2021GG0111); Science and Technology Major Project of Inner Mongolia; Research and Innovation Project for Doctoral Candidates in Inner Mongolia Autonomous Region, (B20231075Z)","This work was supported by the Fundamental Research Funds of Inner Mongolia Agricultural University (BR221314 and BR221032), the Science and Technology Planning Project of Inner Mongolia Autonomous Region (2021GG0111), and the Research and Innovation Project for Doctoral Candidates in Inner Mongolia Autonomous Region (B20231075Z). Our thanks to Inner Mongolia Beiqi Technology Co., Ltd. for providing assistance and technical support. We are grateful to the reviewers for their suggestions and comments, which significantly improved the quality of this paper. ","C. Xuan; College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, Inner Mongolia, 010018, China; email: xcz@imau.edu.cn","","Oxford University Press","00218812","","","38477672","English","J. Anim. Sci.","Article","Final","","Scopus","2-s2.0-85189285893"
"Yoon H.; Park M.; Lee H.; An J.; Lee T.; Lee S.-H.","Yoon, Heemoon (57219733018); Park, Mira (55485397700); Lee, Hayoung (58503849300); An, Jisoon (58549413700); Lee, Taehyun (58549108100); Lee, Sang-Hee (56375321300)","57219733018; 55485397700; 58503849300; 58549413700; 58549108100; 56375321300","Deep learning framework for bovine iris segmentation","2024","Journal of Animal Science and Technology","66","1","","167","177","10","3","10.5187/jast.2023.e51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175973282&doi=10.5187%2fjast.2023.e51&partnerID=40&md5=a249455c9a965175b3670d0a2d5971f1","School of Information Communication and Technology, University of Tasmania, Hobart, 7005, Australia; College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea","Yoon H., School of Information Communication and Technology, University of Tasmania, Hobart, 7005, Australia; Park M., School of Information Communication and Technology, University of Tasmania, Hobart, 7005, Australia; Lee H., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; An J., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Lee T., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Lee S.-H., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea","Iris segmentation is an initial step for identifying the biometrics of animals when establishing a traceability system for livestock. In this study, we propose a deep learning framework for pixel-wise segmentation of bovine iris with a minimized use of annotation labels utilizing the BovineAAEyes80 public dataset. The proposed image segmentation framework encompasses data collection, data preparation, data augmentation selection, training of 15 deep neural network (DNN) models with varying encoder backbones and segmentation decoder DNNs, and evaluation of the models using multiple metrics and graphical segmentation results. This framework aims to provide comprehensive and in-depth information on each model’s training and testing outcomes to optimize bovine iris segmentation performance. In the experiment, U-Net with a VGG16 backbone was identified as the optimal combination of encoder and decoder models for the dataset, achieving an accuracy and dice coefficient score of 99.50% and 98.35%, respectively. Notably, the selected model accurately segmented even corrupted images without proper annotation data. This study contributes to the advancement of iris segmentation and the establishment of a reliable DNN training framework. Copyright © 2024 Korean Society of Animal Sciences and Technology.","Cow; Deep learning; Identification; Iris; Segmentation","","Institute for Information and Communications Technology Planning and Evaluation; Kangwon National University, KNU; Ministry of Science, ICT and Future Planning, MSIP, (RS-2023-00260267); Ministry of SMEs and Startups, MSS","Funding text 1: This work was supported by the Technology Development Program (S3238047 and RS-2023-00223891), funded by the Ministry of SMEs and Startups and the Ministry of Science and ICT, under the Innovative Human Resource Development for Local; Funding text 2: Intellectualization support program (RS-2023-00260267) supervised by the Institute for Information and Communications Technology Planning and Evaluation, and 2023 Research Grant from Kangwon National University.","S.-H. Lee; College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; email: Sang1799@kangwon.ac.kr","","Korean Society of Animal Sciences and Technology","26720191","","","","English","J.  Anim.  Sci. Technol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175973282"
"Zhang Z.; Xu S.; Lu S.; Chen L.","Zhang, Zongtao (58679410000); Xu, Sijie (58261606000); Lu, Shengan (59094299800); Chen, Ling (57221046707)","58679410000; 58261606000; 59094299800; 57221046707","Advancing Precision Pig Behavior Recognition through Real-Time Detection Transformer","2024","2024 4th International Conference on Neural Networks, Information and Communication Engineering, NNICE 2024","","","","707","710","3","0","10.1109/NNICE61279.2024.10498479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192572498&doi=10.1109%2fNNICE61279.2024.10498479&partnerID=40&md5=73aa0bec66590406124e23a48723ed74","Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China","Zhang Z., Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China; Xu S., Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China; Lu S., Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China; Chen L., Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China","With the growing demand for sustainable animal products, intelligent pig farming has become key to improving the efficiency of livestock agriculture. This paper employs the advanced Real-Time Detection Transformer (RTDETR) algorithm, based on Transformer technology, aiming to enhance the accuracy of pig behavior recognition. To adapt to the variable challenges of real-world environments, we have applied various data augmentation techniques to the Edinburgh Pig Behavior Video Dataset, including random cropping, horizontal flipping, RandomBrightnessContrast, and perspective transformation. By training and comparing the RTDETR with the YOLOv5 model, we observed the exceptional performance of RTDETR in complex environments, achieving a mean Average Precision (mAP) of 0.82. This research not only provides a robust algorithmic foundation for intelligent pig farming but also emphasizes the significant advantages of the Transformer-based RTDETR algorithm in improving the accuracy of pig behavior recognition in challenging environments. © 2024 IEEE.","component; deep learning; Pig Behavior; RTDETR; YOLOv5","Behavioral research; Deep learning; Farms; Metadata; Signal detection; Behaviour recognition; Component; Deep learning; Growing demand; Intelligent pig; Pig behavior; Pig farming; Real-time detection; Real-time detection transformer; YOLOv5; Mammals","","","L. Chen; Beijing Institute of Technology, School of Applied Science and Civil Engineering, Zhuhai, China; email: lingchensh@126.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835039437-5","","","English","Int. Conf. Neural Networks, Inf. Commun. Eng., NNICE","Conference paper","Final","","Scopus","2-s2.0-85192572498"
"Guo Y.; Hong W.; Wu J.; Huang X.; Qiao Y.; Kong H.","Guo, Yangyang (57200132879); Hong, Wenhao (58498983300); Wu, Jiaxin (58498392800); Huang, Xiaoping (57208149375); Qiao, Yongliang (56486770900); Kong, He (57203456679)","57200132879; 58498983300; 58498392800; 57208149375; 56486770900; 57203456679","Vision-Based Cow Tracking and Feeding Monitoring for Autonomous Livestock Farming: The YOLOv5s-CA+DeepSORT-Vision Transformer","2023","IEEE Robotics and Automation Magazine","30","4","3310857","68","76","8","7","10.1109/MRA.2023.3310857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180362552&doi=10.1109%2fMRA.2023.3310857&partnerID=40&md5=eb0dabf7a2cde3fb902a7aeebdb75cf5","Anhui University, School of Internet, Anhui, Hefei, 230039, China; The University of Adelaide, Australian Institute for Machine Learning, 5005, SA, Australia; Southern University of Science and Technology, Guangdong Prov. Key Laboratory of Human-Augmentation and Rehabilitation Robotics in Universities, Shenzhen, 518055, China","Guo Y., Anhui University, School of Internet, Anhui, Hefei, 230039, China; Hong W., Anhui University, School of Internet, Anhui, Hefei, 230039, China; Wu J., Anhui University, School of Internet, Anhui, Hefei, 230039, China; Huang X., Anhui University, School of Internet, Anhui, Hefei, 230039, China; Qiao Y., The University of Adelaide, Australian Institute for Machine Learning, 5005, SA, Australia; Kong H., Southern University of Science and Technology, Guangdong Prov. Key Laboratory of Human-Augmentation and Rehabilitation Robotics in Universities, Shenzhen, 518055, China","Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).  © 1994-2011 IEEE.","","Animals; Complex networks; Deep learning; Farms; Animal feeding; Animal tracking; Farming system; Livestock farming; Online time; Real time tracking; Simple++; Tracking accuracy; Vision based; Welfare measurements; Face recognition","Natural Science Foundation of Anhui Higher Education Institutions of China, (2023AH050082); Natural Science Foundation of Department of Science and Technology of Anhui Province, (1908085QF284); Science, Technology and Innovation Commission of Shenzhen Municipality, (ZDSYS20220330161800001)","This work was supported by the Natural Science Foundation of Department of Science and Technology of Anhui Province under Grant 1908085QF284 and Natural Science Foundation of Anhui Higher Education Institutions of China (2023AH050082). He Kong's work was supported by the Science, Technology, and Innovation Commission of Shenzhen Municipality, China (Grant No. ZDSYS20220330161800001).","","","Institute of Electrical and Electronics Engineers Inc.","10709932","","IRAME","","English","IEEE Rob Autom Mag","Article","Final","","Scopus","2-s2.0-85180362552"
"Bhujade V.G.; Shrawne S.C.; Sambhe V.K.","Bhujade, Vaishali G. (54915591100); Shrawne, S.C. (58032253800); Sambhe, V.K. (24829832100)","54915591100; 58032253800; 24829832100","Implementation and Performance Evaluation of Deep Learning Models for Disease Classification and Severity Estimation of Coffee Leaves","2024","Communications in Computer and Information Science","2092 CCIS","","","3","19","16","0","10.1007/978-3-031-64070-4_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202213790&doi=10.1007%2f978-3-031-64070-4_1&partnerID=40&md5=8efcb5d2af08933997f4e699b4f74cd2","Computer Engineering Department, VJTI Mumbai, Mumbai, India; Department of Information Technology, VJTI Mumbai, Mumbai, India","Bhujade V.G., Computer Engineering Department, VJTI Mumbai, Mumbai, India; Shrawne S.C., Computer Engineering Department, VJTI Mumbai, Mumbai, India; Sambhe V.K., Department of Information Technology, VJTI Mumbai, Mumbai, India","The cultivation area per farmer is consistently increasing while the labor force is reducing, due to which nowadays effective crop management is a major issue in agriculture. Effective crop management needs automatic techniques in farms. The crop gets damaged by biotic and abiotic stresses. Abiotic stresses include salinity, drought, heat, heavy metals, and cold, whereas biotic stresses include crop diseases and plant pests. These stresses affect plant growth and development and produce low-quality yields with reduced crop productivity. Biotic stress causes damage to plants by agents such as viruses, bacteria, fungi, and pests. Agricultural sustainability is a way of controlling biotic agents in an efficient manner and increasing productivity. Deep learning techniques allow correct and early identification of stress-causing agents, which helps a farmer to take preventive and corrective measures as early as possible to mitigate the problem. This paper presents a technique for training the deep neural network model to detect the disease symptom, classify the coffee leaves diseases, estimate the severity of identified diseases, and evaluate the performance of the developed system. The proposed model is a multi-task system for disease classification and severity estimation. In addition, we have experimented with new techniques of data augmentation for accurate results. Computational experiments for the proposed model are performed with several models such as DenseNet121, DenseNet169, DenseNet201, ResNet50, ResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2, VGG16, VGG19, Mobile-Net, MobileNetv2, InceptionV3, InceptionResNetV2. Dense Net outperforms all other models. DenseNet169 obtained 92.421% classification accuracy and DenseNet121 achieved 62.302% accuracy for severity estimation. The experimental results specify the proposed model is an effective tool to assist farmers in the early identification of coffee leaf diseases. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Crop Disease Classification; Deep Learning; Disease Detection; Disease Recognition; Image Processing","Deep neural networks; Livestock; Biotic stress; Crop disease; Crop disease classification; Crop managements; Deep learning; Disease classification; Disease detection; Disease recognition; Disease severity; Images processing; Plant diseases","","","V.G. Bhujade; Computer Engineering Department, VJTI Mumbai, Mumbai, India; email: vgbhujade_p19@ce.vjti.ac.in","Verma A.; Verma P.; Pattanaik K.K.; Dhurandher S.K.; Woungang I.","Springer Science and Business Media Deutschland GmbH","18650929","978-303164069-8","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85202213790"
"Alado D.","Alado, Darios (57213154363)","57213154363","Cassava Disease Classification Using Squeezenet CNN Technique","2024","2024 IEEE 15th Control and System Graduate Research Colloquium, ICSGRC 2024 - Conference Proceeding","","","","239","243","4","0","10.1109/ICSGRC62081.2024.10691308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206659402&doi=10.1109%2fICSGRC62081.2024.10691308&partnerID=40&md5=8728eb0c842b55a16ae6a36c60aa0201","College of Computing Studies, Information and Communication Technology, Isabela State University, Echague Isabela, San Fabian, Philippines","Alado D., College of Computing Studies, Information and Communication Technology, Isabela State University, Echague Isabela, San Fabian, Philippines","Agriculture plays a pivotal role in the economic development of South Asian and African nations. However, adverse climate changes and disease invasions threaten crop productivity annually. Early identification and management of crop diseases are crucial for mitigating these effects. Cassava, a significant crop in the Philippines and other regions, faces challenges from various diseases, necessitating advanced diagnostic methods. This study uses deep learning techniques, specifically Convolutional Neural Networks (CNNs), to identify cassava leaf diseases. Researchers employed the Squeezenet architecture due to its efficiency and suitability for deployment on mobile devices. Our model classified cassava leaf status into four categories: Healthy, CBB (Cassava Bacterial Blight), CPD (Cassava Phytoplasma Disease), and BLS (Bacterial Leaf Streak). Using a robust dataset, the model achieved a high overall accuracy of 92%. The confusion matrix indicated strong performance in identifying diseased leaves, though there were some misclassifications in the Healthy category. These findings demonstrate the potential of deep learning techniques in enhancing crop disease management. They offer a promising tool for farmers in developing nations to protect their crops and improve agricultural productivity.  © 2024 IEEE.","Artificial Intelligence; Cassava; Confusion Matrix; Deep Learning; Neural Networks","Convolutional neural networks; Crops; Livestock; Cassavum; Confusion matrix; Convolutional neural network; Crop disease; Deep learning; Disease classification; Economic development; Learning techniques; Neural network techniques; Neural-networks; Plant diseases","","","D. Alado; College of Computing Studies, Information and Communication Technology, Isabela State University, San Fabian, Echague Isabela, Philippines; email: darios.b.alado@isu.edu.ph","","Institute of Electrical and Electronics Engineers Inc.","","979-835038655-4","","","English","IEEE Control Syst. Grad. Res. Colloq., ICSGRC - Conf. Proceeding","Conference paper","Final","","Scopus","2-s2.0-85206659402"
"Shalini I.S.; Ravikiran H.K.","Shalini, I.S. (59406914700); Ravikiran, H.K. (57208675007)","59406914700; 57208675007","Random search Based Hyperparamerter tunned VGG16 Architecture for Poultry Breed Image Classification","2024","International Conference on Intelligent Algorithms for Computational Intelligence Systems, IACIS 2024","","","","","","","0","10.1109/IACIS61494.2024.10721861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208794643&doi=10.1109%2fIACIS61494.2024.10721861&partnerID=40&md5=0204b27ac949567f1b473de6ca215774","Visvesvaraya Technological University, Navkis College of Engineering, Research Center, Dept. of Ece, Hassan, 573217, India; Visvesvaraya Technological University, Navkis College of Engineering, Dept. of Ece, Hassan, 573217, India","Shalini I.S., Visvesvaraya Technological University, Navkis College of Engineering, Research Center, Dept. of Ece, Hassan, 573217, India; Ravikiran H.K., Visvesvaraya Technological University, Navkis College of Engineering, Dept. of Ece, Hassan, 573217, India","Animal breed classification using deep learning algorithms is increasingly important due to its significant implications for agriculture, biodiversity conservation, and the poultry industry. Accurate breed classification aids in breed identification, disease management, and genetic research. This research aims to evaluate the performance and efficiency of the VGG16 architecture with hyperparameters optimized through Random Search for image classification tasks. Specifically, we compare the traditional CNN with the VGG16 architecture in poultry breed classification. Through rigorous evaluation, including various metrics, training efficiency analysis, and visualization techniques, we investigate the effectiveness of VGG16 with Random Search optimization. Our findings highlight the relative strengths and weaknesses of the proposed methodology, contributing to a deeper understanding of architecture in classifying different hen breeds. Using a dataset of 1010 hen breed images, we found that VGG16 with Random Search-based hyperparameter optimization outperformed the traditional CNN, achieving a 16% higher testing accuracy.  © 2024 IEEE.","CNN Model; Deep Learning; Hen breed Classification; VGG16 Architecture","Deep learning; Federated learning; Invertebrates; Macroinvertebrates; Biodiversity conservation; Breed identifications; CNN models; Deep learning; Hen breed classification; Images classification; Poultry industry; Random searches; Search-based; VGG16 architecture; Livestock","","","I.S. Shalini; Visvesvaraya Technological University, Navkis College of Engineering, Research Center, Dept. of Ece, Hassan, 573217, India; email: shaliniis@aitckm.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835036066-0","","","English","Int. Conf. Intell. Algorithms Comput. Intell. Syst., IACIS","Conference paper","Final","","Scopus","2-s2.0-85208794643"
"Kumar G.D.; Pradhan K.C.; Tyagi S.","Kumar, Guru Dayal (59179413000); Pradhan, Kalandi Charan (57218603400); Tyagi, Shekhar (59179413100)","59179413000; 57218603400; 59179413100","Deep Learning Forecasting: An LSTM Neural Architecture based Approach to Rainfall and Flood Impact Predictions in Bihar","2024","Procedia Computer Science","235","","","1455","1466","11","1","10.1016/j.procs.2024.04.137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196375199&doi=10.1016%2fj.procs.2024.04.137&partnerID=40&md5=c6de3122cba333aefe465693ef7d465a","Indian Institute of Technology Indore, School of Humanities and Social Sciences, Indore, 452020, India; Indian Institute of Technology Indore, Discipline of Computer Science and Engineering, Indore, 452020, India","Kumar G.D., Indian Institute of Technology Indore, School of Humanities and Social Sciences, Indore, 452020, India; Pradhan K.C., Indian Institute of Technology Indore, School of Humanities and Social Sciences, Indore, 452020, India; Tyagi S., Indian Institute of Technology Indore, Discipline of Computer Science and Engineering, Indore, 452020, India","The increasing prominence of natural disasters and the variability in climate change have underscored the significance of research in contemporary and prospective studies. This investigation zeroes in on floods, a ubiquitous natural calamity that plagues regions globally. Bihar stands out as a state exceptionally vulnerable to persistent and intense flooding, with a staggering 69.70 lakh hectares - equivalent to nearly 74 percent of its geographical expanse - enduring its detrimental consequences. The repercussions of such inundations are widespread, permeating multiple facets of life and erecting impediments to both the welfare and the developmental trajectory of local communities. Vital sectors, including infrastructure, agriculture, and livestock, bear the brunt of the damage, catalyzing the dislocation of innumerable households. In an agile response to these adversities, the state's administrative machinery has mobilized disaster response brigades, ensuring timely aid and succor to the distressed populace. To adeptly predict impending floods, this study harnesses data from 1991 to 2022, integrating pivotal metrics such as the 'Total Population affected (in lacs)', 'Mean Annual Rainfall', and 'Crop damage (in INR lakhs)'. By adopting an LSTM (Long Short-Term Memory) model, the research elucidates its findings via illustrative line plots, furnishing an accessible rendition of the flood trajectory predictions over the forthcoming five-year span. These predictive insights are instrumental for disaster management agencies, enhancing their strategic planning and readied responses to imminent flood events in Bihar. Capitalizing on these forecasts enables authorities to architect proactive interventions, ensuring reduced flood repercussions, bolstered community resilience, and an elevated state of disaster preparedness in the region. © 2024 Elsevier B.V.. All rights reserved.","Bihar floods; Climate change variability; Disaster management; Flood prediction; Long Short-Term Memory (LSTM)","Agriculture; Brain; Climate change; Disaster prevention; Disasters; Floods; Forecasting; Population statistics; Rain; Architecture-based; Bihar flood; Climate change variability; Disaster management; Flood prediction; Floodings; Long short-term memory; Natural disasters; Neural architectures; Prospective study; Long short-term memory","","","G.D. Kumar; Indian Institute of Technology Indore, School of Humanities and Social Sciences, Indore, 452020, India; email: phd2201161012@iiti.ac.in","Singh V.; Asari V.K.; Li K.-C.; Crespo R.G.","Elsevier B.V.","18770509","","","","English","Procedia Comput. Sci.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85196375199"
"Zhu J.; Vasseur E.; Wade K.M.","Zhu, Junsheng (58544593600); Vasseur, Elsa (33267999700); Wade, Kevin M. (7006204440)","58544593600; 33267999700; 7006204440","Provision of automatic milking-system alerts for potential incidence of mastitis: a comparative study between stationary classification and time-series regression modelling","2024","11th European Conference on Precision Livestock Farming","","","","1054","1061","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204945278&partnerID=40&md5=efa0729c42a16f4dfa53a968a6084335","Department of Animal Science, McGill University, Montreal, H3A 0G4, QC, Canada","Zhu J., Department of Animal Science, McGill University, Montreal, H3A 0G4, QC, Canada; Vasseur E., Department of Animal Science, McGill University, Montreal, H3A 0G4, QC, Canada; Wade K.M., Department of Animal Science, McGill University, Montreal, H3A 0G4, QC, Canada","Bovine mastitis is one of the most expensive diseases affecting dairy cows globally. It commonly presents as an inflammation of the udder tissue caused by bacteria. Negative consequences include reduced milk production and quality and, in severe cases, the culling of the animal. While automatic milking systems (AMS) perform the milking process without human intervention many also include various sensors that can monitor the milking status and process for each visit (e.g., milking duration, milking flow rate, milk temperature, milk fat and protein percentage, and milk somatic cell count). Data from AMS with the above variables were obtained from 51 dairy farms in Québec, Canada during 2022. Somatic cell count (SCC) is a long-time predictor of mastitis, and both milk quality and mammary health are known to suffer in the case of high counts. Without the availability of a clinical diagnosis of mastitis, this study considered a count of > 2,000,000 cells per ml to be problematic, likely requiring intervention. With a view to providing farmers with alerts five, three, and one day(s) before reaching the critical SCC level of 2,000,000, several methods were tested and compared. Classification methods with a binary outcome of < or >= 2,000,000 SCC only yielded an AUC of 70%. However, for alerts three days prior to reaching the critical level, regression methods improved the prediction process (ARIMA = 75%; LSTM = 83%; and Transformer models = 84%). The SCC in this study is only an approximation of actual laboratory SCC, which makes it hard to be proposed for deterministic actions and/or treatment. In the future, if more accurate SCC protocols could be guaranteed or sensors were installed on AMS, the prediction results could possibly be used as part of decision-support systems, leading to potential preventive treatment. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","dairy-cow mastitis; deep learning; prediction; somatic cell count; stationary classification; time-series regression","Diagnosis; Diseases; Fertilizers; Livestock; Milking machines; Automatic milking; Comparatives studies; Dairy cow; Dairy-cow mastiti; Deep learning; Milk quality; Milking systems; Somatic cell count; Stationary classification; Time-series regression; Mammals","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204945278"
"Thapar S.; Sharma K.; Yadav D.; Kanwer B.; Raj A.","Thapar, Shruti (57221605282); Sharma, Krati (59184923500); Yadav, Dharmveer (57197598430); Kanwer, Budesh (57832476500); Raj, Ashish (56640789400)","57221605282; 59184923500; 57197598430; 57832476500; 56640789400","Integration of Mathematical Operators Based on Decision Boundary Complexity and Combinatorial Optimization for Improved Deep Learning Classifiers","2024","Communications on Applied Nonlinear Analysis","31","2","","370","388","18","0","10.52783/cana.v31.574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196761059&doi=10.52783%2fcana.v31.574&partnerID=40&md5=21931fd7be0d2aa66513cd6ab93a6f6a","Department of Electronics & Communication, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Department of English and Soft Skills, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Department of Computer Science, St. Xavier’s College, Rajasthan, Jaipur, India; Department of AI&DS, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Department of Electrical and Electronics Engineering, Poornima University, Rajasthan, Jaipur, India","Thapar S., Department of Electronics & Communication, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Sharma K., Department of English and Soft Skills, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Yadav D., Department of Computer Science, St. Xavier’s College, Rajasthan, Jaipur, India; Kanwer B., Department of AI&DS, Poornima Institute of Engineering & Technology, Rajasthan, Jaipur, India; Raj A., Department of Electrical and Electronics Engineering, Poornima University, Rajasthan, Jaipur, India","The proliferation of complex diseases in livestock, such as lumpy skin disease, demands advanced diagnostic tools that can accurately classify and predict outbreaks. This study explores the integration of complex mathematical operators within deep learning classifiers to enhance their accuracy and efficiency in diagnosing lumpy skin disease. By focusing on the decision boundary complexity, which delineates different disease states in high-dimensional spaces, and employing combinatorial optimization techniques, we develop a novel framework that significantly improves classification performance. The methodology hinges on optimizing the configuration and combination of mathematical operators, such as gradient operators and higher-order derivatives, to refine feature extraction processes. This approach allows for a more nuanced understanding of the disease features that are critical for accurate classification. Using a dataset comprised of clinical and image data from infected cattle, our enhanced classifiers demonstrate a marked improvement in predictive accuracy compared to traditional deep learning models. The case study not only underscores the potential of integrating advanced mathematical concepts into deep learning but also sets a precedent for tackling similar challenges in veterinary medicine and beyond. © 2024, International Publications. All rights reserved.","Combinatorial Optimization; Decision Boundary Complexity; Deep Learning Classifiers; Feature Extraction; High-Dimensional Data Analysis; Livestock Disease Classification; Lumpy Skin Disease; Mathematical Operators; Predictive Accuracy; Veterinary Medicine","","","","","","International Publications","1074133X","","","","English","Commun. Appl. Nonlinear Anal.","Article","Final","","Scopus","2-s2.0-85196761059"
"Istiak M.A.; Syeed M.M.M.; Hossain M.S.; Uddin M.F.; Hasan M.; Khan R.H.; Azad N.S.","Istiak, Md. Abrar (57224235635); Syeed, M.M. Mahbubul (55205077600); Hossain, Md Shakhawat (57219671703); Uddin, Mohammad Faisal (13008867000); Hasan, Mahady (35105055600); Khan, Razib Hayat (36245156700); Azad, Nafis Saami (58614349100)","57224235635; 55205077600; 57219671703; 13008867000; 35105055600; 36245156700; 58614349100","Adoption of Unmanned Aerial Vehicle (UAV) imagery in agricultural management: A systematic literature review","2023","Ecological Informatics","78","","102305","","","","51","10.1016/j.ecoinf.2023.102305","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171970891&doi=10.1016%2fj.ecoinf.2023.102305&partnerID=40&md5=7bb1795f9f66392eb9d1b9a7223f33cf","Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh; RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh","Istiak M.A., RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Syeed M.M.M., Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh, RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Hossain M.S., Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh, RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Uddin M.F., Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh, RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Hasan M., Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh, RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Khan R.H., Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh, RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh; Azad N.S., RIoT Research Center, Independent University, Bangladesh, Dhaka, Bangladesh","Precision agriculture and Smart farming have become the essential backbone for sustainable agricultural production by leveraging cutting edge remote sensing and communication technologies, meshed with AI driven data processing and decision making approaches. Agricultural segments, such as crop and livestock monitoring, crop/plant classification, yield prediction, weed detection, automatic harvesting, early detection, and prevention of diseases are being served for efficient, cost-effective process monitoring with increased profitability. With the remarkable development in recent decades, Unmanned Aerial Vehicles (UAV) based remote sensing technologies have gained rapid proliferation and exploitation in precision agriculture. Consequently, over the past decades, researchers have explored the capabilities of UAVs for real-time imagery data acquisition and processing through powerful Deep Learning (DL) algorithms to optimize agricultural process management. Being a prevalent research domain of high-tech field with constant advancement, there is a need for systematic review to recapitulate the contemporary literature and reveal the domain's intellectual structure. This systematic literature review (SLR) research has methodically scrutinized 214 peer reviewed articles on the concerned domain that are published in ranked journals and conferences over the past 14 years. Several pressing dimensions are investigated, including, the feasibility assessment of the UAVs in precision agriculture, determine the impact of imaging modalities and imagery datasets in relation to agricultural applications, categorically evaluate the UAV configuration and offer detailed scrutiny of AI methods in relation to real-time control, decision making and action performance in agricultural applications. Alongside, the taxonomy of crops across the world is documented for which UAV is utilized. Finally, the main challenges and directions of future research along the track is presented. © 2023","Deep learning; Precision agriculture; Remote sensing; Smart farming; Systematic literature review; UAV; Unmanned aerial vehicle; Visual imagery","agricultural production; exploitation; precision agriculture; remote sensing; unmanned vehicle","Department of Crop Botany at Bangladesh Agricultural University","We would like to thank Dr. Md. Ashrafuzzaman, Professor of Department of Crop Botany at Bangladesh Agricultural University, for his invaluable contributions to this research. He provided significant domain expertise, feedback, and guidance throughout the course of this study, which greatly improved the quality and depth of our research findings.","M.S. Hossain; Department of Computer Science and Engineering, Independent University, Bangladesh, Dhaka, Bangladesh; email: shakhawat@iub.edu.bd","","Elsevier B.V.","15749541","","","","English","Ecol. Informatics","Review","Final","","Scopus","2-s2.0-85171970891"
"Li J.; Chen D.; Qi X.; Li Z.; Huang Y.; Morris D.; Tan X.","Li, Jiajia (58317754400); Chen, Dong (57190854977); Qi, Xinda (57214997813); Li, Zhaojian (57202421579); Huang, Yanbo (7501574352); Morris, Daniel (55459697400); Tan, Xiaobo (8664617000)","58317754400; 57190854977; 57214997813; 57202421579; 7501574352; 55459697400; 8664617000","Label-efficient learning in agriculture: A comprehensive review","2023","Computers and Electronics in Agriculture","215","","108412","","","","17","10.1016/j.compag.2023.108412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178215986&doi=10.1016%2fj.compag.2023.108412&partnerID=40&md5=6b4c6ab79ada60c9e30c5121372b236a","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Department of Mechanical Engineering, Michigan State University, East Lansing, 48824, MI, United States; USDA-ARS Genetics and Sustainable Agriculture Research Unit, Mississippi State, 39762, MS, United States; Department of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, 48824, MI, United States","Li J., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Chen D., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Qi X., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Li Z., Department of Mechanical Engineering, Michigan State University, East Lansing, 48824, MI, United States; Huang Y., USDA-ARS Genetics and Sustainable Agriculture Research Unit, Mississippi State, 39762, MS, United States; Morris D., Department of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, 48824, MI, United States; Tan X., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States","The past decade has witnessed many great successes of machine learning (ML) and deep learning (DL) applications in agricultural systems, including weed control, plant disease diagnosis, agricultural robotics, and precision livestock management. However, a notable limitation of these ML/DL models lies in their reliance on large-scale labeled datasets for training, with their performance closely tied to the quantity and quality of available labeled data. The process of collecting, processing, and labeling such datasets is both expensive and time-consuming, primarily due to escalating labor costs. This challenge has sparked substantial interest among researchers and practitioners in the development of label-efficient ML/DL methods tailored for agricultural applications. In fact, there are more than 50 papers on developing and applying deep-learning-based label-efficient techniques to address various agricultural problems since 2016, which motivates the authors to provide a timely and comprehensive review of recent label-efficient ML/DL methods in agricultural applications. To this end, a principled taxonomy is first developed to organize these methods according to the degree of supervision, including weak supervision (i.e., active learning and semi-/weakly- supervised learning), and no supervision (i.e., un-/self- supervised learning), supplemented by representative state-of-the-art label-efficient ML/DL methods. In addition, a systematic review of various agricultural applications exploiting these label-efficient algorithms, such as precision agriculture, plant phenotyping, and postharvest quality assessment, is presented. Finally, the current problems and challenges are discussed, as well as future research directions. A well-classified paper list that will be actively updated can be accessed at https://github.com/DongChen06/Label-efficient-in-Agriculture. © 2023 Elsevier B.V.","Active learning; Agriculture; Deep learning; Label-efficient learning; Label-free learning; Self-supervised learning; Semi-supervised learning; Unsupervised learning; Weakly-supervised learning","Deep learning; Disease control; Large dataset; Learning systems; Supervised learning; Wages; Weed control; Active Learning; Deep learning; Efficient learning; Label free; Label-efficient learning; Label-free learning; Machine-learning; Self-supervised learning; Semi-supervised learning; Weakly supervised learning; data processing; farming system; literature review; machine learning; performance assessment; precision; supervised learning; unsupervised classification; Diagnosis","","","Z. Li; Department of Mechanical Engineering, Michigan State University, United States; email: lizhaoj1@egr.msu.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85178215986"
"Shinde S.; Gupta J.; Sajwan S.; Raje A.; Yadav S.","Shinde, Samradnyee (59368695400); Gupta, Jaya (57261748900); Sajwan, Sahil (59367895500); Raje, Ashneel (59367566500); Yadav, Saurabh (59368211400)","59368695400; 57261748900; 59367895500; 59367566500; 59368211400","A Comparative Analysis of Deep Learning Models for Wildlife Prediction","2024","Lecture Notes in Networks and Systems","1086 LNNS","","","187","201","14","0","10.1007/978-981-97-6036-7_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206468286&doi=10.1007%2f978-981-97-6036-7_16&partnerID=40&md5=d0ab3a61fd0a0355d71ea7d972104c06","Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India; Department of Computer Engineering (Artificial Intelligence and Machine Learning), A. P. Shah Institute of Technology, Thane, India","Shinde S., Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India; Gupta J., Department of Computer Engineering (Artificial Intelligence and Machine Learning), A. P. Shah Institute of Technology, Thane, India; Sajwan S., Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India; Raje A., Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India; Yadav S., Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India","Our surroundings boast abundant wildlife diversity, however, only a handful of species are easily recognizable to most people. This is commonly observed when hikers and trekkers encounter various wildlife species during their adventures but are often unaware of the diverse array of birds, animals, insects, and fish they come across. To tackle this issue, we propose designing an application which employs machine learning techniques such as Convolutional Neural Network (CNN) to identify these species through a simple image input. A dataset consisting of 200 animals, insects and fish species and 125 bird species, totalling to 325 species was trained and tested using various deep learning algorithms such as VGG16, VGG19, ResNet, AlexNet, and DenseNet to determine which algorithm accurately identified the wildlife species with the maximum accuracy. Targeted at hikers and trekkers, the system enhances nature exploration by providing real-time species recognition, enabling users to gain insights into their surroundings. After a detailed research and implementation of several models, it was concluded that ResNet gave the highest accuracy of 98.23% for identifying the wildlife species. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","AlexNet; Convolutional neural network; Deep learning; DenseNet; Machine learning; ResNet; VGG16; VGG19; Wildlife species","Adversarial machine learning; Contrastive Learning; Livestock; Alexnet; Comparative analyzes; Convolutional neural network; Deep learning; Densenet; Learning models; Machine-learning; VGG16; VGG19; Wildlife species; Invertebrates","","","S. Shinde; Department of Computer Engineering, A. P. Shah Institute of Technology, Thane, India; email: 20102092@apsit.edu.in","Swaroop A.; Kansal V.; Fortino G.; Hassanien A.E.","Springer Science and Business Media Deutschland GmbH","23673370","978-981976035-0","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85206468286"
"Dovdon E.; Agarwal M.; Dajsuren Y.; de Vlieg J.","Dovdon, Enkhzol (57223108547); Agarwal, Manu (58834811300); Dajsuren, Yanja (55317374300); de Vlieg, Jakob (58112751500)","57223108547; 58834811300; 55317374300; 58112751500","Irregular Frame Rate Synchronization of Multi-camera Videos for Data-Driven Animal Behavior Detection","2024","Lecture Notes in Networks and Systems","956 LNNS","","","97","112","15","1","10.1007/978-3-031-56950-0_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190678108&doi=10.1007%2f978-3-031-56950-0_9&partnerID=40&md5=c28f6a33ac07227f3304077f1adec767","Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands","Dovdon E., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Agarwal M., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; Dajsuren Y., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; de Vlieg J., Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands","Deep learning and camera-based monitoring play a pivotal role in effective farm management. However, reliable data availability remains essential for successful deep-learning applications. Cameras are the primary data sources for computer vision deep learning models. For effective farm management, a multi-camera setup is often used. In a multi-camera farm setup, the input dataset for deep learning is prepared by combining the records of the cameras installed on many sides of the farm. However, an irregular frame rate of various cameras in a multi-camera setup can cause issues such as drift. Therefore, the data from different cameras must be in sync before feeding it to a deep learning model. In this work, we present a method for frame rate synchronization that leverages the timestamp information on the video and achieves high accuracy. Our method addresses a critical use case where the frame rate synchronization is performed post-video recording. Its effectiveness is demonstrated in real-world animal behavior detection scenarios, where precise synchronization is vital. Via this work, we contribute to robust deep-learning models for farm management and livestock analysis by addressing frame rate irregularities. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Feature extraction; Frame rate synchronization; Multi-camera video synchronization; Optical character recognition; Precision livestock farming","Animals; Cameras; Deep learning; Farms; Feature extraction; Learning systems; Optical character recognition; Video recording; Farm management; Features extraction; Frame rate synchronization; Frame-rate; Learning models; Multi-camera video; Multi-camera video synchronization; Multi-cameras; Precision livestock farming; Video synchronizations; Synchronization","Technische Universiteit Eindhoven, TU/e; Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO, (P18-19)","This work is supported by the Dutch NWO project IMAGEN [P18-19] of the research program Perspectief. Topigs Norsvin, the Netherlands, offered data from the Volmer facility in Germany. The authors would like to thank EngD Software Technology trainees from the Eindhoven University of Technology for assisting in implementing our method as data pipelines on the IMAGEN Data Analytics Platform.","E. Dovdon; Mathematics and Computer Science Department, Eindhoven University of Technology, Eindhoven, Netherlands; email: e.d.dovdon@tue.nl","Daimi K.; Al Sadoon A.","Springer Science and Business Media Deutschland GmbH","23673370","978-303156949-4","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85190678108"
"Prathima C.; Silpa C.; Charitha A.; Harshitha G.; Sai Charan C.; Sailendra G.R.","Prathima, Ch. (58061524800); Silpa, C. (57992419500); Charitha, A. (58314105700); Harshitha, G. (59410563000); Sai Charan, C. (59301943700); Sailendra, G.R. (59302027300)","58061524800; 57992419500; 58314105700; 59410563000; 59301943700; 59302027300","Detecting and Recognizing Marine Animals Using Advanced Deep Learning Models","2024","Proceedings - 2024 International Conference on Expert Clouds and Applications, ICOECA 2024","","","","950","955","5","0","10.1109/ICOECA62351.2024.00167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202355778&doi=10.1109%2fICOECA62351.2024.00167&partnerID=40&md5=3aec806e5efb5d16a400f9ccd55b8fbb","Data Science Mohan Babu University (Erstwhile Sree Vidyanikethan Engineering College), Tirupati, India; GITAM (Deemed to be University), Department of CSE, Karnataka, Bengaluru, India; Sree Vidyanikethan Engineering College, Department of CSSE, Tirupati, India","Prathima C., Data Science Mohan Babu University (Erstwhile Sree Vidyanikethan Engineering College), Tirupati, India; Silpa C., GITAM (Deemed to be University), Department of CSE, Karnataka, Bengaluru, India; Charitha A., Sree Vidyanikethan Engineering College, Department of CSSE, Tirupati, India; Harshitha G., Sree Vidyanikethan Engineering College, Department of CSSE, Tirupati, India; Sai Charan C., Sree Vidyanikethan Engineering College, Department of CSSE, Tirupati, India; Sailendra G.R., Sree Vidyanikethan Engineering College, Department of CSSE, Tirupati, India","Accurately detecting and recognizing marine animals is crucial for ecological conservation and underwater monitoring. This research analyzes the application of advanced Deep Learning (DL) techniques, including YOLOV5x6, YoloV3, YoloV8, Faster R-CNN SSD, and RetinaNet, to address the challenges inherent in marine environments. It compares the efficiency of these models in handling diverse underwater scenarios, varying lighting conditions, and identifying a wide range of marine species. It evaluates each DL technique meticulously using a comprehensive image dataset representative of marine ecosystems. Evaluation metrics such as precision, recall, and F1-score are employed to assess the models' abilities to detect and recognize marine animals accurately. The comparative analysis highlights each technique's unique strengths and limitations, focusing on their performance in the intricate task of marine animal identification. Scrutinizes Faster R-CNN's region-based approach, YOLO's grid-based methodology, SSD's single-shot detection, and RetinaNet's focal loss mechanism for their effectiveness in differentiating between species, handling occlusions, and robustly capturing the distinct features of marine fauna. It provides valuable insights into selecting and optimizing DL techniques for marine animal detection and recognition tasks. The outcomes aid practitioners and researchers in making informed decisions based on the specific requirements of their underwater monitoring applications, ultimately advancing the state-of-the-art in marine biology and conservation. © 2024 IEEE.","Deep Learning (DL); FasterR-CNN(Region-based convolutional network); Marine animals detection; recognition; RetinaNet; SSD(Single Shot Multibox Detector); YoloV3(You Only Look Once Version 3); YOLOV5x6(You Only Look Once Version 5); YoloV8(You Only Look Once Version 8)","Abiotic; Livestock; Convolutional networks; Deep learning; Fasterr-CNN(region-based convolutional network); Marine animal detection; Marine animals; Recognition; Region-based; Retinanet; Single shot multibox detector; Single-shot; YOLOV5x6(you only look once version 5); You only look once version 3; You only look once version 8; Invertebrates","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038579-3","","","English","Proc. - Int. Conf. Expert Clouds Appl., ICOECA","Conference paper","Final","","Scopus","2-s2.0-85202355778"
"Peng Y.; Jiang H.; Wang F.","Peng, Yihang (36976085400); Jiang, Honghui (59313521900); Wang, Fu (59313571600)","36976085400; 59313521900; 59313571600","A PVT based wheat detection method","2024","Proceedings of SPIE - The International Society for Optical Engineering","13250","","132502I","","","","0","10.1117/12.3038561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203106758&doi=10.1117%2f12.3038561&partnerID=40&md5=acee5a9060b911cc7d457127dd527ff0","School of anhui technical college of mechanical and electrical engineering, Wuhu, 241002, China","Peng Y., School of anhui technical college of mechanical and electrical engineering, Wuhu, 241002, China; Jiang H., School of anhui technical college of mechanical and electrical engineering, Wuhu, 241002, China; Wang F., School of anhui technical college of mechanical and electrical engineering, Wuhu, 241002, China","Wheat detection is significant in agriculture and can help farmers achieve efficient planting management and precision agriculture. Traditional wheat detection methods usually rely on hand-designed feature extractors and classifiers, but the ability and generalization of feature representations limit their performance. In contrast, deep learning-based methods can automatically learn feature representations and perform end-to-end training on large-scale datasets, with more vital representation ability and generalization. In this study, we propose a PVT-based detection method for wheat. PVT is a visual model combining a Transformer and feature pyramid, simultaneously capturing global and local information in images. The experimental results show that the PVT model is adaptable and can be generalized in wheat detection. It can provide accurate detection results of wheat ears and maintain stability when dealing with different light conditions and wheat growth cycles. © 2024 SPIE.","agriculture; Computer vision; PVT; wheat detection","Detection methods; Feature classifiers; Feature extractor; Feature representation; Generalisation; Learning-based methods; Performance; Plantings; Precision Agriculture; Wheat detection; Livestock","","","Y. Peng; School of anhui technical college of mechanical and electrical engineering, Wuhu, 241002, China; email: heizi0126@126.com","Du K.; Zain A.B.M.","SPIE","0277786X","978-151068231-3","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85203106758"
"Husain S.O.; Jagadish S.; Armoogum V.; Siri D.; Palanivel R.","Husain, Saif O. (58121493500); Jagadish, Sripelli (58735645700); Armoogum, Vinaye (20336563500); Siri, Dharmapuri (58675627800); Palanivel, R. (59129711100)","58121493500; 58735645700; 20336563500; 58675627800; 59129711100","Analysis of Machine Learning and Deep Learning Algorithms for Plant Disease Detection and Classification","2024","2nd IEEE International Conference on Networks, Multimedia and Information Technology, NMITCON 2024","","","","","","","0","10.1109/NMITCON62075.2024.10698945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207436830&doi=10.1109%2fNMITCON62075.2024.10698945&partnerID=40&md5=634eb08ecbe129553a2e690bdef91f2c","The Islamic university, College of Technical Engineering, Department of Computers Techniques Engineering, Najaf, Iraq; Sr University, School of Computer Science & Artificial Intelligence, Warangal, India; University of Technology, School of Innovative Technologies and Engineering, Mauritius; Gokaraju Rangaraju Institute of Engineering and Technology, Department of Cse, Hyderabad, India; Nitte Meenakshi Institute of Technology, Department of Ai & Ds, Bengaluru, India","Husain S.O., The Islamic university, College of Technical Engineering, Department of Computers Techniques Engineering, Najaf, Iraq; Jagadish S., Sr University, School of Computer Science & Artificial Intelligence, Warangal, India; Armoogum V., University of Technology, School of Innovative Technologies and Engineering, Mauritius; Siri D., Gokaraju Rangaraju Institute of Engineering and Technology, Department of Cse, Hyderabad, India; Palanivel R., Nitte Meenakshi Institute of Technology, Department of Ai & Ds, Bengaluru, India","Agriculture is the fundamental pillar of civilization and raising the livestock, and economic prosperity of the country. Detection of plant diseases is a most important aspect of agriculture, aimed at identifying and diagnosing diseases that affect plants. Early detection of plant disease is key to reducing yield loss and ensuring quality of crops. Traditional detection methods done by researchers, experts, and farmers are subjective matter, time-consuming, and sometimes the results may not be accurate. In this paper, recent technological approaches like computer vision, Machine Learning (ML), and Deep Learning (DL) have been used for detection and classification of diseases. Moreover, techniques like image processing, and spectral imaging efficiently identify the symptoms of disease from leaf samples as faster compared to previous detecting methods. This real-time monitoring and detection system are big support to the farmers to identify diseases and prevent loss in agriculture production. The outcome illustrates that ML and DL models in this paper are well suited for detection and classification of plant disease. © 2024 IEEE.","agriculture; and plant disease; classification; deep learning; detection; image processing; machine learning","And plant disease; Deep learning; Detection; Disease classification; Disease detection; Economic prosperity; Images processing; Machine-learning; Plant disease; Yield loss","","","S.O. Husain; The Islamic university, College of Technical Engineering, Department of Computers Techniques Engineering, Najaf, Iraq; email: saifobeed.aljanabi@iunajaf.edu.iq","","Institute of Electrical and Electronics Engineers Inc.","","979-835037289-2","","","English","IEEE Int. Conf. Networks, Multimed. Inf. Technol., NMITCON","Conference paper","Final","","Scopus","2-s2.0-85207436830"
"Xue J.-B.; Xia S.; Wang X.-Y.; Huang L.-L.; Huang L.-Y.; Hao Y.-W.; Zhang L.-J.; Li S.-Z.","Xue, Jing-Bo (57191156611); Xia, Shang (36728593100); Wang, Xin-Yi (57221485247); Huang, Lu-Lu (57201185550); Huang, Liang-Yu (57698620300); Hao, Yu-Wan (56079675100); Zhang, Li-Juan (55917753900); Li, Shi-Zhu (26641946500)","57191156611; 36728593100; 57221485247; 57201185550; 57698620300; 56079675100; 55917753900; 26641946500","Recognizing and monitoring infectious sources of schistosomiasis by developing deep learning models with high-resolution remote sensing images","2023","Infectious Diseases of Poverty","12","1","6","","","","5","10.1186/s40249-023-01060-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147461219&doi=10.1186%2fs40249-023-01060-9&partnerID=40&md5=c8eb6143b6fefd1279f884bd02a82ce1","National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; School of Global Health, Chinese Center for Tropical Diseases Research, Shanghai Jiao Tong University School of Medicine, Shanghai, 200025, China","Xue J.-B., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China, School of Global Health, Chinese Center for Tropical Diseases Research, Shanghai Jiao Tong University School of Medicine, Shanghai, 200025, China; Xia S., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China, School of Global Health, Chinese Center for Tropical Diseases Research, Shanghai Jiao Tong University School of Medicine, Shanghai, 200025, China; Wang X.-Y., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; Huang L.-L., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; Huang L.-Y., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; Hao Y.-W., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; Zhang L.-J., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; Li S.-Z., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China, School of Global Health, Chinese Center for Tropical Diseases Research, Shanghai Jiao Tong University School of Medicine, Shanghai, 200025, China","Background: China is progressing towards the goal of schistosomiasis elimination, but there are still some problems, such as difficult management of infection source and snail control. This study aimed to develop deep learning models with high-resolution remote sensing images for recognizing and monitoring livestock bovine, which is an intermediate source of Schistosoma japonicum infection, and to evaluate the effectiveness of the models for real-world application. Methods: The dataset of livestock bovine’s spatial distribution was collected from the Chinese National Platform for Common Geospatial Information Services. The high-resolution remote sensing images were further divided into training data, test data, and validation data for model development. Two recognition models based on deep learning methods (ENVINet5 and Mask R-CNN) were developed with reference to the training datasets. The performance of the developed models was evaluated by the performance metrics of precision, recall, and F1-score. Results: A total of 50 typical image areas were selected, 1125 bovine objectives were labeled by the ENVINet5 model and 1277 bovine objectives were labeled by the Mask R-CNN model. For the ENVINet5 model, a total of 1598 records of bovine distribution were recognized. The model precision and recall were 81.9% and 80.2%, respectively. The F1 score was 0.81. For the Mask R-CNN mode, 1679 records of bovine objectives were identified. The model precision and recall were 87.3% and 85.2%, respectively. The F1 score was 0.87. When applying the developed models to real-world schistosomiasis-endemic regions, there were 63 bovine objectives in the original image, 53 records were extracted using the ENVINet5 model, and 57 records were extracted using the Mask R-CNN model. The successful recognition ratios were 84.1% and 90.5% for the respectively developed models. Conclusion: The ENVINet5 model is very feasible when the bovine distribution is low in structure with few samples. The Mask R-CNN model has a good framework design and runs highly efficiently. The livestock recognition models developed using deep learning methods with high-resolution remote sensing images accurately recognize the spatial distribution of livestock, which could enable precise control of schistosomiasis. Graphical Abstract: [Figure not available: see fulltext.] © 2023, The Author(s).","Deep learning; High-resolution remote sensing; Infectious source; Monitoring; Recognizing; Schistosomiasis","Animals; Cattle; China; Deep Learning; Livestock; Remote Sensing Technology; Schistosomiasis; Schistosomiasis japonica; accuracy; algorithm; animal dispersal; Article; bovine; comparative effectiveness; controlled study; convolutional neural network; deep learning; disease control; disease surveillance; envinet5 model; environmental monitoring; feature learning (machine learning); image processing; information service; livestock; mask r-cnn model; nonhuman; recall; remote sensing; Schistosoma japonicum; schistosomiasis; sensitivity and specificity; spatial analysis; training; validation process; animal; China; remote sensing; schistosomiasis; schistosomiasis japonica; veterinary medicine","Science and Technology; Shanghai Municipal Health Commission, (20194Y0359); Shanghai Municipal Health Commission; National Natural Science Foundation of China, NSFC, (32161143036, 81960374, 82173633); National Natural Science Foundation of China, NSFC; Key Technologies Research and Development Program; National Key Research and Development Program of China, NKRDPC, (2021YFC2300800, 2021YFC2300803); National Key Research and Development Program of China, NKRDPC","This research work was supported by the National Natural Science Foundation of China (No. 32161143036, No. 82173633, No. 81960374); Science and Technology research project of Shanghai Municipal Health Commission (No. 20194Y0359); the National Key Research and Development Program of China (No. 2021YFC2300800, 2021YFC2300803). ","S.-Z. Li; National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research); NHC Key Laboratory of Parasite and Vector Biology; WHO Collaborating Centre for Tropical Diseases, National Center for International Research On Tropical Diseases, Shanghai, 200025, China; email: lisz@chinacdc.cn","","BioMed Central Ltd","20955162","","","36747280","English","Infect. Dis. Pover.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85147461219"
"Van De Putte T.; Van Kerschaver C.; Hostens M.; Degroote J.","Van De Putte, T. (59344001600); Van Kerschaver, C. (57219030400); Hostens, M. (25960883600); Degroote, J. (55830765200)","59344001600; 57219030400; 25960883600; 55830765200","Evaluation of an image classifier for estimating feed intake and feeding behavior in weaned piglets","2024","11th European Conference on Precision Livestock Farming","","","","514","521","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204963972&partnerID=40&md5=314e9b9288dffcffb5f06ad0090dd652","Department of Animal Sciences and Aquatic Ecology, Ghent University, Coupure Links 653, Gent, 9000, Belgium; Department of Animal Science, College of Agriculture and Life Sciences, Cornell University, 215 Garden Ave, Ithaca, 14850, NY, United States","Van De Putte T., Department of Animal Sciences and Aquatic Ecology, Ghent University, Coupure Links 653, Gent, 9000, Belgium; Van Kerschaver C., Department of Animal Sciences and Aquatic Ecology, Ghent University, Coupure Links 653, Gent, 9000, Belgium; Hostens M., Department of Animal Sciences and Aquatic Ecology, Ghent University, Coupure Links 653, Gent, 9000, Belgium, Department of Animal Science, College of Agriculture and Life Sciences, Cornell University, 215 Garden Ave, Ithaca, 14850, NY, United States; Degroote J., Department of Animal Sciences and Aquatic Ecology, Ghent University, Coupure Links 653, Gent, 9000, Belgium","In the first days post-weaning feed consumption can be delayed and aberrantly low in piglets. Accessible tools that accurately assess feeding behavior can be valuable to optimize nutritional programs. A total of 288 22-day-old weaned piglets were housed in 24 pens and fed 2 distinct feeds in different feeders, a complete creep feed and a standard weaning dry feed. Feed intake was monitored by daily manual weighing the feeders as well as video recording during the first three days post-weaning. From these videos, two datasets were created containing images labeled by a human expert and indicating the number of animals eating from either the creep feeder or standard dry feeder. For each feeder type, a deep learning model, comprising of the ResNet50 base with custom image classification layers, was trained using the Tensorflow library in Python. These models were used to process all video recordings and predict the time expenditure at each feeder. Feed intake for each diet was compared to time expenditure for the corresponding feeder type. The correlation yielded an R2 of 0.92 and 0.70 for the weaner and creep feed respectively, suggesting feeding time is a good estimate of actual feed intake. Further analysis revealed that feed intake rates changed daily. Preference for creep feed was observed when available, significantly increasing total intake and feeding behavior during the early post-weaning period. In summary, the proposed image classification method using computer vision shows promise to evaluate the immediate post-weaning feeding behavior of piglets. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","Behavior analysis; Feed intake patterns; Image classification; Weaned pigs","Image classification; Video recording; Behavior analysis; Dry feed; Feed consumption; Feed intake; Feed intake pattern; Feeding behavior; Human expert; Image Classifiers; Images classification; Weaned pig; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204963972"
"Mahato S.; Neethirajan S.","Mahato, Shubhangi (59362780200); Neethirajan, Suresh (57217318680)","59362780200; 57217318680","Integrating Artificial Intelligence in dairy farm management − biometric facial recognition for cows","2024","Information Processing in Agriculture","","","","","","","2","10.1016/j.inpa.2024.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206101856&doi=10.1016%2fj.inpa.2024.10.001&partnerID=40&md5=9b1c857209154f8e2341a2469544351d","Faculty of Computer Science, Dalhousie University, 6050 University Ave, Halifax, B3H 1W5, NS, Canada; Department of Animal Science and Aquaculture, Faculty of Agriculture, Dalhousie University, PO Box 550, Truro, B2N 5E3, NS, Canada","Mahato S., Faculty of Computer Science, Dalhousie University, 6050 University Ave, Halifax, B3H 1W5, NS, Canada; Neethirajan S., Faculty of Computer Science, Dalhousie University, 6050 University Ave, Halifax, B3H 1W5, NS, Canada, Department of Animal Science and Aquaculture, Faculty of Agriculture, Dalhousie University, PO Box 550, Truro, B2N 5E3, NS, Canada","The integration of Artificial Intelligence (AI) into dairy farm management through biometric facial recognition of cows marks a significant milestone in livestock care. This comprehensive review explores the development, implementation, and challenges associated with AI-powered biometric facial identification in dairy agriculture. It emphasizes the pivotal role of this innovation in enabling precise monitoring of individual cows, thereby facilitating thorough tracking of their health, behaviors, and productivity levels. Derived from facial recognition technologies originally designed for humans, this approach harnesses distinctive features of cow faces for gentle and immediate observation within large-scale farming operations. The evolution of AI from basic pattern recognition to advanced Convolutional Neural Networks (CNNs) and deep learning frameworks signifies a transition toward data-driven agriculture. This analysis addresses notable challenges such as environmental variability, data collection difficulties, ethical considerations, and technological limitations. Furthermore, it compares various AI frameworks, highlighting their unique advantages and suitability in the dairy farming context. Despite these obstacles, facial recognition technology holds promise for enhancing farm efficiency, improving animal welfare, and promoting sustainable practices, underscoring the need for ongoing research and innovation. We advocate for future investigations focused on enhancing adaptability to diverse environments, ensuring ethical AI deployment, fostering compatibility across different breeds, and integrating with complementary agricultural technologies. Ultimately, this review underscores the transformative impact of AI in advancing dairy farming towards a data-centric future while prioritizing responsible agricultural practices. © 2024 The Author(s)","AI-driven livestock Management; Animal identification technology; Dairy cow biometrics; Dairy welfare; Digital livestock farming; Facial recognition technology; Precision dairy farming; Sustainable dairy practices","Convolutional neural networks; Animal identification; Animal identification technology; Artificial intelligence-driven livestock management; Dairy cow; Dairy cow biometric; Dairy farming; Dairy welfare; Digital livestock farming; Facial recognition; Facial recognition technology; Identification technology; Livestock farming; Precision dairy farming; Sustainable dairy practice","Mitacs; Department of Agriculture, Aquaculture and Fisheries; Natural Sciences and Engineering Research Council of Canada, NSERC; Department of Agriculture, Nova Scotia; Net Zero Atlantic Agency of Canada","The authors sincerely thank the Net Zero Atlantic Agency of Canada, Mitacs Canada, Nova Scotia Department of Agriculture, Department of Agriculture, Aquaculture and Fisheries and the Natural Sciences and Engineering Research Council of Canada for funding this study. ","S. Neethirajan; Faculty of Computer Science, Dalhousie University, Halifax, 6050 University Ave, B3H 1W5, Canada; email: sneethir@gmail.com","","China Agricultural University","20970153","","","","English","Inf. Process. Agric.","Review","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85206101856"
"Anithaa J.; Avanthika R.; Kavipriya B.; Vishnupriya S.","Anithaa, J. (57065181500); Avanthika, R. (59204639700); Kavipriya, B. (59242543800); Vishnupriya, S. (59242567600)","57065181500; 59204639700; 59242543800; 59242567600","Cattle identifcation using muzzle images","2024","Applied Data Science and Smart Systems","","","","370","377","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200182353&partnerID=40&md5=2cfaeaa738dfa045dd5e751b3e0c7be4","Sri Ramakrishna Engineering College, Tamil Nadu, Coimbatore, India","Anithaa J., Sri Ramakrishna Engineering College, Tamil Nadu, Coimbatore, India; Avanthika R., Sri Ramakrishna Engineering College, Tamil Nadu, Coimbatore, India; Kavipriya B., Sri Ramakrishna Engineering College, Tamil Nadu, Coimbatore, India; Vishnupriya S., Sri Ramakrishna Engineering College, Tamil Nadu, Coimbatore, India","Nowadays livestock management is critical for a country’s economy, which includes identification of breed, total count of cattle in a region, and identification of unique cattle. The livestock management is very important for the government, when insurance claims are made during floods or epidemic. Hence advanced techniques are required to use biometrics like muzzle images to uniquely identify the cattle. The cattle identification system’s aim is to identify individual cattle with its unique muzzle print. Similar to finger print of human being, every individual cattle possess unique muzzle patterns. With the feature extraction techniques, the unique extracted features could be matched against the template of cattle to identify it. The feature matching based on YOLO V5 algorithm has obtained an accuracy of 80%, whereas the system that uses feature extraction and deep learning methodology like SIFT, CNN and VGG16 trained with more than 200 cattle images has obtained accuracy of 95%. The extracted features are stored and could be used for matching and identifying the cattle in future. This would prevent many issues like false insurance claim, help the abattoir to track their cattle, etc. © 2025 selection and editorial matter, Jaiteg Singh, S B Goyal, Rajesh Kumar Kaushal, Naveen Kumar and Sukhjit Singh Sehra; individual chapters, the contributors.","CNN; Feature extraction; Muzzle images; Muzzle pattern; VGG16; YoloV5 model","","","","J. Anithaa; Sri Ramakrishna Engineering College, Coimbatore, Tamil Nadu, India; email: anitha.j@srec.ac.in","","CRC Press","","978-104001723-4; 978-103274814-6","","","English","Appl. Data Science and Smart Systems","Book chapter","Final","","Scopus","2-s2.0-85200182353"
"Meng Y.; Yoon S.; Han S.; Fuentes A.; Park J.; Jeong Y.; Park D.S.","Meng, Yao (57907038700); Yoon, Sook (35779575000); Han, Shujie (57201776906); Fuentes, Alvaro (57194569998); Park, Jongbin (56095724400); Jeong, Yongchae (7202332048); Park, Dong Sun (7403245797)","57907038700; 35779575000; 57201776906; 57194569998; 56095724400; 7202332048; 7403245797","Improving Known–Unknown Cattle’s Face Recognition for Smart Livestock Farm Management","2023","Animals","13","22","3588","","","","3","10.3390/ani13223588","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178085572&doi=10.3390%2fani13223588&partnerID=40&md5=d5824a6a1d9dbf4267d942521e68548f","Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea; Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Department of Computer Engineering, Mokpo National University, Mokpo, 58554, South Korea","Meng Y., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Yoon S., Department of Computer Engineering, Mokpo National University, Mokpo, 58554, South Korea; Han S., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Fuentes A., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Park J., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Jeong Y., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea; Park D.S., Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea","Accurate identification of individual cattle is of paramount importance in precision livestock farming, enabling the monitoring of cattle behavior, disease prevention, and enhanced animal welfare. Unlike human faces, the faces of most Hanwoo cattle, a native breed of Korea, exhibit significant similarities and have the same body color, posing a substantial challenge in accurately distinguishing between individual cattle. In this study, we sought to extend the closed-set scope (only including identifying known individuals) to a more-adaptable open-set recognition scenario (identifying both known and unknown individuals) termed Cattle’s Face Open-Set Recognition (CFOSR). By integrating open-set techniques to enhance the closed-set accuracy, the proposed method simultaneously addresses the open-set scenario. In CFOSR, the objective is to develop a trained model capable of accurately identifying known individuals, while effectively handling unknown or novel individuals, even in cases where the model has been trained solely on known individuals. To address this challenge, we propose a novel approach that integrates Adversarial Reciprocal Points Learning (ARPL), a state-of-the-art open-set recognition method, with the effectiveness of Additive Margin Softmax loss (AM-Softmax). ARPL was leveraged to mitigate the overlap between spaces of known and unknown or unregistered cattle. At the same time, AM-Softmax was chosen over the conventional Cross-Entropy loss (CE) to classify known individuals. The empirical results obtained from a real-world dataset demonstrated the effectiveness of the ARPL and AM-Softmax techniques in achieving both intra-class compactness and inter-class separability. Notably, the results of the open-set recognition and closed-set recognition validated the superior performance of our proposed method compared to existing algorithms. To be more precise, our method achieved an AUROC of 91.84 and an OSCR of 87.85 in the context of open-set recognition on a complex dataset. Simultaneously, it demonstrated an accuracy of 94.46 for closed-set recognition. We believe that our study provides a novel vision to improve the classification accuracy of the closed set. Simultaneously, it holds the potential to significantly contribute to herd monitoring and inventory management, especially in scenarios involving the presence of unknown or novel cattle. © 2023 by the authors.","animal welfare; cattle’s face recognition; deep learning; open-set recognition; precision livestock farming","accuracy; agricultural worker; algorithm; animal welfare; Article; artificial intelligence; artificial neural network; artificial ventilation; behavior; bovine; breed; classifier; convolutional neural network; deep learning; deterioration; facial recognition; feeding; herd; human; Korea; learning; livestock; multinomial logistic regression; receiver operating characteristic; reproduction; training","Korea Smart Farm Foundation; Ministry of Education, MOE, (2019R1A6A1A09031717, 2020R1A2C2013060); Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (1545027423); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET; National Research Foundation of Korea, NRF","This work was supported by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm Foundation (KosFarm) through the Smart Farm Innovation Technology Development Program, funded by the Ministry of Agriculture, Food and Rural Affairs (MAFRA) and the Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (1545027423). This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2019R1A6A1A09031717). This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (2020R1A2C2013060).","D.S. Park; Department of Electronic Engineering, Jeonbuk National University, Jeonju, 54896, South Korea; email: dspark@jbnu.ac.kr; S. Yoon; Department of Computer Engineering, Mokpo National University, Mokpo, 58554, South Korea; email: syoon@mokpo.ac.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85178085572"
"Dayan J.; Goldman N.; Halevy O.; Uni Z.","Dayan, Jonathan (57219095070); Goldman, Noam (58296159200); Halevy, Orna (7004134111); Uni, Zehava (7004268021)","57219095070; 58296159200; 7004134111; 7004268021","Predictive precision: AI histology for preventing breast muscle myopathy and food loss","2024","11th European Conference on Precision Livestock Farming","","","","987","991","4","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204983586&partnerID=40&md5=f511061accfbf0ce16c5bc8ef426388f","Department of Animal Science, The Robert H. Smith Faculty of Agriculture, Food and Environment, The Hebrew University of Jerusalem, Rehovot, 7610001, Israel","Dayan J., Department of Animal Science, The Robert H. Smith Faculty of Agriculture, Food and Environment, The Hebrew University of Jerusalem, Rehovot, 7610001, Israel; Goldman N., Department of Animal Science, The Robert H. Smith Faculty of Agriculture, Food and Environment, The Hebrew University of Jerusalem, Rehovot, 7610001, Israel; Halevy O., Department of Animal Science, The Robert H. Smith Faculty of Agriculture, Food and Environment, The Hebrew University of Jerusalem, Rehovot, 7610001, Israel; Uni Z., Department of Animal Science, The Robert H. Smith Faculty of Agriculture, Food and Environment, The Hebrew University of Jerusalem, Rehovot, 7610001, Israel","The global demand for high-yield and quality chicken meat emphasizes the need for innovative tools in poultry research. Moreover, the increasing incidence of breast muscle abnormalities, such as White Striping (WS), Wooden Breast (WB), and Spaghetti Meat (SM), amplifies the urgency for tools capable of rapid and precise diagnosis of breast muscle development and morphology in broiler chickens. In response to this critical need, we present a novel deep learning-based automated image analysis workflow, combining Fiji (ImageJ) with Cellpose and MorphoLibJ plugins to provide automated diameter and cross-sectional area quantification for broiler breast muscle. Previous research in our lab has demonstrated the efficiency and accuracy of this tool. Comparing the AI image analysis tool with manual analysis demonstrated an impressive accuracy rate of up to 99.91%, coupled with significantly enhanced speed and productivity. The automated workflow processed 70 times more data sets in 38-fold less time. The implementation of this method is intended to detect breast muscle myopathies (BMM) in broiler flocks at two weeks of age. We introduce a new concept of relative myofiber size, providing a unified metric and predictive threshold, showcasing a clear separation between moderate and severe myopathy broilers. This approach will enable early adjustment of poultry management, implementing preventive actions such as modifications of feeding programs and/or lighting regimen, to reduce growth rates, a factor known to be associated with increased myopathy prevalence. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","automated image analysis; breast muscle; broiler chicken; histology; myopathy","Medical imaging; Poultry; Automated image analysis; Breast muscle; Broiler chickens; Chicken meat; Food loss; Global demand; High quality; Higher yield; Myopathy; Predictive precision; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204983586"
"Durand M.; Largouët C.; Bonneau de Beaufort L.; Dourmad J.Y.; Gaillard C.","Durand, M. (57226159788); Largouët, C. (8925681800); Bonneau de Beaufort, L. (56925448600); Dourmad, J.Y. (56268318900); Gaillard, C. (56988804400)","57226159788; 8925681800; 56925448600; 56268318900; 56988804400","A dataset to study group-housed sows’ individual behaviours and production responses to different short-term events","2023","Animal - Open space","2","","100039","","","","2","10.1016/j.anopes.2023.100039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160385454&doi=10.1016%2fj.anopes.2023.100039&partnerID=40&md5=7ad64a1e1e4320faf1c2accfcda286f7","PEGASE, INRAE, Institut Agro, Saint Gilles, 35590, France; Institut Agro, Univ Rennes1, CNRS, INRIA, IRISA, Rennes, 35000, France","Durand M., PEGASE, INRAE, Institut Agro, Saint Gilles, 35590, France; Largouët C., Institut Agro, Univ Rennes1, CNRS, INRIA, IRISA, Rennes, 35000, France; Bonneau de Beaufort L., Institut Agro, Univ Rennes1, CNRS, INRIA, IRISA, Rennes, 35000, France; Dourmad J.Y., PEGASE, INRAE, Institut Agro, Saint Gilles, 35590, France; Gaillard C., PEGASE, INRAE, Institut Agro, Saint Gilles, 35590, France","The relational database SOWELL was created to better understand the behaviour and individual responses of gestating sows facing different short-term events induced: a competitive situation for feed, hot and cold thermal conditions, a sound event, an enrichment (straw, ropes and bags available) and an impoverishment (no straw, no objects) of the pen. The data were collected on 102 crossbred sows equipped with activity sensors, group-housed in video-recorded pens (16–18 sows per pen), with access to automatons. Feeding and drinking behaviours were extracted from the electronic feeders and drinkers’ recordings. Social behaviours, physical activities and locations in the pen were recorded thanks to manual video analysis labelling at the individual scale. Accelerometer fixed on the sows’ ears also recorded individual physical activities. The physical activity was also determined at a group scale by automatic video analysis using deep learning techniques. BWs, back fat thickness, and body condition (cleanliness, body damages) were recorded weekly during the whole gestation. Last gestation room data regarding environmental conditions (temperature, humidity, noise level) were recorded using automatic sensors. The database can fulfil different research purposes, namely sows’ nutrition for example to better calculate the energy requirements regarding environmental factors, or also on welfare or health during gestation by providing indicators. © 2023 The Author(s)","Health; Nutrition; Precision livestock farming; Sensor; Welfare","","Institut National de Recherche pour l'Agriculture, l'Alimentation et l'Environnement, INRAE; Agence Nationale de la Recherche, ANR, (ANR‐16‐CONV‐0004); Agence Nationale de la Recherche, ANR","This work was supported by the French National Research Agency under the Investments for the Future Program, referred as ANR\u201016\u2010CONV\u20100004 (#DIGITAG) and the department PHASE of INRAE.","M. Durand; PEGASE, INRAE, Institut Agro, Saint Gilles, 35590, France; email: maeva.durand@inrae.fr","","Elsevier B.V.","27726940","","","","English","Anim. Open Sp.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85160385454"
"Mate S.; Somani V.; Dahiwale P.","Mate, Sanjay (58137443700); Somani, Vikas (58312950500); Dahiwale, Prashant (36052433800)","58137443700; 58312950500; 36052433800","Applications of Machine Learning to Address Complex Problems in Livestock","2024","2024 3rd International Conference for Innovation in Technology, INOCON 2024","","","","","","","1","10.1109/INOCON60754.2024.10511471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193640294&doi=10.1109%2fINOCON60754.2024.10511471&partnerID=40&md5=1127fc086e241cccdffcce5fa00454fa","Sangam University, Govt. Polytechnic Daman, Dept. of Cse, Daman, India; Sangam University, Dept. of Cse, Bhilwara, India; Govt. Polytechnic Daman, Dept. of Computer Engg., Daman, India","Mate S., Sangam University, Govt. Polytechnic Daman, Dept. of Cse, Daman, India; Somani V., Sangam University, Dept. of Cse, Bhilwara, India; Dahiwale P., Govt. Polytechnic Daman, Dept. of Computer Engg., Daman, India","Machine Learning and Deep Learning-based applications have a vast potential to monitor livestock. Precise Livestock Farming (PLF) is essential to the smooth functioning of a farm to make it profitable in all aspects. PLF derives through feed monitoring, reproductively cycling, health, farm shelter, etc. It helps to maintain overall productivity and recognize health issues like foot-and-mouth disease, lameness, stress level, respiration rate, heart rates, lumpy skin disease etc., and bio-waste management. Assessment helps plan timely medication, vaccination, precaution and treatment for viral diseases. PLF also provides a solution for better health through emotional checkups to avoid loss from livestock. Precise Livestock Farming (PLF) can be practical when associated activities are effectively implemented and categorized in SLF and DLF. © 2024 IEEE.","Body Condition Score (BCS); Digital Livestock Farming (DLF); Precise Livestock Farming (PLF); Smart Livestock Farming (SLF)","Diseases; E-learning; Farms; Learning systems; Waste management; Body condition score; Complex problems; Digital livestock farming; Livestock farming; Machine-learning; Precise livestock farming; Smart livestock farming; Deep learning","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038193-1","","","English","Int. Conf. Innov. Technol., INOCON","Conference paper","Final","","Scopus","2-s2.0-85193640294"
"Patil R.; Hemavathy B.; Sarangi S.; Singh D.; Chakraborty R.; Junagade S.; Pappula S.","Patil, Ruturaj (58710008400); Hemavathy, B. (57222260106); Sarangi, Sanat (55633353700); Singh, Dineshkumar (56046410400); Chakraborty, Rupayan (55752065000); Junagade, Sanket (57427587500); Pappula, Srinivasu (6505567619)","58710008400; 57222260106; 55633353700; 56046410400; 55752065000; 57427587500; 6505567619","Identifying Indian Cattle Behaviour Using Acoustic Biomarkers","2024","International Conference on Pattern Recognition Applications and Methods","1","","","594","602","8","0","10.5220/0012576800003654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190654461&doi=10.5220%2f0012576800003654&partnerID=40&md5=f2cd079b3bc7aff21346ca1c3c7e1a9a","TCS Research, Tata Consultancy Services Limited, India","Patil R., TCS Research, Tata Consultancy Services Limited, India; Hemavathy B., TCS Research, Tata Consultancy Services Limited, India; Sarangi S., TCS Research, Tata Consultancy Services Limited, India; Singh D., TCS Research, Tata Consultancy Services Limited, India; Chakraborty R., TCS Research, Tata Consultancy Services Limited, India; Junagade S., TCS Research, Tata Consultancy Services Limited, India; Pappula S., TCS Research, Tata Consultancy Services Limited, India","A system to recognise sounds from some major cattle breeds commonly found in India and linking them to intents reflecting specific behaviour along with associated needs is proposed. Cattle breeds in India consist of a mix of indigenous and exotic breeds where Sindhi, Sahiwal, and Gir make up a significant fraction of the indigenous breeds. Exotic breeds are Jersey and Holstein Friesian. Vocalisation from the animals in this cattle group is used to create a sound dataset comprising 120 utterances for over six intents where the intents were labelled by domain experts familiar with the animals and their behaviour. MFCCs and OpenSMILE global features from the audio signal with 6552 properties are used to model for intent recognition. The dataset is scaled and augmented with four different methods to 870 cattle sounds for the six classes. Two model architectures are created and tested on data for each method independently and with all of them together. The models are also tested on unseen cattle sounds for speaker independent verification. An accuracy of 97% was obtained for intent classification with MFCCs and OpenSMILE features. This indicates that behaviour recognition from sounds for Indian cattle breeds is possible with a good confidence level. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.","Acoustic Feature Recognition; Cattle Vocalisation; Deep Learning; Livestock Behaviour; OpenSMILE","","","","","Castrillon-Santana M.; De Marsico M.; Fred A.","Science and Technology Publications, Lda","21844313","978-989758684-2","","","English","Int. Conf. Pattern. Recognit. Appl. Method.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85190654461"
"Fang H.; Shi B.; Sun Y.; Xiong N.; Zhang L.","Fang, Hao (59380928600); Shi, Binbin (59381241600); Sun, Yongpeng (58644800100); Xiong, Neal (35231569200); Zhang, Lijuan (57202038419)","59380928600; 59381241600; 58644800100; 35231569200; 57202038419","APEST-YOLO: A MULTI-SCALE AGRICULTURAL PEST DETECTION MODEL BASED ON DEEP LEARNING","2024","Applied Engineering in Agriculture","40","5","","553","564","11","0","10.13031/aea.15987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207286522&doi=10.13031%2faea.15987&partnerID=40&md5=a361e6a08eaa8fc2c6901389b97e11ae","Zhejiang Academy of Agricultural Sciences, Zhejiang, Hangzhou, China; School of Computer Science & Technology, Hangzhou Dianzi University, Qiantang, Hangzhou, China; Department of Computer Science and Mathematics, Sul Ross State University, Alpine, TX, United States; Zhejiang University of Science and Technology, Xihu, Hangzhou, China","Fang H., Zhejiang Academy of Agricultural Sciences, Zhejiang, Hangzhou, China; Shi B., School of Computer Science & Technology, Hangzhou Dianzi University, Qiantang, Hangzhou, China; Sun Y., Zhejiang Academy of Agricultural Sciences, Zhejiang, Hangzhou, China; Xiong N., Department of Computer Science and Mathematics, Sul Ross State University, Alpine, TX, United States; Zhang L., Zhejiang University of Science and Technology, Xihu, Hangzhou, China","Crop pests and diseases pose a significant threat to smart agriculture, making pest detection a critical component in agricultural applications. However, current detection methods often struggle to effectively identify multi-scale pest data. In response, we present a novel agricultural pest detection model (APest-YOLO) based on a lightweight approach. The APest-YOLO model enhances pest detection efficiency while reducing model size, which is different from the baseline models. Our model features an original grouping atrous spatial pyramid pooling fast module, comprising four convolution layers with varying rates to capture multi-scale and multi-level pest characteristics. Additionally, we incorporate a convolutional block attention module to extract smoother features from pest images with noisy and complex backgrounds. We evaluated the APest-YOLO model on a large-scale multi-pest dataset encompassing 37 pest species. Furthermore, the APest- YOLO model achieved 99.3% mAP0.5 and found that it outperforms baseline models, demonstrating effective pest species detection capabilities. © 2024 American Society of Agricultural and Biological Engineers.","Attention mechanism; Convolutional neural network; Intelligent agriculture; Pest detection; YOLO","Agricultural pests; Attention mechanisms; Baseline models; Convolutional neural network; Detection models; Intelligent agriculture; Model-based OPC; Multi-scales; Pest detection; YOLO; Livestock","Natural Science Foundation of Zhejiang Province, ZJNSF, (LQ23F010004); Natural Science Foundation of Zhejiang Province, ZJNSF; National Natural Science Youth Science Foundation Project, (62201508)","The research was partially funded by the Zhejiang Provincial Natural Science Foundation Youth Fund Project (Grant No. LQ23F010004) and the National Natural Science Youth Science Foundation Project (Grant No. 62201508).","L. Zhang; Zhejiang University of Science and Technology, Hangzhou, Xihu, China; email: zhanglijuan931026@foxmail.com","","American Society of Agricultural and Biological Engineers","08838542","","AEAGE","","English","Appl Eng Agric","Article","Final","","Scopus","2-s2.0-85207286522"
"Kim J.-J.; Kim G.-W.; Choi J.-H.; Kim K.-H.; Bae J.-H.; Koo K.-W.","Kim, Jung-Ju (44961339200); Kim, Gi-Won (58788775700); Choi, Jae-Hoon (58788540900); Kim, Ki-Hyun (57295522500); Bae, Jung-Hyoun (57211240405); Koo, Kyung-Wan (14919506100)","44961339200; 58788775700; 58788540900; 57295522500; 57211240405; 14919506100","A study on the multi-sensing of complex environment for smart livestock farming and biometric information measurement method for respective livestock based on BLE","2023","Transactions of the Korean Institute of Electrical Engineers","72","12","","1795","1801","6","0","10.5370/KIEE.2023.72.12.1795","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181053934&doi=10.5370%2fKIEE.2023.72.12.1795&partnerID=40&md5=d8c0457d4c6b4233c3a9282c9f8246c1","Dept. of Automotive ICT Engineering, Hoseo University, South Korea; Dept. of Robotics, Hoseo University, South Korea; Dept. of Robotics and Automation Engineering, Hoseo University, South Korea; Azita Co., Ltd., South Korea","Kim J.-J., Dept. of Robotics, Hoseo University, South Korea; Kim G.-W., Dept. of Robotics and Automation Engineering, Hoseo University, South Korea; Choi J.-H., Dept. of Robotics and Automation Engineering, Hoseo University, South Korea; Kim K.-H., Dept. of Automotive ICT Engineering, Hoseo University, South Korea; Bae J.-H., Azita Co., Ltd., South Korea; Koo K.-W., Dept. of Automotive ICT Engineering, Hoseo University, South Korea","Declining population and aging farmers are causing difficulties in agricultural management. In this study, we propose a system that combines BLE technologies and indoor positioning technologies for smart livestock farming to track the complex environment and the position of respective livestock. sensor data measuring the complex environment is transmitted via the MQTT protocol. BLE beacons and cameras are employed to enable real-time tracking of livestock positions and monitor the body temperature of respective livestock for prompt assessment of their health status. In particular, we develop a new algorithm, adaptive triangulation, to accurately track the location of livestock products using the BLE beacon. The developed adaptive triangulation algorithm shows 90.12 % accuracy because of the experiment, showing higher location recognition accuracy than the existing triangulation method. The smart livestock farming system based on BLE is expected to improve productivity and reduce costs in the agricultural industry and contribute to the stable growth and development of farmers. Copyright ©The Korean Institute of Electrical Engineers.","BLE beacon; Computer vision; Deep learning; Indoor positioning; MQTT; Smart livestock farming monitoring system","Computer vision; Deep learning; Farms; Indoor positioning systems; Biometrics informations; BLE beacon; Complex environments; Deep learning; Indoor positioning; Information measurement; Livestock farming; Monitoring system; MQTT; Smart livestock farming monitoring system; Triangulation","Ministry of Agriculture, Food and Rural Affairs, MAFRA, (322102-03); Ministry of Agriculture, Food and Rural Affairs, MAFRA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry(IPET) through 2025 Livestock Industrialization Technology Develop-ment Program, funded by Ministry of Agriculture, Food and Rural Affairs(MAFRA) (NO. 322102-03).","K.-W. Koo; Dept. of Automotive ICT Engineering, Hoseo University, South Korea; email: alarmkoo@Hoseo.edu","","Korean Institute of Electrical Engineers","19758359","","","","Korean","Trans. Korean Inst. Electr. Eng.","Article","Final","","Scopus","2-s2.0-85181053934"
"Ocholla I.A.; Pellikka P.; Karanja F.N.; Vuorinne I.; Odipo V.; Heiskanen J.","Ocholla, Ian A. (58829676200); Pellikka, Petri (7007042259); Karanja, Faith N. (22134890000); Vuorinne, Ilja (57221478085); Odipo, Victor (57192682740); Heiskanen, Janne (9842163500)","58829676200; 7007042259; 22134890000; 57221478085; 57192682740; 9842163500","Livestock detection in African rangelands: Potential of high-resolution remote sensing data","2024","Remote Sensing Applications: Society and Environment","33","","101139","","","","5","10.1016/j.rsase.2024.101139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182871441&doi=10.1016%2fj.rsase.2024.101139&partnerID=40&md5=b6eb0615a81434bfaef64950a39a0bf1","Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland; Institute for Atmosphere and Earth System Research, University of Helsinki, Helsinki, FI-00014, Finland; Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197, Nairobi, Kenya; International Livestock Research Institute, P.O. Box 30709, Nairobi, Kenya; Finnish Meteorological Institute, P.O. Box 503, Helsinki, FI-00101, Finland","Ocholla I.A., Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland, Institute for Atmosphere and Earth System Research, University of Helsinki, Helsinki, FI-00014, Finland; Pellikka P., Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland, Institute for Atmosphere and Earth System Research, University of Helsinki, Helsinki, FI-00014, Finland; Karanja F.N., Department of Geospatial and Space Technology, University of Nairobi, P.O. Box 30197, Nairobi, Kenya; Vuorinne I., Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland, Institute for Atmosphere and Earth System Research, University of Helsinki, Helsinki, FI-00014, Finland; Odipo V., International Livestock Research Institute, P.O. Box 30709, Nairobi, Kenya; Heiskanen J., Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland, Finnish Meteorological Institute, P.O. Box 503, Helsinki, FI-00101, Finland","Livestock production is vital in eradicating poverty, malnutrition, and in attainment of the Sustainable Development Goals (SDG) in developing regions such as Africa. The livestock sector of Africa contributes 10%–44% of the gross domestic product and more than 70% of the greenhouse gas emissions of the continent. With the anticipated increase in demand for livestock products, the need to mitigate climate change, and lack of accurate livestock census data, innovative remote sensing technologies and methods for livestock census become crucial for the livestock sector. In this paper, we present a review of current technological advancements in remote sensing and detection algorithms in livestock censuses, identifying weaknesses in sensors and detection methods, and highlighting issues that currently limit adoption of these technologies in African countries. We observed that the last four years (2019–2022) accounted for 69% of all livestock detection studies. This surge was driven by development of Unmanned Aerial Vehicles, which offer high resolution images and flexibility for detection. In addition, the use of automated detection methods are fast, efficient and accurate. However, the surrounding background of different livestock species, herd size and spatial resolution of the datasets affects detection accuracy. We suggest the need for publicly accessible aerial labelled livestock databases covering the various livestock breeds in Africa to develop customized detection models for the heterogeneous landscapes in the rangelands. Efficient detection methods are vital for monitoring livestock population trends and environmental impacts of grazing practises. © 2024 The Authors","Deep learning; Livestock; Object detection; Unmanned aerial vehicles; VHR imagery","","European Commission, EC, (FOOD/2020/418–132); European Commission, EC","This work was funded by European Union DG International Partnerships under DeSIRA (Development of Smart Innovation through Research in Agriculture) programme ( FOOD/2020/418–132 ) through the Earth observation and environmental sensing for climate-smart sustainable agropastoral ecosystem transformation in East Africa (ESSA) project . ","I.A. Ocholla; Earth Change Observation Laboratory, Department of Geosciences and Geography, University of Helsinki, P.O. Box 64, FI-00014, Finland; email: ian.ocholla@helsinki.fi","","Elsevier B.V.","23529385","","","","English","Remote Sens. Appl. Soc. Environ.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85182871441"
"Deepak D.; D'Mello D.A.; Divakarla U.","Deepak, D. (58834211000); D'Mello, Demian Antony (57485362500); Divakarla, Usha (36871439100)","58834211000; 57485362500; 36871439100","Advancements in Automated Livestock Monitoring: A Concise Review of Deep Learning-Based Cattle Activity Recognition","2024","10th International Conference on Advanced Computing and Communication Systems, ICACCS 2024","","","","321","327","6","0","10.1109/ICACCS60874.2024.10717327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208652738&doi=10.1109%2fICACCS60874.2024.10717327&partnerID=40&md5=b64ba7705b93e29302fa83aaec23499d","Canara Engineering College, Department of Computer Science and Engineering, Karnataka, Bantwal, India; Nmam Institute of Technology, Department of Information Science and Engineering, Karnataka, Nitte, India","Deepak D., Canara Engineering College, Department of Computer Science and Engineering, Karnataka, Bantwal, India; D'Mello D.A., Canara Engineering College, Department of Computer Science and Engineering, Karnataka, Bantwal, India; Divakarla U., Nmam Institute of Technology, Department of Information Science and Engineering, Karnataka, Nitte, India","Cattle activity serves as a critical indicator of animal health and welfare, offering valuable insights into both physical and mental conditions. The livestock industry has embraced cutting-edge AI and computer vision techniques, particularly empowered by deep learning (DL) models, serving as decision-making tools. These models have successfully tackled various challenges, ranging from individual cattle identification, tracking movement, and body part recognition to cattle classification. The incorporation of cutting-edge machine learning and deep learning technologies has been instrumental in advancing precision livestock management. This includes functions like disease detection, tracking, and health monitoring. This paper conducts a review of DL based cattle recognition, focusing on the exploration and analysis of research concerning cattle activity recognition through Deep Learning (DL). It provides a concise overview of DL-based approaches, organized based on a taxonomy. Drawing from our review, prominent DL models such as Convolutional Neural Network (CNN), CNN-LSTM, Faster R-CNN, and You Only Look Once (YOLO) emerge as widely adopted choices for cattle activity recognition. The insights gained from this review not only inspire the advancement of robust automated activity recognition but also contribute valuable guidance for the evolution of deep learning techniques in this domain. In conclusion, this review paves the way for future research focused on the development of effective automated systems for recognizing cattle activities © 2024 IEEE.","Cattle activity; Cattle detection; Cattle health; Cattle identification; CNN; CNN-LSTM; Faster R-CNN; YOLO","Automatic guidance (agricultural machinery); Convolutional neural networks; Cattle activity; Cattle detection; Cattle health; Cattle identification; Cattles; Convolutional neural network; Convolutional neural network-LSTM; Fast R-convolutional neural network; You only look once","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038436-9","","","English","Int. Conf. Adv. Comput. Commun. Syst., ICACCS","Conference paper","Final","","Scopus","2-s2.0-85208652738"
"Perez-Garcia C.; Benni S.; Tassinari P.; Torreggiani D.; Bovo M.","Perez-Garcia, C. (58618608000); Benni, S. (24334072400); Tassinari, P. (13609849700); Torreggiani, D. (24336908000); Bovo, M. (55504159800)","58618608000; 24334072400; 13609849700; 24336908000; 55504159800","Predicting Equivalent Temperature Index in Dairy Cows with the NeuralProphet Model","2024","11th European Conference on Precision Livestock Farming","","","","1421","1428","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204952393&partnerID=40&md5=8b772e538dad55d35dd70f98b7aadb5f","Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy","Perez-Garcia C., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Benni S., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Tassinari P., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Torreggiani D., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Bovo M., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy","Precision Livestock Farming (PLF) tools have become an integral part of sustainable livestock management, offering increased productivity while minimizing environmental impact. This study explores the application of the NeuralProphet model to predict Equivalent Temperature Index (ETI) in dairy cows, leveraging the development of powerful statistical algorithms and advances in artificial intelligence, particularly in the deep learning subset. The data used come from a smart monitoring system implemented in a pilot dairy farm located in Budrio (Emilia-Romagna, Italy) while the data of the main external environmental parameters were obtained from the weather service provider Open-Meteo, taking advantage of the performance of predictive models already tested in various applications. In this study, a predictive model for ETI was developed through an initial data exploration that includes preprocessing tasks such as outlier detection, unit adjustment, and calculation of the target variable. Exogenous variables were analyzed for sensitivity, resulting in the selection of 24-hour forecasts for outdoor temperature, relative humidity, solar radiation, and wind speed as model inputs. In addition, values for indoor temperature, relative humidity, and wind speed were included as initial conditions for model reproducibility in future studies. As a final step, the evaluation of the accuracy of the model was performed by comparing the indoor environmental parameters of the building provided by the model with raw measurements. As a result, a reliable tool was synthesized to support the decision-making process of farmers to improve livestock welfare and increase animal productivity. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","data analysis; deep learning; environmental parameters; smart farming","Dairy cow; Deep learning; Environmental parameter; Equivalent temperature; Precision livestock farming; Predictive models; Smart farming; Temperature index; Temperature-relative humidity; Wind speed; Decision making","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204952393"
"Shaik M.A.; Parveen M.; Qureshi I.","Shaik, Mohammed Ali (57189589334); Parveen, Masrath (58711220700); Qureshi, Imran (57209492353)","57189589334; 58711220700; 57209492353","Leveraging Machine Learning and Drone Technology for Effective Insect Pest Management in Agriculture","2024","2024 Asia Pacific Conference on Innovation in Technology, APCIT 2024","","","","","","","1","10.1109/APCIT62007.2024.10673597","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205593620&doi=10.1109%2fAPCIT62007.2024.10673597&partnerID=40&md5=d6fb66b5cf7b0e3a577364fe73c069ce","SR University, School of Computer Science & Artificial Intelligence, Telangana State, Warangal, India; Vidya Jyothi Institute of Technology, Department of Information Technology, Telangana State, Hyderabad, India; AlMusanna College of Technology, College of Computing and Information Sciences, Oman","Shaik M.A., SR University, School of Computer Science & Artificial Intelligence, Telangana State, Warangal, India; Parveen M., Vidya Jyothi Institute of Technology, Department of Information Technology, Telangana State, Hyderabad, India; Qureshi I., AlMusanna College of Technology, College of Computing and Information Sciences, Oman","Insect pests pose a serious threat to food security and agricultural profitability, as they cause direct damage to crops and transmit plant diseases. Manual monitoring methods are inadequate to detect and prevent infestations, especially in large-scale farming operations. This article explores the potential of using drone technology and machine learning algorithms to enhance insect pest management in agriculture. Drones can capture high-resolution images of fields, while machine learning can analyze the data and identify signs of insect activity. However, this approach faces several challenges, such as the diversity and complexity of insect species, their behavior, and their impact on crops. Therefore, a multidisciplinary collaboration among researchers, farmers, technologists, and entomologists is essential to develop effective and robust solutions. This article aims to provide an overview of the current state and future prospects of using drone technology and machine learning for insect pest management in agriculture. © 2024 IEEE.","decision tree; Deep learning; drone; Insect Pest; machine learning","Adversarial machine learning; Drones; Invertebrates; Livestock; Deep learning; Food security; Insects pests; Large-scales; Machine-learning; Manual monitoring; Monitoring methods; Pest management; Plant disease; Technology learning; Plant diseases","","","M.A. Shaik; SR University, School of Computer Science & Artificial Intelligence, Warangal, Telangana State, India; email: niharali@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835036153-7","","","English","Asia Pac. Conf. Innov. Technol., APCIT","Conference paper","Final","","Scopus","2-s2.0-85205593620"
"Gowri A.S.; Yovan I.; Sundarsingh Jebaseelan S.D.; Anitha Selvasofia S.D.; Nandhana N.","Gowri, A.S. (57211669131); Yovan, Immanuel (56648167700); Sundarsingh Jebaseelan, S.D. (56429493200); Anitha Selvasofia, S.D. (57209499282); Nandhana, N. (59249479200)","57211669131; 56648167700; 56429493200; 57209499282; 59249479200","Satellite Image Based Animal Identification System Using Deep Learning Assisted Remote Sensing Strategy","2024","Proceedings - 3rd International Conference on Advances in Computing, Communication and Applied Informatics, ACCAI 2024","","","","","","","0","10.1109/ACCAI61061.2024.10602411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200555051&doi=10.1109%2fACCAI61061.2024.10602411&partnerID=40&md5=9001227a41ff18a22f95693f0d21599d","Srm Institute of Science and Technology, Communications School of Computing, Chennai, India; Sathyabama Institute of Science and Technology, Department of Maths, Chennai, India; Sathyabama Institute of Science and Technology, Department of Eee, Chennai, India; Sri Ramakrishna Engineering College, Department of Civil, India","Gowri A.S., Srm Institute of Science and Technology, Communications School of Computing, Chennai, India; Yovan I., Sathyabama Institute of Science and Technology, Department of Maths, Chennai, India; Sundarsingh Jebaseelan S.D., Sathyabama Institute of Science and Technology, Department of Eee, Chennai, India; Anitha Selvasofia S.D., Sri Ramakrishna Engineering College, Department of Civil, India; Nandhana N., Sri Ramakrishna Engineering College, Department of Civil, India","Over the past ten years, tiny Unmanned Aerial Vehicles (UAVs) have exploded in popularity for a variety of aerial monitoring applications, including livestock counting, wildlife tracking in their natural environments, and agricultural region monitoring. When used in conjunction with deep learning, they make it possible to analyze and recognize images automatically. Recently, species population detection and monitoring in remotely sensed data has been made possible with the use of deep learning, an efficient machine learning technology. Animal recognition in satellite and aerial photos is one area where deep learning approaches are finding practical use right now, and this paper intends to give an experimental review of those areas. To simplify the process of animal identification from satellite images, this study presented a new deep learning method called the Learning based Animal Sensing and Classification Model (LASCM). To test how well the method worked, it was cross-validated with the traditional Random Forest (RF) model. The primary obstacles to implementing these deep learning techniques are unbalanced datasets, tiny samples, tiny objects, picture annotation techniques, picture backgrounds, animal counting, evaluation of model performance, and uncertainty calculation. Barely and self-supervised techniques for learning, optimizing either favorable or adverse instances, improving network architecture, and sample annotation methods were all considered as potential answers. The following areas are projected to get increased attention in the next years: video-based detection; detection based on extremely high-resolution satellite images; identification of several species; novel methods for annotation; and the creation of specialized network frameworks and big foundational modelling. The proposed methodology is designed to sort out all these problems and provide an efficient animal identification scheme based on deep learning model from the satellite images.  © 2024 IEEE.","Animal Classification; Animal Identification; Deep Learning; LASCM; Random Forest; Remote Sensing; RF; Satellite Image","Aircraft detection; Antennas; Deep learning; Forestry; Image classification; Learning algorithms; Learning systems; Network architecture; Population statistics; Remote sensing; Satellites; Animal classification; Animal identification; Classification models; Deep learning; Learning based animal sensing and classification model; Random forests; Remote-sensing; Satellite images; Sensing model; Animals","","","A.S. Gowri; Srm Institute of Science and Technology, Communications School of Computing, Chennai, India; email: sivakgowri@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835038943-2","","","English","Proc. - Int. Conf. Adv. Comput., Commun. Appl. Informatics, ACCAI","Conference paper","Final","","Scopus","2-s2.0-85200555051"
"Sreekanth G.R.; Latha R.S.; Roja R.; Sankarnanth S.; Gayathri A.","Sreekanth, G.R. (57211025955); Latha, R.S. (59484419700); Roja, R. (59507599500); Sankarnanth, S. (59156028500); Gayathri, A. (57610371900)","57211025955; 59484419700; 59507599500; 59156028500; 57610371900","Detection of Corn Leaf Infection Using CNN with Various Optimizers","2024","2nd International Conference on Artificial Intelligence and Machine Learning Applications: Healthcare and Internet of Things, AIMLA 2024","","","","","","","0","10.1109/AIMLA59606.2024.10531505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195111942&doi=10.1109%2fAIMLA59606.2024.10531505&partnerID=40&md5=d449b77bb74e5e34d3b1d189c4f7495c","Kongu Engineering College, Department of Computer Science and Engineering, Erode, India; Kongu Engineering College, Department of Computer Technology, Erode, India","Sreekanth G.R., Kongu Engineering College, Department of Computer Science and Engineering, Erode, India; Latha R.S., Kongu Engineering College, Department of Computer Science and Engineering, Erode, India; Roja R., Kongu Engineering College, Department of Computer Technology, Erode, India; Sankarnanth S., Kongu Engineering College, Department of Computer Science and Engineering, Erode, India; Gayathri A., Kongu Engineering College, Department of Computer Science and Engineering, Erode, India","Corn, a grain categorized within the grass family, stands as a fundamental staple crop globally. It plays a crucial role in supplying sustenance for both humans and livestock, in addition to serving as a raw material for various industries. In 2017, Africa produced 7.4% of the 1135 million tons produced worldwide in 40 million hectares. Throughout its growing process, corn plant is impacted by numerous illnesses. Lack of early detection of its diseases may result in a loss in productivity and even crop failure. It is proposed to analyze the leaves of the corn plant for identifying the diseases by exploiting deep learning methods. A wide range of deep learning algorithms are being utilized to detect the corn leaf with infection with good range of accuracy. To categorize the leaf, image processing models are mostly used for these kind of disease detections. Since it has in-built convolutional layer, CNN can reduce the image with high dimensionality without losing any kind of information. In this proposed work CNN with Adam optimizer has been implemented for better results and achieved 91.68 % of accuracy.  © 2024 IEEE.","Accuracy; Classification; Convolutional Neural Networks; Corn leaf; Deep learning; Optimizers","Convolution; Crops; Deep learning; Image processing; Learning algorithms; Learning systems; Plants (botany); Accuracy; Convolutional neural network; Corn leaf; Corn plant; Crop failures; Deep learning; Growing process; Learning methods; Optimizers; Staple crops; Convolutional neural networks","","","G.R. Sreekanth; Kongu Engineering College, Department of Computer Science and Engineering, Erode, India; email: grsreekanth25@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835034922-1","","","English","Int. Conf. Artif. Intell. Mach. Learn. Appl.: Healthc. Internet Things, AIMLA","Conference paper","Final","","Scopus","2-s2.0-85195111942"
"Su Y.; Xiao Z.; Bao P.","Su, Yu (58920227500); Xiao, Zhiyun (8894949600); Bao, Pengfei (58920017500)","58920227500; 8894949600; 58920017500","Livestock detection in pastoral areas using improved YOLOv5s; [采用改进 YOLOv5s 检测牧区牲畜]","2023","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","39","24","","165","176","11","0","10.11975/j.issn.1002-6819.202308231","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186705050&doi=10.11975%2fj.issn.1002-6819.202308231&partnerID=40&md5=d730620d5d4f1e7085c63397b4b6f545","(1. School of Electric Power, Inner Mongolia University of Technology, Hohhot 010051, China; 2. Key Laboratory of Electromechanical Control of Inner Mongolia Autonomous Region, Huhhot 010051, China; 3. Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Huhhot 010051, China)","Su Y., (1. School of Electric Power, Inner Mongolia University of Technology, Hohhot 010051, China; 2. Key Laboratory of Electromechanical Control of Inner Mongolia Autonomous Region, Huhhot 010051, China; 3. Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Huhhot 010051, China); Xiao Z., (1. School of Electric Power, Inner Mongolia University of Technology, Hohhot 010051, China; 2. Key Laboratory of Electromechanical Control of Inner Mongolia Autonomous Region, Huhhot 010051, China; 3. Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Huhhot 010051, China); Bao P., (1. School of Electric Power, Inner Mongolia University of Technology, Hohhot 010051, China; 2. Key Laboratory of Electromechanical Control of Inner Mongolia Autonomous Region, Huhhot 010051, China; 3. Intelligent Energy Technology and Equipment Engineering Research Centre of Colleges and Universities in the Inner Mongolia Autonomous Region, Huhhot 010051, China)","Animal husbandry is ever-increasing in artificial intelligence (AI) at present, in terms of scale, informatization, and refinement. Collective cattle farms can be expected to gradually replace the small-scale mode, such as individual farming. An important challenge has posed on the automated management in animal husbandry. It is very necessary to accurately identify the livestock population in the large-scale grazing and breeding, in order to determine the quantity and then update the population information in real-time. However, it is very difficult for the scale and automation of livestock detection, due to the complex environment in the pastoral areas, which was often obstructed by trees, sunlight, and sandstorms. The current object algorithms often encounter missed and error detections, even leading to monitoring accurately. In this article, a livestock detection called LDHorNet (livestock detection HorNet) was proposed using the YOLOV5s object detection network. Firstly, a HorNB module was designed to detect the small targets in the long-distance and complex environments. The recursive gated convolution in the HorNB was used to extend the second-order interactions in self-attention to any order. As such, the C3 module was then replaced in the Backbone and Neck sections, in order to improve the performance of the network model. The high-order spatial information interaction was realized without introducing significant additional calculations. The better performance of the improved model was achieved to understand the relationship between different targets in the image, and then better capture the spatial structure and contextual information between targets. The accuracy of detection was also enhanced to locate and detect the small targets. Secondly, the gradient accumulation was utilized to decrease the variance of gradient, in order to enhance both the stability of the model and the efficiency of feature extraction. The CBAM (convolutional block attention module) attention mechanism was also embedded in the Neck section of the network structure to better capture the key information in the image. The detection accuracy and attention weight of small targets were improved to effectively solve the interference of light, wind and sand, noise, and livestock motion blur on livestock detection in complex pastoral environments. At the same time, there was a decrease in the missed detection, due to overlapping occlusion of livestock in the pastoral areas. The repulsion loss function was utilized to handle the overlapping occlusion. There was an enhancement in the recall and accuracy of the model. Finally, accurate detection was achieved in the complex scenes, such as the complex lighting and environmental interference. The experimental results show that the LDHorNet model shared the better performance and high accuracy, with a Precision and Recall of 95.24% and 88.87%, respectively, and with the mAP_0.5 and mAP_0.5:0.95 of 94.11% and 77.01%, respectively. The precision of the improved model increased by 2.83, 2.93, and 9.79 percentage points, respectively, compared with the YOLOv5s, YOLOv8s, and YOLOv7 Tiny, and the Recall increased by 6.66, 4.95, and 13.42 percentage points, respectively, mAP_0.5 increased by 3.98, 3.4, and 10.78 percentage points, respectively, with mAP_0.5:0.95 increased by 12.46, 5.26, and 20.97 percentage points, respectively. This network performed the best to detect the livestock in the small targets and occlusion scenes. Therefore, this model can provide a strong reference for livestock detection under large-scale grazing conditions. © 2023 Chinese Society of Agricultural Engineering. All rights reserved.","attention mechanism; deep learning; LDHorNet; livestock detection; loss function; pastoral areas; target detection","Agriculture; Animals; Complex networks; Convolution; Deep learning; Feature extraction; Image enhancement; Informatization; Storms; Attention mechanisms; Deep learning; Livestock detection; Livestock detection hornet; Loss functions; Pastoral area; Percentage points; Performance; Small targets; Targets detection; Object detection","","","Z. Xiao; email: xiaozhiyun@imut.edu.cn","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85186705050"
"Wang W.; Xie M.; Jiang C.; Zheng Z.; Bian H.","Wang, Wei (59170926100); Xie, Mujun (7202255491); Jiang, Changhong (9737854400); Zheng, Zhong (57544115300); Bian, Heyu (59143291400)","59170926100; 7202255491; 9737854400; 57544115300; 59143291400","Cow Detection Model Based on Improved YOLOv5","2024","Proceedings - 2024 39th Youth Academic Annual Conference of Chinese Association of Automation, YAC 2024","","","","1697","1701","4","0","10.1109/YAC63405.2024.10598535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200747665&doi=10.1109%2fYAC63405.2024.10598535&partnerID=40&md5=fee34d98bfea70c0ee5a52e1c49091b8","Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China","Wang W., Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China; Xie M., Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China; Jiang C., Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China; Zheng Z., Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China; Bian H., Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China","With the increasing demand for livestock products in the market, the number of cow raised is also on the rise. However, currently, manual management and quantity statistics of cow are difficult and inefficient. In recent years, deep learning technology has been widely applied in the field of object detection. This paper proposes an improved YOLOv5 model by introducing a Bi-Level Routing Attention mechanism to address the problem of detecting a large number of cow in a circle, with significant differences in target location distance and occlusion. This model adds a dual Bi-Level Routing Attention (BRA) to the Neck layer of YOLOv5 to improve its feature processing and enhance the detection performance of the model. This article verifies the detection results through a self-built cow detection dataset. The results showed that the improved YOLOv5 model achieved Mean Average Precision (mAP) of 94.2% in individual cow detection tasks, which was 2.1% higher than the unimproved YOLOv5 model. This model provides a new approach for detecting cow in pens, laying the foundation for subsequent tracking and counting of cow. © 2024 IEEE.","Bi-Level Routing Attention; cow detection; mAP improvement; Yolov5","Agriculture; Deep learning; Bi-level routing attention; Cow detection; Detection models; Learning technology; Manual management; Mean average precision improvement; Model-based OPC; Precision improvement; Routings; Yolov5; Object detection","","","M. Xie; Changchun University of Technology, School of Electrical and Electronic Engineering, Changchun, China; email: xiemujun@ccut.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","979-835037922-8","","","English","Proc. - Youth Acad. Annu. Conf. Chin. Assoc. Autom., YAC","Conference paper","Final","","Scopus","2-s2.0-85200747665"
"Min P.; Su H.; He M.; Hou R.; Que P.; Chen P.","Min, Peng (59343670500); Su, Han (36617960500); He, Mengnan (56431813400); Hou, Rong (34769868300); Que, Pinjia (55588594600); Chen, Peng (56881800500)","59343670500; 36617960500; 56431813400; 34769868300; 55588594600; 56881800500","ReconGait: Giant Panda Gait Recognition Based on Spatio-Temporal Feature Reconstruction","2024","Proceedings of the International Joint Conference on Neural Networks","","","","","","","0","10.1109/IJCNN60899.2024.10649970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205020202&doi=10.1109%2fIJCNN60899.2024.10649970&partnerID=40&md5=abcb5de782369d167cac9d40ecd92ec4","Sichuan Normal University, School of Computer Science, Chengdu, China; Visual Computing and Virtual Reality Key Laboratory of Sichuan Province, Chengdu, China; Chengdu Research Base of Giant Panda Breeding, Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China","Min P., Sichuan Normal University, School of Computer Science, Chengdu, China; Su H., Sichuan Normal University, School of Computer Science, Chengdu, China, Visual Computing and Virtual Reality Key Laboratory of Sichuan Province, Chengdu, China; He M., Chengdu Research Base of Giant Panda Breeding, Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China; Hou R., Chengdu Research Base of Giant Panda Breeding, Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China; Que P., Chengdu Research Base of Giant Panda Breeding, Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China; Chen P., Sichuan Normal University, School of Computer Science, Chengdu, China, Chengdu Research Base of Giant Panda Breeding, Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China","As an endangered species, protecting and identifying individual giant pandas has always been a focus. However, traditional methods based on feces and molecular biology have inherent limitations. With the progress of deep learning, image analysis methods can deliver good results, but they rely on high-quality frontal facial images, which are challenging to acquire in the wild. Furthermore, these approaches have neglected the fact that a substantial portion of data from the wild is predominantly in the form of video, and have inadequately exploited the temporal information. Therefore, we apply gait recognition to identify individual giant pandas. By examining the unique walking patterns of individuals in the video, gait recognition allows accurate identification from various angles, long ranges, and without subject cooperation. This makes it ideal to identify wild individual giant pandas, but it also introduces new challenges, such as quadrupeds having more complex body gestures compared to humans, and the generally lower image quality taken by cameras in the wild. To tackle issues in natural environments, we design ReconGait, a giant panda gait recognition model capable of aligning multi-view images and restoring low-quality regions effectively. Experiments shows state-of-the-art results, proving the effectiveness of using gait analysis for individual giant panda identification. Through our work, we strive to contribute significantly to the preservation of this rare species and set a precedent for the integration of gait recognition in wildlife conservation efforts. © 2024 IEEE.","Deep Learning; Feature Reconstruction; Gait Recognition; Giant Panda Identification","Invertebrates; Livestock; Deep learning; Endangered species; Feature reconstruction; Gait recognition; Giant panda; Giant panda identification; High quality; Image analysis method; Inherent limitations; Spatiotemporal feature","Ministry of Human Resources and Social Security; Chengdu Municipal Science and Technology Program, (2022-YF09-00019-SN); Chengdu Municipal Science and Technology Program; Natural Science Foundation of Sichuan Province, (2023YFS0202, 2023NSFSC1080, 2023NSFSC0210); Natural Science Foundation of Sichuan Province; Chengdu Research Base of Giant Panda Breeding, CRBGPB, (2021CPB-B06); Chengdu Research Base of Giant Panda Breeding, CRBGPB","The work is supported by Returned Overseas Chinese Program of Ministry of Human Resources and Social Security, Natural Science Foundation of Sichuan Province(2023NSFSC1080, 2023NSFSC0210, 2023YFS0202), Chengdu Science and Technology Program (2022-YF09-00019-SN), The Chengdu Research Base of Giant Panda Breeding (NO. 2021CPB-B06). We thank the Wildlife Conservation Department of the National Forestry and Grassland Administration of P.R.China support of this research.","P. Chen; Sichuan Normal University, School of Computer Science, Chengdu, China; email: capricorncp@163.com; H. Su; Sichuan Normal University, School of Computer Science, Chengdu, China; email: jkxy_sh@sicnu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","979-835035931-2","85OFA","","English","Proc Int Jt Conf Neural Networks","Conference paper","Final","","Scopus","2-s2.0-85205020202"
"Tian M.; Zhao F.; Chen N.; Zhang D.; Li J.; Li H.","Tian, Maohong (58177165300); Zhao, Feiyang (58539624100); Chen, Nengmei (59227789300); Zhang, Dequan (58949902500); Li, Jie (59227343700); Li, Hualin (58177377200)","58177165300; 58539624100; 59227789300; 58949902500; 59227343700; 58177377200","Application of Convolutional Neural Network in Livestock Target Detection of IoT Images","2024","2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology, AINIT 2024","","","","1146","1151","5","0","10.1109/AINIT61980.2024.10581525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199152735&doi=10.1109%2fAINIT61980.2024.10581525&partnerID=40&md5=b75f13adadb57958381a7dd59d74c8e6","School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering, Shu Yi Xin Credit Management Co., Ltd, Chongqing, China; School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering Intelligent Application of Financial Big Data, Chongqing Colleges and Universities Engineering Research Center, Chongqing, China; Chongqing Da Tong Experimental School, Chongqing, China; Shu Yi Xin Credit Management Co., Ltd, Chongqing, China","Tian M., School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering, Shu Yi Xin Credit Management Co., Ltd, Chongqing, China; Zhao F., School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering Intelligent Application of Financial Big Data, Chongqing Colleges and Universities Engineering Research Center, Chongqing, China; Chen N., Chongqing Da Tong Experimental School, Chongqing, China; Zhang D., Shu Yi Xin Credit Management Co., Ltd, Chongqing, China; Li J., School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering, Shu Yi Xin Credit Management Co., Ltd, Chongqing, China; Li H., School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering, Shu Yi Xin Credit Management Co., Ltd, Chongqing, China","The national rural revitalization strategy has advanced, and new requirements for modern internet-of-things (IoT)-based farm supervision tools have been proposed. This study proposes a 'you-only-look-once' (YOLO) v5 convolutional neural network to optimize the candidate frame's center point algorithm. Five YOLOv5 models are used to identify and classify captive pigs. According to the experimental results, the K-median algorithm has a better effect than using the K-means algorithm, and its average intersection over union is approximately 1-2% higher. In our scenario, the commercial application value of the YOLOv5s model is higher, and the corresponding performance results of F1-score, mAP0.5, mAP0.5:0.95, FPS, and R2 are 92.21%, 95.18%, 64.85%, 75 FPS, and 85.54%, respectively. © 2024 IEEE.","computer vision; convolutional neural network; Deep learning; internet-of-things; object detection; YOLOv5","Agriculture; Computer vision; Convolution; Deep learning; Internet of things; K-means clustering; Mammals; Object detection; Center points; Convolutional neural network; Deep learning; K-mean algorithms; K-median algorithm; Objects detection; Point algorithms; Supervision tools; Targets detection; YOLOv5; Convolutional neural networks","Natural Science Foundation of Chongqing Municipality, (cstc2020jcyj-msxmX0666, CSTB2022NSCQ-MSX1419); Natural Science Foundation of Chongqing Municipality; Chongqing Institute of Engineering, China, (2022gcky03, 2023xzcr05, 2022xzcr06, 2022xzcr05, 2023xzcr04); Scientific and Technological Research Key Program of Chongqing, (KJZD- K202001901, KJZD-M202201901)","This study was funded by the Scientific Research Fund of the Chongqing Institute of Engineering, China, Grant Number (2022gcky03, 2022xzcr05, 2022xzcr06, 2023xzcr04, 2023xzcr05), in part by the Natural Science Foundation of Chongqing, Grant Number (cstc2020jcyj-msxmX0666, CSTB2022NSCQ-MSX1419), and the Scientific and Technological Research Key Program of Chongqing, grant number (KJZD-M202201901, KJZD- K202001901).","H. Li; School of Big Data and Artificial Intelligence, Chongqing Institute of Engineering, Shu Yi Xin Credit Management Co., Ltd, Chongqing, China; email: hualinli@hotmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835038555-7","","","English","Int. Semin. Artif. Intell., Netw. Inf. Technol., AINIT","Conference paper","Final","","Scopus","2-s2.0-85199152735"
"Liu Y.; Zhou H.; Ni Z.; Jiang Z.; Wang X.","Liu, Yu (58610686700); Zhou, Haibo (56066024700); Ni, Zhengshuai (58750914600); Jiang, Zhangjun (57986267800); Wang, Xin (57211222546)","58610686700; 56066024700; 58750914600; 57986267800; 57211222546","An Accurate and Lightweight Algorithm for Caged Chickens Detection based on Deep Learning","2024","Pakistan Journal of Agricultural Sciences","61","2","","403","415","12","0","10.21162/PAKJAS/24.138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198044356&doi=10.21162%2fPAKJAS%2f24.138&partnerID=40&md5=327341abadb98c4617de1962ab93b9d4","National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China; Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China","Liu Y., National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China, Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China; Zhou H., National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China, Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China; Ni Z., National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China, Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China; Jiang Z., National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China, Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China; Wang X., National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China, Tianjin Key Laboratory for Advanced Mechatronic System Design and Intelligent Control, School of Mechanical Engineering, Tianjin University of Technology, Tianjin, 300384, China","The animal husbandry industry is undergoing a transition towards intelligent breeding with advancements in artificial intelligence technology. Ensuring the health and safety of livestock and poultry requires automated assessment of their condition, with object detection being the primary focus. However, the existing object detection methods demonstrate subpar accuracy when applied to caged chicken detection. Moreover, their deployment on embedded devices is hindered by the significant size of the network models. To address these issues, an Improved-You Only Look Once version 5 (YOLOv5s) network detection model is proposed, which is based on YOLOv5s object detection algorithms. The cross-stage partial structure of the neck is modified to enhance the residual structure. Additionally, a convolutional block attention module is incorporated to extract crucial features. The distance intersection over union non-maximum suppression algorithm is employed to improve the identification of overlapping chickens. Furthermore, depthwise separable convolution is implemented to reduce network complexity. Experimental results on the custom dataset demonstrate that the Improved-YOLOv5s model achieves a mean average precision of 98.28%, surpassing the original model by 5.33%. It also exhibits strong performance across all other evaluation metrics while significantly reducing network parameters. In comparison to SSD, Faster-RCNN, YOLOv3, YOLOv4, YOLOv5s, and YOLOX-m, the Improved-YOLOv5s model demonstrates notable advantages in detection accuracy, network complexity, and detection speed. © 2024, University of Agriculture. All rights reserved.","caged chickens; Deep learning; image recognition; smart farming; YOLOv5s","","Special Project for Research and Development in Key areas of Guangdong Province, (2019B090922002); Special Project for Research and Development in Key areas of Guangdong Province; Natural Science Foundation of Tianjin Municipality, (17JCZDJC30400); Natural Science Foundation of Tianjin Municipality","Acknowledgments: This work was supported by the Research and Development projects in Key Areas of Guangdong Province [grant number 2019B090922002]; the Key projects of Tianjin Natural Science Foundation [grant number 17JCZDJC30400].","X. Wang; National Demonstration Center for Experimental Mechanical and Electrical Engineering Education, Tianjin University of Technology, Tianjin, 300384, China; email: winx@tju.edu.cn","","University of Agriculture","05529034","","","","English","Pak. J. Agric. Sci.","Article","Final","","Scopus","2-s2.0-85198044356"
"Pathak U.; Das A.; Bora P.K.; Rajkhowa S.","Pathak, Upasana (57220603904); Das, Abhichandan (57220602642); Bora, Pranjal Kumar (58070696100); Rajkhowa, Sanchaita (58408405700)","57220603904; 57220602642; 58070696100; 58408405700","Applications and future perspectives of computational approaches in livestock animals","2023","Systems Biology, Bioinformatics and Livestock Science","","","","279","309","30","0","10.2174/9789815165616123010018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205618293&doi=10.2174%2f9789815165616123010018&partnerID=40&md5=991c196fbeb2b12e986219ae9f2657cd","Centre for Biotechnology and Bioinformatics, Dibrugarh University, Dibrugarh, Assam, 786004, India; Centre for Computer Science and Applications, Dibrugarh University, Dibrugarh, Assam, 786004, India","Pathak U., Centre for Biotechnology and Bioinformatics, Dibrugarh University, Dibrugarh, Assam, 786004, India; Das A., Centre for Biotechnology and Bioinformatics, Dibrugarh University, Dibrugarh, Assam, 786004, India; Bora P.K., Centre for Computer Science and Applications, Dibrugarh University, Dibrugarh, Assam, 786004, India; Rajkhowa S., Centre for Biotechnology and Bioinformatics, Dibrugarh University, Dibrugarh, Assam, 786004, India","Livestock is regarded as a critical point of access for enhanced food and nutrition. With the population explosion, an increase in the successful fulfillment of livestock production, including meat and dairy products, is necessary in the most ethical way. Fundamentally keeping the overall nutrition intact along with the health of both human and livestock animals is vital. Although there is an increment in production, it contributes to rising greenhouse gas (methane) emissions, thus damaging the environment. Inheriting novel technologies will not only help in the surplus upliftment of livestock products but also the emission of greenhouse gases. Omics and Systems Biology are such approaches. Omics is a combination of different aspects dealing with complete molecular levels ranging from DNA to protein, protein to metabolites, whereas Systems Biology is the analysis of both mathematical and computational along with biological system modeling. Omics gives a broad overview of both pathways and traits controlling various characters. Thus, showing detailed links between genotype-phenotype. It can yield an enormous amount of data with incredible speed. In addition, Systems Biology lines up to give an overview of the complete biological system rather than just examining a single biological molecule. It combines mathematical modelling, statistics, and bioinformatics for a better grip and understanding of the enormous data sets. In this chapter, we discuss the latest cuttingedge technologies in the field of livestock and how omics can be implemented in creating disease resistant livestock animals without hampering the quality of the products. The chapter also discusses the various applications and future scopes involving computational approaches towards animal science. © 2023 Bentham Science Publishers. All rights reserved.","Dairy; Deep learning; Livestock; Meat; Metabolomics; Omics; Systems biology; Vaccines","","","","S. Rajkhowa; Centre for Biotechnology and Bioinformatics, Dibrugarh University, Dibrugarh, Assam, 786004, India; email: s_rajkhowa@dibru.ac.in","","Bentham Science Publishers","","978-981516561-6; 978-981516562-3","","","English","Syst. biol., bioinform. and livest. sci.","Book chapter","Final","","Scopus","2-s2.0-85205618293"
"Guo Z.; Lyu L.; He Z.; Guo C.; Mao A.; Huang E.; Liu K.","Guo, Z. (58804462600); Lyu, L. (58805389800); He, Z. (58444762800); Guo, C. (59343517800); Mao, A. (57238219000); Huang, E. (57226523976); Liu, K. (55823366100)","58804462600; 58805389800; 58444762800; 59343517800; 57238219000; 57226523976; 55823366100","DeMVpp-YOLO: A lightweight pig behaviour detection model for improving pig health management in farrowing pens","2024","11th European Conference on Precision Livestock Farming","","","","1110","1117","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204999354&partnerID=40&md5=3ee454968014d73ea38fcff6eba75dd9","Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; School of Communication Engineering, Hangzhou Dianzi University, Zhejiang, Hangzhou, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","Guo Z., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Lyu L., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; He Z., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Guo C., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong; Mao A., School of Communication Engineering, Hangzhou Dianzi University, Zhejiang, Hangzhou, China; Huang E., Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Liu K., Department of Infectious Diseases and Public Health, City University of HongKong, Hong Kong","The behaviours exhibited by pigs are closely tied to their health and welfare status, making it crucial to monitor them for effective management. Automatic real-time detection methods can significantly improve pig health management and reduce the workload of caretakers, particularly in farrowing pens where piglets and sows are vulnerable. In recent years, computer vision, particularly empowered by deep learning, has gained traction for animal behaviour recognition. However, existing deep learning methods either require high computational resources or lack the detection accuracy needed for real-world applications. To balance the detection precision and model complexity, we propose a novel lightweight network called DeMVpp-YOLO. We improve the backbone and neck of the Transformer-based MobileViTv2 and PPyolov5 models, respectively, to reduce the model size and enhance deep feature extraction. Additionally, we employ a decoupled head to separate the classification and regression tasks, thereby accelerating the training process. In our pig detection experiment, the detection mean Average Precision (mAP) for piglet and sow behaviours reached 93.1% in the ablation study, demonstrating significant improvement in detection compared with YOLOv-5. Our experiments successfully identified four postures including standing, left lying, right lying, and sternal lying for piglets and sows, with an average accuracy rate of 96.3%. Furthermore, to assess the activity levels of piglets and the sow during the early stages of development in the farrowing pens, we utilized the model to characterize the pig posture patterns on two different days. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","animal behaviours; animal welfare; computer vision; precision livestock farming","mHealth; Animal behaviour; Animal welfare; Behavior detection; Detection methods; Detection models; Effective management; Health management; Pig behavior; Precision livestock farming; Real-time detection; Mammals","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204999354"
"Daker M.; Elsayaad F.; Atia A.","Daker, Mahmoud (58743318500); Elsayaad, Farida (58743425000); Atia, Ayman (35101350300)","58743318500; 58743425000; 35101350300","The Classification Of Cattle Behaviors Using Deep Learning","2024","6th International Conference on Computing and Informatics, ICCI 2024","","","","28","33","5","0","10.1109/ICCI61671.2024.10485159","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190661139&doi=10.1109%2fICCI61671.2024.10485159&partnerID=40&md5=38e532d29fcf25d877a10e91a6da49fe","Faculty of Computer Science, October University for Modern Sciences and Arts (MSA), Giza, Egypt; HCI-LAB, Faculty of Computers and Artificial Intelligence, Helwan University, Egypt","Daker M., Faculty of Computer Science, October University for Modern Sciences and Arts (MSA), Giza, Egypt; Elsayaad F., Faculty of Computer Science, October University for Modern Sciences and Arts (MSA), Giza, Egypt; Atia A., Faculty of Computer Science, October University for Modern Sciences and Arts (MSA), Giza, Egypt, HCI-LAB, Faculty of Computers and Artificial Intelligence, Helwan University, Egypt","Cows have different behaviors that act as indicators of health and well-being. Monitoring the behaviors of cows is an essential component of livestock management since it offers the help needed by farmers to make decisions that contribute to the overall welfare of cattle. Traditional manual monitoring of cows can be costly, inefficient, and subjective to each human observer. In this paper, we propose an approach for the classification of two specific cow behaviors which are Drinking and Grazing. The dataset we used consisted of labelled videos containing 3 classes which are ""Drinking"", ""Grazing"", and ""Other"". The number of videos in each class were 360,413, and 941 videos respectively. We used a CNN-RNN architecture to handle both the temporal and spatial information of the videos. We carried out 2 experiments. Firstly, we inputted the imbalanced data as it is to the model. Due to the imbalanced nature of the classes, we used proper evaluation metrics which were weighted average precision, recall, and Fl-Score in addition to the testing accuracy. The results were a testing accuracy of 73.24%, a precision of 72.72%, a recall of 73.18%, and an Fl-Score of 72.94%. Secondly, we used data augmentation techniques to increase the size of the dataset. We used rotation using different angles combined with horizontal flipping to make the number consistent across all classes. The final data contained 942 videos in each class. This produced a testing accuracy of 84.88%, a precision of 85.92%, a recall of 84.89%, and an Fl-Score of 85.5%. © 2024 IEEE.","Animal Behaviors; Cow Behavior Classification; Deep Learning; Smart Farming; Supervised Learning","Farms; Animal behaviour; Behaviour classification; Cow behavior; Cow behavior classification; Deep learning; Human observers; Manual monitoring; Smart farming; Testing accuracy; Well being; Deep learning","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037387-5","","","English","Int. Conf. Comput. Informatics, ICCI","Conference paper","Final","","Scopus","2-s2.0-85190661139"
"Bueno I.T.; Antunes J.F.G.; Dos Reis A.A.; Werner J.P.S.; Toro A.P.S.G.D.D.; Figueiredo G.K.D.A.; Esquerdo J.C.D.M.; Lamparelli R.A.C.; Coutinho A.C.; Magalhães P.S.G.","Bueno, Inacio T. (56603977800); Antunes, João F.G. (15020172200); Dos Reis, Aliny A. (55320336300); Werner, João P.S. (57212420665); Toro, Ana P.S.G.D.D. (57741497200); Figueiredo, Gleyce K.D.A. (57185295400); Esquerdo, Júlio C.D.M. (15020462500); Lamparelli, Rubens A.C. (6602713722); Coutinho, Alexandre C. (55513012600); Magalhães, Paulo S.G. (7003731076)","56603977800; 15020172200; 55320336300; 57212420665; 57741497200; 57185295400; 15020462500; 6602713722; 55513012600; 7003731076","Mapping integrated crop-livestock systems in Brazil with planetscope time series and deep learning","2023","Remote Sensing of Environment","299","","113886","","","","7","10.1016/j.rse.2023.113886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175474237&doi=10.1016%2fj.rse.2023.113886&partnerID=40&md5=c5c45b75181d502b9a47020f66b17ff6","Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil; School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil","Bueno I.T., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil, School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Antunes J.F.G., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil, Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Dos Reis A.A., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil, School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Werner J.P.S., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Toro A.P.S.G.D.D., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Figueiredo G.K.D.A., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Esquerdo J.C.D.M., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil, Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Lamparelli R.A.C., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil, School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Coutinho A.C., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Magalhães P.S.G., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil","Accurate mapping of crops with high spatiotemporal resolution plays a critical role in achieving the Sustainable Development Goals (SDGs), especially in the context of integrated crop-livestock systems (ICLS). Stakeholders can make informed decisions and implement targeted strategies to achieve multiple SDGs related to agriculture, rural development, and sustainable livelihoods by understanding the spatial dynamics of these systems. Accurate information on the extent of ICLS derived from multitemporal remote sensing and emerging map techniques such as deep learning can help in the implementation of sustainable agricultural practices. However, far too little attention has been paid to ICLS map accuracy because it may not be at the forefront of research agendas compared to those of other agricultural practices. This paper aims to map ICLS using high spatiotemporal resolution imagery and deep learning neural network classifiers at two different sites located in Brazil. The pipeline involves four interpretation approaches based on the ICLS class: evaluating deep neural network classifiers with different image composition intervals, explaining commission and omission errors, evaluating the temporal transferability of the method, and evaluating the influence of variables. The study area consists of two locations in São Paulo (study site 1, SS1) and Mato Grosso state (study site 2, SS2), Brazil. We derived nine spectral variables from PlanetScope (PS) images and four metrics through object-based image analysis (OBIA) using two time intervals, 10 and 15 days, to generate the image compositions. These input variables were used in three deep neural network classifiers: convolutional neural network in one dimension (Conv1D), long short-term memory (LSTM), and LSTM with a fully convolutional network (LSTM-FCN). Our results showed that mapping dynamic land use such as ICLS is possible by using high-spatiotemporal-resolution imagery and deep neural network classifiers. The 15-day LSTM-FCN classifier returned the highest map accuracies for both sites, with the following class-level accuracies: producer accuracy (PA) = 97.0% and user accuracy (UA) = 97.0% for SS1 and PA = 82.0% and UA = 96.5% for SS2. Meanwhile, we found map uncertainties arising from the diverse crop calendars and spectro-temporal similarities between ICLS and other land use. The best approaches revealed that temporal generalization was suitable for mapping ICLS, but some classifiers could not generalize due to the inherent characteristics of the class. Most variables were considered efficient for predicting ICLS, although spectral indices revealed better functional relationships, while the PS bands had a lower influence on the predictions. The accuracies achieved with the proposed method represent promising opportunities for the sufficiently accurate mapping of ICLS and other complex crop activities. © 2023","Crop classification; OBIA; Shapley additive exPlanations; Sustainable agriculture; Temporal generalization","Convolution; Convolutional neural networks; Crops; Deep neural networks; Image classification; Land use; Long short-term memory; Mapping; Regional planning; Remote sensing; Sustainable development; Crop classification; Generalisation; Image-analysis; Object based; Object-based image analyse; Objects-based; Shapley; Shapley additive explanation; Sustainable agriculture; Temporal generalization; alternative agriculture; livestock farming; machine learning; mapping method; remote sensing; Sustainable Development Goal; time series; Image analysis","Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2017/50205-9, 2018/24985-0, 2021/15001-9); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (305271/2020-2)","The authors are thankful for the financial support from The São Paulo Research Foundation (FAPESP) (Grants: 2021/15001-9 , 2018/24985-0 , and 2017/50205-9 ), the National Council for Scientific and Technological Development (CNPq) (Grant: 305271/2020-2 ), and the Coordination for the Improvement of Higher Education Personnel (CAPES, Finance code 001). Additionally, the authors would like to acknowledge Prof. Dr. Jansle Rocha for his substantial contributions to the inception and conceptualization of this study, and extend their sincere gratitude to the Editor and all the reviewers whose insightful suggestions have significantly enhanced the quality of this paper.","I.T. Bueno; Interdisciplinary Center of Energy Planning, University of Campinas, Campinas, SP, 13083-896, Brazil; email: ibueno@unicamp.br","","Elsevier Inc.","00344257","","RSEEA","","English","Remote Sens. Environ.","Article","Final","","Scopus","2-s2.0-85175474237"
"Bandaru J.; Basa N.; Raghavendra P.; Sirisha A.","Bandaru, Jabili (59261401600); Basa, Nikitha (59261769600); Raghavendra, P. (59261401700); Sirisha, A. (57219553783)","59261401600; 59261769600; 59261401700; 57219553783","Review on Various Techniques for Wildlife Monitoring and Alerting Systems","2024","2024 International Conference on Knowledge Engineering and Communication Systems, ICKECS 2024","","","","","","","0","10.1109/ICKECS61492.2024.10616522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201257494&doi=10.1109%2fICKECS61492.2024.10616522&partnerID=40&md5=fe39a1ca2a435559c9a5b1795abd293f","Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India","Bandaru J., Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India; Basa N., Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India; Raghavendra P., Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India; Sirisha A., Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India","Monitoring wildlife is crucial for understanding animal movement patterns, habitat utilization, population demographics, human-wildlife conflicts, incidents of vehicle-animal collisions, occurrences of snares and poaching, as well as outbreaks of epidemics. In recent times, the frequency of accidents involving wildlife and vehicles has risen, particularly on rural roads and highways in hilly terrain. The identification and classification of animals through image and video sequences represent a widely studied field. This paper provides a comprehensive overview of recent advancements in wildlife monitoring technologies and alerting systems. The review encompasses a wide spectrum of methodologies, including Internet of Things (IOT), camera traps etc. The paper begins by exploring the fundamental objectives of wildlife monitoring. It then delves into the evolution of monitoring technologies, highlighting the shift from traditional methods to state-of-the-art, automated systems such as deep neural networks, different versions of YOLO and the Internet of Things (IOT), which have transformed data processing and decision-making in wildlife monitoring. A critical aspect of wildlife monitoring is the development of efficient alerting systems that enable rapid response to threats. Overall this survey identifies gaps in existing knowledge, outlines future research directions, and advocates for the adoption of integrated, technology-driven solutions to address the problem.  © 2024 IEEE.","Deep learning; Deep Neural Networks; IOT; YOLO","Livestock; Alerting systems; Animal movement; Deep learning; Habitat utilization; Monitoring system; Monitoring technologies; Movement pattern; Neural-networks; Wildlife monitoring; YOLO; Deep neural networks","","","J. Bandaru; Chaitanya Bharathi Institute of Technology, Dept. of Information Technology, Hyderabad, India; email: ugs207221_it.jabili@cbit.org.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835035968-8","","","English","Int. Conf. Knowl. Eng. Commun. Syst., ICKECS","Conference paper","Final","","Scopus","2-s2.0-85201257494"
"Ismail M.S.; Samad R.; Pebrianti D.; Mustafa M.; Hasma Abdullah N.R.","Ismail, Muhammad Syahmie (55482485300); Samad, Rosdiyana (8546771300); Pebrianti, Dwi (55268466200); Mustafa, Mahfuzah (36069366700); Hasma Abdullah, Nor Rul (57781812800)","55482485300; 8546771300; 55268466200; 36069366700; 57781812800","Comparative Analysis of Deep Learning Models for Sheep Detection in Aerial Imagery","2024","Proceedings of the 9th International Conference on Mechatronics Engineering, ICOM 2024","","","","234","239","5","0","10.1109/ICOM61675.2024.10652292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204286474&doi=10.1109%2fICOM61675.2024.10652292&partnerID=40&md5=d06d9f3832749afc6997ce04e4d68460","Universiti Malaysia Pahang Al-Sultan Abdullah, Faculty of Electrical and Electronics Engineering Technology, Pahang, Malaysia; International Islamic University Malaysia, Faculty of Engineering, Department of Mechanical and Aerospace Engineer, Kuala Lumpur, Malaysia","Ismail M.S., Universiti Malaysia Pahang Al-Sultan Abdullah, Faculty of Electrical and Electronics Engineering Technology, Pahang, Malaysia; Samad R., Universiti Malaysia Pahang Al-Sultan Abdullah, Faculty of Electrical and Electronics Engineering Technology, Pahang, Malaysia; Pebrianti D., International Islamic University Malaysia, Faculty of Engineering, Department of Mechanical and Aerospace Engineer, Kuala Lumpur, Malaysia; Mustafa M., Universiti Malaysia Pahang Al-Sultan Abdullah, Faculty of Electrical and Electronics Engineering Technology, Pahang, Malaysia; Hasma Abdullah N.R., Universiti Malaysia Pahang Al-Sultan Abdullah, Faculty of Electrical and Electronics Engineering Technology, Pahang, Malaysia","This research evaluates You Look Only Once - YOLOv5, YOLO-NAS, and Detection Transformer (DETR) and provides a thorough evaluation of deep learning models for sheep identification in aerial pictures. A carefully selected collection of 4,212 aerial photos of sheep in various environments was used to thoroughly evaluate model performance. The implementation involved preprocessing, augmentation, model parameter optimization, training on Google Collab GPU s, and quantitative test results analysis. Important results show that on the sheep dataset, YOLOv5 and YOLO-NAS achieved an impressive accuracy of 97%, exceeding DETR's initial accuracy range of 70-80%. However, after adjusting the hyperparameters, DETR's accuracy significantly increased to 86%, showing less overfitting and more stability. The increased accuracy of YOLO models highlights how useful they are for sheep counting and aerial surveillance to support modern farming techniques. However, improvements to the transformer based DETR may increase its usefulness even more. This research offers valuable insights into the real-world applications of deep learning for livestock detection in aerial imagery, providing a foundation for future advancements in the field. © 2024 IEEE.","deep learning; DETR; sheep detection; YOLO NAS; YOLOv5","Aircraft detection; Aerial imagery; Aerial photos; Comparative analyzes; Deep learning; Detection transformer; Learning models; Modeling performance; Sheep detection; YOLO NAS; YOLOv5; Aerial photography","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835034978-8","","","English","Proc. Int. Conf. Mechatronics Eng., ICOM","Conference paper","Final","","Scopus","2-s2.0-85204286474"
"Iftikhar S.; Khattak H.A.; Saadat A.; Ameer Z.; Zakarya M.","Iftikhar, Sadaf (59257886100); Khattak, Hasan Ali (57208818911); Saadat, Ahsan (59265939100); Ameer, Zoobia (55780222300); Zakarya, Muhammad (55546416300)","59257886100; 57208818911; 59265939100; 55780222300; 55546416300","Efficient fruit disease diagnosis on resource-constrained agriculture devices","2024","Journal of the Saudi Society of Agricultural Sciences","","","","","","","0","10.1016/j.jssas.2024.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201094764&doi=10.1016%2fj.jssas.2024.07.002&partnerID=40&md5=413f698e7a84d735e91863023e529eeb","National University of Sciences and Technology (NUST), Islamabad, 44500, Pakistan; Shaheed Benazir Bhutto Women University Peshawar, Pakistan; Faculty of Information Technology, Sohar University, Sohar, 311, Oman","Iftikhar S., National University of Sciences and Technology (NUST), Islamabad, 44500, Pakistan; Khattak H.A., National University of Sciences and Technology (NUST), Islamabad, 44500, Pakistan, Faculty of Information Technology, Sohar University, Sohar, 311, Oman; Saadat A., National University of Sciences and Technology (NUST), Islamabad, 44500, Pakistan; Ameer Z., Shaheed Benazir Bhutto Women University Peshawar, Pakistan; Zakarya M., Faculty of Information Technology, Sohar University, Sohar, 311, Oman","Climate change in recent years has caught the attention of researchers worldwide, with drastic effects on many economic sectors, especially agriculture and livestock. Due to global warming and declining water levels, food security is becoming an increasingly urgent concern. The situation is more challenging in underdeveloped nations such as Pakistan, where little attention has been paid to improving the processes to produce effective seeds and herbicides. The problem has been exacerbated due to financial constraints and a lack of technology to monitor crops. It has been observed that even a noncritical disease can ruin the crop if timely inspections aren't performed. Although many solutions have been proposed recently, most are based on heavy mobile applications and cloud-based solutions, which results in considerable computational cost, power consumption, and latency. This work proposes a Deep Neural Networks (DNNs)-based light-weight solution for diagnosing and classifying apple crop diseases for farmers working in rural places (where the internet is not accessible). With the increasing need for efficient and robust Machine Learning (ML) applications on edge devices, such as mobile phones, Raspberry Pi, and Jetson Nano-based devices, there has been a demand for more efficient Deep Neural Networks (DNN) models. To meet this demand, various DNN-based models have been experimented with, including Basic CNN Architecture, AlexNet, and EfficientNet Lite to measure performance After a thorough examination and evaluation of each model's performance, efficiency, and resource usage, the most optimal model was picked and developed applications for classifying apple crop disease. Using a transfer learning strategy on a specially developed EfficientNet DNN architecture, achieving 85% test accuracy. © 2024 The Authors","Deep learning models; Fruit disease classification; On-device disease classification; Precision agriculture","","National University of Sciences and Technology, NUST","Project No. NUST-22\u201341-44 titled \u201DTowards on-device Classification of Fruit Diseases\u201D by the National University of Sciences and Technology (NUST), Islamabad, Pakistan, partially funded this research.","H.A. Khattak; National University of Sciences and Technology (NUST), Islamabad, 44500, Pakistan; email: hasan.alikhattak@acm.org","","King Saud University","1658077X","","","","English","J. Saudi Soc. Agric. Sci.","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85201094764"
"Harshitha S.; Likhitha K.; Varshini K.R.G.; Mahure S.J.; Yasaswi A.","Harshitha, S. (57220240817); Likhitha, K. (59128903700); Varshini, Katta R G (59531176900); Mahure, Sonali Jagadish (59340655200); Yasaswi, Anaparthi (59341076100)","57220240817; 59128903700; 59531176900; 59340655200; 59341076100","Poultry Pathogen Detection: Using Deep Learning for Coccidiosis Identification","2024","Proceedings - 2024 5th International Conference on Image Processing and Capsule Networks, ICIPCN 2024","","","","178","182","4","0","10.1109/ICIPCN63822.2024.00037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204795055&doi=10.1109%2fICIPCN63822.2024.00037&partnerID=40&md5=7817d565b65b09b549a47f50064c6fd0","New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India","Harshitha S., New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India; Likhitha K., New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India; Varshini K.R.G., New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India; Mahure S.J., New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India; Yasaswi A., New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India","Poultry farming plays a crucial part in fulfilling the need for animal protein worldwide, but the industry faces significant challenges in controlling infectious diseases that impact bird health and productivity. Protozoan parasites of the genus Eimeria are the source of coccidiosis, which is common and economically significant poultry disease. Traditional methods of coccidiosis diagnosis rely on labor-intensive and time-consuming microscopic examination of fecal samples, leading to delayed detection and response. This study investigates the use of deep learning methods for the quick and precise diagnosis of coccidiosis in poultry. A novel deep learning model trained on a comprehensive dataset of microscopic images of fecal samples obtained from infected birds has been proposed in this study. This study presents an innovative approach to coccidiosi detection using the VGGNet architecture, a deep convolutional neural network renowned for its hierarchical feature learning capabilities. Leveraging this architecture, we explore the potential for automated identification of coccidia-infected samples through microscopic image analysis. This study evaluates the performance of the deep learning model against traditional diagnostic methods and also architectures like YOLOv5, ResNet and demonstrating its ability to significantly reduce the time required for coccidiosis detection without compromising accuracy. We also discuss the model's flexibility in a variety of environments and its potential for early identification, enabling prompt intervention and disease management. This research contributes to the ongoing efforts to modernize poultry disease diagnostics through the utilization of deep learning.  © 2024 IEEE.","Coccidiosis; Convolutional Neural Network; Deep learning; Droppings; Poultry","Birds; Deep neural networks; Diagnosis; Diseases; Livestock; Animal proteins; Coccidiosis; Convolutional neural network; Deep learning; Dropping; Faecal samples; Infectious disease; Learning models; Pathogen detection; Protozoan parasites; Convolutional neural networks","","","S. Harshitha; New Horizon College of Engineering, Computer Science and Engineering, Bengaluru, India; email: harshithaskumar67@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835036717-1","","","English","Proc. - Int. Conf. Image Process. Capsul. Networks, ICIPCN","Conference paper","Final","","Scopus","2-s2.0-85204795055"
"Zhang Q.; Ahmed K.; Sharda N.; Wang H.","Zhang, Qianqian (58743314600); Ahmed, Khandakar (58586796500); Sharda, Nalin (6603082357); Wang, Hua (58465923400)","58743314600; 58586796500; 6603082357; 58465923400","A Comprehensive Survey of Animal Identification: Exploring Data Sources, AI Advances, Classification Obstacles and the Role of Taxonomy","2024","International Journal of Intelligent Systems","2024","","","","","","0","10.1155/2024/7033535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207392879&doi=10.1155%2f2024%2f7033535&partnerID=40&md5=77fc5d773e4a21128477b57a39523a0c","Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, VIC, Australia","Zhang Q., Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, VIC, Australia; Ahmed K., Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, VIC, Australia; Sharda N., Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, VIC, Australia; Wang H., Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, VIC, Australia","With the rapid development of entity recognition technology, animal recognition has gradually become essential in modern society, supporting labour-intensive agriculture and animal husbandry tasks. Severe problems such as maintaining biodiversity can also beneft from animal identifcation technology. However, certain invasive recognition systems have resulted in permanent harm to animals, while noninvasive identifcation methods also exhibit certain drawbacks. Tis paper conducts a systematic literature review (SLR), presenting a comprehensive overview of various animal recognition technologies and their applications. Specifcally, it examines methodologies such as deep learning, image processing and acoustic analysis used for diferent animal characteristics and identifcation purposes. Te contribution of machine learning to animal feature extraction is highlighted, emphasising its signifcance for animal taxonomy and wild species monitoring. Additionally, this review addresses the challenges and limitations of current technologies, including data scarcity, model accuracy and computational requirements, and suggests opportunities for future research to overcome these obstacles. © 2024 Qianqian Zhang et al.","animal identifcation; image processing; machine learning; neural network; signal processing","Contrastive Learning; Livestock; Taxonomies; Animal husbandry; Animal identifcation; Animal identification; Data-source; Entity recognition; Images processing; Labor intensive agriculture; Machine-learning; Neural-networks; Signal-processing; Invertebrates","","","Q. Zhang; Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, Australia; email: yindie36@gmail.com; K. Ahmed; Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Footscray, 3011, Australia; email: khandakar.ahmed@vu.edu.au","","John Wiley and Sons Inc","08848173","","IJISE","","English","Int J Intell Syst","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85207392879"
"Anuprabha K.; Kaliappan V.K.; Baniekal Hiremath G.","Anuprabha, K. (58695378600); Kaliappan, Vishnu Kumar (24829297500); Baniekal Hiremath, Gangadhar (55914453600)","58695378600; 24829297500; 55914453600","Transformative Technology in Poultry Management: 3D CNNs for Broiler Chicken Weight Prediction","2024","2024 International Conference on Cognitive Robotics and Intelligent Systems, ICC - ROBINS 2024","","","","432","436","4","0","10.1109/ICC-ROBINS60238.2024.10534012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195118854&doi=10.1109%2fICC-ROBINS60238.2024.10534012&partnerID=40&md5=511d559a85f61999c31f47e7b47e2a43","Kpr Institute of Engineering and Technology, Department of Computer Science Engineering, Tamilnadu, Coimbatore, 641407, India; Wasp Autonomous Systems Pvt Ltd, Karnataka, Ballari, India","Anuprabha K., Kpr Institute of Engineering and Technology, Department of Computer Science Engineering, Tamilnadu, Coimbatore, 641407, India; Kaliappan V.K., Kpr Institute of Engineering and Technology, Department of Computer Science Engineering, Tamilnadu, Coimbatore, 641407, India; Baniekal Hiremath G., Wasp Autonomous Systems Pvt Ltd, Karnataka, Ballari, India","In global food production, broiler chickens hold significant importance, which are traditionally managed through manual labour for both meat and egg production. However, the poultry farming industry has increasingly turned to automation to address challenges associated with manual labour and enhance efficiency. This research introduces a novel methodology for estimating chicken weight utilizing top-view videos as input and leveraging 3D Convolutional Neural Networks (CNNs). Accurate weight assessment is a critical aspect of poultry farming, influencing feeding strategies and resource allocation. Traditional manual weighing methods are laborious and time-intensive, necessitating the exploration of automated alternatives. The methodology involves dataset collection, preprocessing, and employing 3D CNNs for effective feature extraction. 3D CNN architecture, adept at capturing spatial and temporal features, consists of convolutional and pooling layers, followed by flatten and dense layers for weight prediction. The model achieves a remarkable accuracy of 95%, as evidenced in the experimental results. Experimental validation showcases the potential of this method as a non-invasive and efficient tool for estimating chicken weight. It contributes to the growing field of automated livestock management in agriculture.  © 2024 IEEE.","3D CNN; 3D-CNN; Deep learning; Model Evaluation; Preprocessing","Animals; Automation; Convolution; Deep learning; Farms; 3d convolutional neural network; 3d-convolutional neural network; Broiler chickens; Convolutional neural network; Deep learning; Food production; Manual labors; Model evaluation; Preprocessing; Convolutional neural networks","","","K. Anuprabha; Kpr Institute of Engineering and Technology, Department of Computer Science Engineering, Coimbatore, Tamilnadu, 641407, India; email: anuprabhakandasamy@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835037274-8","","","English","Int. Conf. Cogn. Robot. Intell. Syst., ICC - ROBINS","Conference paper","Final","","Scopus","2-s2.0-85195118854"
"Yang X.; Bist R.; Paneru B.; Chai L.","Yang, Xiao (57743060600); Bist, Ramesh (57866456200); Paneru, Bidur (57216784609); Chai, Lilong (57222280822)","57743060600; 57866456200; 57216784609; 57222280822","Advanced Machine learning Techniques for Monitoring Poultry Movement Patterns","2024","2024 ASABE Annual International Meeting","","","","","","","0","10.13031/aim.202400107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206106765&doi=10.13031%2faim.202400107&partnerID=40&md5=50e27132359f66f91ce7c873529da8fb","Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States","Yang X., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Bist R., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Paneru B., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States; Chai L., Department of Poultry Science, University of Georgia, Athens, 30602, GA, United States","Poultry locomotion is an important indicator of animal health, welfare and productivity. Traditional methodologies such as manual observation or the use of wearable devices, encounter significant challenges, including potential stress induction and behavioral alteration in animals. This research introduced an innovative approach that employs an enhanced track anything model (TAM) to track chickens in various experimental settings for locomotion analysis. Utilizing a dataset comprising both dyed and undyed broilers and layers, the TAM model was adapted and rigorously evaluated for its capability in non-intrusively tracking and analyzing poultry movement by intersection over union (mIoU) and the root mean square error (RMSE). The findings underscore TAM's superior segmentation and tracking capabilities, particularly its exemplary performance against other state-of-the-art models, such as YOLO (You only look once) models of YOLOv5 and YOLOv8, and its high mIoU values (93.12%) across diverse chicken categories. Moreover, the model demonstrated notable accuracy in speed detection, as evidenced by an RMSE value of 0.02 m/s, offering a technologically advanced, consistent, and non-intrusive method for tracking and estimating the speed of chickens. This research not only substantiates TAM as a potent tool for detailed poultry behavior analysis and monitoring but also illuminates its potential applicability in broader livestock monitoring scenarios, thereby contributing to the enhancement of animal welfare and management in poultry farming through automated, non-intrusive monitoring and analysis. © 2024 ASABE Annual International Meeting. All rights reserved.","animal welfare; deep learning; non-intrusive tracking; Poultry locomotion; track anything model","Fertilizers; Animal health; Animal welfare; Deep learning; Machine learning techniques; Movement pattern; Non-intrusive; Non-intrusive tracking; Poultry locomotion; Root mean square errors; Track anything model; Wearable technology","","","L. Chai; Department of Poultry Science, University of Georgia, Athens, 30602, United States; email: lchai@uga.edu","","American Society of Agricultural and Biological Engineers","","979-833130221-4","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85206106765"
"","","","2024 24th Symposium of Image, Signal Processing, and Artificial Vision, STSIVA 2024","2024","2024 24th Symposium of Image, Signal Processing, and Artificial Vision, STSIVA 2024","","","","","","35","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203189599&partnerID=40&md5=0c61e16a33fb586ad8cd426ec813affb","","","The proceedings contain 7 papers. The topics discussed include: deep learning-based pulmonary arterial segmentation in computed tomography images; visual and inertial odometry based on sensor fusion; 3D reconstruction of cultural heritage pieces using depth sensors; transforming agriculture: drones enhancing livestock management with ai precision; data-driven based preconditioning for one-bit MIMO detection; point spread function estimation using principal component analysis for a double diffractive optical element system; and detecting false arrhythmias alarms in the ICU using a deep learning approach.","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835035521-5","","","English","Symp. Image, Signal Process., Artif. Vis., STSIVA","Conference review","Final","","Scopus","2-s2.0-85203189599"
"Tsai Y.-J.; Huang Y.-C.; Lin E.-C.; Lai S.-C.; Hong X.-C.; Tsai J.; Chiang C.-E.; Kuo Y.-F.","Tsai, Yu-Jung (57226895310); Huang, Yi-Che (58616767000); Lin, En-Chung (7201721121); Lai, Sheng-Chieh (59244295400); Hong, Xu-Chu (59243266400); Tsai, Jonas (59243060500); Chiang, Cheng-En (59243266500); Kuo, Yan-Fu (55823045200)","57226895310; 58616767000; 7201721121; 59244295400; 59243266400; 59243060500; 59243266500; 55823045200","Monitoring the lactation-related behaviors of sows and their piglets in farrowing crates using deep learning","2024","Frontiers in Animal Science","5","","1431285","","","","0","10.3389/fanim.2024.1431285","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200257971&doi=10.3389%2ffanim.2024.1431285&partnerID=40&md5=4f586e5881677e708cb826828b41a22a","Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Lab of Animal Breeding and Applied Resources, Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan","Tsai Y.-J., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Huang Y.-C., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Lin E.-C., Lab of Animal Breeding and Applied Resources, Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan; Lai S.-C., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Hong X.-C., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Tsai J., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Chiang C.-E., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; Kuo Y.-F., Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan","Pig farming is a major sector of livestock production. The preweaning stage is a critical period in the pig farming process, where lactation-related behaviors between sows and their piglets directly influence the preweaning survivability of the piglets. Lactation-related behaviors are mutual interactions that require the combined monitoring of both the sow and her piglets. Conventional naked-eye observation is discontinuous and labor-intensive and may result in undetected abnormal behavior and economic losses. Thus, this study proposed to monitor the lactation-related behaviors of sows and their piglets simultaneously and continuously using computer vision. Videos were recorded from farrowing crates using embedded systems equipped with regular RGB cameras. The sow posture recognition model (SPRM), comprising a convolutional neural network (CNN) of the architecture EfficientNet and a long short-term memory network, was trained to identify seven postures of sows. The piglet localization and tracking model (PLTM), comprising a CNN of the architecture YOLOv7 and a simple online and realtime tracking algorithm, was trained to localize and track piglets in the farrowing crate. The sow posture information was then combined with the piglet activity to detect unfed piglets. The trained SPRM and PLTM reached an accuracy of 91.36% and a multiple object tracking accuracy of 94.6%. The performance of the proposed unfed piglet detection achieved a precision of 98.4% and a recall of 90.7%. A long-term experiment was conducted to monitor lactation-related behaviors of sows and their piglets from the birth of the piglets to day 15. The overall mean daily percentages ± standard deviations (SDs) of sow postures were 6.8% ± 2.9% for feeding, 8.8% ± 6.6% for standing, 11.8% ± 4.5% for sitting, 20.6% ± 16.3% for recumbency, 14.1% ± 6.5% for lying, and 38.1% ± 7.5% for lactating. The overall mean daily percentages ± SDs of piglet activities were 38.1% ± 7.5% for suckling, 22.2% ± 5.4% for active, and 39.7% ± 10.5% for rest. The proposed approach provides a total solution for the automatic monitoring of sows and their piglets in the farrowing house. This automatic detection of abnormal lactation-related behaviors can help in preventing piglet preweaning mortality and therefore aid pig farming efficiency. Copyright © 2024 Tsai, Huang, Lin, Lai, Hong, Tsai, Chiang and Kuo.","pig feeding; piglet movement; sow posture; suckling; suckling sow posture; unfed piglet detection","","Ministry of Agriculture, Taiwan, (108AS-13.2.11-ST-a3, 109AS-11.3.2-STa1)","The author(s) declare financial support was received for the research, authorship, and/or publication of this article. This research was supported by the Ministry of Agriculture, Taiwan, under grants 108AS-13.2.11-ST-a3 and 109AS-11.3.2-STa1. Acknowledgments ","Y.-F. Kuo; Lab of Machine Learning and Machine Vision, Department of Biomechatronics Engineering, National Taiwan University, Taipei, Taiwan; email: ykuo@ntu.edu.tw","","Frontiers Media SA","26736225","","","","English","Front. Anim. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85200257971"
"Yen J.-J.; Pan Y.-H.; Wang C.-H.","Yen, Jun-Jie (59357747600); Pan, Yu-Hao (59357343000); Wang, Chi-Hung (59357333800)","59357747600; 59357343000; 59357333800","Deer Species and Gender Detection system based on YOLO v9","2024","11th IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2024","","","","463","464","1","0","10.1109/ICCE-Taiwan62264.2024.10674650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205776308&doi=10.1109%2fICCE-Taiwan62264.2024.10674650&partnerID=40&md5=52d35779081e61e06c39d7c3c03cdb56","Feng Chia University, Department of Artificial Intelligence Technology and Application, Taichung City, Taiwan; Feng Chia University, Dept. of Artificial Intelligence Technology and Applications, Taichung City, Taiwan","Yen J.-J., Feng Chia University, Department of Artificial Intelligence Technology and Application, Taichung City, Taiwan; Pan Y.-H., Feng Chia University, Department of Artificial Intelligence Technology and Application, Taichung City, Taiwan; Wang C.-H., Feng Chia University, Dept. of Artificial Intelligence Technology and Applications, Taichung City, Taiwan","Earth currently hosts 18 genera and 41 deer species, yet various human activities have degraded their natural habitats. According to the IUCN Red List, 10 deer species are now at a heightened risk of extinction. To detect and conserve endangered deer populations, this work proposes a YOLOv9 model combined with the Squeeze-and-excitation networks (SENet) to build a Deer Species and Gender Detection System. Compared to other deer detection systems, the proposed system demonstrates the capability to do deer detection, deer species classification, and deer gender identification with minimal data, which has 94% mAP@0.5. © 2024 IEEE.","Attention mechanism; Biometric recognition; Computer vision; Deep learning; YOLOv9","Biotic; Invertebrates; Attention mechanisms; Biometric recognition; Deep learning; Deer population; Detection system; Gender detection; Human activities; Natural habitat; Species classification; YOLOv9; Livestock","","","C.-H. Wang; Feng Chia University, Dept. of Artificial Intelligence Technology and Applications, Taichung City, Taiwan; email: chihwang@o365.fcu.edu.tw","","Institute of Electrical and Electronics Engineers Inc.","","979-835038684-4","","","English","IEEE Int. Conf. Consum. Electron. - Taiwan, ICCE-Taiwan","Conference paper","Final","","Scopus","2-s2.0-85205776308"
"Chen C.; Qiu J.; He L.; Wu Q.; Lu X.","Chen, Chen (59195264100); Qiu, Jing (37011683500); He, Lei (59373249700); Wu, Qiuhong (59373096600); Lu, Xiaolei (57222422442)","59195264100; 37011683500; 59373249700; 59373096600; 57222422442","Research on the application of image recognition technology in livestock and poultry behavior recognition","2024","Proceedings of SPIE - The International Society for Optical Engineering","13272","","132722C","","","","0","10.1117/12.3048224","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206815962&doi=10.1117%2f12.3048224&partnerID=40&md5=9a5226017e46386c034d42adc8bb3295","College of Big Data (College of Information Engineering), Yunnan Agricultural University, Yunnan, Kunming, 650201, China; Teaching Affairs Department, Yunnan Agricultural University, Yunnan, Kunming, 650201, China; School of Information and Intelligent Engineering, Yunnan College of Business and Management, Yunnan, Kunming, 650106, China","Chen C., College of Big Data (College of Information Engineering), Yunnan Agricultural University, Yunnan, Kunming, 650201, China; Qiu J., Teaching Affairs Department, Yunnan Agricultural University, Yunnan, Kunming, 650201, China; He L., College of Big Data (College of Information Engineering), Yunnan Agricultural University, Yunnan, Kunming, 650201, China; Wu Q., College of Big Data (College of Information Engineering), Yunnan Agricultural University, Yunnan, Kunming, 650201, China; Lu X., School of Information and Intelligent Engineering, Yunnan College of Business and Management, Yunnan, Kunming, 650106, China","Image recognition technology, leveraging the analytical capabilities of deep learning models, has become an advanced non-contact monitoring tool in the livestock and poultry industry, achieving efficient monitoring and analysis of livestock and poultry behavior, thereby improving production efficiency and animal welfare. This article outlines the development and basic principles of image recognition technology, with a focus on the current application of image recognition techniques such as object detection, image segmentation, and pose estimation in the recognition of animal behavior. It also highlights the challenges faced by image recognition technology in the application of animal behavior recognition and proposes optimization strategies. This paper aims to provide a reference for accelerating the efficient and sustainable development of the breeding industry, promoting the deep integration of artificial intelligence and breeding, and driving the continuous development of the domestic modern breeding industry. © 2024 SPIE.","Behavior recognition; Deep learning; Image recognition; Livestock and poultry","Animal behaviour; Behaviour recognition; Deep learning; Efficient monitoring; Image recognition technology; Learning models; Livestock and poultry; Monitoring tools; Non-contact monitoring; Poultry industry; Livestock","","","J. Qiu; Teaching Affairs Department, Yunnan Agricultural University, Kunming, Yunnan, 650201, China; email: qiujingyn@qq.com; X. Lu; School of Information and Intelligent Engineering, Yunnan College of Business and Management, Kunming, Yunnan, 650106, China; email: kemlxl@163.com","Yin M.; Zhang X.","SPIE","0277786X","978-151068286-3","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85206815962"
"Tan Z.; Liu J.; Xiao D.; Liu Y.; Huang Y.","Tan, Zujie (58508968300); Liu, Junbin (58198765500); Xiao, Deqin (24469363100); Liu, Youfu (57206825175); Huang, Yigui (57225910332)","58508968300; 58198765500; 24469363100; 57206825175; 57225910332","Dual-Stream Fusion Network with ConvNeXtV2 for Pig Weight Estimation Using RGB-D Data in Aisles","2023","Animals","13","24","3755","","","","8","10.3390/ani13243755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180475873&doi=10.3390%2fani13243755&partnerID=40&md5=2dc157f245f73ccc707f8d9d4144737f","College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China; Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China","Tan Z., College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China, Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China; Liu J., College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China, Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China; Xiao D., College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China, Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China; Liu Y., College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China, Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China; Huang Y., College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China, Key Laboratory of Smart Agricultural Technology in Tropical South China, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China","In the field of livestock management, noncontact pig weight estimation has advanced considerably with the integration of computer vision and sensor technologies. However, real-world agricultural settings present substantial challenges for these estimation techniques, including the impacts of variable lighting and the complexities of measuring pigs in constant motion. To address these issues, we have developed an innovative algorithm, the moving pig weight estimate algorithm based on deep vision (MPWEADV). This algorithm effectively utilizes RGB and depth images to accurately estimate the weight of pigs on the move. The MPWEADV employs the advanced ConvNeXtV2 network for robust feature extraction and integrates a cutting-edge feature fusion module. Supported by a confidence map estimator, this module effectively merges information from both RGB and depth modalities, enhancing the algorithm’s accuracy in determining pig weight. To demonstrate its efficacy, the MPWEADV achieved a root-mean-square error (RMSE) of 4.082 kg and a mean absolute percentage error (MAPE) of 2.383% in our test set. Comparative analyses with models replicating the latest research show the potential of the MPWEADV in unconstrained pig weight estimation practices. Our approach enables real-time assessment of pig conditions, offering valuable data support for grading and adjusting breeding plans, and holds broad prospects for application. © 2023 by the authors.","computer vision; deep learning; mass measurement","accuracy; algorithm; animal experiment; animal model; Article; autoencoder; body weight; breeding; computer vision; convolutional neural network; deep learning; feature extraction; illumination; image processing; image quality; limit of detection; livestock; nonhuman; pig; root mean squared error; scoring system","National Key Research and Development Program of China, NKRDPC, (2021YFD200802, 2023B10564002); National Key Research and Development Program of China, NKRDPC","This research was supported by the National Key R&D Program of China (2021YFD200802), as a key technology research and integrated application of smart agriculture (2023B10564002).","D. Xiao; College of Mathematics Informatics, South China Agricultural University, Guangzhou, 510642, China; email: deqinx@scau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180475873"
"Gavojdian D.; Mincu M.; Lazebnik T.; Oren A.; Nicolae I.; Zamansky A.","Gavojdian, Dinu (55250015500); Mincu, Madalina (57225025213); Lazebnik, Teddy (57220855077); Oren, Ariel (57712226000); Nicolae, Ioana (57195260952); Zamansky, Anna (8731254400)","55250015500; 57225025213; 57220855077; 57712226000; 57195260952; 8731254400","BovineTalk: machine learning for vocalization analysis of dairy cattle under the negative affective state of isolation","2024","Frontiers in Veterinary Science","11","","1357109","","","","2","10.3389/fvets.2024.1357109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185282772&doi=10.3389%2ffvets.2024.1357109&partnerID=40&md5=d12ed2c19911e9ee5c903d897a3ee0c9","Cattle Production Systems Laboratory, Research and Development Institute for Bovine, Balotesti, Romania; Department of Mathematics, Ariel University, Ariel, Israel; Department of Cancer Biology, University College London, London, United Kingdom; Tech4Animals Laboratory, Information Systems Department, University of Haifa, Haifa, Israel","Gavojdian D., Cattle Production Systems Laboratory, Research and Development Institute for Bovine, Balotesti, Romania; Mincu M., Cattle Production Systems Laboratory, Research and Development Institute for Bovine, Balotesti, Romania; Lazebnik T., Department of Mathematics, Ariel University, Ariel, Israel, Department of Cancer Biology, University College London, London, United Kingdom; Oren A., Tech4Animals Laboratory, Information Systems Department, University of Haifa, Haifa, Israel; Nicolae I., Cattle Production Systems Laboratory, Research and Development Institute for Bovine, Balotesti, Romania; Zamansky A., Tech4Animals Laboratory, Information Systems Department, University of Haifa, Haifa, Israel","There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry, and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types of vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts, and open mouth emitted high-frequency calls (HF), produced for long-distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. One contribution of this study is providing the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here, we present two computational frameworks—deep learning based and explainable machine learning based, to classify high and low-frequency cattle calls and individual cow voice recognition. Our models in these two frameworks reached 87.2 and 89.4% accuracy for LF and HF classification, with 68.9 and 72.5% accuracy rates for the cow individual identification, respectively. Copyright © 2024 Gavojdian, Mincu, Lazebnik, Oren, Nicolae and Zamansky.","affective states; animal communication; cattle; vocal parameters; welfare indicators","animal experiment; Article; audio recording; dairy cattle; lactation; machine learning; milk yield; nonhuman; speech analysis; vocalization","Ministry of Research; Corporation for National and Community Service, CNCS; Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii, UEFISCDI, (PN-III-P1-1.1-TE-2021-0027)","The author(s) declare financial support was received for the research, authorship, and/or publication of this article. This work was supported by a grant of the Ministry of Research, Innovation and Digitization, CNCS—UEFISCDI, project number PN-III-P1-1.1-TE-2021-0027, within PNCDI III. ","D. Gavojdian; Cattle Production Systems Laboratory, Research and Development Institute for Bovine, Balotesti, Romania; email: gavojdian_dinu@animalsci-tm.ro","","Frontiers Media SA","22971769","","","","English","Front. Vet. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85185282772"
"Guo C.; Guo Z.; Ede T.; Lyu L.; He Z.; Mao A.; Parsons T.D.; Liu K.","Guo, Chuanyi (59343517800); Guo, Zhaojin (58804462600); Ede, Thomas (57202640938); Lyu, Li (58805389800); He, Zheng (58444762800); Mao, Axiu (57238219000); Parsons, Thomas D. (57210708339); Liu, Kai (55823366100)","59343517800; 58804462600; 57202640938; 58805389800; 58444762800; 57238219000; 57210708339; 55823366100","A Two-Stage Computer Vision Framework for Individual Recognition and Precision Data Collection Illustrated through Piglets in Farrowing Pens","2024","11th European Conference on Precision Livestock Farming","","","","1698","1706","8","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204969948&partnerID=40&md5=1221594d52ae0028b8fd95022244fe07","Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, PA, United States; School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China","Guo C., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Guo Z., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Ede T., Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, PA, United States; Lyu L., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; He Z., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Mao A., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong, School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China; Parsons T.D., Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, PA, United States; Liu K., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong","Intensive livestock farming practices in the modern era are characterized by large flock sizes and high animal densities. These conditions make traditional manual observation methods inefficient, impractical, and error-prone. While computer vision-based research has made notable progress in studying animal populations, it often falls short in enabling long-term, precise individual recognition for comprehensive behavioral analysis and individual health management. This limitation hampers in-depth research on animal physiology, nutrition, health, behavior, and welfare. In this study, we propose a two-stage individual recognition and tracking scheme for piglets in commercial farrowing pen settings. Our approach provides a valuable tool for pig research by continuously recognizing and tracking individual piglets, accurately measuring their behavioral responses to different experimental treatments. Furthermore, this method can be adapted to other species as well. In the first stage, we employ a lightweight model based on improved YOLO to detect the presence of the sow and piglets within the farrowing pen. In the second stage, we utilize a fine-tuned handwritten digit recognition model based on color-adjusted Mnist to identify the colored digits (e.g., digits 0 to 3 in blue, green, and red) on the bodies of piglets. By combining the two stages, our approach enables individual piglet detection and identification. Experimental results demonstrate an average recognition accuracy of 97.6%, successfully identifying 12 piglets over long periods. In practical applications, this approach has the potential to greatly facilitate animal science research. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","animal welfare; applied ethology; deep learning; piglet recognition; precision livestock farming","Invertebrates; Mammals; Animal welfare; Applied ethology; Data collection; Deep learning; Individual recognition; Livestock farming; Model-based OPC; Piglet recognition; Precision livestock farming; Vision frameworks; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204969948"
"Wang M.; Qian K.","Wang, Meili (55694491200); Qian, Kun (57213057429)","55694491200; 57213057429","Smart Rural Services for Managing Plant Diseases, Livestock, and Medical Care","2024","Digital Transformation with AI and Smart Servicing Technologies for Sustainable Rural Development","","","","83","115","32","0","10.1201/9781032686691-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195724602&doi=10.1201%2f9781032686691-5&partnerID=40&md5=0ab91c71df1046d854080249312261eb","College of Information Engineering, Northwest A&F University, Xianyang, China; School of Biomedical Engineering & Imaging Sciences, King’s College London, London, United Kingdom","Wang M., College of Information Engineering, Northwest A&F University, Xianyang, China; Qian K., School of Biomedical Engineering & Imaging Sciences, King’s College London, London, United Kingdom","In this chapter, we introduce the present research and work progress related to plant disease identification, livestock management, medical technology, and facial disease detection. For plant disease identification, we propose an improved detection algorithm based on YOLOv5s and design a plant disease detection application system. For livestock management, we propose an improved method of fusion of multiple optimizations to verify the high similarity of cashmere goat ID. For medical technology, we propose a cutting-based surface parameterization transfer algorithm that can be used to transfer the surface parameterization attribute. For facial disease detection, we propose a platform to embed deep learning methods, train and generate a network that can recognize and classify facial diseases firstly, and then embed the network into the system to assist the rural health system. Finally, the future work is discussed. © 2024 selection and editorial matter, Shengfeng Qin, Hongan Wang, and Cuixia Ma; individual chapters, the contributors.","","","","","","","CRC Press","","978-104004859-7; 978-103268667-7","","","English","Digital Transformation with AI and Smart Servicing Technologies for Sustainable Rural Development","Book chapter","Final","","Scopus","2-s2.0-85195724602"
"Ramakrishna S.; Boccaccini A.R.; Zare M.","Ramakrishna, Seeram (57205523298); Boccaccini, Aldo R. (55937239600); Zare, Mina (57195981836)","57205523298; 55937239600; 57195981836","Futures of BME: Sustainable medical materials 2023","2023","Current Opinion in Biomedical Engineering","28","","100507","","","","0","10.1016/j.cobme.2023.100507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175354048&doi=10.1016%2fj.cobme.2023.100507&partnerID=40&md5=47330fd5a45f9c15bce7a622cad06956","Center for Nanotechnology and Sustainability, Department of Mechanical Engineering, National University of Singapore, Singapore; Institute of Biomaterials, University of Erlangen-Nuremberg, Cauerstr. 6, Erlangen, 91058, Germany; Department of Food and Nutrition, University of Helsinki, Helsinki, 00014, Finland; Helsinki Institute of Sustainability Science (HELSUS), University of Helsinki, Helsinki, 00014, Finland","Ramakrishna S., Center for Nanotechnology and Sustainability, Department of Mechanical Engineering, National University of Singapore, Singapore; Boccaccini A.R., Institute of Biomaterials, University of Erlangen-Nuremberg, Cauerstr. 6, Erlangen, 91058, Germany; Zare M., Department of Food and Nutrition, University of Helsinki, Helsinki, 00014, Finland, Helsinki Institute of Sustainability Science (HELSUS), University of Helsinki, Helsinki, 00014, Finland","[No abstract available]","","alginic acid; biobased material; biomaterial; biopolymer; chitosan; gelatin; gold; graphene; solvent; sustainable material; unclassified drug; agricultural waste; aqueous solution; artificial intelligence; biocompatibility; biodegradability; biomedical engineering; bone regeneration; bone tissue; chemical reaction; clinical research; cost effectiveness analysis; deep learning; drug delivery system; economic development; Editorial; electrochemical analysis; electrospinning; food waste; freeze drying; green chemistry; health care; human; immune response; in vivo study; livestock; machine learning; marine biology; mechanics; nonhuman; plant; solubility; sustainable development; three dimensional printing; tissue engineering; valorization; wound healing; Young modulus","","","M. Zare; Department of Food and Nutrition, University of Helsinki, Helsinki, 00014, Finland; email: mina.zare@helsinki.fi","","Elsevier B.V.","24684511","","","","English","Curr. Opin. Biomed. Eng.","Editorial","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85175354048"
"Zhang C.; Lyu C.; Hao T.; Liu J.; Sarhan N.; Awwad E.M.; Ghadi Y.Y.","Zhang, Chengping (58992881700); Lyu, Chengzhi (57204920740); Hao, Tang (59179291600); Liu, Jinru (59179138800); Sarhan, Nadia (58486356400); Awwad, Emad Mahrous (59149084400); Ghadi, Yazeed Yasin (55797735700)","58992881700; 57204920740; 59179291600; 59179138800; 58486356400; 59149084400; 55797735700","Global warming’s grip on agriculture: Strategies for sustainable production amidst climate change using regression based prediction","2024","Emirates Journal of Food and Agriculture","36","","","","","","0","10.3897/ejfa.2024.125630","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196356662&doi=10.3897%2fejfa.2024.125630&partnerID=40&md5=0c002de4771b9fd70a553c518dfa8a6e","Mechanical and Electrical Engineering College, Hainan Vocational University of Science and Technology, Haikou, 571126, China; Hubei University of Automotive Technology, Hubei Province, Hubei, China; School of information and Communication Engineering, Hainan University, Haikou, China; Department of Quantitative Analysis, College of Business Administration, King Saud University, Riyadh, Saudi Arabia; Department of Electrical Engineering, College of Engineering, King Saud University, P.O. Box 800, Riyadh, 11421, Saudi Arabia; Department of Computer Science and Software Engineering, Al Ain University, Abu Dhabi, 12555, United Arab Emirates","Zhang C., Mechanical and Electrical Engineering College, Hainan Vocational University of Science and Technology, Haikou, 571126, China; Lyu C., Hubei University of Automotive Technology, Hubei Province, Hubei, China; Hao T., School of information and Communication Engineering, Hainan University, Haikou, China; Liu J., School of information and Communication Engineering, Hainan University, Haikou, China; Sarhan N., Department of Quantitative Analysis, College of Business Administration, King Saud University, Riyadh, Saudi Arabia; Awwad E.M., Department of Electrical Engineering, College of Engineering, King Saud University, P.O. Box 800, Riyadh, 11421, Saudi Arabia; Ghadi Y.Y., Department of Computer Science and Software Engineering, Al Ain University, Abu Dhabi, 12555, United Arab Emirates","The intersection of climate change and food production is emerging as a critical area of research, focusing on both the potential benefits and the significant challenges posed by changing climate conditions. Elevated levels of carbon dioxide alongside rising global temperatures could theoretically boost crop yields, benefiting both human and animal consumption. This study examines the impact of various climate variables—temperature, humidity, precipitation, and soil moisture—on the primary production of essential foods such as rice, wheat, livestock, milk, eggs, vegetables, and fruits. Utilizing data from different countries spanning from 2000 to 2020, drawn from world development indicators, this research employs econometric analysis coupled with deep learning-based cluster analysis. Additionally, it projects future production trends up to 2100 using the moving average time series forecasting method. The findings reveal a direct correlation between climate variables and the production levels of vegetables and other food items, highlighting the immediate effects of climatic changes on agriculture. The study also points out the uneven distribution of these climate impacts, with developing countries facing more severe challenges due to their limited resources and adaptive capacities. This uneven impact contributes to increased uncertainty in food supply and affects market stability. Furthermore, concerns about food safety are intensifying under the influence of climate change, although some regions have implemented effective food conservation and control measures to mitigate these risks. This research underscores a complex landscape where the risks and benefits of climate change on food production are not uniformly distributed, but rather are influenced by a myriad of factors including geographic location, economic conditions, and the level of technological advancement in food safety practices. The nuanced understanding of these dynamics is crucial for developing targeted strategies to enhance food security in the face of a changing climate. © Zhang, et al.","Climate Change; Deep Learning; Food Production; Food Resources","","Hubei Institute of Automotive Technology, (BK202102); King Saud University, KSU, (RSPD2024R1052); King Saud University, KSU","This study is supported by PhD research fund of Hubei Institute of Automotive Technology BK202102. The authors present their appreciation to King Saud University for funding this research through Researchers Supporting Program number (RSPD2024R1052), King Saud University, Riyadh, Saudi Arabia.","C. Lyu; Hubei University of Automotive Technology, Hubei, Hubei Province, China; email: sanchilongquan1@outlook.com","","Pensoft Publishers","2079052X","","","","English","Emirates J. Food Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85196356662"
"Wang Y.; Chu M.; Kang X.; Liu G.","Wang, Yanchao (57375336600); Chu, Mengyuan (57224800553); Kang, Xi (57056967700); Liu, Gang (57374481500)","57375336600; 57224800553; 57056967700; 57374481500","A deep learning approach combining DeepLabV3+ and improved YOLOv5 to detect dairy cow mastitis","2024","Computers and Electronics in Agriculture","216","","108507","","","","9","10.1016/j.compag.2023.108507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179763711&doi=10.1016%2fj.compag.2023.108507&partnerID=40&md5=58dfbf6dfc8c5114d9ebd5cc51315952","Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; School of Computing and Data Engineering, NingboTech University, Zhejiang, Ningbo, 315200, China","Wang Y., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; Chu M., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; Kang X., School of Computing and Data Engineering, NingboTech University, Zhejiang, Ningbo, 315200, China; Liu G., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China","Dairy cow mastitis has a great impact on the productivity of dairy cows and the profits of livestock farms. Early detection is of great significance to improve the efficiency of mastitis treatment. However, due to the low resolution of thermal infrared images and the complexity of the living environment of dairy cows, it is difficult to detect the eyes and udders of cows, which reduces the detection accuracy of mastitis. To solve this problem, this paper proposes a two-stage model (DCYOLO) integrating the DeepLabV3 + semantic segmentation network and an improved YOLOv5 target recognition network, which is used to detect the eyes and udders of dairy cows under complex background and applied to the severity classification of dairy cow mastitis. In the first stage, the DeepLabV3 + model was used to segment the eyes and udders of dairy cows from thermal infrared images. In the second stage, the segmented image was input into the target recognition YOLOv5 network for key parts recognition. Finally, to further improve the detection accuracy of the model, a convolutional block attention module (CBAM) was added at the end of the main part of the YOLOv5 model. After comparing different semantic segmentation and target recognition networks, the DeepLabV3 + network and YOLOv5 network performed best. The mIoU and mean pixel accuracy (mPA) of the DeepLabV3 + network reached 86.98 % and 92.92 %, respectively. The mean average precision (mAP) and F1 scores of the YOLOv5 network for unsegmented thermal infrared images reached 93.4 % and 90.9 %, respectively. The CBAM-added YOLOv5 (CAYOLOv5) model was combined with the DeepLabV3 + model. Compared with the single YOLOv5 model, the mAP and F1 scores of DCYOLO increased by 5.4 % and 5.3 %, respectively. Therefore, the proposed model can achieve more accurate positioning of key parts of dairy cows. Based on this model, the eye and udder temperature differences of 50 dairy cows were extracted for mastitis detection, and the detection results were compared with the results of the somatic cell count (SCC) approach. The results showed that the classification accuracy of mastitis was 86 %, and the average sensitivity and specificity were 79.41 % and 92.49 %, respectively. The dairy cow mastitis detection method based on the two-stage model can accurately locate the key parts of dairy cows and realize the automatic detection and classification of dairy cow mastitis, and the accuracy is high. © 2023 Elsevier B.V.","Attention mechanism; Dairy cow; Mastitis automatic detection; Segmentation and recognition; Two-stage detection","Complex networks; Convolutional neural networks; Deep learning; Diseases; Farms; Infrared imaging; Semantics; Attention mechanisms; Automatic Detection; Dairy cow; Detection accuracy; Key parts; Mastiti automatic detection; Segmentation and recognition; Target recognition; Thermal infrared images; Two-stage detections; detection method; livestock farming; machine learning; pixel; recognition; semantic standardization; Semantic Segmentation","China Agricultural University, CAU; Chinese Universities Scientific Fund; National Key Research and Development Program of China, NKRDPC, (2021YFD1300502); National Key Research and Development Program of China, NKRDPC","Funding text 1: We would like to thank the National Key R&D Program of China and China Agricultural University for their support. And we acknowledge the dairy farms of the Dadi Group Farm in Yanqing District for using their animals and facilities as well as all the farmers involved in the experimental data collection. In addition, we also acknowledge the Laboratory Animal Welfare and Ethics Review Committee of China Agricultural University for its approval of our experiment. ; Funding text 2: This work was supported by the National Key R&D Program of China (no. 2021YFD1300502 ), and the Chinese Universities Scientific Fund, Beijing. ","G. Liu; College of Information and Electrical Engineering, China Agricultural University, China; email: pac@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85179763711"
"Kala K.U.; Nandhini M.; Kishore Chakkravarthi M.N.; Thangadarshini M.; Madhusudhana Verma S.","Kala, K.U. (57211870141); Nandhini, M. (57219780255); Kishore Chakkravarthi, M.N. (58785838100); Thangadarshini, M. (57222228471); Madhusudhana Verma, S. (57665457800)","57211870141; 57219780255; 58785838100; 57222228471; 57665457800","DEEP LEARNING TECHNIQUES FOR CROP NUTRIENT DEFICIENCY DETECTION-A COMPREHENSIVE SURVEY","2024","Precision Agriculture for Sustainability: Use of Smart Sensors, Actuators, and Decision Support Systems","","","","319","326","7","3","10.1201/9781003435228-18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180818621&doi=10.1201%2f9781003435228-18&partnerID=40&md5=bf2f61043444ca8c59c46c65f370e28b","Department of Computer Science, Pondicherry University, Puducherry, India; Department of Electrical and Electronics, Puducherry Technical University (PTU), Puducherry, India; Department of Computer Science and Engineering, Puducherry Technical University (PTU), Puducherry, India; Department of Operations Research & Statistical Quality Control, Rayalaseema University, Andhra Pradesh, India","Kala K.U., Department of Computer Science, Pondicherry University, Puducherry, India; Nandhini M., Department of Computer Science, Pondicherry University, Puducherry, India; Kishore Chakkravarthi M.N., Department of Electrical and Electronics, Puducherry Technical University (PTU), Puducherry, India; Thangadarshini M., Department of Computer Science and Engineering, Puducherry Technical University (PTU), Puducherry, India; Madhusudhana Verma S., Department of Operations Research & Statistical Quality Control, Rayalaseema University, Andhra Pradesh, India","Agriculture is the backbone of the economic system in developing countries. Agriculture is the science or practice of farming together with the cultivation of crops and the rearing of livestock for providing food, wool, and other products. The major source of food for the world population is agriculture, especially, the cultivation of cereals, vegetables, and fruits. The health of the plants is essential to the profitable and sustainable production of the crops. Balanced nutrition has a high impact on the health of plants. It is essential for 320achieving increased quantity and quality of yields in production. Nutrients serve as the first line of defense in a plant. Over the growth cycle of the crops, the need for nutrients, and their ratio may vary. The farmers are facing difficulty in identifying the concentration of the nutrients which the crop needs. In most of the crop varieties, the deficiency of nutrients gives the impression on the leaves as the variation in their color and shape. Artificial intelligence, especially, deep learning techniques are efficient in diagnosing nutrient deficiencies by analyzing these symptoms. Although the deep convolutional neural networks (DCNNs) have established their efficacy in classifying and detecting diseases in plants and humans, their use in identifying deficiency of nutrients has attained little attention. This study analyses the effectiveness of DCNN for identifying nutrient deficiency by surveying the state-of-theart research works in this area. This study will help the researchers to attain a brief idea of recent trends and techniques in detecting nutrient deficiencies in the crops and motivate them to do more innovations in this area of research. © 2024 by Apple Academic Press, Inc.","banana plants; convolutional neural network; crop protection; deep learning; nutrient defciency; precision agriculture","","","","","","Apple Academic Press","","978-100095553-8; 978-177491373-4","","","English","Precision Agriculture for Sustainability: Use of Smart Sensors, Actuators, and Decision Support Systems","Book chapter","Final","","Scopus","2-s2.0-85180818621"
"Liu H.; Reibman A.R.; Boerman J.P.","Liu, He (57190393767); Reibman, Amy R. (7005098846); Boerman, Jacquelyn P. (55959044200)","57190393767; 7005098846; 55959044200","Feature extraction using multi-view video analytics for dairy cattle body weight estimation","2023","Smart Agricultural Technology","6","","100359","","","","3","10.1016/j.atech.2023.100359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177868601&doi=10.1016%2fj.atech.2023.100359&partnerID=40&md5=7dda9f181a1405ba394402c592513651","Purdue University, School of Electrical and Computer Engineering, 501 Northwestern Ave., West Lafayette, 47907, IN, United States; Purdue University, Department of Animal Sciences, 270 S. Russell St., West Lafayette, 47907, IN, United States","Liu H., Purdue University, School of Electrical and Computer Engineering, 501 Northwestern Ave., West Lafayette, 47907, IN, United States; Reibman A.R., Purdue University, School of Electrical and Computer Engineering, 501 Northwestern Ave., West Lafayette, 47907, IN, United States; Boerman J.P., Purdue University, Department of Animal Sciences, 270 S. Russell St., West Lafayette, 47907, IN, United States","Accurate body weight (BW) estimation of livestock provides valuable information about animal health and welfare and can be used to assess changes in nutrient status of individuals or groups of animals. To obtain BW information regularly, automated low-cost methods that do not intrude upon normal farm operation are essential. Measurements of BW from 196 lactating, Holstein dairy cattle were collected across 5 days (655 +/- 77.1 kg; range 458 - 876 kg). This population of animals was used to develop a BW estimation system for dairy cattle using a synchronized two-camera system. The system applies deep-learning models and domain knowledge to both camera views to estimate BW of dairy cattle despite challenges such as occluding fences and crowded backgrounds. Videos from side and front-view were used to extract features about the anatomical locations and shape of the individual cows, and a regression model was applied to the features for BW prediction. We demonstrate that each view provides valuable information for BW estimation, where combining two views outperforms either view separately. This is despite the side view having distractors such as occluding fences and crowded backgrounds. The experimental results, applied to videos captured within the constraints of an operating dairy farm, demonstrate a root-mean-squared prediction error of 34 kg and a mean absolute percentage error of 4.1%, when a Random Forest is trained on four days of data and tested on the fifth unseen day. This model enabled 57% of cattle BW to be estimated within 25 kg of their measured BW. In addition, we demonstrated that linear regression generalized well to unseen data. © 2023 The Author(s)","Body weight estimation; Dairy cattle; Video analytics","","Foundation for Food and Agriculture, (534662)","This work was supported by the Foundation for Food and Agriculture Research Grant # 534662 and the Open Ag Technologies and Systems (OATS) Center . ","A.R. Reibman; Purdue University, School of Electrical and Computer Engineering, West Lafayette, 501 Northwestern Ave., 47907, United States; email: reibman@purdue.edu","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85177868601"
"Lin H.; Zhang K.; Li H.; Liu Y.; Chen Z.; Ma Q.","Lin, Huapu (58850775800); Zhang, Kai (59041724400); Li, Hao (56652563600); Liu, Yufei (58748475600); Chen, Zilin (58850694200); Ma, Qin (35332370300)","58850775800; 59041724400; 56652563600; 58748475600; 58850694200; 35332370300","Detecting herd pigs using multi-scale fusion attention mechanism; [基于多尺度融合注意力机制的群猪检测方法]","2023","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","39","21","","188","195","7","3","10.11975/j.issn.1002-6819.202306166","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183472314&doi=10.11975%2fj.issn.1002-6819.202306166&partnerID=40&md5=c505e03fa1ad492e4c8b3ff4a2d907e6","College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China","Lin H., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; Zhang K., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; Li H., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; Liu Y., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; Chen Z., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; Ma Q., College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China","Target detection has been widely applied to many scenarios in daily life, including people flow management, items counting, and object searching. Deep learning has also effectively improved the accuracy of target detection in recent years. Therefore, target detection can be expected to improve the efficiency of intelligent management in the livestock industry, such as pig farms. Daily population counting has been one of the most important steps in modern pig farms, such as target tracking and behavior recognition. However, the small targets or occluded pigs are difficult to accurately detect during counting. In this study, an accurate and rapid detection of the pig population was proposed to treat the small targets and occlusion in the dataset using a multi-scale fusion attention mechanism. YOLOpig network was constructed to detect the individual pig target using YOLOv7. Then, the scale network structure was proposed to enhance the detection of small targets. The residual thought of network structure was used to improve the convolution module for the high accuracy of the experiments. A parameter-free attention mechanism was also added to reduce the network weights while accelerating the detection speed. GradCAM was used for the feature visualization to verify the effectiveness of the experimental feature extraction. Finally, target tracking (StrongSORT) was adopted to accurately track the individual IDs of the pigs that were detected by the improved model, providing the identity information required for pig detection tasks. Experiments were conducted to verify the accuracy and real-time performance of the improved model on Large White pigs in the fattening stage. A series of experiments were conducted, including model ablation, model comparison, pig feature information extraction, and tracking. The effectiveness and feasibility of the models were verified to detect the pig groups. The great potential was obtained to solve the difficulties and challenges in pig population counting, providing important support in the agricultural breeding field. The experimental results show that the accuracy, recall, and average accuracy of the counting were 90.4%, 85.5%, and 92.4%, respectively. Furthermore, the average accuracy and the detection speed were improved by 5.1 percentage points and 7.14%, respectively, compared with the basic YOLOv7 model. The average accuracies of the YOLOv5, YOLOv7tiny, and YOLOv8n models were also improved by 12.1, 16.8, and 5.7 percentage points, respectively. Specifically, the pig population counting with a multi-scale fusion attention mechanism can be expected to rapidly and accurately complete the counting task, and then effectively deal with small targets and occlusion. The widespread application of the improved model can greatly contribute to the operational efficiency of pig farms for labor cost-saving, in order to promote the development of intelligent technology in the field of agricultural breeding. The more accurate and efficient counting of pigs can provide strong technical support for the field of agricultural breeding and intelligent farming. © 2023 Chinese Society of Agricultural Engineering. All rights reserved.","attention mechanism; image recognition; machine vision; multi-target tracking; pig counting; small object detection; YOLOv7","Behavioral research; Clutter (information theory); Computer vision; Deep learning; Farms; Feature extraction; Mammals; Object detection; Object recognition; Target tracking; Attention mechanisms; Machine-vision; Multi-target-tracking; Multiscale fusion; Pig counting; Pig farms; Small object detection; Small targets; Targets detection; YOLOv7; Image recognition","","","Q. Ma; College of Information and ELectrical Engineering, China Agricultural University, Beijing, 100083, China; email: sockline@163.com","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85183472314"
"Wang J.; Hu Y.; Xiang L.; Morota G.; Brooks S.A.; Wickens C.L.; Miller-Cushon E.K.; Yu H.","Wang, Jin (58403238200); Hu, Yu (58536006900); Xiang, Lirong (57201118152); Morota, Gota (55356558800); Brooks, Samantha A. (7401651238); Wickens, Carissa L. (16317988100); Miller-Cushon, Emily K. (30567600400); Yu, Haipeng (57195972353)","58403238200; 58536006900; 57201118152; 55356558800; 7401651238; 16317988100; 30567600400; 57195972353","Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision","2024","Journal of Animal Science","102","","skad416","","","","0","10.1093/jas/skad416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186494898&doi=10.1093%2fjas%2fskad416&partnerID=40&md5=5b1569914eb9986bb8c9e489f3a3328f","Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Department of Biological and Agricultural Engineering, North Carolina State University, Raleigh, 27695, NC, United States; School of Animal Sciences, Virginia Polytechnic Institute, State University, Blacksburg, 24061, VA, United States","Wang J., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Hu Y., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Xiang L., Department of Biological and Agricultural Engineering, North Carolina State University, Raleigh, 27695, NC, United States; Morota G., School of Animal Sciences, Virginia Polytechnic Institute, State University, Blacksburg, 24061, VA, United States; Brooks S.A., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Wickens C.L., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Miller-Cushon E.K., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States; Yu H., Department of Animal Sciences, University of Florida, Gainesville, 32611, FL, United States","Computer vision (CV), a non-intrusive and cost-effective technology, has furthered the development of precision livestock farming by enabling optimized decision-making through timely and individualized animal care. The availability of affordable two- and three-dimensional camera sensors, combined with various machine learning and deep learning algorithms, has provided a valuable opportunity to improve livestock production systems. However, despite the availability of various CV tools in the public domain, applying these tools to animal data can be challenging, often requiring users to have programming and data analysis skills, as well as access to computing resources. Moreover, the rapid expansion of precision livestock farming is creating a growing need to educate and train animal science students in CV. This presents educators with the challenge of efficiently demonstrating the complex algorithms involved in CV. Thus, the objective of this study was to develop ShinyAnimalCV, an open-source cloud-based web application designed to facilitate CV teaching in animal science. This application provides a user-friendly interface for performing CV tasks, including object segmentation, detection, three-dimensional surface visualization, and extraction of two- and three-dimensional morphological features. Nine pre-trained CV models using top-view animal data are included in the application. ShinyAnimalCV has been deployed online using cloud computing platforms. The source code of ShinyAnimalCV is available on GitHub, along with detailed documentation on training CV models using custom data and deploying ShinyAnimalCV locally to allow users to fully leverage the capabilities of the application. ShinyAnimalCV can help to support the teaching of CV, thereby laying the groundwork to promote the adoption of CV in the animal science community. © The Author(s) 2023.","computer vision; morphological features; object detection; object segmentation; shiny application; three-dimensional visualization","Animal Husbandry; Animals; Cloud Computing; Computers; Imaging, Three-Dimensional; Livestock; Software; animal; animal husbandry; cloud computing; livestock; software; three-dimensional imaging; veterinary medicine","University of Florida, UF","This work was supported by the University of Florida startup funds to H.Y. We thank Oleksandr (Alex) Moskalenko for assisting us in setting up the instance on HiPerGator to deploy ShinyAnimalCV. ","H. Yu; Department of Animal Sciences, University of Florida, Gainesville, 32611, United States; email: haipengyu@ufl.edu","","Oxford University Press","00218812","","","38134209","English","J. Anim. Sci.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85186494898"
"Sonmez D.; Cetin A.","Sonmez, Doruk (57216415347); Cetin, Aydin (14047836100)","57216415347; 14047836100","An End-to-End Deployment Workflow for AI Enabled Agriculture Applications at the Edge","2024","6th International Conference on Computing and Informatics, ICCI 2024","","","","506","511","5","0","10.1109/ICCI61671.2024.10485167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190696511&doi=10.1109%2fICCI61671.2024.10485167&partnerID=40&md5=16a2103075fbc8cd7c0b8ece8ac674be","Computer Engineering Department, Gazi University Graduate School of Natural and Applied Sciences, Ankara, Turkey; Computer Engineering Department, Gazi University Faculty of Technology, Ankara, Turkey","Sonmez D., Computer Engineering Department, Gazi University Graduate School of Natural and Applied Sciences, Ankara, Turkey; Cetin A., Computer Engineering Department, Gazi University Faculty of Technology, Ankara, Turkey","The use of deep learning enabled computer vision applications gained momentum after AlexNet convolutional neural network architecture won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. This led implementation of such methods in various application fields including defense industry, smart cities, intelligent video analytics, industrial inspection, and healthcare. Success of these applications enabled agriculture and livestock management to benefit from such implementations in recent years. In this work, we proposed an end-to-end workflow for deep learning-based computer vision applications to facilitate the AI model training, model optimization, model deployment, metadata streaming, metadata recording, result filtering/searching and data visualization. The source data of the visualizations as result of the analysis are stored in a high-performance database presented within the scope of the study. On the other hand, using the state-of-the-art deep learning algorithms and infrastructures, both model training and inference performance-oriented classification model comparison were carried out. In order to compare the developed models with the models developed in other studies in the literature, the training and inference process were tested on the same open-source data set. The workflow is proposed as a hybrid solution that can be deployed both on streamline x86 computer systems and ARM64 embedded devices. Therefore, deployment benchmarks for servers and NVIDIA Jetson devices are also provided within the paper. © 2024 IEEE.","agriculture; component; computer vision; deep learning; edge deployment; embedded devices","Agriculture; Convolutional neural networks; Data visualization; Deep learning; Embedded systems; Inference engines; Learning algorithms; Learning systems; Metadata; Network architecture; Visualization; Agriculture applications; Component; Computer vision applications; Convolutional neural network; Deep learning; Edge deployment; Embedded device; End to end; Model training; Work-flows; Computer vision","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835037387-5","","","English","Int. Conf. Comput. Informatics, ICCI","Conference paper","Final","","Scopus","2-s2.0-85190696511"
"Negreiro A.; Alves A.; Ferreira R.; Bresolin T.; Menezes G.; Casella E.; Rosa G.J.M.; Dórea J.R.R.","Negreiro, A. (58803971000); Alves, A. (57192072071); Ferreira, R. (57362855900); Bresolin, T. (55618681200); Menezes, G. (57481294200); Casella, E. (59513158600); Rosa, G.J.M. (57219659850); Dórea, J.R.R. (37057402900)","58803971000; 57192072071; 57362855900; 55618681200; 57481294200; 59513158600; 57219659850; 37057402900","Siamese Networks for identification of Holstein cattle during growth and across different physiological stages","2024","11th European Conference on Precision Livestock Farming","","","","467","474","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204955314&partnerID=40&md5=cb1e0134b077f71cf912032bf5c759c6","University of Wisconsin-Madison, Madison, WI, United States; University of Georgia, Athens, GA, United States; University of Illinois Urbana-Champaign, Champaign, IL, United States","Negreiro A., University of Wisconsin-Madison, Madison, WI, United States; Alves A., University of Georgia, Athens, GA, United States; Ferreira R., University of Wisconsin-Madison, Madison, WI, United States; Bresolin T., University of Illinois Urbana-Champaign, Champaign, IL, United States; Menezes G., University of Wisconsin-Madison, Madison, WI, United States; Casella E., University of Wisconsin-Madison, Madison, WI, United States; Rosa G.J.M., University of Wisconsin-Madison, Madison, WI, United States; Dórea J.R.R., University of Wisconsin-Madison, Madison, WI, United States","In dairy production, accurate individual animal identification is essential for farm management and addressing concerns related to food security and consumer trust. While computer vision systems (CVS) have been proposed for non-invasive animal recognition, limited research has explored their capability to identify the same individual across various life stages. This study aims to bridge this gap by developing a CVS using Siamese Neural Networks (SNN), designed for open-set identification of Holstein calves based on images captured during their initial weeks of life, with the capability to recognize these same individuals after a year of growth. The training dataset consisted of top-down view infrared images of 51 calves aged 1 to 6 weeks, collected on six separate days, resulting in 300 images per calf. These images were used to train a SNN for individual identification with 12,000 image pairs, implemented in Python using Tensorflow and Keras. The trained model was tested on 10 infrared images of each animal after one year (60 weeks of age) using 5 support images captured during their initial weeks of life. Furthermore, the network's ability to recognize individuals not seen in the training set was assessed using 14 additional animals. The results show an F1-score of 73% for identifying individual calves after a year and 83% for recognizing individuals outside the 51-animal training group. These findings highlight the effectiveness of SNN in open-set animal identification and across life stages, demonstrating the potential of reliable traceability systems for animals throughout their lifespan using CVS. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","artificial intelligence; calves; deep learning; identification","Infrared imaging; Invertebrates; Animal identification; Calf; Computer vision system; Deep learning; Farm management; Food security; Holstein cattle; Identification; Life stages; Neural-networks; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204955314"
"Mao A.; Zhu M.; Huang E.; Guo Z.; He Z.; Lyu L.; Norton T.; Liu K.","Mao, A. (57238219000); Zhu, M. (57200920263); Huang, E. (57226523976); Guo, Z. (58804462600); He, Z. (58444762800); Lyu, L. (58805389800); Norton, T. (35273348100); Liu, K. (55823366100)","57238219000; 57200920263; 57226523976; 58804462600; 58444762800; 58805389800; 35273348100; 55823366100","Cross-species knowledge sharing for improved animal activity recognition with limited labelled data","2024","11th European Conference on Precision Livestock Farming","","","","556","562","6","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204965907&partnerID=40&md5=2d7b0015dc9f3382a9a19fcf54250521","School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China; Department of Mechanical Engineering, City University of HongKong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Department of Biosystems, Division Animal and Human Health Engineering, M3-BIORES, Katholieke University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium","Mao A., School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China; Zhu M., Department of Mechanical Engineering, City University of HongKong, Hong Kong; Huang E., Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Guo Z., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; He Z., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Lyu L., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong; Norton T., Department of Biosystems, Division Animal and Human Health Engineering, M3-BIORES, Katholieke University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Liu K., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of HongKong, Hong Kong","Deep learning dominates automated animal activity recognition (AAR) tasks due to high performance on large-scale labelled data. However, such data are often limited due to the laborious and time-consuming data labelling process, leading to weak feature representation ability and poor model performance. Existing works generally augment data sizes based on labelled data, exploit unlabelled data via semi-supervised learning, or pre-train models on large-scale datasets (e.g., ImageNet dataset). In this paper, we boost the capability of models trained on limited labelled data from a new perspective. Notably, there are increasing open-source datasets of various animal species in the field of wearable sensor-aided AAR. Different species may have similar movement patterns and thus possess shared features that can improve the model's feature representation ability. Nevertheless, data distributions across species are distinct due to their inherent discrepancy, bringing difficulty in learning generic representations. Therefore, we develop a Cross-species Knowledge Sharing network (CKS-Net), where convolutional layers capture species-shared features and a Species-specific Batch-Normalization (SBN) module is designed following each convolutional layer to avoid inter-species conflicts. The SBN module involves multiple BN layers that separately fit the distributions of different species. To verify our method's effectiveness, a case study of cattle behaviour recognition is conducted on data collected from six cattle via accelerometers. Two distinct public datasets from sheep and horses are used to provide shared knowledge. The results demonstrate that our method remarkably enhances the cattle behaviour classification performance with increments in precision, recall, F1-score, and accuracy of 14.27%, 0.22%, 8.71%, and 4.65%, respectively. © 2024 11th European Conference on Precision Livestock Farming. All rights reserved.","animal activity recognition; batch-normalization; data limitation; deep learning; wearable sensor","Invertebrates; Labeled data; Macroinvertebrates; Self-supervised learning; Activity recognition; Animal activities; Animal activity recognition; Batch-normalization; Cattles; Cross-species; Data limitations; Deep learning; Labeled data; Normalisation; Livestock","","","","Berckmans D.; Tassinari P.; Torreggiani D.","European Conference on Precision Livestock Farming","","979-122106736-1","","","English","European Conf. Precis. Livest. Farming","Conference paper","Final","","Scopus","2-s2.0-85204965907"
"Yeon J.S.; Ma R.; Kim S.-C.","Yeon, Jeong Se (59075013900); Ma, Ruihan (57580081500); Kim, Sang-Cheol (55561975800)","59075013900; 57580081500; 55561975800","Pig Face Recognition Application Using YOLO Algorithm and Transformer Model","2024","Lecture Notes in Networks and Systems","795","","","647","654","7","0","10.1007/978-3-031-44851-5_52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192489615&doi=10.1007%2f978-3-031-44851-5_52&partnerID=40&md5=4defaff5a3e49e11ba3f16fc960c33da","Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju-Si, South Korea; Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju-Si, South Korea","Yeon J.S., Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju-Si, South Korea; Ma R., Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju-Si, South Korea; Kim S.-C., Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju-Si, South Korea","The research revolves around the key utilization of the You Only Look Once (YOLO) algorithm for pig face detection, renowned for its real-time object recognition with impressive speed and accuracy. The increasing demand for intensified livestock farming necessitates accurate identification and traceability of animals, including cows and pigs. In this study, we propose a non-invasive, deep-learning-based biometric system for pig facial recognition. Our methodology involves several stages: firstly, we develop a ROS data collection module to capture facial information from ten pigs; subsequently, employing the SSIM method, we execute a preprocessing phase to eliminate highly similar images; finally, we employ an enhanced CNN image classification model (ViT), incorporating both fine-tuning and pretraining techniques for pig face recognition. Our proposed approach achieves an impressive 98.66% accuracy rate. In conclusion, smart farm technology, particularly employing the YOLO algorithm for pig face detection, holds immense potential for the livestock industry, especially in pig farming. Through image detection, movement analysis, and classification techniques, farmers can efficiently produce high-quality pigs with reduced labor requirements and minimized stress levels. This technology not only enhances pig grading and selection but also promotes animal welfare and sustainable farming practices. As it continues to evolve, it is expected to revolutionize the livestock industry and significantly contribute to the advancement of modern agriculture. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Deep learning; Pig face detection; Smart farm","Classification (of information); Deep learning; Farms; Grading; Image classification; Image enhancement; Learning systems; Mammals; Object recognition; Algorithm model; Biometric systems; Deep learning; Faces detection; Facial recognition; Livestock farming; Pig face detection; Real-time object recognition; Smart farm; Transformer modeling; Face recognition","Ministry of Agriculture, Food and Rural Affairs, MAFRA; Ministry of Science, ICT and Future Planning, MSIP, (421023-04); Ministry of Science, ICT and Future Planning, MSIP","This work was supported by the Ministry of Agriculture, Food and Rural Affairs, the Ministry of Science and ICT, the Rural Development administration, and the Smart Farm R&D Project Group\u2019s Smart Farm Multi-ministry Package Innovation Technology Development project (421023-04).","S.-C. Kim; Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju-Si, South Korea; email: sckim7777@jbnu.ac.kr","Lee S.-G.; An J.; Chong N.Y.; Strand M.; Kim J.H.","Springer Science and Business Media Deutschland GmbH","23673370","978-303144850-8","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85192489615"
"Pomar C.; Remus A.","Pomar, Candido (6701585743); Remus, Aline (54416199800)","6701585743; 54416199800","Review: Fundamentals, limitations and pitfalls on the development and application of precision nutrition techniques for precision livestock farming","2023","Animal","17","","100763","","","","7","10.1016/j.animal.2023.100763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151462919&doi=10.1016%2fj.animal.2023.100763&partnerID=40&md5=fe1a03968ff6453568688974cf4c6376","Sherbrooke Research and Development Centre, Agriculture and Agri-Food Canada, 2000 College Street, Sherbrooke, J1M 0C8, QC, Canada","Pomar C., Sherbrooke Research and Development Centre, Agriculture and Agri-Food Canada, 2000 College Street, Sherbrooke, J1M 0C8, QC, Canada; Remus A., Sherbrooke Research and Development Centre, Agriculture and Agri-Food Canada, 2000 College Street, Sherbrooke, J1M 0C8, QC, Canada","Precision livestock farming (PLF) concerns the management of livestock using the principles and technologies of process engineering. Precision nutrition (PN) is part of the PLF approach and involves the use of feeding techniques that allow the proper amount of feed with the suitable composition to be supplied in a timely manner to individual animals or groups of animals. Automatic data collection, data processing, and control actions are required activities for PN applications. Despite the benefits that PN offers to producers, few systems have been successfully implemented so far. Besides the economical and logistical challenges, there are conceptual limitations and pitfalls that threaten the widespread adoption of PN. Developers have to avoid the temptation of looking for the application of available sensors and instead concentrate on identifying the most appropriate and relevant information needed for the optimal functioning of PN applications. Efficient PN applications are obtained by controlling the nutrient requirement variations occurring between animals and over time. The utilization of feedback control algorithms for the automatic determination of optimal nutrient supply is not recommended. Mathematical models are the preferred data processing method for PN, but these models have to be designed to operate in real time using up-to-date information. These models are therefore structurally different than traditional nutrition or growth models. Combining knowledge- and data-driven models using machine learning and deep learning algorithms will enhance our ability to use real-time farm data, thus opening up new opportunities for PN. To facilitate the implementation of PN in farms, different experts and stakeholders should be involved in the development of the fully integrated and automatic PLF system. Precision livestock farming and PN should not be seen as just being a question of technology, but a successful marriage between knowledge and technology. © 2023","Automatic data collection; Control actions; Nutrition; Precision feeding; Real-time data processing","Agriculture; Animals; Farms; Livestock; Nutritional Status; Technology; agricultural worker; agriculture; animal; livestock; nutritional status; technology","Agriculture and Agri-Food Canada, AAFC","None. Financial support received from Agriculture and Agri-Food Canada. This article is part of a supplement entitled Selected keynote lectures of the 73rd Annual Meeting of the European Federation of Animal Science (Porto, Portugal) supported by the Animal Consortium.","C. Pomar; Sherbrooke Research and Development Centre, Agriculture and Agri-Food Canada, Sherbrooke, 2000 College Street, J1M 0C8, Canada; email: Candido.Pomar@agr.gc.ca","","Elsevier B.V.","17517311","","","36966025","English","Animal","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85151462919"
"Zhao F.; Tian M.; Hu S.; Liang J.; Zhou L.; Li H.","Zhao, Feiyang (58539624100); Tian, Maohong (58177165300); Hu, Sheng (58539892000); Liang, Jian (58176848700); Zhou, Longfu (58539732200); Li, Hualin (58177377200)","58539624100; 58177165300; 58539892000; 58176848700; 58539732200; 58177377200","Livestock Recognition and Identification with Deep Convolutional Neural Networks: A Case Study of Pigs","2023","ACM International Conference Proceeding Series","","","","185","194","9","0","10.1145/3594315.3594642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168235568&doi=10.1145%2f3594315.3594642&partnerID=40&md5=c370bdf47f6d970c3dbf91aab5b1fe7f","ChongQing Institute of Engineering Intelligent Application of Financial Big Data Chongqing Colleges, Universities Engineering Research Center, ChongQing, China; ChongQing Institute of Engineering Shu Yi Xin Credit Management Co,LTD, ChongQing, China; ChongQing Institute of Engineering, ChongQing, China","Zhao F., ChongQing Institute of Engineering Intelligent Application of Financial Big Data Chongqing Colleges, Universities Engineering Research Center, ChongQing, China; Tian M., ChongQing Institute of Engineering Shu Yi Xin Credit Management Co,LTD, ChongQing, China; Hu S., ChongQing Institute of Engineering, ChongQing, China; Liang J., ChongQing Institute of Engineering, ChongQing, China; Zhou L., ChongQing Institute of Engineering, ChongQing, China; Li H., ChongQing Institute of Engineering Shu Yi Xin Credit Management Co,LTD, ChongQing, China","Animal recognition with Object Detection (OD) method by using deep learning techniques has gained popularities in biodiversity preservation and “smart farm” program in recent years. Current research mainly focuses on model training and parameter optimization of different forms of livestock in a single scene. To meet the developmental needs of farms in modern agriculture, this paper proposes an image recognition method which preprocesses the morphological characteristics of livestock in different captivity scenarios, by combining Kennard Stone algorithm, K-means II algorithm and deep learning models for different scenes and live pigs of different behavior characteristics. Our results show that the F1-socre, mAP0.5, and mAP0.5-0.95 of the model were 98.48%, 99.27% and 73.03%, respectively. Our research shows significant improvements for promoting smart animal husbandry and saving labor costs for breeding enterprises, which can also speed up research of high accuracy livestock auto-weighing systems and subsequent applied in animal husbandry. © 2023 Copyright held by the owner/author(s).","Deep learning(CNN); livestock monitoring system; object detection; YOLOv5","Agriculture; Biodiversity; Computer vision; Convolutional neural networks; Deep neural networks; Image recognition; K-means clustering; Learning systems; Mammals; Object recognition; Wages; Animal husbandry; Case-studies; Convolutional neural network; Deep learning(CNN); Learning techniques; Livestock monitoring system; Monitoring system; Object detection method; Objects detection; YOLOv5; Object detection","scientific research fund of Chongqing Institute of Engineering, China, (2022gcky03, 2022xzcr05, 2022xzcr06)","This study was funded by the scientific research fund of Chongqing Institute of Engineering, China (2022gcky03, 2022xzcr05, 2022xzcr06).","H. Li; ChongQing Institute of Engineering Shu Yi Xin Credit Management Co,LTD, ChongQing, China; email: hualinli@hotmail.com","","Association for Computing Machinery","","978-145039902-9","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85168235568"
"Bence T.; István S.; János T.","Bence, Tarr (59307876600); István, Szabó (57196249144); János, Tőzsér (59307418400)","59307876600; 57196249144; 59307418400","Develping artificial intelligence technology to support cattle identification, animal health and welfare solutions; [Mesterséges intelligencia alkalmazása a szarvasmarha-tenyésztés egyes területein: egyedazonosítás, állategészségügy és állatjóllét Irodalmi összefoglaló]","2023","Magyar Allatorvosok Lapja","145","11","","651","660","9","0","10.56385/magyallorv.2023.11.651-660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202777861&doi=10.56385%2fmagyallorv.2023.11.651-660&partnerID=40&md5=599079d380ef2187ee70aefd7b6d99ab","Magyar Agrár-és Élettudományi Egyetem, Műszaki Tudományok Intézet, Szent István Campus, Páter Károly u. 1, Gödöllő, H-2100, Hungary; Széchenyi István Egyetem, Albert Kázmér Mosonmagyaróvári Kar, Állattudományi Tanszék, Mosonmagyaróvár, Hungary","Bence T., Magyar Agrár-és Élettudományi Egyetem, Műszaki Tudományok Intézet, Szent István Campus, Páter Károly u. 1, Gödöllő, H-2100, Hungary; István S., Magyar Agrár-és Élettudományi Egyetem, Műszaki Tudományok Intézet, Szent István Campus, Páter Károly u. 1, Gödöllő, H-2100, Hungary; János T., Széchenyi István Egyetem, Albert Kázmér Mosonmagyaróvári Kar, Állattudományi Tanszék, Mosonmagyaróvár, Hungary","Artificial Intelligence (AI) has become an important tool for optimising breeding processes in several areas of animal production. In this thesis, we have presented examples from the literature, mainly for the identification and counting of cattle. The individual identification of animals, the monitoring of their behaviour and the control of their movements support a number of conclusions from both animal welfare and veterinary point of view. Automation of the processing of captured images has also become essential. This process is supported by Artificial Intelligence. Deep learning and neural networks are excellent tools for segmenting images and processing their content based on different features. Convolutional neural networks are specifically powerful for such tasks and we have seen that further developments of these networks (e.g. Faster R-CNN) allow even more efficient image analysis procedures. Processing animal images can be a major step forward for automatic analysis and identification of livestock. It also allows early intervention in the event of disease. In the context of individual identification, it is important to underline that, when complemented with other measurement options, e.g. sensor measurements, it offers even more complex applications that have not been available so far. © 2023, Herman Otto Intezet. All rights reserved.","","","","","","","Herman Otto Intezet","0025004X","","","","Hungarian","Magyar Allatorv. Lapja","Article","Final","","Scopus","2-s2.0-85202777861"
"Kopler I.; Marchaim U.; Tikász I.E.; Opaliński S.; Kokin E.; Mallinger K.; Neubauer T.; Gunnarsson S.; Soerensen C.; Phillips C.J.C.; Banhazi T.","Kopler, Idan (57203875564); Marchaim, Uri (6601937110); Tikász, Ildikó E. (57201281739); Opaliński, Sebastian (28767994400); Kokin, Eugen (35747799700); Mallinger, Kevin (57221224243); Neubauer, Thomas (19933872600); Gunnarsson, Stefan (7004247570); Soerensen, Claus (57214329310); Phillips, Clive J. C. (57216214020); Banhazi, Thomas (12785390200)","57203875564; 6601937110; 57201281739; 28767994400; 35747799700; 57221224243; 19933872600; 7004247570; 57214329310; 57216214020; 12785390200","Farmers’ Perspectives of the Benefits and Risks in Precision Livestock Farming in the EU Pig and Poultry Sectors","2023","Animals","13","18","2868","","","","18","10.3390/ani13182868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172210434&doi=10.3390%2fani13182868&partnerID=40&md5=bbb0fe6601b051bd443e8196aa4f2741","European Wing Unit, Galilee Research Institute, Kiryat Shmona, 11016, Israel; Agricultural Economics Directorate, Institute of Agricultural Economics, Budapest, H-1093, Hungary; Department of Environmental Hygiene and Animal Welfare, Wroclaw University of Environmental and Life Sciences, Wrocław, 50-375, Poland; Institute of Forestry and Engineering, Estonian University of Life Science, Tartu, 51014, Estonia; SBA Research, Vienna, 1040, Austria; Department of Animal Environment and Health, Swedish University of Agricultural Sciences, Skara, SE-532 23, Sweden; Department of Electrical and Computer Engineering, Aarhus University, Aarhus, 8000, Denmark; CUSP Institute, Curtin University, Bentley, 6102, WA, Australia; AgHiTech Kft, Budapest, H-1101, Hungary; International College, National Taiwan University, Taipei, 10617, Taiwan","Kopler I., European Wing Unit, Galilee Research Institute, Kiryat Shmona, 11016, Israel; Marchaim U., European Wing Unit, Galilee Research Institute, Kiryat Shmona, 11016, Israel; Tikász I.E., Agricultural Economics Directorate, Institute of Agricultural Economics, Budapest, H-1093, Hungary; Opaliński S., Department of Environmental Hygiene and Animal Welfare, Wroclaw University of Environmental and Life Sciences, Wrocław, 50-375, Poland; Kokin E., Institute of Forestry and Engineering, Estonian University of Life Science, Tartu, 51014, Estonia; Mallinger K., SBA Research, Vienna, 1040, Austria; Neubauer T., SBA Research, Vienna, 1040, Austria; Gunnarsson S., Department of Animal Environment and Health, Swedish University of Agricultural Sciences, Skara, SE-532 23, Sweden; Soerensen C., Department of Electrical and Computer Engineering, Aarhus University, Aarhus, 8000, Denmark; Phillips C.J.C., Institute of Forestry and Engineering, Estonian University of Life Science, Tartu, 51014, Estonia, CUSP Institute, Curtin University, Bentley, 6102, WA, Australia; Banhazi T., AgHiTech Kft, Budapest, H-1101, Hungary, International College, National Taiwan University, Taipei, 10617, Taiwan","More efficient livestock production systems are necessary, considering that only 41% of global meat demand will be met by 2050. Moreover, the COVID-19 pandemic crisis has clearly illustrated the necessity of building sustainable and stable agri-food systems. Precision Livestock Farming (PLF) offers the continuous capacity of agriculture to contribute to overall human and animal welfare by providing sufficient goods and services through the application of technical innovations like digitalization. However, adopting new technologies is a challenging issue for farmers, extension services, agri-business and policymakers. We present a review of operational concepts and technological solutions in the pig and poultry sectors, as reflected in 41 and 16 European projects from the last decade, respectively. The European trend of increasing broiler-meat production, which is soon to outpace pork, stresses the need for more outstanding research efforts in the poultry industry. We further present a review of farmers’ attitudes and obstacles to the acceptance of technological solutions in the pig and poultry sectors using examples and lessons learned from recent European projects. Despite the low resonance at the research level, the investigation of farmers’ attitudes and concerns regarding the acceptance of technological solutions in the livestock sector should be incorporated into any technological development. © 2023 by the authors.","animal welfare; digitalization; farmers’ adoption; farmers’ engagement; ICT; livestock; PLF; technology-adoption","adoption; agricultural worker; animal food; animal welfare; animal well-being; attitude assessment; broiler; classifier; consumer; coronavirus disease 2019; deep learning; digitalization; egg production; electrocardiography; environmental monitoring; farming system; food intake; food safety; global positioning system; imagery; livestock; meat industry; meat production; mitigation; nonhuman; pandemic; physiological stress; pig; pig farming; poultry farming; poultry product; precision livestock farming system; public sector; Review; sanitation; technology; thermal imagery; workforce","Estonia University of Life Sciences; NRDI; Aarhus Universitet, AU; Horizon 2020 Framework Programme, H2020, (861665 ERA-NET ICT-Agri-Food); Horizon 2020 Framework Programme, H2020; Svenska Forskningsrådet Formas; Sveriges Lantbruksuniversitet, SLU; Narodowe Centrum Badań i Rozwoju, NCBR; Ministry of Rural Affairs, MRA; Uniwersytet Przyrodniczy we Wroclawiu, UPWr; Grønt Udviklings- og Demonstrations Program, GUDP; Israel Innovation Authority, IIA","Funding text 1: The authors acknowledge the contributions of AgHiTech Kft (HU), Institute of Agricultural Economics (HU), Galilei Research Institute Ltd. (IL), SBA Research (AT), Innvite ApS. (DK), Swedish University of Agricultural Sciences (SE), Wroclaw University of Environmental and Life Sciences (PL), Estonia University of Life Sciences (EE), Aarhus University (DK) and the co-funding of the following organisations: NRDI Funds (HU), Israel Innovation Authority (IL), Bundesministerium, LRT Fund (AT), GUDP (DK), Ministry of Rural Affairs (EE), The National Centre for Research and Development (PL) and FORMAS (SE). ; Funding text 2: This article reports on the results of the LivestockSense project funded by European Union’s Horizon 2020 research and innovation program, under grant agreement no. 861665 ERA-NET ICT-Agri-Food.","I. Kopler; European Wing Unit, Galilee Research Institute, Kiryat Shmona, 11016, Israel; email: idank@migal.org.il","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172210434"
"Zhang L.; Han G.; Qiao Y.; Xu L.; Chen L.; Tang J.","Zhang, Lianyue (58667541400); Han, Gaoge (58667078300); Qiao, Yongliang (56486770900); Xu, Liu (58373272800); Chen, Ling (58741118400); Tang, Jinglei (35176589500)","58667541400; 58667078300; 56486770900; 58373272800; 58741118400; 35176589500","Interactive Dairy Goat Image Segmentation for Precision Livestock Farming","2023","Animals","13","20","3250","","","","1","10.3390/ani13203250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175056375&doi=10.3390%2fani13203250&partnerID=40&md5=be74833b4263198a13f7a82f0e29f77c","College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China; Australian Institute for Machine Learning (AIML), The University of Adelaide, Adelaide, 5005, Australia; The Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, Xianyang, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Xianyang, Yangling, 712100, China","Zhang L., College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China; Han G., College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China; Qiao Y., Australian Institute for Machine Learning (AIML), The University of Adelaide, Adelaide, 5005, Australia; Xu L., College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China; Chen L., College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China; Tang J., College of Information Engineering, Northwest A&F University, Yangling, Xianyang, 712100, China, The Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, Xianyang, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Xianyang, Yangling, 712100, China","Semantic segmentation and instance segmentation based on deep learning play a significant role in intelligent dairy goat farming. However, these algorithms require a large amount of pixel-level dairy goat image annotations for model training. At present, users mainly use Labelme for pixel-level annotation of images, which makes it quite inefficient and time-consuming to obtain a high-quality annotation result. To reduce the annotation workload of dairy goat images, we propose a novel interactive segmentation model called UA-MHFF-DeepLabv3+, which employs layer-by-layer multi-head feature fusion (MHFF) and upsampling attention (UA) to improve the segmentation accuracy of the DeepLabv3+ on object boundaries and small objects. Experimental results show that our proposed model achieved state-of-the-art segmentation accuracy on the validation set of DGImgs compared with four previous state-of-the-art interactive segmentation models, and obtained 1.87 and 4.11 on mNoC@85 and mNoC@90, which are significantly lower than the best performance of the previous models of 3 and 5. Furthermore, to promote the implementation of our proposed algorithm, we design and develop a dairy goat image-annotation system named DGAnnotation for pixel-level annotation of dairy goat images. After the test, we found that it just takes 7.12 s to annotate a dairy goat instance with our developed DGAnnotation, which is five times faster than Labelme. © 2023 by the authors.","dairy goat; deep learning; deepLabv3+; interactive segmentation; precision stock farming","agricultural worker; algorithm; animal experiment; animal husbandry; animal model; article; attention; dairy goat; deep learning; human; image segmentation; livestock; nonhuman; workload","Xianyang Key Project of Research and Development Plan, (L2022ZDYFSF050); Xi’an Science and Technology Plan Project, (22NYYF013); National Key Research and Development Program of China, NKRDPC, (2021YFD1600704); Key Research and Development Projects of Shaanxi Province, (2023-YBNY-121)","This work was supported by Key Research and Development projects in Shaanxi Province (grant number 2023-YBNY-121), Xi’an Science and Technology Plan Project (grant number 22NYYF013), Xianyang Key Project of Research and Development Plan (grant number L2022ZDYFSF050) and the National Key Research and Development Program of China (grant number 2021YFD1600704).","J. Tang; College of Information Engineering, Northwest A&F University, Xianyang, Yangling, 712100, China; email: tangjinglei@nwsuaf.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175056375"
"Delplanque A.; Foucher S.; Théau J.; Bussière E.; Vermeulen C.; Lejeune P.","Delplanque, Alexandre (57226592039); Foucher, Samuel (6701728686); Théau, Jérôme (6507809197); Bussière, Elsa (58094868100); Vermeulen, Cédric (26428620400); Lejeune, Philippe (8700431500)","57226592039; 6701728686; 6507809197; 58094868100; 26428620400; 8700431500","From crowd to herd counting: How to precisely detect and count African mammals using aerial imagery and deep learning?","2023","ISPRS Journal of Photogrammetry and Remote Sensing","197","","","167","180","13","19","10.1016/j.isprsjprs.2023.01.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147603824&doi=10.1016%2fj.isprsjprs.2023.01.025&partnerID=40&md5=ebf1f9a25f3ade526016feafeedb5fc3","TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, 2 Passage des Déportés, Gembloux, 5030, Belgium; Departement of Applied Geomatics, Université de Sherbrooke, 2500 Boulevard de l'Université, Sherbrooke, J1K 2R1, QC, Canada; Quebec Centre for Biodiversity Science (QCBS), Stewart Biology, McGill University, Montréal, H3A 1B1, QC, Canada; Wild Sense, PostNet #270, Private Bag X11326, Nelspruit, 1200, South Africa","Delplanque A., TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, 2 Passage des Déportés, Gembloux, 5030, Belgium; Foucher S., Departement of Applied Geomatics, Université de Sherbrooke, 2500 Boulevard de l'Université, Sherbrooke, J1K 2R1, QC, Canada; Théau J., Departement of Applied Geomatics, Université de Sherbrooke, 2500 Boulevard de l'Université, Sherbrooke, J1K 2R1, QC, Canada, Quebec Centre for Biodiversity Science (QCBS), Stewart Biology, McGill University, Montréal, H3A 1B1, QC, Canada; Bussière E., Wild Sense, PostNet #270, Private Bag X11326, Nelspruit, 1200, South Africa; Vermeulen C., TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, 2 Passage des Déportés, Gembloux, 5030, Belgium; Lejeune P., TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, 2 Passage des Déportés, Gembloux, 5030, Belgium","Rapid growth of human populations in sub-Saharan Africa has led to a simultaneous increase in the number of livestock, often leading to conflicts of use with wildlife in protected areas. To minimize these conflicts, and to meet both communities’ and conservation goals, it is therefore essential to monitor livestock density and their land use. This is usually done by conducting aerial surveys during which aerial images are taken for later counting. Although this approach appears to reduce counting bias, the manual processing of images is time-consuming. The use of dense convolutional neural networks (CNNs) has emerged as a very promising avenue for processing such datasets. However, typical CNN architectures have detection limits for dense herds and close-by animals. To tackle this problem, this study introduces a new point-based CNN architecture, HerdNet, inspired by crowd counting. It was optimized on challenging oblique aerial images containing herds of camels (Camelus dromedarius), donkeys (Equus asinus), sheep (Ovis aries) and goats (Capra hircus), acquired over heterogeneous arid landscapes of the Ennedi reserve (Chad). This approach was compared to an anchor-based architecture, Faster-RCNN, and a density-based, adapted version of DLA-34 that is typically used in crowd counting. HerdNet achieved a global F1 score of 73.6 % on 24 megapixels images, with a root mean square error of 9.8 animals and at a processing speed of 3.6 s, outperforming the two baselines in terms of localization, counting and speed. It showed better proximity-invariant precision while maintaining equivalent recall to that of Faster-RCNN, thus demonstrating that it is the most suitable approach for detecting and counting large mammals at close range. The only limitation of HerdNet was the slightly weaker identification of species, with an average confusion rate approximately 4 % higher than that of Faster-RCNN. This study provides a new CNN architecture that could be used to develop an automatic livestock counting tool in aerial imagery. The reduced image analysis time could motivate more frequent flights, thus allowing a much finer monitoring of livestock and their land use. © 2023 The Author(s)","Aerial survey; Convolutional neural networks; Deep learning; Herd; Livestock; Protected area","Africa; Chad; Agriculture; Antennas; Conservation; Convolution; Convolutional neural networks; Deep neural networks; Environmental protection; Land use; Mean square error; Network architecture; Population statistics; Aerial imagery; Aerial images; Aerial surveys; Convolutional neural network; Deep learning; Herd; Livestock; Neural network architecture; Protected areas; Rapid growth; airborne sensing; artificial neural network; cattle; detection method; machine learning; mammal; monitoring; nature reserve; Mammals","African Parks Network; Ennedi Massif and south-western plains; Fund for Research Training in Industry and Agriculture; Zoological Society of London, ZSL; European Commission, EC; Fonds pour la Formation à la Recherche dans l’Industrie et dans l’Agriculture, FRIA","Funding text 1: We are very grateful to the European Union and the Dutch Postcode Lottery for providing the financial means to conduct the 2019 aerial survey of the Ennedi Massif and south-western plains. We thank the African Parks Network (APN), especially the ENCR management team for the planning, preparation, and implementation of the survey, as well as the aircrew, composed of Tim Wacher, Alexis Peltier, Moussa Sougui and Issa Hamid. We also thank the Zoological Society of London for analyzing the data and producing the final report of the survey. Finally, we thank both East and West Ennedi Provinces for authorizing low-altitude flights for the purpose of this study. ; Funding text 2: This work was supported by the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS). The 2019 aerial survey of the Ennedi Massif and south-western plains was financially supported by the European Union and the Dutch Postcode Lottery. ; Funding text 3: We are very grateful to the European Union and the Dutch Postcode Lottery for providing the financial means to conduct the 2019 aerial survey of the Ennedi Massif and south-western plains. We thank the African Parks Network (APN), especially the ENCR management team for the planning, preparation, and implementation of the survey, as well as the aircrew, composed of Tim Wacher, Alexis Peltier, Moussa Sougui and Issa Hamid. We also thank the Zoological Society of London for analyzing the data and producing the final report of the survey. Finally, we thank both East and West Ennedi Provinces for authorizing low-altitude flights for the purpose of this study. This work was supported by the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS). The 2019 aerial survey of the Ennedi Massif and south-western plains was financially supported by the European Union and the Dutch Postcode Lottery. All code for training and testing HerdNet, as well as pre-trained models, is available at https://github.com/Alexandre-Delplanque/HerdNet. Any updates will be published on this GitHub repository.","A. Delplanque; TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, Gembloux, 2 Passage des Déportés, 5030, Belgium; email: alexandre.delplanque@uliege.be","","Elsevier B.V.","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85147603824"
"Park J.; Jo G.; Jung M.; Oh Y.","Park, Junsu (57222137944); Jo, Gwanggon (57211341824); Jung, Minwoong (55890039300); Oh, Youngmin (58555862100)","57222137944; 57211341824; 55890039300; 58555862100","Comparative Analysis of Neural Network Models for Predicting Ammonia Concentrations in a Mechanically Ventilated Sow Gestation Facility in Korea","2023","Atmosphere","14","8","1248","","","","0","10.3390/atmos14081248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169169389&doi=10.3390%2fatmos14081248&partnerID=40&md5=d91e286b0a2d907503825da6eb0207a3","Animal Environment Division, National Institute of Animal Science (NIAS), Rural Development Administration (RDA), Wanju-gun, 55365, South Korea; Monitoring and Analysis Division, Geum River Basin Environment Office, Ministry of Environment (ME), Daejeon-si, 34142, South Korea; School of Computing, Gachon University, Seongnam-si, 13120, South Korea","Park J., Animal Environment Division, National Institute of Animal Science (NIAS), Rural Development Administration (RDA), Wanju-gun, 55365, South Korea; Jo G., Monitoring and Analysis Division, Geum River Basin Environment Office, Ministry of Environment (ME), Daejeon-si, 34142, South Korea; Jung M., Animal Environment Division, National Institute of Animal Science (NIAS), Rural Development Administration (RDA), Wanju-gun, 55365, South Korea; Oh Y., School of Computing, Gachon University, Seongnam-si, 13120, South Korea","Conventional methods for monitoring ammonia (NH3) emissions from livestock farms have several challenges, such as a poor environment for measurement, difficulty in accessing livestock, and problems with long-term measurement. To address these issues, we applied various neural network models for the long-term prediction of NH3 concentrations from sow farms in this study. Environmental parameters, including temperature, humidity, ventilation rate, and past records of NH3 concentrations, were given as inputs to the models. These neural network models took the encoder or the feature extracting parts from the representative deep learning models, including Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Transformer, to encode temporal patterns of time series. However, all of these models adopted dense layers for the decoder to format the task of long-term prediction as a regression problem. Due to their regression nature, all models showed a robust performance in predicting long-term NH3 concentrations at a scale of weeks or even months despite there being a relatively short period of input signals (a few days to a week). Given one week of input, LSTM showed the minimum mean absolute errors (MAE) of 1.83, 1.78, and 1.87 ppm for the prediction of one, two, and three weeks, respectively, whereas Transformer performed best with a MAE of 1.73 ppm for a four-week prediction. In the long-term estimation of spanning months, LSTM showed the minimum MAEs of 1.95 and 1.90 ppm when trained on predicting two and three weeks of windows. At the same condition, Transformer gave the minimum MAEs of 1.87 and 1.83 when trained on predicting one and four weeks of windows. Overall, the neural network models can facilitate the prediction of national-level NH3 emissions, the development of mitigation strategies for NH3-derived air pollutants, odor management, and the monitoring of animal-rearing environments. Further, their integration of real-time measurement devices can significantly prolong device longevity and offer substantial cost savings. © 2023 by the authors.","ammonia; mechanical ventilation; neural network models; sow","South Korea; Agriculture; Air quality; Ammonia; Convolutional neural networks; Forecasting; Long short-term memory; Ventilation; Ammonia concentrations; Comparative analyzes; Conventional methods; Environmental parameter; Long-term measurements; Long-term prediction; Mean absolute error; Mechanical ventilation; Neural network model; Sow; ammonia; artificial neural network; atmospheric pollution; comparative study; emission; livestock farming; longevity; prediction; regression analysis; ventilation; Neural network models","Rural Development Administration, RDA","This research was supported by the “Development of NH and greenhouse gases reduction technologies in livestock sector” Rural Development Administration, Republic of Korea. 3 ","Y. Oh; School of Computing, Gachon University, Seongnam-si, 13120, South Korea; email: youngminoh@gachon.ac.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20734433","","","","English","Atmosphere","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85169169389"
"Guo Y.; Du S.; Qiao Y.; Liang D.","Guo, Yangyang (57200132879); Du, Shuzeng (58303911100); Qiao, Yongliang (56486770900); Liang, Dong (58054905900)","57200132879; 58303911100; 56486770900; 58054905900","Advances in the Applications of Deep Learning Technology for Livestock Smart Farming; [深度学习在家畜智慧养殖中研究应用进展]","2023","Smart Agriculture","5","1","","52","65","13","2","10.12133/j.smartag.SA202205009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161078113&doi=10.12133%2fj.smartag.SA202205009&partnerID=40&md5=64f132d59adadc00617921c255abe98c","School of Internet, Anhui University, Hefei, 230039, China; Nanyang Vocational College of Agriculture, Nanyang, 473000, China; Faculty of Engineering, The University of Sydney, Sydney, NSW2006, Australia","Guo Y., School of Internet, Anhui University, Hefei, 230039, China; Du S., Nanyang Vocational College of Agriculture, Nanyang, 473000, China; Qiao Y., Faculty of Engineering, The University of Sydney, Sydney, NSW2006, Australia; Liang D., School of Internet, Anhui University, Hefei, 230039, China","Accurate and efficient monitoring of animal information, timely analysis of animal physiological and physical health conditions, and automatic feeding and farming management combined with intelligent technologies are of great significance for large-scale livestock farming. Deep learning techniques, with automatic feature extraction and powerful image representation capabilities, solve many visual challenges, and are more suitable for application in monitoring animal information in complex livestock farming environments. In order to further analyze the research and application of artificial intelligence technology in intelligent animal farming, this paper presents the current state of research on deep learning techniques for tag detection recognition, body condition evaluation and weight estimation, and behavior recognition and quantitative analysis for cattle, sheep and pigs. Among them, target detection and recognition is conducive to the construction of electronic archives of individual animals, on which basis the body condition and weight information, behavior information and health status of animals can be related, which is also the trend of intelligent animal farming. At present, intelligent animal farming still faces many problems and challenges, such as the existence of multiple perspectives, multi-scale, multiple scenarios and even small sample size of a certain behavior in data samples, which greatly increases the detection difficulty and the generalization of intelligent technology application. In addition, animal breeding and animal habits are a long-term process. How to accurately monitor the animal health information in real time and effectively feed it back to the producer is also a technical difficulty. According to the actual feeding and management needs of animal farming, the development of intelligent animal farming is prospected and put forward. First, enrich the samples and build a multi perspective dataset, and combine semi supervised or small sample learning methods to improve the generalization ability of in-depth learning models, so as to realize the perception and analysis of the animal's physical environment. Secondly, the unified cooperation and harmonious development of human, intelligent equipment and breeding animals will improve the breeding efficiency and management level as a whole. Third, the deep integration of big data, deep learning technology and animal farming will greatly promote the development of intelligent animal farming. Last, research on the interpretability and security of artificial intelligence technology represented by deep learning model in the breeding field. And other development suggestions to further promote intelligent animal farming. Aiming at the progress of research application of deep learning in livestock smart farming, it provides reference for the modernization and intelligent development of livestock farming. © 2023 Chinese Journal of Animal Nutrition. All rights reserved.","behavior recognition; deep learning; individual identification; information perception; intelligent farming; livestock husbandry","","","","Y. Qiao; Faculty of Engineering, The University of Sydney, Sydney, NSW2006, Australia; email: yongliang.qiao@outlook.com","","Agricultural Information Institute, Chinese Academy of Agricultural Sciences","20968094","","","","Chinese","Smart. Agric.","Article","Final","","Scopus","2-s2.0-85161078113"
"Xiang T.; Li T.; Li J.; Li X.; Wang J.","Xiang, Tao (56699559900); Li, Tao (58263182700); Li, Jielin (57808184600); Li, Xin (58412902800); Wang, Jia (56027231300)","56699559900; 58263182700; 57808184600; 58412902800; 56027231300","Using machine learning to realize genetic site screening and genomic prediction of productive traits in pigs","2023","FASEB Journal","37","6","e22961","","","","6","10.1096/fj.202300245R","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159738798&doi=10.1096%2ffj.202300245R&partnerID=40&md5=77507ce0643e3040d9b9b221f05a75dc","Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Laboratory of Swine Genetics and Breeding of Ministry of Agriculture, Huazhong Agricultural University, Wuhan, China; College of Informatics, Huazhong Agricultural University, Wuhan, China; Key Laboratory of Smart Farming for Agricultural Animals, Huazhong Agricultural University, Wuhan, China; Hubei Key Laboratory of Agricultural Bioinformatics, Huazhong Agricultural University, Wuhan, China","Xiang T., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Laboratory of Swine Genetics and Breeding of Ministry of Agriculture, Huazhong Agricultural University, Wuhan, China; Li T., College of Informatics, Huazhong Agricultural University, Wuhan, China, Key Laboratory of Smart Farming for Agricultural Animals, Huazhong Agricultural University, Wuhan, China, Hubei Key Laboratory of Agricultural Bioinformatics, Huazhong Agricultural University, Wuhan, China; Li J., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Laboratory of Swine Genetics and Breeding of Ministry of Agriculture, Huazhong Agricultural University, Wuhan, China; Li X., College of Informatics, Huazhong Agricultural University, Wuhan, China, Key Laboratory of Smart Farming for Agricultural Animals, Huazhong Agricultural University, Wuhan, China, Hubei Key Laboratory of Agricultural Bioinformatics, Huazhong Agricultural University, Wuhan, China; Wang J., College of Informatics, Huazhong Agricultural University, Wuhan, China, Key Laboratory of Smart Farming for Agricultural Animals, Huazhong Agricultural University, Wuhan, China, Hubei Key Laboratory of Agricultural Bioinformatics, Huazhong Agricultural University, Wuhan, China","Genomic prediction, which is based on solving linear mixed-model (LMM) equations, is the most popular method for predicting breeding values or phenotypic performance for economic traits in livestock. With the need to further improve the performance of genomic prediction, nonlinear methods have been considered as an alternative and promising approach. The excellent ability to predict phenotypes in animal husbandry has been demonstrated by machine learning (ML) approaches, which have been rapidly developed. To investigate the feasibility and reliability of implementing genomic prediction using nonlinear models, the performances of genomic predictions for pig productive traits using the linear genomic selection model and nonlinear machine learning models were compared. Then, to reduce the high-dimensional features of genome sequence data, different machine learning algorithms, including the random forest (RF), support vector machine (SVM), extreme gradient boosting (XGBoost) and convolutional neural network (CNN) algorithms, were used to perform genomic feature selection as well as genomic prediction on reduced feature genome data. All of the analyses were processed on two real pig datasets: the published PIC pig dataset and a dataset comprising data from a national pig nucleus herd in Chifeng, North China. Overall, the accuracies of predicted phenotypic performance for traits T1, T2, T3 and T5 in the PIC dataset and average daily gain (ADG) in the Chifeng dataset were higher using the ML methods than the LMM method, while those for trait T4 in the PIC dataset and total number of piglets born (TNB) in the Chifeng dataset were slightly lower using the ML methods than the LMM method. Among all the different ML algorithms, SVM was the most appropriate for genomic prediction. For the genomic feature selection experiment, the most stable and most accurate results across different algorithms were achieved using XGBoost in combination with the SVM algorithm. Through feature selection, the number of genomic markers can be reduced to 1 in 20, while the predictive performance on some traits can even be improved compared to using the full genome data. Finally, we developed a new tool that can be used to execute combined XGBoost and SVM algorithms to realize genomic feature selection and phenotypic prediction. © 2023 Federation of American Societies for Experimental Biology.","deep learning; feature selection; genomic prediction; machine learning; pigs","Algorithms; Animals; Genome; Genomics; Machine Learning; Phenotype; Reproducibility of Results; Swine; animal experiment; animal model; article; average daily gain; China; controlled study; convolutional neural network; deep learning; feasibility study; feature selection; genetic marker; machine learning; nonhuman; piglet; prediction; random forest; reliability; support vector machine; algorithm; animal; genetics; genome; genomics; phenotype; pig; reproducibility","Yingzi Tech & Huazhong Agricultural University Intelligent Research Institute of Food Health, (IRIFH202209); National Key Research and Development Program of China, NKRDPC, (2022YFD1301900, NK2022110602); Fundamental Research Funds for the Central Universities, (2662018JC034, 2662022DKYJ004)","This work was supported by the National Key Research and Development Program of China (No. 2022YFD1301900); Tackling project for agricultural key core technologies (NO. NK2022110602); Fundamental Research Funds for the Central Universities of China (NO. 2662018JC034 & 2662022DKYJ004); and The Yingzi Tech & Huazhong Agricultural University Intelligent Research Institute of Food Health (IRIFH202209). ","J. Wang; College of Informatics, Huazhong Agricultural University, Wuhan, No. 1, Shizishan Street, 430070, China; email: wang.jia@mail.hzau.edu.cn","","John Wiley and Sons Inc","08926638","","FAJOE","37178007","English","FASEB J.","Article","Final","","Scopus","2-s2.0-85159738798"
"Balasso P.; Taccioli C.; Serva L.; Magrin L.; Andrighetto I.; Marchesini G.","Balasso, Paolo (57193552530); Taccioli, Cristian (16302702000); Serva, Lorenzo (24345300500); Magrin, Luisa (56525437000); Andrighetto, Igino (6601957162); Marchesini, Giorgio (35218966400)","57193552530; 16302702000; 24345300500; 56525437000; 6601957162; 35218966400","Uncovering Patterns in Dairy Cow Behaviour: A Deep Learning Approach with Tri-Axial Accelerometer Data","2023","Animals","13","11","1886","","","","6","10.3390/ani13111886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163109095&doi=10.3390%2fani13111886&partnerID=40&md5=503b5d99320d479e5b969abe3123f393","Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy","Balasso P., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; Taccioli C., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; Serva L., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; Magrin L., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; Andrighetto I., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; Marchesini G., Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy","The accurate detection of behavioural changes represents a promising method of detecting the early onset of disease in dairy cows. This study assessed the performance of deep learning (DL) in classifying dairy cows’ behaviour from accelerometry data acquired by single sensors on the cows’ left flanks and compared the results with those obtained through classical machine learning (ML) from the same raw data. Twelve cows with a tri-axial accelerometer were observed for 136 ± 29 min each to detect five main behaviours: standing still, moving, feeding, ruminating and resting. For each 8 s time interval, 15 metrics were calculated, obtaining a dataset of 211,720 observation units and 15 columns. The entire dataset was randomly split into training (80%) and testing (20%) datasets. The DL accuracy, precision and sensitivity/recall were calculated and compared with the performance of classical ML models. The best predictive model was an 8-layer convolutional neural network (CNN) with an overall accuracy and F1 score equal to 0.96. The precision, sensitivity/recall and F1 score of single behaviours had the following ranges: 0.93–0.99. The CNN outperformed all the classical ML algorithms. The CNN used to monitor the cows’ conditions showed an overall high performance in successfully predicting multiple behaviours using a single accelerometer. © 2023 by the authors.","animal welfare; convolutional neural network; machine learning; precision livestock farming","algorithm; animal behavior; animal experiment; animal model; animal welfare; Article; controlled study; convolutional neural network; dairy cattle; deep learning; livestock; machine learning; nonhuman; predictive model; sensitivity and specificity; support vector machine","Università degli Studi di Padova, UNIPD, (MARC_COMM17_01)","This research was funded by the University of Padova, project SID 2020 Marchesini, and by Smart Unipd, project MARC_COMM17_01.","G. Marchesini; Dipartimento di Medicina Animale, Produzioni e Salute, Università degli Studi di Padova, Legnaro, 35020, Italy; email: giorgio.marchesini@unipd.it","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85163109095"
"Shankar A.; Bhardwaj N.; Goswami I.; Mishra V.K.","Shankar, Ansh (58178625400); Bhardwaj, Navodit (59169540500); Goswami, Indresh (59169632800); Mishra, Vikash Kumar (56723528500)","58178625400; 59169540500; 59169632800; 56723528500","Food Crop Disease Identification system using ML","2023","Proceedings - IEEE 2023 5th International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2023","","","","820","824","4","0","10.1109/ICAC3N60023.2023.10541380","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195805165&doi=10.1109%2fICAC3N60023.2023.10541380&partnerID=40&md5=d1f6e4bc020fef05a08723b39ebfc2b6","Galgotias University, School Of Computing Science And Engineering, Greater Noida, India","Shankar A., Galgotias University, School Of Computing Science And Engineering, Greater Noida, India; Bhardwaj N., Galgotias University, School Of Computing Science And Engineering, Greater Noida, India; Goswami I., Galgotias University, School Of Computing Science And Engineering, Greater Noida, India; Mishra V.K., Galgotias University, School Of Computing Science And Engineering, Greater Noida, India","Agriculture has played a crucial role in domesticating essential crops and livestock for thousands of years. However, food insecurity caused by plant diseases remains a significant global problem. These diseases result in crop damage and reduced yield, leading to food shortages. According to the Food and Agriculture Organization, plant parasites and diseases are responsible for up to 40 percent of the decline in global agricultural production. This has severe consequences, including widespread hunger and substantial damage to the agriculture industry. Smallholder farmers, who contribute over 80 percent of agricultural output in developing countries, are particularly vulnerable to pathogen-related disruptions in food supply. As a result, the development of new techniques for detecting plant diseases can significantly increase food production and mitigate losses.The limited accuracy and efficiency of previous methods in detecting and classifying crop leaf diseases have hindered agricultural productivity. To address these shortcomings, this research proposes the utilization of the Plant Village Kaggle dataset, which presents challenges due to its varying image capture conditions and diverse image samples. Through the development of a 28-layer deep learning model, we achieved an impressive accuracy rate exceeding 97%. Our model not only overcomes the limitations of previous methods but also demonstrates its effectiveness in accurately detecting and classifying crop leaf diseases. By leveraging this model, we aim to enhance agricultural productivity and mitigate losses caused by plant diseases, thereby contributing to global food security. © 2023 IEEE.","agriculture; classification; Deep learning; Machine Learning; plant disease","Classification (of information); Crops; Deep learning; Developing countries; Learning systems; Agricultural productivity; Crop disease; Crop leaves; Deep learning; Food crops; Food insecurity; Global problems; Leaf disease; Machine-learning; Plant disease; Food supply","","","A. Shankar; Galgotias University, School Of Computing Science And Engineering, Greater Noida, India; email: anshshankar@gmail.com","Sharma V.; Sinha J.; Chandraprabha M.; Kumar S.; Rana A.K.","Institute of Electrical and Electronics Engineers Inc.","","979-835033086-1","","","English","Proc. - IEEE Int. Conf. Adv. Comput., Commun. Control Netw., ICAC3N","Conference paper","Final","","Scopus","2-s2.0-85195805165"
"Ayub M.Y.; Hussain A.; Hassan M.F.U.; Khan B.; Khan F.A.; Al-Jumeily D.; Khan W.","Ayub, Muhammad Yaseen (57880708800); Hussain, Abir (56212648400); Hassan, Muhammad Furqan Ul (59283650200); Khan, Bilal (57216733975); Khan, Farman Ali (55660602700); Al-Jumeily, Dhiya (57382902800); Khan, Wasiq (55319932200)","57880708800; 56212648400; 59283650200; 57216733975; 55660602700; 57382902800; 55319932200","A non-Restraining Sheep Activity Detection and Surveillance using Deep Machine Learning","2023","Proceedings - International Conference on Developments in eSystems Engineering, DeSE","","","","66","72","6","0","10.1109/DeSE60595.2023.10469582","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189286440&doi=10.1109%2fDeSE60595.2023.10469582&partnerID=40&md5=1eb22b5c6fadf92c1c77b93d9b52b36e","Comsats University Islamabad, Department of Computer Science, Attock Campus, Pakistan; Sharjah University, College of Engineering, United Arab Emirates; California State University, School of Computer Science & Eng, San Bernardino, CA, United States; Liverpool John Moores University, Faculty of Engineering & Technology, Liverpool, United Kingdom","Ayub M.Y., Comsats University Islamabad, Department of Computer Science, Attock Campus, Pakistan; Hussain A., Sharjah University, College of Engineering, United Arab Emirates; Hassan M.F.U., Comsats University Islamabad, Department of Computer Science, Attock Campus, Pakistan; Khan B., California State University, School of Computer Science & Eng, San Bernardino, CA, United States; Khan F.A., Comsats University Islamabad, Department of Computer Science, Attock Campus, Pakistan; Al-Jumeily D., Liverpool John Moores University, Faculty of Engineering & Technology, Liverpool, United Kingdom; Khan W., Liverpool John Moores University, Faculty of Engineering & Technology, Liverpool, United Kingdom","The number of livestock farms and their sizes (particularly the sheep farms) are on the rise, in response to the growing demands of food supply chain for increasing population. The detection and monitoring of sheep activities particularly in huge farms is tedious and challenging task. Therefore, a reliable and cost-effective sheep activity detection system which can be utilized for the virtual fencing, is of high demand. Existing data-driven approaches use accelerometer data for sheep monitoring and activity detection however, there are several limitations with these methods such as generating high volume of data with noise, relatively expensive, and not very reliable. This study presents a non-invasive computer vision-based approach along with deep transfer learning for sheep detection and determining whether the corresponding state is 'active' or 'inactive'. We complied a primary dataset comprising sheep in diverse poses and activities in a realistic outdoor environment. A custom YOLOV5s model is trained over new dataset and validated on purely unseen sheep instances for the model evaluation. The statistical outcomes demonstrate the robustness of proposed approach for various sheep activity detection. Our method has diverse implications and uses in the development of reliable and economical systems for monitoring sheep, particularly in extensive farms and virtual fencing applications.  © 2023 IEEE.","Animal welfare; Livestock monitoring; Sheep activity detection; Virtual fencing","Cost effectiveness; E-learning; Farms; Food supply; Supply chains; Activity detection; Animal welfare; Cost effective; Detection system; Food supply chain; Growing demand; Livestock monitoring; Machine-learning; Sheep activity detection; Virtual fencing; Deep learning","","","M.Y. Ayub; Comsats University Islamabad, Department of Computer Science, Attock Campus, Pakistan; email: yaseen.ayub@ieee.org","Obe D.A.-J.; Assi S.; Jayabalan M.; Hind J.; Hussain A.; Tawfik H.; Rowe N.; Mustafina J.","Institute of Electrical and Electronics Engineers Inc.","21611343","979-835038134-4","","","English","Proc. - Int. Conf. Dev. eSystems Eng., DeSE","Conference paper","Final","","Scopus","2-s2.0-85189286440"
"Chang X.; Zhang B.; Zhu H.; Song W.; Ren D.; Dai J.","Chang, Xinyue (58512897000); Zhang, Bing (57193427787); Zhu, Hongbo (58187549700); Song, Weidong (56512910400); Ren, Dongfeng (57927133700); Dai, Jiguang (36677171900)","58512897000; 57193427787; 58187549700; 56512910400; 57927133700; 36677171900","A Spatial and Temporal Evolution Analysis of Desert Land Changes in Inner Mongolia by Combining a Structural Equation Model and Deep Learning","2023","Remote Sensing","15","14","3617","","","","2","10.3390/rs15143617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166248906&doi=10.3390%2frs15143617&partnerID=40&md5=8993caf6562a6bca9336b1a862a2c1c1","School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China; Collaborative Innovation Institute of Geospatial Information Service, Liaoning Technical University, Fuxin, 123000, China","Chang X., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China; Zhang B., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China, Collaborative Innovation Institute of Geospatial Information Service, Liaoning Technical University, Fuxin, 123000, China; Zhu H., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China; Song W., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China, Collaborative Innovation Institute of Geospatial Information Service, Liaoning Technical University, Fuxin, 123000, China; Ren D., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China; Dai J., School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China, Collaborative Innovation Institute of Geospatial Information Service, Liaoning Technical University, Fuxin, 123000, China","With the wide application of remote sensing technology, target detection based on deep learning has become a research hotspot in the field of remote sensing. In this paper, aimed at the problems of the existing deep-learning-based desert land intelligent extraction methods, such as the spectral similarity of features and unclear texture features, we propose a multispectral remote sensing image desert land intelligent extraction method that takes into account band information. Firstly, we built a desert land intelligent interpretation dataset based on band weighting to enhance the desert land foreground features of the images. On this basis, we introduced the deformable convolution adaptive feature extraction capability to U-Net and developed the Y-Net model to extract desert land from Landsat remote sensing images covering the Inner Mongolia Autonomous Region. Finally, in order to analyze the spatial and temporal trends of the desert land in the study area, we used a structural equation model (SEM) to evaluate the direct and indirect effects of natural conditions and human activities, i.e., population density (PD), livestock volume (LS), evaporation (Evp), temperature (T), days of sandy wind conditions (LD), humidity (RH), precipitation (P), anthropogenic disturbance index (Adi), and cultivated land (CL). The results show that the F1-score of the Y-Net model proposed in this paper is 95.6%, which is 11.5% more than that of U-Net. Based on the Landsat satellite images, the area of desert land in the study area for six periods from 1990 to 2020 was extracted. The results show that the area of desert land in the study area first increased and then decreased. The main influencing factors have been precipitation, humidity, and anthropogenic disturbance, for which the path coefficients are 0.646, 0.615, and 0.367, respectively. This study will be of great significance in obtaining large-scale and long-term time series of desert land cover and revealing the inner mechanism of desert land area change. © 2023 by the authors.","desert land; driving factors; multispectral images; structural equation model; Y-Net model","Agriculture; Convolution; Deep learning; Extraction; Image enhancement; Landforms; Landsat; Learning systems; Population statistics; Textures; Anthropogenic disturbance; Desert land; Driving factors; Extraction method; Multispectral images; Net model; Spatial evolution; Structural equation models; Study areas; Y-net model; Remote sensing","National Natural Science Foundation of China, NSFC, (42071343, 42204031, 42071428); UK Research and Innovation, UKRI, (104829)","This work was supported in part by the National Natural Science Foundation of China under Grants 42071343, 42071428 and 42204031.","B. Zhang; School of Mapping and Geoscience, Liaoning Technical University, Fuxin, 123000, China; email: zhangbing@lntu.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85166248906"
"Kijima M.; Fujita K.","Kijima, Masato (59004351200); Fujita, Katsuhide (7404058642)","59004351200; 7404058642","Effectiveness of Reward Functions for Deep Reinforcement Learning in Chick-Feeding System","2023","Proceedings - 2023 15th International Congress on Advanced Applied Informatics Winter, IIAI-AAI-Winter 2023","","","","236","241","5","0","10.1109/IIAI-AAI-Winter61682.2023.00051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191476935&doi=10.1109%2fIIAI-AAI-Winter61682.2023.00051&partnerID=40&md5=c38e89f11265d93a4129366d5381e3ef","Graduate School of Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan; Institute of Global Innovation Research, Tokyo University of Agriculture and Technology, Tokyo, Japan","Kijima M., Graduate School of Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan; Fujita K., Institute of Global Innovation Research, Tokyo University of Agriculture and Technology, Tokyo, Japan","Animal welfare has become a universal standard since the establishment of global standards. Livestock is raised cage free, and it is crucial to develop an appropriate livestock management system. The key to constructing an appropriate system is the animal-computer interaction technology of understanding livestock and controlling feeders automatically. However, it is difficult to control feeders without targeting because there are many unknown things about behaviors of livestock. Therefore, optimizing the system using reinforcement learning is an effective approach to constructing a production management system. This research aims to develop and analyze a reward function for deep reinforcement learning in a chick-feeding system. We developed a simulator to model an environment with many chicks and feeder robots and assessed the proposed approaches using the simulator. The effectiveness of the proposed reward function was evaluated by comparing its accuracy in post-training tests. The experimental findings demonstrated that rewarding food consumption with a bonus and penalizing food dispersion resulted in the shortest number of steps. Furthermore, the dispersal reward effectively reduced the number of steps.  © 2023 IEEE.","chick-feeding system; deep reinforcement learning; reward function","Agriculture; Animals; Deep learning; Feeding; Food supply; Learning systems; Materials handling equipment; Animal welfare; Animal-computer interactions; Chick-feeding system; Deep reinforcement learning; Feeding system; Global standards; Interaction technology; Management systems; Reinforcement learnings; Reward function; Reinforcement learning","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835038382-9","","","English","Proc. - Int. Congr. Adv. Appl. Informatics Winter, IIAI-AAI-Winter","Conference paper","Final","","Scopus","2-s2.0-85191476935"
"Saga S.K.; Ishikawa S.; Mitani T.; Morita S.; Hara R.; Tanaka T.; Komiya M.; Ishii K.; Ochiai S.; Ham G.-Y.","Saga, Samuel Kinari (58923742100); Ishikawa, Shiho (56027320300); Mitani, Tomohiro (9533868000); Morita, Shigeru (55704989500); Hara, Ryoichi (7006065276); Tanaka, Takayuki (58923318200); Komiya, Michio (14520445200); Ishii, Kazuei (20433457200); Ochiai, Satoru (55466500100); Ham, Geun-Yong (57211556093)","58923742100; 56027320300; 9533868000; 55704989500; 7006065276; 58923318200; 14520445200; 20433457200; 55466500100; 57211556093","APPLICABILITY OF VITAL DATA COLLECTED FROM A NON-CONTACT SENSOR FOR ESTIMATING AN INDIVIDUAL COW’S METHANE EMISSION WITH A LASER METHANE DETECTOR","2023","Journal of Japan Society of Civil Engineers","11","2","","23","31","8","0","10.2208/JOURNALOFJSCE.23-26006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186939386&doi=10.2208%2fJOURNALOFJSCE.23-26006&partnerID=40&md5=a82670c7f1f981ab58223448ac303cee","Graduate School of Engineering, Hokkaido University, Kita 13 Nishi 8, Kita-ku, Sapporo, Hokkaido, 060-8628, Japan; College of Agriculture, Food and Environment Sciences, Rakuno Gakuen University, 582 Midori-machi, Bunkyodai, Ebetsu-shi, Hokkaido, 069-8501, Japan; Research Faculty of Agriculture, Hokkaido University, Kita 9 Nishi 9, Kita-ku, Sapporo, Hokkaido, 060-8589, Japan; College of Agriculture, Food and Environment Sciences, Rakuno Gakuen University, Japan; Faculty of Information Science and Technology, Hokkaido University, Kita14 Nishi9, Sapporo, 060-0814, Japan; Faculty of Information Science and Technology, Hokkaido University, Japan; Graduate School of Engineering, Hokkaido University, Japan","Saga S.K., Graduate School of Engineering, Hokkaido University, Kita 13 Nishi 8, Kita-ku, Sapporo, Hokkaido, 060-8628, Japan; Ishikawa S., College of Agriculture, Food and Environment Sciences, Rakuno Gakuen University, 582 Midori-machi, Bunkyodai, Ebetsu-shi, Hokkaido, 069-8501, Japan; Mitani T., Research Faculty of Agriculture, Hokkaido University, Kita 9 Nishi 9, Kita-ku, Sapporo, Hokkaido, 060-8589, Japan; Morita S., College of Agriculture, Food and Environment Sciences, Rakuno Gakuen University, Japan; Hara R., Faculty of Information Science and Technology, Hokkaido University, Kita14 Nishi9, Sapporo, 060-0814, Japan; Tanaka T., Faculty of Information Science and Technology, Hokkaido University, Japan; Komiya M.; Ishii K., Graduate School of Engineering, Hokkaido University, Japan; Ochiai S.; Ham G.-Y.","This paper explores the applicability of estimating methane emission from cows using non-contact sensors and deep learning techniques. The study was conducted on a Holstein cow housed in a tie-stall barn at Rakuno Gakuen University in Ebetsu-shi, Hokkaido, Japan. Methane concentration in the cow's breath and vital data, including heart rate, respiratory rate, and body movements, were measured using a laser methane detector (LMD) and a non-contact sensor, respectively. The LMD data was preprocessed to “extracted Mini-peaks”, which represent exhalation events, to be used as the target variable dataset. In addition, the vital data was used as the explanatory variable dataset. The Long Short-Term Memory (LSTM) model was implemented to estimate methane concentration in a cow’s breath, and the performance of the model was evaluated based on the root mean square error (RMSE) value. The results showed that the LSTM model trained with the “extracted Mini-peaks” data outperformed the model trained with the “raw LMD” data, indicating that the Mini-peaks were more closely related to vital data. Furthermore, the LSTM model trained with the “extracted Mini-peaks” data exhibited a relative error ranging from 1.9% to 11.6% in estimating daily methane emissions, compared to that calculated from observed methane concentration. The study demonstrated the applicability of estimating methane emissions from cows using non-contact sensors and the LSTM model with achieving estimation accuracy that is comparable to the LMD method. This approach could provide a cost-effective and efficient method for monitoring methane emissions from cows, contributing to the development of sustainable livestock farming practices. © 2023 Japan Society of Civil Engineers. All rights reserved.","cow; laser methane detector; long short-term memory; methane emission; non-contact sensor","","","","K. Ishii; Graduate School of Engineering, Hokkaido University, Japan; email: k-ishii@eng.hokudai.ac.jp","","Japan Society of Civil Engineers","21875103","","","","English","J.Japan Soc. Civil Engi.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186939386"
"Andrade A.C.; da Silva L.O.; Souza V.F.; Rufino L.M.D.A.; da Silva T.E.; dos Santos A.D.F.; Gomes D.D.A.; Rodrigues J.P.P.","Andrade, André Cascalho (58083450400); da Silva, Luan Oliveira (58083513800); Souza, Victor Ferreira (58083549300); Rufino, Luana Marta de Almeida (58083386100); da Silva, Tadeu Eder (56485229400); dos Santos, Adam Dreyton Ferreira (58083549400); Gomes, Diego de Azevedo (58083483900); Rodrigues, João Paulo Pacheco (57206914884)","58083450400; 58083513800; 58083549300; 58083386100; 56485229400; 58083549400; 58083483900; 57206914884","Identifying pregnancy in cows using ovarian ultrasound images and convolutional neural networks - a proof-of-concept study","2023","Computers and Electronics in Agriculture","206","","107674","","","","3","10.1016/j.compag.2023.107674","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147093196&doi=10.1016%2fj.compag.2023.107674&partnerID=40&md5=bdd2aa6737b0392bab63f67ae9d317ac","Faculty of Animal Science, Federal University of Southern and Southeastern Pará, Pará, Xinguara, Brazil; Laboratory of Scientific Computing, Federal University of Southern and Southeastern Pará, Pará, Marabá, Brazil; Department of Animal and Dairy Science, University of Wisconsin, Madison, United States; Department of Animal Production, Federal Rural University of Rio de Janeiro, Rio de Janeiro, Seropédica, Brazil","Andrade A.C., Faculty of Animal Science, Federal University of Southern and Southeastern Pará, Pará, Xinguara, Brazil; da Silva L.O., Laboratory of Scientific Computing, Federal University of Southern and Southeastern Pará, Pará, Marabá, Brazil; Souza V.F., Laboratory of Scientific Computing, Federal University of Southern and Southeastern Pará, Pará, Marabá, Brazil; Rufino L.M.D.A., Faculty of Animal Science, Federal University of Southern and Southeastern Pará, Pará, Xinguara, Brazil, Department of Animal Production, Federal Rural University of Rio de Janeiro, Rio de Janeiro, Seropédica, Brazil; da Silva T.E., Department of Animal and Dairy Science, University of Wisconsin, Madison, United States; dos Santos A.D.F., Laboratory of Scientific Computing, Federal University of Southern and Southeastern Pará, Pará, Marabá, Brazil; Gomes D.D.A., Laboratory of Scientific Computing, Federal University of Southern and Southeastern Pará, Pará, Marabá, Brazil; Rodrigues J.P.P., Faculty of Animal Science, Federal University of Southern and Southeastern Pará, Pará, Xinguara, Brazil, Department of Animal Production, Federal Rural University of Rio de Janeiro, Rio de Janeiro, Seropédica, Brazil","Visual analysis of ovarian structures by ultrasound in cows is a relevant support tool for improving reproductive performance in livestock. However, with human visual analysis, subjectiveness is a limiting factor, and computer vision technologies are a way to overcome this. We thus aimed to evaluate the use of convolutional neural networks (CNNs) for identifying pregnancy in cows using ovarian ultrasound images obtained 30 days after artificial insemination. The dataset consisted of 510 images from individual independent functional ovaries of 238 pregnant and 272 non-pregnant Nellore cows. All the images were collected from the same commercial farm. To evaluate the dependency of CNN performance on image quality, the images were classified by two independent veterinarians with significant experience in ultrasound evaluation as good, regular, or bad for visual identification of ovarian structures. Four CNN architectures were evaluated, namely ResNet50, ResNeXt50, InceptionResNetV2, and DenseNet121. All the CNNs were evaluated using the complete dataset (ALL; n = 510) and a subset of good and regular images (GR; n = 462). Ten evaluations were conducted for each architecture and dataset combination. In each run, the datasets were randomly divided into 70 % for training and 30 % for testing according to the holdout method. A regularization-based method was used during the training phase using dropout, data augmentation, and a dynamic learning rate. The model's performance was evaluated in terms of accuracy, precision, sensitivity, and specificity, which ranged from 46.5 to 78.8, 34.4–80.0, 28.3–75.4, and 11.2–82.2 %, respectively. The DenseNet121 architecture performed better based on higher accuracy, sensitivity, and specificity, and both lower SD and variation due to dataset. ResNet50 and DenseNet121 models performed better when using the GR dataset. InceptionResNetV2 had a more precise architecture when using the ALL dataset but had lower performance for the GR dataset. We concluded that CNNs are a promising tool to identify the pregnancy status of cows in ovarian images collected 30 days after artificial insemination. The effect of image quality on the performance depends on the CNN architecture. © 2023 Elsevier B.V.","Bovine reproduction; Computer vision; Corpus luteum; Deep learning; Ultrasonography","Agriculture; Cell proliferation; Computer vision; Convolution; Convolutional neural networks; Deep learning; Image quality; Mammals; Network architecture; Quality control; Sensitivity analysis; Artificial insemination; Bovine reproduction; Convolutional neural network; Corpus luteum; Deep learning; Neural network architecture; Performance; Sensitivity and specificity; Ultrasound images; Visual analysis; accuracy assessment; artificial neural network; computer vision; image analysis; pregnancy; reproduction; ultrasonics; Ultrasonics","","","J.P.P. Rodrigues; Seropédica, BR-465, km 7, University Campus, Rio de Janeiro, 23897-000, Brazil; email: joao.rodrigues@ufrrj.br","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85147093196"
"Gu Z.; Zhang H.; He Z.; Niu K.","Gu, Zishuo (58542988600); Zhang, Haoyu (57219634925); He, Zhiqiang (7403885147); Niu, Kai (7007085232)","58542988600; 57219634925; 7403885147; 7007085232","A two-stage recognition method based on deep learning for sheep behavior","2023","Computers and Electronics in Agriculture","212","","108143","","","","14","10.1016/j.compag.2023.108143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168407957&doi=10.1016%2fj.compag.2023.108143&partnerID=40&md5=40e56dbb8ee28f8ed1e2e89cd2e9d8b2","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Gu Z., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Zhang H., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; He Z., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Niu K., Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Recently, computer vision has been widely used in livestock behavior detection. In animal multi-behavior detection, there is a common problem of low recognition accuracy. To address this, a deep learning-based method for sheep-behavior detection is proposed in this paper. Six types of sheep behavior can be recognized by the method. Among these behaviors, standing, feeding, and lying are normal physiological activities. Attacking, biting, and climbing are disruptive behaviors that could result in ranch losses and require immediate concern from the breeder. The method consists of two stages: the detection stage and the classification stage. In the detection stage, a detection network is used to determine whether every sheep's behavior belongs to normal physiological activities or disruptive behaviors respectively. Based on a classic network, the multi-scale feature aggregation, attention mechanism and depthwise convolution module are imported to it, which makes the network trades off between detection accuracy and model size. These improvements make the network more suitable for sheep-behavior detection. In the classification stage, the VGG network is utilized to classify the behavior of each sheep specifically. Experimental results demonstrate that the two-stage method achieves desirable results in sheep-behavior recognition. In the detection stage, the mAP of the two types of behaviors exceeds 98%. In the classification stage, the classification accuracy of all behaviors exceeds 94%. At the same time, the memory of the detection model is less than 130 MB. © 2023 Elsevier B.V.","Classification; Deep learning; Detection; Sheep Behavior; Two-stage","Agriculture; Behavioral research; Learning systems; Physiology; Behavior detection; Deep learning; Detection; Detection models; Detection stage; Physiological activity; Recognition methods; Sheep behaviour; Two-stage; Two-stage recognition; behavioral ecology; computer vision; detection method; machine learning; physiological response; sheep; Deep learning","National Natural Science Foundation of China, NSFC, (92067202)","This work was supported by the National Natural Science Foundation of China (No. 92067202 ). The authors should express their support for Zhengning Xinle Agriculture and Animal Husbandry in terms of data collection.","K. Niu; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, 100876, China; email: niukai@bupt.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85168407957"
"He C.; Qiao Y.; Mao R.; Li M.; Wang M.","He, Chong (57199719193); Qiao, Yongliang (56486770900); Mao, Rui (36167161700); Li, Mei (57199160408); Wang, Meili (55694491200)","57199719193; 56486770900; 36167161700; 57199160408; 55694491200","Enhanced LiteHRNet based sheep weight estimation using RGB-D images","2023","Computers and Electronics in Agriculture","206","","107667","","","","21","10.1016/j.compag.2023.107667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146954157&doi=10.1016%2fj.compag.2023.107667&partnerID=40&md5=88f0fa2eb697ef42a4a66551ba9cea40","College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW, 2006, Australia","He C., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW, 2006, Australia; Mao R., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Li M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Wang M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Sheep farming is a strategic sector of livestock husbandry, and its production has large market demand in many countries. The live weight of sheep provides important information about the health state and the time point for marketing. Manual weighing sheep is time-consuming for farmers even with the help of a ground scale. With the development of Artificial Intelligence (AI) and smart sensors, non-contact sheep weighing methods have gradually been used to estimate weight. However, the performance of prior studies tends to degenerate with varying postures and light conditions in practical natural environments. In this study, we propose a sheep live weight estimation approach based on LiteHRNet (a Lightweight High-Resolution Network) using RGB-D images. Class Activation Mapping (CAM) guided the design of efficient network heads embracing visual explanation and applicability in practical natural environments. Experiments are conducted on our challenging dataset (of 726 sheep RGB-D images, weight range between 19.5 to 94 kg). Comparative experiment results reveal that the lightweight Convolutional Neural Network (CNN) model trained on RGB-D images can reach an acceptable weight estimation result, Mean Average Percentage Error (MAPE) is 14.605% (95% confidence interval: [13.821%, 15.390%], t test) with only 1.06M parameters. Our works can be viewed as preliminary work that confirms the ability to use lightweight CNNs for sheep weight estimation on RGB-D data. The results of this study are potential to develop an embedded device to automatically estimate sheep live weight and would contribute to the development of precision livestock farming. © 2023","Deep learning; Depth image; Lightweight CNNs; Sheep weight estimation","Commerce; Deep learning; Farms; Image enhancement; Neural network models; Weighing; Deep learning; Depth image; High resolution; Lightweight CNN; Natural environments; Network-based; Sheep farming; Sheep weight estimation; Strategic sectors; Weights estimation; animal husbandry; depth determination; estimation method; image resolution; optical depth; sheep; weight; Convolutional neural networks","Key Laboratory of Agricultural Internet of Things; National Natural Science Foundation of China, NSFC, (61702433, NYKJ-2021-YL); National Natural Science Foundation of China, NSFC; Ministry of Agriculture, Food and Rural Affairs, MAFRA, (2018AIOT-09); Ministry of Agriculture, Food and Rural Affairs, MAFRA","Funding text 1: This work was partially funded by Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi 712100, China (2018AIOT-09). National Natural Science Foundation of China (61702433). Shaanxi Agricultural Science and Technology Innovation Drive Project (NYKJ-2021-YL (XN) 48). The authors express their gratitude to Hongke Zhao, Haotian Zhang, Yingnan Wang, and Feiyu Zhang for their help in experiment organization and data collection.; Funding text 2: This work was partially funded by Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, Shaanxi 712100, China ( 2018AIOT-09 ). National Natural Science Foundation of China ( 61702433 ). Shaanxi Agricultural Science and Technology Innovation Drive Project ( NYKJ-2021-YL (XN) 48 ). The authors express their gratitude to Hongke Zhao, Haotian Zhang, Yingnan Wang, and Feiyu Zhang for their help in experiment organization and data collection.","M. Wang; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: wml@nwsuaf.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85146954157"
"Sanjel A.; Khanal B.; Rivas P.; Speegle G.","Sanjel, Arun (58102866900); Khanal, Bikram (57795523900); Rivas, Pablo (57207780751); Speegle, Greg (6602883048)","58102866900; 57795523900; 57207780751; 6602883048","Non-Invasive Muzzle Matching for Cattle Identification Using Deep Learning","2023","Proceedings - 2023 Congress in Computer Science, Computer Engineering, and Applied Computing, CSCE 2023","","","","1998","2002","4","0","10.1109/CSCE60160.2023.00328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191191875&doi=10.1109%2fCSCE60160.2023.00328&partnerID=40&md5=d1452cfdf2a605e97b299039bced52a3","School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States","Sanjel A., School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States; Khanal B., School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States; Rivas P., School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States; Speegle G., School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States","Accurate cattle identification is an essential but complicated issue in the field of livestock management. Traditional identifying methods can involve invasive procedures, posing ethical difficulties and compromising animal welfare. This paper addresses this pressing issue by proposing a deep learning-based methodology for non-invasive cattle identification through muzzle matching. Our approach leverages a comprehensive dataset of 4923 cleaned and cropped muzzle images from 268 distinct cattle breeds. The model showcases exceptional performance with a training accuracy of 98.88 % and a test accuracy of 100 %. Importantly, our methodology avoids invasive procedures and exhibits adaptability, effectively handling introducing new animals into the system. This versatility ensures the model's reliability across diverse operational scenarios, making it a suitable candidate for insurance fraud prevention and animal trading applications. The paper also highlights critical future research directions, including expanding the dataset to encompass a broader range of cattle breeds and muzzle variations and the potential integration with other identification modalities. © 2023 IEEE.","Adaptability; Cattle recognition; Deep Learning; Muzzle point; Non-invasive Techniques","Agriculture; Deep learning; Insurance; Learning systems; Adaptability; Animal welfare; Cattle recognition; Deep learning; Identifying methods; Matchings; Muzzle point; Noninvasive technique; Performance; Pressung; Animals","","","A. Sanjel; School of Engineering and Computer Science, Baylor University, Department of Computer Science, United States; email: Arun_Sanjel1@baylor.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835032759-5","","","English","Proc. - Congr. Comput. Sci., Comput. Eng., Appl. Comput., CSCE","Conference paper","Final","","Scopus","2-s2.0-85191191875"
"Nakaguchi V.M.; Ahamed T.","Nakaguchi, Victor Massaki (57853537400); Ahamed, Tofael (9269729600)","57853537400; 9269729600","Thermal Imaging and Deep Learning Object Detection Algorithms for Early Embryo Detection: A Methodology Development Addressed to Quail Precision Hatching","2023","IoT and AI in Agriculture: Self-sufficiency in Food Production to Achieve Society 5.0 and SDG’s Globally","","","","253","281","28","1","10.1007/978-981-19-8113-5_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201433925&doi=10.1007%2f978-981-19-8113-5_13&partnerID=40&md5=ff5ab72d8b5334346d7ad9a048cadfb2","Graduate School of Science and Technology, University of Tsukuba, Ibaraki, Tsukuba, Japan; Faculty of Life & Environmental Sciences, University of Tsukuba, Ibaraki, Tsukuba, Japan","Nakaguchi V.M., Graduate School of Science and Technology, University of Tsukuba, Ibaraki, Tsukuba, Japan; Ahamed T., Faculty of Life & Environmental Sciences, University of Tsukuba, Ibaraki, Tsukuba, Japan","Poultry production utilized many available technologies in terms of farmindustry automation and sanitary control. However, there is a lack of robust techniques and affordable equipment for avian embryo detection and sexual segregation at the early stages. In this work, we aimed to evaluate the potential use of thermal microcameras for detecting embryos in quail eggs via thermal images during the first 168 h (7 days) of incubation. We propose a methodology to collect data during incubation period. Additionally, to support the visual analysis, YOLO deep learning object detection algorithms were applied to detect unfertilized eggs; the results showed its potential to distinguish fertilized eggs from unfertilized eggs during the incubation period after filtering radiometric images. We compared YOLOv4, YOLOv5, and SSD-MobileNet V2 trained models. The mAP@0.50 of the YOLOv4, YOLOv5, and SSD-MobileNet V2 was 98.62%, 99.5%, and 91.8%, respectively. We also compared three testing datasets for different intervals of rotation of eggs, as our hypothesis was that fewer turning periods could improve the visualization of fertilized eggs features, and applied three treatments: 1.5, 6, and 12 h. The results showed that turning eggs in different periods did not exhibit a linear relation, as the F1 Score for YOLOv4 of detection for the 12 h period was 0.569, that for the 6 h period was 0.404 and that for the 1.5 h period was 0.384. YOLOv5 F1 Scores for 12, 6, and 1.5 h was 1, 0.545, and 0.386, respectively. SSD-MobileNet V2 performed F1 score of 0.60 for 12 h, 0.22 for 6 h, and 0 for 1.5 h turning periods. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023.","Deep learning; Embryo detection; Precision livestock farming; Quail eggs; Thermal imaging; YOLO","","","","T. Ahamed; Faculty of Life & Environmental Sciences, University of Tsukuba, Tsukuba, Ibaraki, Japan; email: tofael.ahamed.gp@u.tsukuba.ac.jp","","Springer Nature","","978-981198113-5; 978-981198112-8","","","English","IoT and AI in Agriculture: Self-sufficiency in Food Production to Achieve Society 5.0 and SDG’s Globally","Book chapter","Final","","Scopus","2-s2.0-85201433925"
"Wang J.; Li W.; Liu Y.; Li Z.","Wang, Jing (59067351200); Li, Weiran (57310991800); Liu, Yeqiang (58938458400); Li, Zhenbo (55613118900)","59067351200; 57310991800; 58938458400; 55613118900","Review of Vision Counting Methods and Applications for Farmed Animals; [基于计算机视觉的养殖动物计数方法研究综述]","2023","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","54","","","315","329","14","3","10.6041/j.issn.1000-1298.2023.S1.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187673091&doi=10.6041%2fj.issn.1000-1298.2023.S1.034&partnerID=40&md5=376bc806810b7a42c082cf240437f867","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China","Wang J., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China; Li W., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China; Liu Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China; Li Z., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China","Quantitative measurement is the basic work of biological research and breeding management, and its results are of great significance to the production efficiency, cost control of animal breeding and assessment of economic benefits. In recent years, with the development of image acquisition equipment, image processing technology and computer vision algorithms, the research on animal counting based on computer vision has also made great progress. Artificial counting often needs to rely on breeding personnel to observe and count the animals one by one, which is not only prone to omissions and errors, but also requires a lot of time and human resources. Computer vision-based counting methods can realize automated counting, which to a certain extent reduces the workload of breeding personnel and improves the breeding efficiency. The research related to farm animal counting in the past ten years was counted, and the farm animal counting algorithms were analyzed and discussed from both traditional machine learning and deep learning. Among them, the traditional machine learning method mainly relied on manually extracted features for recognition and counting, with fast computation speed and small resource consumption, but lacked the understanding of the global semantic information of the image ; counting algorithms based on deep learning had a stronger generalization ability to complex scenes, and achieved better results in the counting task for farmed animals, which was the mainstream direction of the current research. In addition, the applications of farmed animal counting in the fields of aquaculture, livestock and poultry farming and special animal farming were sorted out and summarized. At the same time, the current publicly released farmed animal counting datasets were summarized. Finally, the main challenges of farmed animal counting research were analyzed and discussed in terms of datasets, application scenarios and counting methods, and the future development trend was outlooked. Specifically, by constructing larger and richer public datasets, improving the accuracy and generalization ability of counting algorithms, and expanding the counting models in specific scenarios to a wider range of application scenarios, the research on farmed animal counting would make greater progress and development, so as to truly play its role in supporting agricultural production. © 2023 Chinese Society of Agricultural Machinery. All rights reserved.","computer vision; counting; deep learning; farm animal; machine learning","Agriculture; Animals; Computer vision; Deep learning; Image acquisition; Learning algorithms; Learning systems; Personnel; Semantics; 'current; Application scenario; Biological breeding; Counting; Deep learning; Farm animals; Farmed animals; Generalization ability; Machine-learning; Quantitative measurement; Production efficiency","","","Z. Li; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; email: lizb@cau.edu.en","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85187673091"
"Ahmad M.; Abbas S.; Fatima A.; Ghazal T.M.; Alharbi M.; Khan M.A.; Elmitwally N.S.","Ahmad, Munir (57226132376); Abbas, Sagheer (57202833192); Fatima, Areej (57210681299); Ghazal, Taher M. (57224545303); Alharbi, Meshal (57483830000); Khan, Muhammad Adnan (57226797214); Elmitwally, Nouh Sabri (57209421201)","57226132376; 57202833192; 57210681299; 57224545303; 57483830000; 57226797214; 57209421201","AI-Driven livestock identification and insurance management system","2023","Egyptian Informatics Journal","24","3","100390","","","","14","10.1016/j.eij.2023.100390","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168012964&doi=10.1016%2fj.eij.2023.100390&partnerID=40&md5=c87434cc825b1b1ee049b141c6821f77","School of Computer Science, National College of Business Administration & Economics, Lahore, 54000, Pakistan; Department of Computer Science, Lahore Garrison University, Lahore, 54000, Pakistan; School of Information Technology, Skyline University College, University City Sharjah, Sharjah, 1797, United Arab Emirates; Applied Science Research Center, Applied Science Private University, Amman, 11931, Jordan; Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Alkharj, 11942, Saudi Arabia; Department of Software, Faculty of Artificial Intelligence and Software, Gahcon University, Seongnam, 13120, South Korea; School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, United Kingdom; Department of Computer Science, Faculty of Computers and Artificial Intelligence, Cairo University, Giza, 12613, Egypt","Ahmad M., School of Computer Science, National College of Business Administration & Economics, Lahore, 54000, Pakistan; Abbas S., School of Computer Science, National College of Business Administration & Economics, Lahore, 54000, Pakistan; Fatima A., Department of Computer Science, Lahore Garrison University, Lahore, 54000, Pakistan; Ghazal T.M., School of Information Technology, Skyline University College, University City Sharjah, Sharjah, 1797, United Arab Emirates, Applied Science Research Center, Applied Science Private University, Amman, 11931, Jordan; Alharbi M., Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Alkharj, 11942, Saudi Arabia; Khan M.A., Department of Software, Faculty of Artificial Intelligence and Software, Gahcon University, Seongnam, 13120, South Korea; Elmitwally N.S., School of Computing and Digital Technology, Birmingham City University, Birmingham, B4 7XG, United Kingdom, Department of Computer Science, Faculty of Computers and Artificial Intelligence, Cairo University, Giza, 12613, Egypt","Cattle identification is pivotal for many reasons. Animal health management, traceability, bread classification, and verification of insurance claims are largely depended on the accurate identification of the animals. Conventionally, animals have been identified by various means such as ear tags, tattoos, rumen implants, and hot brands. Being non-scientific approaches, these controls can be easily circumvented. The emerging technologies of biometric identification are extensively applied for Human recognition via thumb impression, face features, or eye retina patterns. The application of biometric recognition technology has now moved towards animals. Cattle identification with the help of muzzle patterns has shown tremendous results. For precise identification, nature has awarded a unique Muzzle pattern that can be utilized as a primary biometric feature. Muzzle pattern image scanning for biometric identification has now been extensively applied for identification. Animal recognition via Muzzle pattern image for different applications has been proliferating gradually. One of those applications includes the identification of fake insurance claims under livestock insurance. Fraudulent animal owners tend to lodge fake claims against livestock insurance with proxy animals. In this paper, we proposed the solution to avoid and/or discard fraudulent claims of livestock insurance by intelligently identifying the proxy animals. Data collection of animal muzzle patterns remained challenging. Key aspects of the proposed system include: (1) the Animal face will be detected through visual using YOLO v7 object detector. (2) After face detection, the same procedures will apply to detect muzzle point (3) the muzzle pattern is extracted and then stored in the database. The System has a mean average precision of 100% for the face and 99.43% for the nose/muzzle point of the animal. Once the animal is registered in the database, the identification process is initiated by extracting unique nose pattern features with ORB and/or SIFT. Then it is matched using the pattern matchers like BFMatcher and/or FLANNMatcher for animal identification. The proposed model is more efficient and accurate as compared to concurrent approaches. The results extracted from this research study show 100% accurate identification. © 2023","Artificial Intelligence; Deep Learning; Machine Learning; Transfer Learning","Agriculture; Anthropometry; Biometrics; Deep learning; Face recognition; Insurance; Learning systems; Object detection; Animal health; Biometric identifications; Deep learning; Emerging technologies; Health management; Insurance claims; Machine-learning; Management systems; Pattern images; Transfer learning; Animals","United Insurance Company of Pakistan Ltd., (No.UICL-06–2022/USTI-RND-006/11/2022)","This research program is partially funded by the United Insurance Company of Pakistan Ltd. in collaboration with United Software and Technologies International (Pvt) Ltd. under Grant No.UICL-06–2022/USTI-RND-006/11/2022. ","M.A. Khan; Department of Software, Faculty of Artificial Intelligence and Software, Gahcon University, Seongnam, 13120, South Korea; email: adnan@gachon.ac.kr","","Elsevier B.V.","11108665","","","","English","Egypt. Informatics J.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168012964"
"Xiao R.; Gao J.; Liu A.; Hou P.; Zhang W.; Yang Y.; Li Y.; Fu Z.; Jin C.; Yang X.; Zheng S.; Yin S.","Xiao, Rulin (33568432600); Gao, Jixi (57189334684); Liu, Aijun (56784857300); Hou, Peng (7102218643); Zhang, Wenguo (57219963879); Yang, Yong (59322666500); Li, Yunbao (58976144200); Fu, Zhuo (44761049200); Jin, Chuanping (57220584082); Yang, Xu (58976413100); Zheng, Shuhua (58976543800); Yin, Shoujing (15840951400)","33568432600; 57189334684; 56784857300; 7102218643; 57219963879; 59322666500; 58976144200; 44761049200; 57220584082; 58976413100; 58976543800; 15840951400","Remote sensing monitoring method of livestock in grassland based on multi-scale features and multi-models fusion; [多尺度特征和多模型相融合的草原区牧畜遥感监测]","2023","National Remote Sensing Bulletin","27","10","","2383","2394","11","1","10.11834/jrs.20222099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189818730&doi=10.11834%2fjrs.20222099&partnerID=40&md5=5f427c26da18fc877d13cc59de47c754","Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Inner Mongolia Autonomous Region Grassland Remote Sensing and Emergency Technical Reserve Key Laboratory, Hohhot, 010020, China; Information Center of Ecology and Environment of Mianyang, Mianyang, 621050, China","Xiao R., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Gao J., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Liu A., Inner Mongolia Autonomous Region Grassland Remote Sensing and Emergency Technical Reserve Key Laboratory, Hohhot, 010020, China; Hou P., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Zhang W., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Yang Y., Inner Mongolia Autonomous Region Grassland Remote Sensing and Emergency Technical Reserve Key Laboratory, Hohhot, 010020, China; Li Y., Information Center of Ecology and Environment of Mianyang, Mianyang, 621050, China; Fu Z., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Jin C., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Yang X., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; Zheng S., Inner Mongolia Autonomous Region Grassland Remote Sensing and Emergency Technical Reserve Key Laboratory, Hohhot, 010020, China; Yin S., Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China","China is a large country of grassland and animal husbandry. Overloading and overgrazing is one of the main causes of grassland degradation in China. To protect the grassland, it is necessary to precisely monitoring livestock carrying capacity, which is the key to evaluation and control grass-livestock balance. However, traditional method of livestock carrying capacity such as hierarchical statistics, sampling field survey and online camera monitoring is whether time-consuming, labor-intensive, costly or poor quality. So it is very urgent to find a kind of efficient and precise monitoring method of livestock carrying capacity in grassland. To achieve this goal, this research proposes an efficient and precise monitoring method of the livestock in grassland by using of sub-meter resolution satellite image. The method not only fuses multi-scale features of livestock in sub-meter resolution satellite image such as “blob feature”、“flock feature”、“moving feature”, but also integrates deep learning technology and object oriented recognition technology. Firstly, considering that the livestock in satellite image is a kind of small (tiny) target, it uses kinds of image enhancement method such as bilateral filtering and Laplace of Gaussian (LoG) operator to enhance the weak livestock signal successfully. Secondly, in consideration of “flock feature”of livestock flock, it use a kind of“livestock flock detection model”based on deep learning technology to get the rough distribution area of livestock flocks. Thirdly, in consideration of“blob feature”and“moving feature”of livestock, it use a kind of“livestock blob detection method”based on LoG Gradient Difference and object oriented recognition technology to get the possible livestock blobs. Finally, by integrating the detecting result of both livestock flocks and livestock blobs, it uses the livestock blobs result to enhance and verify the livestock flocks result, and the enhanced and verified livestock flocks and the livestock blobs within them is finally get by using some simple manually revising work. Through an experiment in Xilingol grassland, it is found that the approach has good effect on livestock flock detection: with positive detection rate about 0.802 and false detection rate about 0.244, especially as to big livestock flock, the positive detection rate is up to 0.937, the false detection rate is low to 0.072. It is very helpful for the monitoring and supervision of livestock flock in grassland, and can also provide reference for remote sensing monitoring of other“small (tiny) targets”. It is of great significance both in terms of technological innovation and business application. It makes a litter effort in promoting the livestock satellite remote sensing monitoring into an intuitive and fine monitoring era- “Number-Counting Era”. © 2023 Science Press. All rights reserved.","grassland; livestock; livestock carrying; remote sensing; small (tiny) target","Agriculture; Deep learning; Engineering education; Feature extraction; Learning systems; Object detection; Remote sensing; Sampling; Satellites; Detection rates; Grassland; Livestock; Livestock carrying; Monitoring methods; Multi-scale features; Remote sensing monitoring; Remote-sensing; Satellite images; Small (tiny) target; Image enhancement","National Key Research and Development Program of China, NKRDPC, (2021YFB3901102); National Key Research and Development Program of China, NKRDPC","National Key Research and Development Program of China (No. 2021YFB3901102)","J. Gao; Satellite Application Center for Ecology and Environment of MEE, Beijing, 100094, China; email: gjx@nies.org","","Science Press","10074619","","","","Chinese","National Remote Sensing Bulletin","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85189818730"
"Mishra S.","Mishra, Shailendra (55463276900)","55463276900","Internet of things enabled deep learning methods using unmanned aerial vehicles enabled integrated farm management","2023","Heliyon","9","8","e18659","","","","7","10.1016/j.heliyon.2023.e18659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166359488&doi=10.1016%2fj.heliyon.2023.e18659&partnerID=40&md5=f18227c136a2b1c2cd77af64ce3f097c","Department of Computer Engineering, College of Computer and Information Sciences, Majmaah University, Al Majmaah, 11952, Saudi Arabia","Mishra S., Department of Computer Engineering, College of Computer and Information Sciences, Majmaah University, Al Majmaah, 11952, Saudi Arabia","Smart livestock farming strives to make farming more lucrative, efficient, and ecologically beneficial by using digital technologies. Precision livestock fencing, in which each animal is followed and studied independently, is the most promising kind of smart livestock farming. The Internet of Things (IoT) allows farmers to save money and effort by keeping tabs on crops, mapping out their land, and giving them data to develop sensible management strategies for their farms. Surveillance, disaster management, firefighting, border patrol, and courier services employ Unmanned Aerial Vehicles (UAVs) that are originally created for the military. The segment focuses on UAVs in livestock and agricultural production. This is achieved via employing robots, drones, remote sensors, and computer imagery in unison with ever-improving in-Depth Learning for farming. Deep learning (DL) algorithms find many uses in the agricultural sector, from identifying plant diseases to estimating yields to detecting weeds to forecasting the weather and determining how much water is in the soil. The challenging characteristics of smart livestock farming are climate change, biodiversity loss, and continuous monitoring. Hence, in this research, the Unmanned Aerial Vehicles enabled Integrated Farm Management (UAV-IFM) has been designed to improve smart livestock farming. Safe and reliable tracking of livestock from farm to fork is made possible by this sensor, which has far-reaching implications for detecting and containing disease outbreaks and preventing the resulting financial losses and food-related health pandemics. UAV-IFM aims to improve the assessment process so that smart livestock farming may be more widely adopted and offers growth-supportive help to farmers. Conclusions gathered from this study's examination of the UAV-IFM reveal that these instruments correctly forecast and verify smart livestock farming management within the framework of the assessment procedure. The experimental analysis of UAV-IFM outperforms smart livestock farming in terms of efficiency ratio, performance, accuracy, and prediction. © 2023 The Author","Deep learning; Integrated farm management; Internet of things; Smart livestock farming; Unmanned aerial vehicles","","Majmaah University, MU, (R-2023-408)","The author would like to thank Deanship of Scientific Research at Majmaah University for supporting this work under Project Number No. R-2023-408.","","","Elsevier Ltd","24058440","","","","English","Heliyon","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85166359488"
"Cui L.; Tang W.; Deng X.; Jiang B.","Cui, Lihang (57571477900); Tang, Wenjie (58151350100); Deng, Xiaoshang (57452099100); Jiang, Bing (57758826500)","57571477900; 58151350100; 57452099100; 57758826500","Farm Animal Welfare Is a Field of Interest in China: A Bibliometric Analysis Based on CiteSpace","2023","Animals","13","19","3143","","","","8","10.3390/ani13193143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173888709&doi=10.3390%2fani13193143&partnerID=40&md5=02e7ca80a9a085e62dd8961d3de14ef7","College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Development Research Center of Modern Agriculture, Northeast Agricultural University, Harbin, 150030, China","Cui L., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Tang W., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Deng X., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Jiang B., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China, Development Research Center of Modern Agriculture, Northeast Agricultural University, Harbin, 150030, China","Farm animal welfare research conducted in China is not commonly accessed or known outside of China, which may lead to the assumption that farm animal welfare receives relatively little attention in China. Therefore, a bibliometric analysis was conducted on the existing Chinese farm animal welfare literature to provide robust evidence to refute this assumption. A total of 1312 peer-reviewed Chinese studies on farm animal welfare published between March 1992 and June 2023 were retrieved from the Web of Science (WoS) and the China National Knowledge Infrastructure (CNKI) database. CiteSpace software was used to analyze and visualize the number, species, authors, institutions, journals, and keywords of the papers. In China, farm animal welfare research has gone through the processes of an early stage (1992–2001), rapid-growth stage (2002–2007), and mature stage (2008–present), and the scale of research continues to grow. Notably, swine and chickens have received priority attention in this area. A Matthew effect was observed for authors and institutions, with relatively little collaboration among authors and institutions. Most of the papers were published in a small number of journals, with an apparent agglomeration characteristic. The research hotspots, summarized as “feed and diet”, “environmental impacts and control”, “integrated rearing management”, “injury and disease”, “behavior and technologies for behavior monitoring”, “genetic analysis”, “welfare during transport and slaughter”, “welfare-friendly animal product consumption”, “attitudes toward farm animal welfare”, and “healthy breeding”. The keywords “computer vision”, “recognition”, “temperature”, “precision livestock farming”, “laying hen”, and “behavior”, represent the major research frontiers in the field, which could indicate potential areas of significant future research. The findings of the present bibliometric analysis confirm the fact that farm animal welfare is a field of interest in China. Farm animal welfare research in China tends to be pragmatic, with a strong emphasis on enhancing growth and production performance, as well as product quality, rather than solely concentrating on improving farm animal welfare. This paper provides insightful references that researchers can use to identify and understand the current status and future direction of the farm animal welfare field in China. © 2023 by the authors.","bibliometric analysis; China; CiteSpace; farm animal welfare; research frontiers; research hotspots; research trends","agriculture; animal food; animal product; animal welfare; attention; attitude; behavior; bibliometrics; breeding; broiler; chicken meat; China; citation analysis; computer vision; deep learning; egg laying; environmental impact; farm animal; genetic analysis; growth curve; heat stress; hen; injury; livestock; medical research; nonhuman; physiological stress; poultry product; product quality; publication; quantitative trait locus; rearing; review; Review; ruminant; slaughtering; stereotypy; technology; temperature; veterinary medicine","National Natural Science Foundation of China, NSFC, (71640017, 72203034); Natural Science Foundation of Heilongjiang Province, (LH2021G002)","This research was funded by the National Natural Science Foundation of China for Young Scholars (grant number 72203034); the Emergency Project Fund of National Natural Science Foundation of China (grant number 71640017); and the Natural Science Foundation of Heilongjiang Province (grant number LH2021G002).","B. Jiang; College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; email: jiangbing2020@neau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85173888709"
"Zhou H.; Li Q.; Xie Q.","Zhou, Hong (55649568604); Li, Qingda (27168640800); Xie, Qiuju (59157795100)","55649568604; 27168640800; 59157795100","Individual Pig Identification Using Back Surface Point Clouds in 3D Vision","2023","Sensors","23","11","5156","","","","6","10.3390/s23115156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161594508&doi=10.3390%2fs23115156&partnerID=40&md5=78d48a1505c90dca24c80e14cf0f7092","College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; Key Laboratory of Swine Facilities Engineering, Ministry of Agriculture, Harbin, 150030, China","Zhou H., College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; Li Q., College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; Xie Q., College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Swine Facilities Engineering, Ministry of Agriculture, Harbin, 150030, China","The individual identification of pigs is the basis for precision livestock farming (PLF), which can provide prerequisites for personalized feeding, disease monitoring, growth condition monitoring and behavior identification. Pig face recognition has the problem that pig face samples are difficult to collect and images are easily affected by the environment and body dirt. Due to this problem, we proposed a method for individual pig identification using three-dimension (3D) point clouds of the pig’s back surface. Firstly, a point cloud segmentation model based on the PointNet++ algorithm is established to segment the pig’s back point clouds from the complex background and use it as the input for individual recognition. Then, an individual pig recognition model based on the improved PointNet++LGG algorithm was constructed by increasing the adaptive global sampling radius, deepening the network structure and increasing the number of features to extract higher-dimensional features for accurate recognition of different individuals with similar body sizes. In total, 10,574 3D point cloud images of ten pigs were collected to construct the dataset. The experimental results showed that the accuracy of the individual pig identification model based on the PointNet++LGG algorithm reached 95.26%, which was 2.18%, 16.76% and 17.19% higher compared with the PointNet model, PointNet++SSG model and MSG model, respectively. Individual pig identification based on 3D point clouds of the back surface is effective. This approach is easy to integrate with functions such as body condition assessment and behavior recognition, and is conducive to the development of precision livestock farming. © 2023 by the authors.","3D sensors; deep learning; pig individual identification; point clouds; PointNet++","Agriculture; Algorithms; Animals; Body Size; Facial Recognition; Farms; Livestock; Swine; Agriculture; Behavioral research; Condition monitoring; Deep learning; Mammals; 3D point cloud; 3D sensor; Deep learning; Individual identification; Model-based OPC; Pig individual identification; Point-clouds; Pointnet++; Precision livestock farming; Surface points; agricultural worker; agriculture; algorithm; animal; body size; facial recognition; livestock; pig; Face recognition","Heilongjiang Bayi Agricultural University Support Program for San Zong, (ZDZX202102); National Natural Science Foundation of China, NSFC, (32072787); National Natural Science Foundation of China, NSFC; Northeast Agricultural University, NEAU, (19YJXG02); Northeast Agricultural University, NEAU; Heilongjiang Provincial Postdoctoral Science Foundation, (LBH-Q21070); Heilongjiang Provincial Postdoctoral Science Foundation","This work was supported by the project of the National Natural Science Foundation of China (NSFC) (32072787); the project of the Postdoctoral Science Foundation of Heilongjiang Province (LBH-Q21070), China; the project of the Heilongjiang Bayi Agricultural University Support Program for San Zong (ZDZX202102); the project of Scholar Plan at Northeast Agriculture University (19YJXG02), China.","Q. Li; College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; email: liqingda23@126.com; Q. Xie; College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; email: xqj197610@163.com","","MDPI","14248220","","","37299883","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85161594508"
"Shi H.; Gao F.; Liu T.; Wang H.; Hasi B.; Yang T.; Yuan C.","Shi, Hongxiao (55570423000); Gao, Fangyu (57724377100); Liu, Tonghai (56131813200); Wang, Hai (57225058481); Hasi, Bagen (57724377000); Yang, Tingting (57217802402); Yuan, Chuangchuang (57724628200)","55570423000; 57724377100; 56131813200; 57225058481; 57724377000; 57217802402; 57724628200","Research progress of the recognition of free-range sheep behavior using sensor technology; [基于传感器技术的自由放牧羊行为识别研究进展]","2023","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","39","17","","1","18","17","1","10.11975/j.issn.1002-6819.202305037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179488261&doi=10.11975%2fj.issn.1002-6819.202305037&partnerID=40&md5=7c29d9f929a2341a2788ccef24cd237b","Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China; College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300384, China","Shi H., Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China; Gao F., Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China, College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300384, China; Liu T., College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300384, China; Wang H., Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China; Hasi B., Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China; Yang T., Grassland Research Institute, Chinese Academy of Agricultural Sciences, Hohhot, 010010, China; Yuan C., College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300384, China","An accurate and rapid recognition of the behavior of sheep can fully reflect the internal physiological state and health level for the better welfare of livestock breeding. At the same time, the health status of grazing grassland can also be analyzed to combine with the location characteristics. Then, the grass condition can be grasped in time to ensure the balance and sustainable development of grass and livestock in the grassland ecosystem. However, it is very difficult to monitor the behavior of free-grazing sheep under the influence of spatial scale. The computer vision with high accuracy is also inconvenient to arranging the equipment and facilities, and then collecting data under the harsh field environment and vast area, compared with the indoor intensive farming mode. The traditional manual observation cannot fully meet the requirement of large-scale production at this stage, due to the low efficiency and high subjectivity. Contact sensors can be expected to identify the behavior of sheep for the relationship between sheep and grassland environment, particularly with the rapid development of key technologies, such as communication and big data. Behavior monitoring included sustained behavior (grazing, walking, ruminating, running, and resting), and transient behavior (parturition, estrus, and urination) recognition. A systematic analysis has been implemented to determine the feed intake distribution, feeding intensity distribution, grazing intensity, and the spatiotemporal change of sheep in the field of grassland. A theoretical basis and technical means can be provided for the formulation of a grazing system in the monitoring of livestock and grass conditions for accurate animal husbandry. In this review, three commonly-used sensor technologies were introduced in sheep behavior monitoring at present. Data acquisition, processing, feature extraction, and modelling were also analyzed to summarize the existing methods and challenges in the key technologies. The research object was mainly for the large ruminants, where the research on sheep behavior monitoring was still in its infancy. There were many similarities between sheep and cattle, as the main grazing livestock in grassland. There were still some differences to need further exploration. Tri-Axis acceleration sensors were selected to compare the different deployment positions and time intervals. Appropriate positions and time intervals were then optimized, according to the target behavior combined with the grassland condition. However it is still lacking to consider the sampling frequency. The satellite signal was generally blocked by some obstacles in the condition of free grazing, which was easy to cause data loss. Current imaging data was collected from the artificial pastures with the small area, in order to simulate the living environment of sheep. But there were still differences from the real complex pastures. It was difficult to replace the equipment in the vast pasture area as the survival basis for sheep grazing. Frequent equipment replacement should be given special attention to reducing human participation in intelligent grazing management. In addition, the application status of key technologies in the different behaviors was illustrated in combination with the behavior pattern of sheep. Finally, the development direction of the contact sensor was proposed in the future. Firstly, the multi-sensor fusion system can be combined as complementary, according to the characteristics of the sensor, in order to more accurately infer the behavior of sheep, and then assess the health status of grazing pasture. Secondly, a deep learning network can be utilized to analyze the image data, in order to dig deeper and then distinguish the feature information. The manual extraction of features can be reduced to overcome the data imbalance and insufficient data for the recognition of complex patterns. A lightweight end-to-end network model was established for deployment in the embedded systems or mobile devices, fully meeting the practical applications. © 2023 Chinese Society of Agricultural Engineering. All rights reserved.","behavior recognition; ecological environment; grazing; sensors; sheep","Acceleration; Behavioral research; Data acquisition; Data handling; Ecosystems; Efficiency; Farms; Sustainable development; Behaviour monitoring; Behaviour recognition; Condition; Ecological environments; Grazing; Health status; Key technologies; Sensor technologies; Sheep; Sheep behaviour; Mammals","","","T. Liu; College of Computer and Information Engineering, Tianjin Agricultural University, Tianjin, 300384, China; email: tonghai_1227@126.com","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Review","Final","","Scopus","2-s2.0-85179488261"
"Franzo G.; Legnardi M.; Faustini G.; Tucciarone C.M.; Cecchinato M.","Franzo, Giovanni (55961557100); Legnardi, Matteo (57200640902); Faustini, Giulia (57604891100); Tucciarone, Claudia Maria (56593241700); Cecchinato, Mattia (6506249397)","55961557100; 57200640902; 57604891100; 56593241700; 6506249397","When Everything Becomes Bigger: Big Data for Big Poultry Production","2023","Animals","13","11","1804","","","","18","10.3390/ani13111804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163061048&doi=10.3390%2fani13111804&partnerID=40&md5=49abc6ea64d74ae5b6e6cfeafcef8f4b","Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy","Franzo G., Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy; Legnardi M., Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy; Faustini G., Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy; Tucciarone C.M., Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy; Cecchinato M., Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy","In future decades, the demand for poultry meat and eggs is predicted to considerably increase in pace with human population growth. Although this expansion clearly represents a remarkable opportunity for the sector, it conceals a multitude of challenges. Pollution and land erosion, competition for limited resources between animal and human nutrition, animal welfare concerns, limitations on the use of growth promoters and antimicrobial agents, and increasing risks and effects of animal infectious diseases and zoonoses are several topics that have received attention from authorities and the public. The increase in poultry production must be achieved mainly through optimization and increased efficiency. The increasing ability to generate large amounts of data (“big data”) is pervasive in both modern society and the farming industry. Information accessibility—coupled with the availability of tools and computational power to store, share, integrate, and analyze data with automatic and flexible algorithms—offers an unprecedented opportunity to develop tools to maximize farm profitability, reduce socio-environmental impacts, and increase animal and human health and welfare. A detailed description of all topics and applications of big data analysis in poultry farming would be infeasible. Therefore, the present work briefly reviews the application of sensor technologies, such as optical, acoustic, and wearable sensors, as well as infrared thermal imaging and optical flow, to poultry farming. The principles and benefits of advanced statistical techniques, such as machine learning and deep learning, and their use in developing effective and reliable classification and prediction models to benefit the farming system, are also discussed. Finally, recent progress in pathogen genome sequencing and analysis is discussed, highlighting practical applications in epidemiological tracking, and reconstruction of microorganisms’ population dynamics, evolution, and spread. The benefits of the objective evaluation of the effectiveness of applied control strategies are also considered. Although human-artificial intelligence collaborations in the livestock sector can be frightening because they require farmers and employees in the sector to adapt to new roles, challenges, and competencies—and because several unknowns, limitations, and open-ended questions are inevitable—their overall benefits appear to be far greater than their drawbacks. As more farms and companies connect to technology, artificial intelligence (AI) and sensing technologies will begin to play a greater role in identifying patterns and solutions to pressing problems in modern animal farming, thus providing remarkable production-based and commercial advantages. Moreover, the combination of diverse sources and types of data will also become fundamental for the development of predictive models able to anticipate, rather than merely detect, disease occurrence. The increasing availability of sensors, infrastructures, and tools for big data collection, storage, sharing, and analysis—together with the use of open standards and integration with pathogen molecular epidemiology—have the potential to address the major challenge of producing higher-quality, more healthful food on a larger scale in a more sustainable manner, thereby protecting ecosystems, preserving natural resources, and improving animal and human welfare and health. © 2023 by the authors.","big data; deep learning; IoT; machine learning; pathogens; poultry; production; sensors; sequencing","antiinfective agent; growth promotor; probiotic agent; volatile organic compound; acoustic technology; agricultural land; air velocity; algorithm; animal welfare; artificial intelligence; artificial neural network; automation; Avian infectious bronchitis virus; bioinformatics; Campylobacter; classification; coccidiosis; data processing; deep learning; environmental impact; farming system; gene expression; generalizability; genotyping; health care planning; high throughput sequencing; human; infection; infrared thermal imaging; innate immunity; internet of things; intervention study; livestock production; lung ventilation; machine learning; metabolic pattern; metagenomics; microbial community; molecular epidemiology; near infrared spectroscopy; nonhuman; nutrition; optical flow analysis; performance; phylodynamics; phylogeny; population density; population dynamics; population growth; population structure; poultry; poultry farming; poultry meat; poultry production; process optimization; Raman spectrometry; Review; social interaction; sound analysis; spermatozoon motility; support vector machine; temperature; temperature stress; thermography; third generation sequencing; whole genome sequencing; workflow; zoonosis","","","G. Franzo; Department of Animal Medicine, Production and Health (MAPS), University of Padua, Legnaro, 35020, Italy; email: giovanni.franzo@unipd.it","","MDPI","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85163061048"
"Naik B.N.; Ramanathan M.; Ponnusamy P.","Naik, Bhookya Nageswararao (57216789573); Ramanathan, Malmathanraj (57204091550); Ponnusamy, Palanisamy (57212368594)","57216789573; 57204091550; 57212368594","Refined single-stage object detection deep-learning technique for chilli leaf disease detection","2023","Journal of Electronic Imaging","32","3","033039","","","","4","10.1117/1.JEI.32.3.033039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164037909&doi=10.1117%2f1.JEI.32.3.033039&partnerID=40&md5=9a51934db6e601a72c4f274b65d77dda","National Institute of Technology, Department of Ece, Tiruchirapalli, India","Naik B.N., National Institute of Technology, Department of Ece, Tiruchirapalli, India; Ramanathan M., National Institute of Technology, Department of Ece, Tiruchirapalli, India; Ponnusamy P., National Institute of Technology, Department of Ece, Tiruchirapalli, India","Agriculture is a crucial sector for every country's economy, as it provides livestock and crops that are essential for farmers and their families' survival. Crop diseases significantly impact farmers' livelihood, making early detection and accurate diagnosis essential for informed crop management decisions that can improve yield and crop quality. While the manual diagnosis of plant diseases has limitations, research on automatic plant disease identification and categorization using computer vision algorithms is a popular research topic. We focus on leaf disease recognition and compare classification techniques to object detection techniques. Classification techniques in leaf disease recognition involve assigning an input image of a leaf to one of the pre-defined classes or categories, such as ""healthy""or ""diseased.""Object detection techniques, on the other hand, involve identifying the presence of an object of interest (e.g., a leaf) in an image and locating its boundaries, in addition to classifying the object. The advantage of object detection techniques over classification techniques in leaf disease recognition is that they provide additional information about the location of the disease on the leaf, which can be useful for tasks such as disease diagnosis and monitoring. In our work, 5100 chilli leaf images of 6 different classes were collected and labeled using the makeSense artificial intelligence (AI) annotating tool; we applied the YOLOv5s object detection model, producing a precision of 0.951, recall of 0.926, mAP@0.5 of 0.959, mAP@0.5:0.95 of 0.826, and accuracy of 95%. We also proposed an enhanced single-stage object detection model that employs the exponential linear unit activation function in the convolution layers and the sigmoid activation function at the output layer, resulting in a precision of 0.998, recall of 0.998, mAP@0.5 of 0.995, mAP@0.5:0.95 of 0.975, and accuracy of 99.83% for the chilli leaf dataset.  © 2023 SPIE and IS & T.","chilli leaf disease; classification; feature fusion; image annotation; mean average precision; object detection; YOLOv5s","Chemical activation; Crops; Deep learning; Image annotation; Learning systems; Object detection; Object recognition; Chili leaf disease; Classification technique; Features fusions; Image annotation; Leaf disease; Mean average precision; Objects detection; Plant disease; Single stage; YOLOv5; Classification (of information)","NIT Trichy","The first author (Bhookya Nageswararao Naik) gratefully acknowledges the NIT Trichy for providing financial assistance through Research Assistantship. ","B.N. Naik; National Institute of Technology, Department of Ece, Tiruchirapalli, India; email: nagnaik133@gmail.com","","SPIE","10179909","","JEIME","","English","J. Electron. Imaging","Article","Final","","Scopus","2-s2.0-85164037909"
"Naikar M.L.; Nandeppanavar A.S.; Kudari M.","Naikar, Manjunath L (59126594600); Nandeppanavar, Anupama S (42962100400); Kudari, Medha (57202606351)","59126594600; 42962100400; 57202606351","Lumpy Skin Disease Detection Using GUI Based Deep Learning Model in Cattle","2023","Proceedings - 2023 3rd International Conference on Ubiquitous Computing and Intelligent Information Systems, ICUIS 2023","","","","182","187","5","3","10.1109/ICUIS60567.2023.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193052975&doi=10.1109%2fICUIS60567.2023.00039&partnerID=40&md5=4adca345b23112bd80b756367f530a75","K.L.E. Institute of Technology, Dept. of Master of Computer Applications, Hubballi, 580 027, India","Naikar M.L., K.L.E. Institute of Technology, Dept. of Master of Computer Applications, Hubballi, 580 027, India; Nandeppanavar A.S., K.L.E. Institute of Technology, Dept. of Master of Computer Applications, Hubballi, 580 027, India; Kudari M., K.L.E. Institute of Technology, Dept. of Master of Computer Applications, Hubballi, 580 027, India","Cattle are susceptible to the extremely contagious viral infection known as Lumpy Skin Disease (LSD), which results in skin lesions and substantial financial losses for the livestock industry. Visual Geometry Group 16(VGG16), Visual Geometry Group 19(VGG19), and DenseNet121 are a few examples of Convolutional Neural Networks (CNNs) that have shown success in correctly diagnosing LSD lesions. Transfer learning is frequently used to optimize previously trained models on certain datasets. Model performance is measured using evaluation measures like accuracy, precision, recall, and F1-score. In comparison to VGG16 and VGG19, CNN and DenseNet121 provide a balance between accuracy and processing economy. To implement effective control measures and lessen the financial burden on cow populations, it is important to accurately classify LSD lesions. The proposed system with DenseNet121 model enables early detection and intervention. DenseNet121 has given the highest accuracy of 99.10% with approximately balanced dataset of size 4759. The proposed model also includes an attractive user interface to display and compare results of all models for a given input image. Categorization report, a confusion matrix, and a test accuracy calculation all are obtained as results. © 2023 IEEE.","accuracy; computational efficiency; control strategies; Convolutional Neural Networks; dataset; DenseNet121; disease management; early detection; F1-score; pre-trained models; precision; recall; transfer learning; Visual Geometry Group 16; Visual Geometry Group19","Agriculture; Computational efficiency; Convolution; Convolutional neural networks; Deep learning; Dermatology; Diagnosis; Disease control; Diseases; Learning systems; Losses; Transfer learning; Accuracy; Control strategies; Convolutional neural network; Dataset; Densenet121; Disease management; Early detection; F1 scores; Pre-trained model; Precision; Recall; Transfer learning; Visual geometry group 16; Visual geometry group19; Geometry","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030698-9","","","English","Proc. - Int. Conf. Ubiquitous Comput. Intell. Inf. Syst., ICUIS","Conference paper","Final","","Scopus","2-s2.0-85193052975"
"Gan H.; Guo J.; Liu K.; Deng X.; Zhou H.; Luo D.; Chen S.; Norton T.; Xue Y.","Gan, Haiming (56531479600); Guo, Jingfeng (57557116900); Liu, Kai (55823366100); Deng, Xinru (58235036600); Zhou, Hui (58257864200); Luo, Dehuan (58235257700); Chen, Shiyun (58235257800); Norton, Tomas (35273348100); Xue, Yueju (12241464400)","56531479600; 57557116900; 55823366100; 58235036600; 58257864200; 58235257700; 58235257800; 35273348100; 12241464400","Counting piglet suckling events using deep learning-based action density estimation","2023","Computers and Electronics in Agriculture","210","","107877","","","","5","10.1016/j.compag.2023.107877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158046186&doi=10.1016%2fj.compag.2023.107877&partnerID=40&md5=eb6f8ae424d45493d3ca370b4928a9eb","College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Faculty of Bioscience Engineering, Katholieke Universiteit Leuven (KU LEUVEN), Kasteelpark Arenberg 30, Heverlee/ Leuven, 3001, Belgium; Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong; Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China","Gan H., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China, Faculty of Bioscience Engineering, Katholieke Universiteit Leuven (KU LEUVEN), Kasteelpark Arenberg 30, Heverlee/ Leuven, 3001, Belgium, Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong, Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China; Guo J., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Liu K., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong; Deng X., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Zhou H., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Luo D., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Chen S., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Norton T., Faculty of Bioscience Engineering, Katholieke Universiteit Leuven (KU LEUVEN), Kasteelpark Arenberg 30, Heverlee/ Leuven, 3001, Belgium; Xue Y., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China, Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China","Analysis of piglet suckling behaviour is important for the evaluation of piglet nutrient ingestion, health, welfare, and affinity with the sow. In this study, an action density estimation network (ADEN) was proposed for counting the events of piglet suckling followed by automated analysis of suckling behaviour. ADEN is a two-stream network primarily composed of 1) a network stream that processes video images with a higher frame rate (faster stream) and 2) a network stream that processes video images with a lower frame rate (slower stream). Each stream consists of a ResNet-50 with five convolutional stages. A multi-stage attention connection (MSAC), composed of four Spatial-Temporal-Channel (STC) multi-attention structures, is proposed to bridge Faster Stream and Slower Stream and capture discriminative features. The output attention features from each faster stream stage are laterally fused into the corresponding slower stream stage in a concatenating manner. Following this, the features from the last convolutional stages in the Slow stream and Fast stream are fused using concatenation and are then decoded by using three convolutional layers. The last convolutional layer outputs a heatmap on the action density of piglet suckling behaviour. Finally, the number of suckling events is predicted by integrating all the pixel values in the heatmap. Experimental and comparative tests were conducted to validate the effectiveness of the proposed ADEN with a training dataset and a test dataset from 14 pig pens. The 507 video clips (126,750 images for 7 h) from the 1-9th pens were selected as training datasets. The 143 video clips (35,750 images for 2 h) from the 10-13th pens were selected as short-term test datasets. One untrimmed video (162,000 images for 9 h) from the 14th pen was used to ultimately evaluate the action density estimation performance of the ADEN. ADEN was compared with seven approaches and its superiority was demonstrated with an r = 0.938, an RMSE = 1.080, and a MAE = 0.967 in short video clips and r = 0.982, MAE = 0.161, and RMSE = 0.563 in the untrimmed long video. The ADEN proved it feasible to predict the number of suckling events by using action density estimation. © 2023 Elsevier B.V.","Action density; Piglet suckling behaviour; Precision livestock farming; Two-stream network","Convolution; Farms; Mammals; Statistical tests; Video cameras; Action density; Density estimation; Network streams; Piglet suckling behavior; Precision livestock farming; Process video; Stream networks; Two-stream; Two-stream network; Video-clips; data set; experimental study; livestock farming; pig; population density; prediction; stream; suckling; training; Deep learning","Guangdong International Technology Cooperation Project, (2021A0505030058); China Postdoctoral Science Foundation, (2022M721200); South China Agricultural University, SCAU","Funding text 1: Funding was provided in part by the “Special and grand field project for Guangdong Province Higher Education” (grant numbers: 2020ZDZX1041), the “China Postdoctoral Science Foundation” (grant numbers: 2022M721200), and the “Guangdong International Technology Cooperation Project” (grant numbers: 2021A0505030058). ; Funding text 2: Funding was provided in part by the “Special and grand field project for Guangdong Province Higher Education” (grant numbers: 2020ZDZX1041), the “China Postdoctoral Science Foundation” (grant numbers: 2022M721200), and the “Guangdong International Technology Cooperation Project” (grant numbers: 2021A0505030058). All of the animal experiments in this study were conducted in compliance with a protocol reviewed and approved by the Institutional Animal Care and Use Committee at South China Agricultural University.","T. Norton; College of Electronic Engineering, South China Agricultural University, Guangzhou, Wushan 483, Guangdong, China; email: tomas.norton@kuleuven.be","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85158046186"
"Han S.; Fuentes A.; Yoon S.; Jeong Y.; Kim H.; Sun Park D.","Han, Shujie (57201776906); Fuentes, Alvaro (57194569998); Yoon, Sook (35779575000); Jeong, Yongchae (7202332048); Kim, Hyongsuk (23109059600); Sun Park, Dong (55256148900)","57201776906; 57194569998; 35779575000; 7202332048; 23109059600; 55256148900","Deep learning-based multi-cattle tracking in crowded livestock farming using video","2023","Computers and Electronics in Agriculture","212","","108044","","","","16","10.1016/j.compag.2023.108044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165539571&doi=10.1016%2fj.compag.2023.108044&partnerID=40&md5=87a5d12d6566933d6cb45b85aa2427ec","Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea; Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju, South Korea; Department of Computer Engineering, Mokpo National University, Muan, South Korea; Division of Electronics and Information Engineering, IT Convergence Research Center, Jeonbuk National University, Jeonju, South Korea","Han S., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea, Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju, South Korea; Fuentes A., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea, Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju, South Korea; Yoon S., Department of Computer Engineering, Mokpo National University, Muan, South Korea; Jeong Y., Division of Electronics and Information Engineering, IT Convergence Research Center, Jeonbuk National University, Jeonju, South Korea; Kim H., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea, Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju, South Korea; Sun Park D., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea, Core Institute of Intelligent Robots, Jeonbuk National University, Jeonju, South Korea","Cattle monitoring is an essential aspect of precision farming, and recent advancements have greatly contributed to understanding cattle behavior using wearable devices like ear tags and collars, as well as contactless cameras for image-based detection. However, tracking multiple cattle in real farm conditions with cameras, particularly in crowded scenarios, poses significant challenges mainly due to scale variations, random motion, and occlusion. This paper proposes a deep learning-based framework with improved techniques for multi-cattle tracking using video, aiming to overcome these limitations. The proposed algorithm utilizes a detection-based tracking approach, leveraging a YOLO-v5 detector trained specifically for cattle detection to provide initial targets. The main contributions of our research primarily focus on implementing the tracking algorithm to address the aforementioned problems. Several improvements are introduced: first, to handle appearance and scale deformation, a wide residual network with SPP-Net is employed as the backbone to extract cattle appearance information. Second, an ensemble Kalman filter is utilized to adapt to unexpected movements. Additionally, the angle from the centered position of the individuals to the origin of the image is incorporated to predict their location. Third, to tackle occlusion, a novel bench-matching mechanism is designed, allowing for the retrieval of lost trajectories based on the assumption of a known number of cattle in the barn. To validate the performance of the proposed framework, experiments are conducted using video sequences from our Hanwoo cattle tracking dataset. Comparisons with other state-of-the-art trackers are also performed. Our method achieves an accuracy of 84.49 % in data association, which represents a significant improvement considering the challenges involved in precision livestock farming applications. © 2023 Elsevier B.V.","Cattle tracking; Crowded livestock farming; Deep learning; Indoor environment; Video","Agriculture; Deep learning; Cattle monitoring; Cattle tracking; Contact less; Crowded livestock farming; Deep learning; Indoor environment; Livestock farming; Precision-farming; Video; Wearable devices; algorithm; Kalman filter; livestock farming; tracking; videography; Cameras","Korea Smart Farm R&D Foundation; Ministry of Education, MOE, (2019R1A6A1A09031717); Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (1545027423, 2020R1A2C2013060); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET; National Research Foundation of Korea, NRF","This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Education (No. 2019R1A6A1A09031717); by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (RDA)(1545027423); and by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2020R1A2C2013060). ","H. Kim; Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea; email: hskim@jbnu.ac.kr","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85165539571"
"Hammouda N.; Mahfoudh M.; Boukadi K.","Hammouda, Nourelhouda (57820817500); Mahfoudh, Mariem (55803746300); Boukadi, Khouloud (24400647700)","57820817500; 55803746300; 24400647700","MoonCAB : a Modular Ontology for Computational analysis of Animal Behavior","2023","Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA","","","","","","","1","10.1109/AICCSA59173.2023.10479355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190137235&doi=10.1109%2fAICCSA59173.2023.10479355&partnerID=40&md5=0b8fb3e4b01a4e06dfe80e6411055688","University of Sfax, Sfax, Tunisia; Miracl Laboratory, Sfax, Tunisia; University of Kairouan, Kairouan, Tunisia","Hammouda N., University of Sfax, Sfax, Tunisia, Miracl Laboratory, Sfax, Tunisia; Mahfoudh M., University of Sfax, Sfax, Tunisia, Miracl Laboratory, Sfax, Tunisia, University of Kairouan, Kairouan, Tunisia; Boukadi K., University of Sfax, Sfax, Tunisia, Miracl Laboratory, Sfax, Tunisia","Computational analysis of animal behavior (CABA) is a modern approach to studying animal behavior using computer techniques. It provides tools for smart farming and analyzes videos mostly with machine learning and deep learning algorithms. The paper aims to integrate ontology in the field of CABA. An ontology is a formal representation of knowledge that can facilitate the integration of data from different sources, allowing experts to better understand and study animal health and behavior. We propose to build a modular ontology based on the Modular Ontology Modeling (MOMo) methodology. The proposed ontology, MoonCAB (Modular ontology for Computational analysis of Animal Behavior), represents the behavior of livestock animals (sheep and goats) in a pasture: the duration of each activity, the meaning of each duration, the season during which these activities take place, etc. Our ontology is composed of 68 classes, 36 properties, 150 individuals, and 8135 axioms. It has been tested by Fact++ reasoner and SPARQL queries.  © 2023 IEEE.","Animal Behavior; Building Ontology; CABA; Modular Ontology; Smart Farming","Agriculture; Animals; Autonomous agents; Computational methods; Deep learning; Learning algorithms; Animal behaviour; Building ontology; Computational analyse of animal behavior; Computational analysis; Computer techniques; Machine-learning; Modular ontologies; Ontology's; Smart farming; Ontology","","","N. Hammouda; University of Sfax, Sfax, Tunisia; email: hamouda.nourelhouda@gmail.com","","IEEE Computer Society","21615322","979-835031943-9","","","English","Proc. IEEE/ACS Int. Conf. Comput. Syst. Appl., AICCSA","Conference paper","Final","","Scopus","2-s2.0-85190137235"
"Ma W.; Fu Y.; Bao Y.; Wang Z.; Lei B.; Zheng W.; Wang C.; Liu Y.","Ma, Wenlong (57203481073); Fu, Yang (57715971500); Bao, Yongzhou (58533515300); Wang, Zhen (59293330800); Lei, Bowen (58533279500); Zheng, Weigang (57205272631); Wang, Chao (57862339200); Liu, Yuwen (56023509100)","57203481073; 57715971500; 58533515300; 59293330800; 58533279500; 57205272631; 57862339200; 56023509100","DeepSATA: A Deep Learning-Based Sequence Analyzer Incorporating the Transcription Factor Binding Affinity to Dissect the Effects of Non-Coding Genetic Variants","2023","International Journal of Molecular Sciences","24","15","12023","","","","1","10.3390/ijms241512023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167761695&doi=10.3390%2fijms241512023&partnerID=40&md5=30528a7986f24b8e5a6d14fa816b4c30","Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China; Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China; School of Life Sciences, Henan University, Kaifeng, 475004, China; Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Lab of Swine Genetics and Breeding of Ministry of Agriculture and Rural Affairs, Huazhong Agricultural University, Wuhan, 430070, China; Kunpeng Institute of Modern Agriculture at Foshan, Chinese Academy of Agricultural Sciences, Foshan, 528226, China","Ma W., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China; Fu Y., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China; Bao Y., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, School of Life Sciences, Henan University, Kaifeng, 475004, China; Wang Z., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, School of Life Sciences, Henan University, Kaifeng, 475004, China; Lei B., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Lab of Swine Genetics and Breeding of Ministry of Agriculture and Rural Affairs, Huazhong Agricultural University, Wuhan, 430070, China; Zheng W., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Lab of Swine Genetics and Breeding of Ministry of Agriculture and Rural Affairs, Huazhong Agricultural University, Wuhan, 430070, China; Wang C., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction of Ministry of Education & Key Lab of Swine Genetics and Breeding of Ministry of Agriculture and Rural Affairs, Huazhong Agricultural University, Wuhan, 430070, China; Liu Y., Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Innovation Group of Pig Genome Design and Breeding, Research Centre for Animal Genome, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China, Kunpeng Institute of Modern Agriculture at Foshan, Chinese Academy of Agricultural Sciences, Foshan, 528226, China","Utilizing large-scale epigenomics data, deep learning tools can predict the regulatory activity of genomic sequences, annotate non-coding genetic variants, and uncover mechanisms behind complex traits. However, these tools primarily rely on human or mouse data for training, limiting their performance when applied to other species. Furthermore, the limited exploration of many species, particularly in the case of livestock, has led to a scarcity of comprehensive and high-quality epigenetic data, posing challenges in developing reliable deep learning models for decoding their non-coding genomes. The cross-species prediction of the regulatory genome can be achieved by leveraging publicly available data from extensively studied organisms and making use of the conserved DNA binding preferences of transcription factors within the same tissue. In this study, we introduced DeepSATA, a novel deep learning-based sequence analyzer that incorporates the transcription factor binding affinity for the cross-species prediction of chromatin accessibility. By applying DeepSATA to analyze the genomes of pigs, chickens, cattle, humans, and mice, we demonstrated its ability to improve the prediction accuracy of chromatin accessibility and achieve reliable cross-species predictions in animals. Additionally, we showcased its effectiveness in analyzing pig genetic variants associated with economic traits and in increasing the accuracy of genomic predictions. Overall, our study presents a valuable tool to explore the epigenomic landscape of various species and pinpoint regulatory deoxyribonucleic acid (DNA) variants associated with complex traits. © 2023 by the authors.","chromatin accessibility; cross-species prediction; deep learning; genomic prediction; non-coding variants; transcription factor binding affinity","Animals; Cattle; Chickens; Chromatin; Deep Learning; DNA; Humans; Mice; Swine; Transcription Factors; transcription factor; DNA; transcription factor; animal experiment; animal genetics; animal tissue; Article; binding affinity; binding site; chromatin; computer prediction; controlled study; deep learning; Duroc pig; female; Gallus gallus; genetic variability; human; human genome; human tissue; male; mouse; Mus musculus; nonhuman; sequence analysis; taurine cattle; animal; bovine; genetics; metabolism; pig","National Natural Science Foundation of China, NSFC, (32070595); National Natural Science Foundation of China, NSFC; National Key Research and Development Program of China, NKRDPC, (2021YFF1000600, 2021YFF1200503); National Key Research and Development Program of China, NKRDPC","This research was funded by the National Key R&D Program of China (2021YFF1000600) to L.Y, the China National Key R&D Program during the 14th Five-year Plan Period (2021YFF1200503) to L.Y, and the National Natural Science Foundation of China (32070595) to L.Y.","Y. Liu; Shenzhen Branch, Guangdong Laboratory for Lingnan Modern Agriculture, Key Laboratory of Livestock and Poultry Multi-Omics of MARA, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, 518124, China; email: liuyuwen@caas.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","16616596","","","37569400","English","Int. J. Mol. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85167761695"
"Chandralekha E.; Muzammil Ali A.; Ritesh V.; Srinivasan M.K.","Chandralekha, E. (56266864800); Muzammil Ali, A. (59010040100); Ritesh, V. (59009234300); Srinivasan, Muthu Kumar (58821870900)","56266864800; 59010040100; 59009234300; 58821870900","Animal Intrusion Detection System using SIFT Features and Transfer Learning with MobileNetV2","2023","3rd International Conference on Innovative Mechanisms for Industry Applications, ICIMIA 2023 - Proceedings","","","","1339","1344","5","1","10.1109/ICIMIA60377.2023.10426584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191842009&doi=10.1109%2fICIMIA60377.2023.10426584&partnerID=40&md5=a2798a473f58ee2075aa98c0af44388d","CSE Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India; AI&DS Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India","Chandralekha E., CSE Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India; Muzammil Ali A., AI&DS Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India; Ritesh V., AI&DS Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India; Srinivasan M.K., AI&DS Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India","Agriculture's impact on India's economy is substantial, offering employment for a significant part of the population and contributing to livelihoods. It plays a crucial role in ensuring food security, providing industrial raw materials, and supporting exports. Yet, agricultural challenges arise from animal-related issues like crop damage, livestock attacks, disease transmission, and farming infrastructure disruptions. These problems hinder productivity, necessitating sustainable farming strategies. Deep learning advancements enhance animal intrusion detection by autonomously analyzing data patterns for accurate identification of wildlife, thus improving farm security. A unique intrusion detection model is introduced, utilizing architectural augmentation and transfer learning. The model, built on MobileNetV2 architecture, successfully detects intruders across diverse image categories, bolstered by a data generator and custom layers. Effective optimization techniques are employed in model construction, and the trained model is stored for future intrusion detection use.  © 2023 IEEE.","Animal Intrusion Detection; Convolutional Neural Network; Data Augmentation; MobileNetV2; Transfer Learning","Animals; Convolutional neural networks; Deep learning; Farms; Food supply; Learning systems; Animal intrusion detection; Convolutional neural network; Data augmentation; Feature learning; Food security; Intrusion Detection Systems; Intrusion-Detection; Mobilenetv2; SIFT Feature; Transfer learning; Intrusion detection","","","E. Chandralekha; CSE Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology Avadi, Chennai, India; email: clchandralekha08@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835034363-2","","","English","Int. Conf. Innov. Mech. Ind. Appl., ICIMIA - Proc.","Conference paper","Final","","Scopus","2-s2.0-85191842009"
"Ji H.; Teng G.; Yu J.; Wen Y.; Deng H.; Zhuang Y.","Ji, Hengyi (57223181066); Teng, Guanghui (10140843100); Yu, Jionghua (57204037705); Wen, Yanbin (57402247900); Deng, Huixiang (58492416800); Zhuang, Yanrong (57204034561)","57223181066; 10140843100; 57204037705; 57402247900; 58492416800; 57204034561","Efficient Aggressive Behavior Recognition of Pigs Based on Temporal Shift Module","2023","Animals","13","13","2078","","","","10","10.3390/ani13132078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164983233&doi=10.3390%2fani13132078&partnerID=40&md5=0e2aa36ff658cfc00b1c53dc8fe64dea","College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Bureau of Agricultural and Rural Affairs, Datong, 037000, China","Ji H., College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Teng G., College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Yu J., Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Wen Y., Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Bureau of Agricultural and Rural Affairs, Datong, 037000, China; Deng H., College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Zhuang Y., College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China","Aggressive behavior among pigs is a significant social issue that has severe repercussions on both the profitability and welfare of pig farms. Due to the complexity of aggression, recognizing it requires the consideration of both spatial and temporal features. To address this problem, we proposed an efficient method that utilizes the temporal shift module (TSM) for automatic recognition of pig aggression. In general, TSM is inserted into four 2D convolutional neural network models, including ResNet50, ResNeXt50, DenseNet201, and ConvNext-t, enabling the models to process both spatial and temporal features without increasing the model parameters and computational complexity. The proposed method was evaluated on the dataset established in this study, and the results indicate that the ResNeXt50-T (TSM inserted into ResNeXt50) model achieved the best balance between recognition accuracy and model parameters. On the test set, the ResNeXt50-T model achieved accuracy, recall, precision, F1 score, speed, and model parameters of 95.69%, 95.25%, 96.07%, 95.65%, 29 ms, and 22.98 M, respectively. These results show that the proposed method can effectively improve the accuracy of recognizing pig aggressive behavior and provide a reference for behavior recognition in actual scenarios of smart livestock farming. © 2023 by the authors.","behavior recognition; CNN; computer vision; deep learning; pigs","aggression; agricultural worker; article; computer vision; convolutional neural network; deep learning; livestock; nonhuman; pig; recall; residual neural network; velocity","Chongqing Technology Innovation and Application Development Project, (cstc2021jscx-dxwtBX0006)","This research was funded by the Chongqing Technology Innovation and Application Development Project (grant number: cstc2021jscx-dxwtBX0006).","G. Teng; College of Water Resources & Civil Engineering, China Agricultural University, Beijing, 100083, China; email: futong@cau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85164983233"
"Jiang B.; Tang W.; Cui L.; Deng X.","Jiang, Bing (57758826500); Tang, Wenjie (58151350100); Cui, Lihang (57571477900); Deng, Xiaoshang (57452099100)","57758826500; 58151350100; 57571477900; 57452099100","Precision Livestock Farming Research: A Global Scientometric Review","2023","Animals","13","13","2096","","","","14","10.3390/ani13132096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164742190&doi=10.3390%2fani13132096&partnerID=40&md5=7dc0e2107ad37bb1a56b65d62188399d","College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Development Research Center of Modern Agriculture, Northeast Agricultural University, Harbin, 150030, China","Jiang B., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China, Development Research Center of Modern Agriculture, Northeast Agricultural University, Harbin, 150030, China; Tang W., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Cui L., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; Deng X., College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China","Precision livestock farming (PLF) utilises information technology to continuously monitor and manage livestock in real-time, which can improve individual animal health, welfare, productivity and the environmental impact of animal husbandry, contributing to the economic, social and environmental sustainability of livestock farming. PLF has emerged as a pivotal area of multidisciplinary interest. In order to clarify the knowledge evolution and hotspot replacement of PLF research, based on the relevant data from the Web of Science database from 1973 to 2023, this study analyzed the main characteristics, research cores and hot topics of PLF research via CiteSpace. The results point to a significant increase in studies on PLF, with countries having advanced livestock farming systems in Europe and America publishing frequently and collaborating closely across borders. Universities in various countries have been leading the research, with Daniel Berckmans serving as the academic leader. Research primarily focuses on animal science, veterinary science, computer science, agricultural engineering, and environmental science. Current research hotspots center around precision dairy and cattle technology, intelligent systems, and animal behavior, with deep learning, accelerometer, automatic milking systems, lameness, estrus detection, and electronic identification being the main research directions, and deep learning and machine learning represent the forefront of current research. Research hot topics mainly include social science in PLF, the environmental impact of PLF, information technology in PLF, and animal welfare in PLF. Future research in PLF should prioritize inter-institutional and inter-scholar communication and cooperation, integration of multidisciplinary and multimethod research approaches, and utilization of deep learning and machine learning. Furthermore, social science issues should be given due attention in PLF, and the integration of intelligent technologies in animal management should be strengthened, with a focus on animal welfare and the environmental impact of animal husbandry, to promote its sustainable development. © 2023 by the authors.","animal welfare; bibliometrics; CiteSpace; precision livestock farming","accuracy; aflatoxicosis; air pollution; animal behavior; animal health; animal husbandry; animal lameness; animal welfare; artificial intelligence; bibliometrics; computer vision; decision making; deep learning; environmental science; estrus; germination; greenhouse effect; heart rate; heat stress; information technology; livestock; locomotion; machine learning; mastitis; nonhuman; Review; scientometrics; support vector machine; sustainable development; water quality","National Natural Science Foundation of China, NSFC, (71640017, 71773134, 72072125, 72203034); Natural Science Foundation of Heilongjiang Province, (LH2021G002)","This research was funded by the National Natural Science Foundation of China for Young Scholars (grant number 72203034); the Emergency Project Fund of National Natural Science Foundation of China (grant number 71640017); the General Program of National Natural Science Foundation of China (grant number 71773134); the General Program of National Natural Science Foundation of China (grant number 72072125); the Natural Science Foundation of Heilongjiang Province (grant number LH2021G002).","B. Jiang; College of Economics and Management, Northeast Agricultural University, Harbin, 150030, China; email: jiangbing2020@neau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85164742190"
"de Paula B.M.; da Silva G.R.; Ferreira S.E.; Maia B.L.C.; Almeida M.E.G.; Júnior V.M.S.; Maciel L.M.D.S.; Chaves A.S.","de Paula, Brenda Marques (57209497198); da Silva, Gabriel Rezende (58123669600); Ferreira, Sabrina Evelin (58123551900); Maia, Brian Luís Coimbra (58123669700); Almeida, Mathews Edwirds Gomes (58123552000); Júnior, Valdo Martins Soares (58123610200); Maciel, Luiz Maurílio da Silva (59158282700); Chaves, Amália Saturnino (56349785400)","57209497198; 58123669600; 58123551900; 58123669700; 58123552000; 58123610200; 59158282700; 56349785400","Dataset of feed bunk score images of cattle feedlot","2023","Data in Brief","47","","108996","","","","1","10.1016/j.dib.2023.108996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172134&doi=10.1016%2fj.dib.2023.108996&partnerID=40&md5=458724bfc454cd4e8d62d900c1dc6d48","Programa de Pós-Graduação em Produção Animal, Universidade Federal de Minas Gerais, MG, Montes Claros, Brazil; Programa de Pós-Graduação em Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Departamento de Medicina Veterinária, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Departamento de Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil","de Paula B.M., Programa de Pós-Graduação em Produção Animal, Universidade Federal de Minas Gerais, MG, Montes Claros, Brazil; da Silva G.R., Programa de Pós-Graduação em Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Ferreira S.E., Departamento de Medicina Veterinária, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Maia B.L.C., Departamento de Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Almeida M.E.G., Departamento de Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Júnior V.M.S., Programa de Pós-Graduação em Produção Animal, Universidade Federal de Minas Gerais, MG, Montes Claros, Brazil; Maciel L.M.D.S., Programa de Pós-Graduação em Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil, Departamento de Ciência da Computação, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil; Chaves A.S., Programa de Pós-Graduação em Produção Animal, Universidade Federal de Minas Gerais, MG, Montes Claros, Brazil, Departamento de Medicina Veterinária, Universidade Federal de Juiz de Fora, MG, Juiz de Fora, Brazil","Bunk management is an important technique to minimize the variations in consumption in feedlot cattle and can be performed according to the South Dakota State University classification system. The use of information and communication technology (ICT) can help, in an objective way, in the interpretation of these measurements. We created a dataset with the objective to develop an automatic classification method of feed bunk score. In May, September and October on the 2021 and September on the 2022 we captured 1511 images in the morning on the farms, in natural lighting conditions with different angles and backgrounds and at a height of about 1.5 m from the bunk. After acquisition data, each image was classified according to its score classification. Additionally, we resized the images to 500 × 500 pixels, generated annotations files, and organized the dataset in folders. The images in this dataset can be used to train and validate a machine learning model to classify feed bunk images. This model can be used to develop an application to support bunk management. © 2023 The Author(s)","Animal nutrition; Computer vision; Deep learning; Precision livestock","Agriculture; Classification (of information); Deep learning; Image classification; Information use; Animal nutritions; Automatic classification; Cattle feedlots; Classification system; Deep learning; Feedlot cattles; Information and Communication Technologies; Management IS; Precision livestock; South dakotas; Computer vision","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq; Fundação de Amparo à Pesquisa do Estado de Minas Gerais, FAPEMIG; Universidade Federal de Minas Gerais, UFMG","The authors would like to thank the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brazil (CAPES), Fundação de Amparo a Pesquisa do Estado de Minas Gerais (FAPEMIG), Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) and Programa de Apoio à Pós-Graduação (PROAP) at the Universidade Federal de Minas Gerais.","L.M.D.S. Maciel; Programa de Pós-Graduação em Ciência da Computação, Universidade Federal de Juiz de Fora, Juiz de Fora, MG, Brazil; email: luiz.maciel@ufjf.br","","Elsevier Inc.","23523409","","","","English","Data Brief","Data paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85149172134"
"Hayes B.J.; Chen C.; Powell O.; Dinglasan E.; Villiers K.; Kemper K.E.; Hickey L.T.","Hayes, Ben J. (7101806388); Chen, Chensong (58513475900); Powell, Owen (57221339021); Dinglasan, Eric (57188556644); Villiers, Kira (57431019600); Kemper, Kathryn E. (25825124500); Hickey, Lee T. (26428733700)","7101806388; 58513475900; 57221339021; 57188556644; 57431019600; 25825124500; 26428733700","Advancing artificial intelligence to help feed the world","2023","Nature Biotechnology","41","9","","1188","1189","1","10","10.1038/s41587-023-01898-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166224904&doi=10.1038%2fs41587-023-01898-2&partnerID=40&md5=f6c29d3879443b2d86b5025c238c38f1","Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Institute for Molecular Bioscience, University of Queensland, Brisbane, QLD, Australia","Hayes B.J., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Chen C., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Powell O., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Dinglasan E., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Villiers K., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia; Kemper K.E., Institute for Molecular Bioscience, University of Queensland, Brisbane, QLD, Australia; Hickey L.T., Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, QLD, Australia","[No abstract available]","","Artificial Intelligence; animal welfare; artificial intelligence; attention network; breeding; comparative study; consumer; crop; deep learning; drought tolerance; food industry; gene editing; genetic gain; genome-wide association study; haplotype; inbreeding; infrared radiation; language; Letter; livestock; machine learning; mate choice; methane emission; milk; nonhuman; phenotype; radiation use efficiency; single nucleotide polymorphism","Australian Research Council, ARC, (LP 70100317)","The authors gratefully acknowledge funding from the Australian Research Council for linkage project LP 70100317 ‘Faststack’. ","B.J. Hayes; Queensland Alliance for Agriculture and Food Innovation, University of Queensland, Brisbane, Australia; email: b.hayes@uq.edu.au","","Nature Research","10870156","","NABIF","37524959","English","Nat. Biotechnol.","Letter","Final","","Scopus","2-s2.0-85166224904"
"Fuentes A.; Han S.; Nasir M.F.; Park J.; Yoon S.; Park D.S.","Fuentes, Alvaro (57194569998); Han, Shujie (57201776906); Nasir, Muhammad Fahad (58419887200); Park, Jongbin (56095724400); Yoon, Sook (35779575000); Park, Dong Sun (7403245797)","57194569998; 57201776906; 58419887200; 56095724400; 35779575000; 7403245797","Multiview Monitoring of Individual Cattle Behavior Based on Action Recognition in Closed Barns Using Deep Learning","2023","Animals","13","12","2020","","","","14","10.3390/ani13122020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163875236&doi=10.3390%2fani13122020&partnerID=40&md5=53de96e2b32439ab2c19b5154b814b22","Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea; Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Department of Computer Engineering, Mokpo National University, Muan, 58554, South Korea","Fuentes A., Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Han S., Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Nasir M.F., Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Park J., Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea; Yoon S., Department of Computer Engineering, Mokpo National University, Muan, 58554, South Korea; Park D.S., Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea, Core Research Institute of Intelligent Robots, Jeonbuk National University, Jeonju, 54896, South Korea","Cattle behavior recognition is essential for monitoring their health and welfare. Existing techniques for behavior recognition in closed barns typically rely on direct observation to detect changes using wearable devices or surveillance cameras. While promising progress has been made in this field, monitoring individual cattle, especially those with similar visual characteristics, remains challenging due to numerous factors such as occlusion, scale variations, and pose changes. Accurate and consistent individual identification over time is therefore essential to overcome these challenges. To address this issue, this paper introduces an approach for multiview monitoring of individual cattle behavior based on action recognition using video data. The proposed system takes an image sequence as input and utilizes a detector to identify hierarchical actions categorized as part and individual actions. These regions of interest are then inputted into a tracking and identification mechanism, enabling the system to continuously track each individual in the scene and assign them a unique identification number. By implementing this approach, cattle behavior is continuously monitored, and statistical analysis is conducted to assess changes in behavior in the time domain. The effectiveness of the proposed framework is demonstrated through quantitative and qualitative experimental results obtained from our Hanwoo cattle video database. Overall, this study tackles the challenges encountered in real farm indoor scenarios, capturing spatiotemporal information and enabling automatic recognition of cattle behavior for precision livestock farming. © 2023 by the authors.","animal welfare; cattle behavior; deep learning; indoor farm; precision livestock farming; video","agricultural worker; animal welfare; article; bovine; deep learning; livestock; nonhuman; videorecording","Korea Smart Farm R&D Foundation; Ministry of Education, MOE, (2019R1A6A1A09031717); Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (1545027423, 2020R1A2C2013060); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET; National Research Foundation of Korea, NRF","This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2019R1A6A1A09031717); by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, and Forestry (IPET) and the Korea Smart Farm R&D Foundation (KosFarm) through the Smart Farm Innovation Technology Development Program, funded by the Ministry of Agriculture, Food, and Rural Affairs (MAFRA) and the Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (1545027423); and by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (2020R1A2C2013060).","D.S. Park; Department of Electronics Engineering, Jeonbuk National University, Jeonju, 54896, South Korea; email: dspark@jbnu.ac.kr; S. Yoon; Department of Computer Engineering, Mokpo National University, Muan, 58554, South Korea; email: syoon@mokpo.ac.kr","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85163875236"
"Benayad M.; Houran N.; Aamir Z.; Maanan M.; Rhinane H.","Benayad, Mohamed (58115137400); Houran, Nouriddine (58115521500); Aamir, Zakaria (57224323495); Maanan, Mehdi (55540824800); Rhinane, Hassan (57189269615)","58115137400; 58115521500; 57224323495; 55540824800; 57189269615","GEOMEMBRANE BASINS DETECTION BASED ON SATELLITE HIGH-RESOLUTION IMAGERY USING DEEP LEARNING ALGORITHMS","2023","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","48","4/W6-2022","","75","79","4","2","10.5194/isprs-archives-XLVIII-4-W6-2022-75-2023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148767361&doi=10.5194%2fisprs-archives-XLVIII-4-W6-2022-75-2023&partnerID=40&md5=c3d608963bb0ab7b6b9db4de259d0502","Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco","Benayad M., Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco; Houran N., Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco; Aamir Z., Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco; Maanan M., Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco; Rhinane H., Geoscience laboratory, Department of geology, Faculty of Sciences Ain Chock, University Hassan II Casablanca, BP 5366, Casablanca, Morocco","Agriculture is a very important economic sector in Morocco, which requires a set of tools to improve agricultural production. Among these tools, the use of geomembrane basins. The latter is of great importance in smart farming planning, management practices or even in livestock use. In this context, this study evaluates a recognition and classification of the geomembrane basin using remote sensing satellite images; based on the Yolov3 deep learning neural network. This paper first adjusts the network model to make it suitable for detecting small targets on remote sensing images, then uses the k-means algorithm to calculate the grid size of the Yolo network model suitable for geomembrane basins, then uses yolov3 to train the data that makes up the satellite remote sensing imagery. The network model for the detection of the geomembrane basins is obtained by the test phase. Finally, the geomembrane basin detection model adapted to the remote sensing image is obtained in the validation phase. Through the research and analysis of the experimental results, it can be seen that this method effectively detects the geomembrane basins in the remote sensing images and ensures the high detection accuracy of the experimental results, which gave us an accuracy of 75%.  Copyright © 2023 M. Benayad et al.","Convolutional Neural Networks; Darknet; Deep learning; Detection; Environment; Geomembrane basin; Water save; Yolo","Convolutional neural networks; Deep learning; Farms; Geomembranes; K-means clustering; Learning algorithms; Learning systems; Satellite imagery; Convolutional neural network; Darknets; Deep learning; Detection; Environment; Geomembrane basin; Network models; Remote sensing images; Water save; Yolo; Remote sensing","","","; ","Abdul Rahman A.; Musliman I.A.; Yong C.Z.","International Society for Photogrammetry and Remote Sensing","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85148767361"
"Islam M.N.; Yoder J.; Nasiri A.; Burns R.T.; Gan H.","Islam, Md Nafiul (57218177002); Yoder, Jonathan (57564101600); Nasiri, Amin (57208238617); Burns, Robert T. (7401945609); Gan, Hao (57201637937)","57218177002; 57564101600; 57208238617; 7401945609; 57201637937","Analysis of the Drinking Behavior of Beef Cattle Using Computer Vision","2023","Animals","13","18","2984","","","","5","10.3390/ani13182984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172279867&doi=10.3390%2fani13182984&partnerID=40&md5=06add85efeb155b7b57dc6b7ff603d5c","Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States","Islam M.N., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Yoder J., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Nasiri A., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Burns R.T., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States; Gan H., Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, TN, United States","Monitoring the drinking behavior of animals can provide important information for livestock farming, including the health and well-being of the animals. Measuring drinking time is labor-demanding and, thus, it is still a challenge in most livestock production systems. Computer vision technology using a low-cost camera system can be useful in overcoming this issue. The aim of this research was to develop a computer vision system for monitoring beef cattle drinking behavior. A data acquisition system, including an RGB camera and an ultrasonic sensor, was developed to record beef cattle drinking actions. We developed an algorithm for tracking the beef cattle’s key body parts, such as head–ear–neck position, using a state-of-the-art deep learning architecture DeepLabCut. The extracted key points were analyzed using a long short-term memory (LSTM) model to classify drinking and non-drinking periods. A total of 70 videos were used to train and test the model and 8 videos were used for validation purposes. During the testing, the model achieved 97.35% accuracy. The results of this study will guide us to meet immediate needs and expand farmers’ capability in monitoring animal health and well-being by identifying drinking behavior. © 2023 by the authors.","animal behavior; beef cattle; computer vision; drinking time; precision livestock farming","algorithm; animal behavior; animal experiment; animal model; Article; beef cattle; computer vision; deep learning; drinking behavior; information processing; livestock; mathematical parameters; nonhuman; short term memory; videorecording; wellbeing","University of Tennessee AgResearch; National Institute of Food and Agriculture, NIFA, (2022-67021-37863)","This research was funded by USDA NIFA (Award No. 2022-67021-37863) and the University of Tennessee AgResearch. Project title: PLF SPRINT: Precision Livestock Farming: Investing for Future Success.","H. Gan; Department of Biosystems Engineering and Soil Science, University of Tennessee, Knoxville, 37996, United States; email: hgan1@utk.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172279867"
"Pech-May F.; Aquino-Santos R.; Delgadillo-Partida J.","Pech-May, Fernando (35759140900); Aquino-Santos, Raúl (23059814000); Delgadillo-Partida, Jorge (58476921300)","35759140900; 23059814000; 58476921300","Sentinel-1 SAR Images and Deep Learning for Water Body Mapping","2023","Remote Sensing","15","12","3009","","","","11","10.3390/rs15123009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164138553&doi=10.3390%2frs15123009&partnerID=40&md5=9a3b398d51efa5b890d78dba04355d2e","Department of Computer Science, TecNM: Instituto Tecnológico Superior de los Ríos, Balancán, 86930, Mexico; Universidad Tecnológica de Manzanillo, Las Humedades s/n Col. Salagua, Manzanillo, 28869, Mexico","Pech-May F., Department of Computer Science, TecNM: Instituto Tecnológico Superior de los Ríos, Balancán, 86930, Mexico; Aquino-Santos R., Universidad Tecnológica de Manzanillo, Las Humedades s/n Col. Salagua, Manzanillo, 28869, Mexico; Delgadillo-Partida J., Universidad Tecnológica de Manzanillo, Las Humedades s/n Col. Salagua, Manzanillo, 28869, Mexico","Floods occur throughout the world and are becoming increasingly frequent and dangerous. This is due to different factors, among which climate change and land use stand out. In Mexico, they occur every year in different areas. Tabasco is a periodically flooded region, causing losses and negative consequences for the rural, urban, livestock, agricultural, and service industries. Consequently, it is necessary to create strategies to intervene effectively in the affected areas. Different strategies and techniques have been developed to mitigate the damage caused by this phenomenon. Satellite programs provide a large amount of data on the Earth’s surface and geospatial information processing tools useful for environmental and forest monitoring, climate change impacts, risk analysis, and natural disasters. This paper presents a strategy for the classification of flooded areas using satellite images obtained from synthetic aperture radar, as well as the U-Net neural network and ArcGIS platform. The study area is located in Los Rios, a region of Tabasco, Mexico. The results show that U-Net performs well despite the limited number of training samples. As the training data and epochs increase, its precision increases. © 2023 by the authors.","CNN; flood mapping with SAR; remote sensing; SAR; SAR images; water bodies","Climate change; Deep learning; Disasters; Earth (planet); Floods; Forestry; Land use; Mapping; Radar imaging; Remote sensing; Risk analysis; Risk assessment; Space-based radar; Agricultural industries; Body mappings; Flood mapping; Flood mapping with SAR; Me-xico; Remote-sensing; SAR Images; Sentinel-1; Service industries; Waterbodies; Synthetic aperture radar","Tecnológico Nacional de México, TNM, (ITESLRIO/PGP-01); Tecnológico Nacional de México, TNM","Thanks to National Technology of Mexico (TecNM). Council reference: ITESLRIO/PGP-01.","R. Aquino-Santos; Universidad Tecnológica de Manzanillo, Manzanillo, Las Humedades s/n Col. Salagua, 28869, Mexico; email: aquinor@ucol.mx","","MDPI","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85164138553"
"Zhao H.; Mao R.; Li M.; Li B.; Wang M.","Zhao, Hongke (57218277654); Mao, Rui (36167161700); Li, Mei (57199160408); Li, Bin (55698698800); Wang, Meili (55694491200)","57218277654; 36167161700; 57199160408; 55698698800; 55694491200","SheepInst: A High-Performance Instance Segmentation of Sheep Images Based on Deep Learning","2023","Animals","13","8","1338","","","","9","10.3390/ani13081338","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153713261&doi=10.3390%2fani13081338&partnerID=40&md5=866dc20e3f70334f8ce870c34e33ffa2","College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China","Zhao H., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Mao R., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Li M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China; Li B., Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, 100097, China; Wang M., College of Information Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Sheep detection and segmentation will play a crucial role in promoting the implementation of precision livestock farming in the future. In sheep farms, the characteristics of sheep that have the tendency to congregate and irregular contours cause difficulties for computer vision tasks, such as individual identification, behavior recognition, and weight estimation of sheep. Sheep instance segmentation is one of the methods that can mitigate the difficulties associated with locating and extracting different individuals from the same category. To improve the accuracy of extracting individual sheep locations and contours in the case of multiple sheep overlap, this paper proposed two-stage sheep instance segmentation SheepInst based on the Mask R-CNN framework, more specifically, RefineMask. Firstly, an improved backbone network ConvNeXt-E was proposed to extract sheep features. Secondly, we improved the structure of the two-stage object detector Dynamic R-CNN to precisely locate highly overlapping sheep. Finally, we enhanced the segmentation network of RefineMask by adding spatial attention modules to accurately segment irregular contours of sheep. SheepInst achieves 89.1%, 91.3%, and 79.5% in box AP, mask AP, and boundary AP metric on the test set, respectively. The extensive experiments show that SheepInst is more suitable for sheep instance segmentation and has excellent performance. © 2023 by the authors.","attention mechanism; computer vision; deep learning; precision livestock farming; sheep instance segmentation","accuracy; animal experiment; Article; comparative study; computer vision; convolutional neural network; deep learning; excitation; high performance instance segmentation; image segmentation; information processing; livestock; nonhuman; sheep; spatial attention; supervised machine learning; training; vaccination","Science and Technology Think Tank Young Talent Program, (20220615ZZ07110314); Key Research and Development Projects of Shaanxi Province, (2022QFY11-03)","This research was funded by Key Research and Development Projects of Shaanxi Province (2022QFY11-03) and the Science and Technology Think Tank Young Talent Program (20220615ZZ07110314).","M. Wang; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: wml@nwsuaf.edu.cn","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153713261"
"Millburn S.; Schmidt T.; Rohrer G.A.; Mote B.","Millburn, Savannah (58617259200); Schmidt, Ty (7402839670); Rohrer, Gary A. (7102792700); Mote, Benny (13807961600)","58617259200; 7402839670; 7102792700; 13807961600","Identifying Early-Life Behavior to Predict Mothering Ability in Swine Utilizing NUtrack System","2023","Animals","13","18","2897","","","","1","10.3390/ani13182897","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172209114&doi=10.3390%2fani13182897&partnerID=40&md5=26d3eff8924579ebff0df5d93360f9c5","Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; United States Meat Animal Research Center, United States Department of Agriculture-Agricultural Research Service, Clay Center, 68933, NE, United States","Millburn S., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Schmidt T., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Rohrer G.A., United States Meat Animal Research Center, United States Department of Agriculture-Agricultural Research Service, Clay Center, 68933, NE, United States; Mote B., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States","Early indicator traits for swine reproduction and longevity support economical selection decision-making. Activity is a key variable impacting a sow’s herd life and productivity. Early-life activities could contribute to farrowing traits including gestation length (GL), number born alive (NBA), and number weaned (NW). Beginning at 20 weeks of age, 480 gilts were video recorded for 7 consecutive days and processed using the NUtrack system. Activity traits included angle rotated (radians), average speed (m/s), distance traveled (m), time spent eating (s), lying lateral (s), lying sternal (s), standing (s), and sitting (s). Final daily activity values were averaged across the period under cameras. Parity one data were collected for all gilts considered. Data were analyzed using linear regression models (R version 4.0.2). GL was significantly impacted by angle rotated (p = 0.03), average speed (p = 0.07), distance traveled (p = 0.05), time spent lying lateral (p = 0.003), and lying sternal (0.02). NBA was significantly impacted by time spent lying lateral (p = 0.01), lying sternal (p = 0.07), and time spent sitting (p = 0.08). NW was significantly impacted by time spent eating (p = 0.09), time spent lying lateral (p = 0.04), and time spent sitting (p = 0.007). This analysis suggests early-life gilt activities are associated with sow productivity traits of importance. Further examination of the link between behaviors compiled utilizing NUtrack and reproductive traits is necessitated to further isolate behavioral differences for potential use in selection decisions. © 2023 by the authors.","automation of behavioral monitoring; livestock precision farming; tracking individual pigs","air temperature; animal behavior; animal culling; animal experiment; Article; behavior; biological trait; birth weight; body position; body weight; daily life activity; deep learning; early life behavior; eating; female; gestation period; gestational age; gilt (swine); lying lateral; lying sternal; male; morbidity; mortality; mother; nonhuman; parity; phenotype; pig; precision agriculture; recumbency; reproductive success; sedentary time; sitting; social behavior; sow (swine); standing; total distance traveled; velocity; videorecording; weaning","National Pork Board, NPB, (19-092)","This research was funded by the National Pork Board, grant number 19-092.","B. Mote; Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, United States; email: bmote8@unl.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172209114"
"Mao A.; Zhu M.; Huang E.; Yao X.; Liu K.","Mao, Axiu (57238219000); Zhu, Meilu (57200920263); Huang, Endai (57226523976); Yao, Xi (56523755000); Liu, Kai (55823366100)","57238219000; 57200920263; 57226523976; 56523755000; 55823366100","A teacher-to-student information recovery method toward energy-efficient animal activity recognition at low sampling rates","2023","Computers and Electronics in Agriculture","213","","108242","","","","2","10.1016/j.compag.2023.108242","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171460032&doi=10.1016%2fj.compag.2023.108242&partnerID=40&md5=dbf73a3b481ac08dc5cdd7800b15b06f","Department of Infectious Diseases and Public Health, City University of Hong Kong, 999077, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, 999077, Hong Kong; Department of Computer Science, City University of Hong Kong, 999077, Hong Kong; Department of Biomedical Sciences, City University of Hong Kong, 999077, Hong Kong","Mao A., Department of Infectious Diseases and Public Health, City University of Hong Kong, 999077, Hong Kong; Zhu M., Department of Mechanical Engineering, City University of Hong Kong, 999077, Hong Kong; Huang E., Department of Infectious Diseases and Public Health, City University of Hong Kong, 999077, Hong Kong, Department of Computer Science, City University of Hong Kong, 999077, Hong Kong; Yao X., Department of Biomedical Sciences, City University of Hong Kong, 999077, Hong Kong; Liu K., Department of Infectious Diseases and Public Health, City University of Hong Kong, 999077, Hong Kong","Automated animal activity recognition (AAR) has advanced greatly through recent advances in sensing technologies and deep learning, and improved livestock management efficiency, animal health, and welfare monitoring. In practical automated AAR systems where animals need to be monitored over a long period, the sampling rate dramatically affects the energy consumption and battery life of sensing devices due to continuous data collection and transmission. Considering real-world benefits, existing works have often lowered the sampling rate to reduce energy costs. However, when the sampling rate falls below a threshold, the AAR performance degrades rapidly due to many relevant signals being missed. Therefore, this study proposed a novel method, dubbed teacher-to-student information recovery (T2S-IR), to improve the performance of AAR at low sampling rates. This approach effectively leverages the knowledge obtained from high-sampling-rate data, to assist in recovering the missing information in features extracted by the classification network trained on low-sampling-rate data. The workflow of the T2S-IR contains two main steps. (1) we utilize high-sampling-rate data for training teacher classification and reconstruction networks sequentially. (2) Then, we train a student classification network using low-sampling-rate data, while promoting its performance by exploiting the knowledge learned by trained teacher networks via two novel modules, namely the reconstruction-based information recovery (RIR) module and the correlation-distillation-based information recovery (CDIR) module. Specifically, the RIR module employs the pretrained teacher reconstruction network to enforce the student classification network to learn complete and descriptive features. The CDIR module enforces the feature maps of student network to mimic internal correlations within feature maps of pretrained teacher classification network along temporal and sensor axes directions. To validate our proposed T2S-IR, we conducted experiments on two public datasets acquired for horses and goats using triaxial accelerometers and gyroscopes with an initial sampling rate of 100 Hz. Data having low sampling rates were obtained by downsampling the original data at different frequencies (i.e., 50, 25, 12.5, 10, 5, and 2 Hz). The results demonstrated that our method remarkably boosted the classification network trained on low-sampling-rate data (e.g., percentage-point increments in the precision, recall, F1-score, and accuracy of 3.33%, 3.58%, 3.45%, and 2.19%, respectively, for the 12.5-Hz horse data and 7.6%, 4.44%, 6.9%, and 0.79%, respectively, for the 5-Hz goat data) while outperforming existing knowledge distillation methods. The enhanced classification network can be directly applied in practical AAR tasks with low sampling rates, significantly beneficial for scenarios with constrained energy sources for wearable devices. © 2023 Elsevier B.V.","Behavioral classification; Deep learning; Knowledge distillation; Reconstruction; Resampling","Agriculture; Animals; Classification (of information); Deep learning; Energy efficiency; Energy utilization; Molecular biology; Pattern recognition; Personnel training; Recovery; Students; Animal activities; Behavioral classification; Classification networks; Deep learning; Information recovery; Knowledge distillation; Reconstruction; Resampling; Sampling rates; Teachers'; accuracy assessment; classification; energy efficiency; instrumentation; knowledge; livestock farming; machine learning; sampling; Distillation","City University of Hong Kong, CityU; University of Twente","Funding text 1: We would like to thank Jacob W.Kamminga et al., at the pervasive systems group, the University of Twente for providing the public dataset. Funding for conducting this study was provided by the new research initiatives at the City University of Hong Kong .; Funding text 2: We would like to thank Jacob W.Kamminga et al. at the pervasive systems group, the University of Twente for providing the public dataset. Funding for conducting this study was provided by the new research initiatives at the City University of Hong Kong.","K. Liu; Department of Infectious Diseases and Public Health, City University of Hong Kong, 999077, Hong Kong; email: kailiu@cityu.edu.hk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85171460032"
"Qi Y.; Du X.; Zhu J.; Gao S.; Liu L.","Qi, Yongsheng (24403028100); Du, Xiaoxu (58249424900); Zhu, Junfeng (58621599700); Gao, Shengli (57206840213); Liu, Liqiang (57199083517)","24403028100; 58249424900; 58621599700; 57206840213; 57199083517","Efficient Livestock Detection in Grazing Areas Based on Enhanced Lightweight Deep Network","2023","Jisuanji Gongcheng/Computer Engineering","49","7","","278","287","9","1","10.19678/j.issn.1000-3428.0064802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172456557&doi=10.19678%2fj.issn.1000-3428.0064802&partnerID=40&md5=9b9eb1c07e6d0a38813d5c77357bf810","Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China; Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, 010051, China; Pastoral Water Conservancy Science Research Institute of the Ministry of Water Resources, Hohhot, 010051, China; Inner Mongolia North Longyuan Wind Power Generation Co., Ltd, Hohhot, 010050, China","Qi Y., Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China, Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, 010051, China; Du X., Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China; Zhu J., Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China, Pastoral Water Conservancy Science Research Institute of the Ministry of Water Resources, Hohhot, 010051, China; Gao S., Inner Mongolia North Longyuan Wind Power Generation Co., Ltd, Hohhot, 010050, China; Liu L., Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China, Institute of Electric Power, Inner Mongolia University of Technology, Hohhot, 010051, China","Realizing big data to manage livestock requires real-time monitoring of livestock, but real-time monitoring of livestock is easily interfered by large changes in target size, lighting, environmental factors, etc., so it is difficult to detection, and existing livestock detection algorithms have the problem of poor robustness. An object detection network called E-YOLOv4-tiny is proposed based on enhanced YOLOv4-tiny, which adopts a pyramid network with multi-scale feature fusion, taking into account shallow local detail features and deep semantic information to solve the problem of livestock size fluctuation in pastoral areas. The number of backbone network parameters is reduced by improving the residual structure to accommodate embedded platform requirements. A new composite clustering algorithm is introduced to design anchor frames to improve the accuracy of the algorithm under the premise of ensuring portability. Finally, according to the characteristics of a pastoral environment, a new Compound Muti-channel Attention(CMA) mechanism is proposed to improve the poor accuracy of the target detection network and enhance the robustness of the algorithm. Experimental results show that the mean Average Precision(mAP) of the E-YOLOv4-tiny algorithm is 0.878 9, and the frame rate is 32 frame/s, and it's mAP is 9.32% higher than that of the traditional YOLOv4-tiny algorithm while maintaining almost the same detection rate. © 2023, Editorial Office of Computer Engineering. All rights reserved.","attention mechanism; computer vision; deep learning; feature fusion; target detection; YOLOv4-tiny algorithm","","","","Y. Qi; Inner Mongolia Key Laboratory of Electrical and Mechanical Control, Hohhot, 010051, China; email: qys@imut.edu.cn","","Editorial Office of Computer Engineering","10003428","","JISGE","","Chinese","Jisuanji Gongcheng","Article","Final","","Scopus","2-s2.0-85172456557"
"Li J.; Chen D.; Li Z.; Huang Y.; Morris D.","Li, Jiajia (58317754400); Chen, Dong (57190854977); Li, Zhaojian (57202421579); Huang, Yanbo (59447268200); Morris, Daniel (55459697400)","58317754400; 57190854977; 57202421579; 59447268200; 55459697400","ML/DL in Agriculture through Label-Efficient Learning","2023","2023 ASABE Annual International Meeting","","","","","","","1","10.13031/aim.202300085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183577182&doi=10.13031%2faim.202300085&partnerID=40&md5=d30f7250f11c0b65dd6bc42b9581e8b0","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; USDA-ARS, Genetics and Sustainable Agriculture Research Unit, Mississippi State, 39762, MS, United States; Department of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, 48824, MI, United States","Li J., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Chen D., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, 48824, MI, United States; Li Z., USDA-ARS, Genetics and Sustainable Agriculture Research Unit, Mississippi State, 39762, MS, United States; Huang Y., Department of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, 48824, MI, United States; Morris D.","The past decade has witnessed many great successes of machine learning (ML) and deep learning (DL) applications in agricultural systems, including weed control, plant disease diagnosis, agricultural robotics, and precision livestock management. Despite tremendous progresses, one downside of such ML/DL models is that they generally rely on large-scale labeled datasets for training, and the performance of such models is strongly influenced by the size and quality of available labeled data samples. In addition, collecting, processing, and labeling such large-scale datasets is extremely costly and time-consuming, partially due to the rising cost in human labor. Therefore, developing label-efficient ML/DL methods for agricultural applications has received significant interests among researchers and practitioners. In fact, there are more than 50 papers on developing and applying deep-learning-based label-efficient techniques to address various agricultural problems since 2016, which motivates the authors to provide a timely and comprehensive review of recent label-efficient ML/DL methods in agricultural applications. To this end, we first develop a principled taxonomy to organize these methods according to the degree of supervision, including weak supervision (i.e., active learning and semi-/weakly- supervised learning), and no supervision (i.e., un-/selfsupervised learning), supplemented by representative state-of-the-art label-efficient ML/DL methods. In addition, a systematic review of various agricultural applications exploiting these label-efficient algorithms, such as plant health, weed and crop management, fruit detection, and aquaculture, is presented. Finally, we discuss the current problems and challenges, as well as future research directions. A well-classified paper list that will be actively updated can be accessed at https://github.com/DongChen06/Label-efficient-in-Agriculture. © 2023 ASABE Annual International Meeting. All Rights Reserved.","active learning; agriculture; deep learning; label-efficient learning; label-free learning; self-supervised learning; semi-supervised learning; unsupervised learning; weakly-supervised learning","Crops; Deep learning; Diagnosis; Disease control; Large datasets; Learning systems; Weed control; Active Learning; Deep learning; Efficient learning; Label free; Label-efficient learning; Label-free learning; Machine-learning; Self-supervised learning; Semi-supervised learning; Weakly supervised learning; Supervised learning","","","","","American Society of Agricultural and Biological Engineers","","978-171388588-7","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85183577182"
"Lee T.; Na Y.; Kim B.G.; Lee S.; Choi Y.","Lee, Taejun (58621938500); Na, Youngjun (56230192800); Kim, Beob Gyun (26654167200); Lee, Sangrak (37056875600); Choi, Yongjun (57199930300)","58621938500; 56230192800; 26654167200; 37056875600; 57199930300","Identification of Individual Hanwoo Cattle by Muzzle Pattern Images through Deep Learning","2023","Animals","13","18","2856","","","","5","10.3390/ani13182856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172482146&doi=10.3390%2fani13182856&partnerID=40&md5=3dd094f753b6e75ee867aeeaf1dc7f96","Department of Animal Science, Konkuk University, Seoul, 05029, South Korea; Animal Data Laboratory, Antller Inc, Seoul, 05029, South Korea","Lee T., Department of Animal Science, Konkuk University, Seoul, 05029, South Korea; Na Y., Department of Animal Science, Konkuk University, Seoul, 05029, South Korea, Animal Data Laboratory, Antller Inc, Seoul, 05029, South Korea; Kim B.G., Department of Animal Science, Konkuk University, Seoul, 05029, South Korea; Lee S., Department of Animal Science, Konkuk University, Seoul, 05029, South Korea; Choi Y., Department of Animal Science, Konkuk University, Seoul, 05029, South Korea","The objective of this study was to identify Hanwoo cattle via a deep-learning model using muzzle images. A total of 9230 images from 336 Hanwoo were used. Images of the same individuals were taken at four different times to avoid overfitted models. Muzzle images were cropped by the YOLO v8-based model trained with 150 images with manual annotation. Data blocks were composed of image and national livestock traceability numbers and were randomly selected and stored as train, validation test data. Transfer learning was performed with the tiny, small and medium versions of Efficientnet v2 models with SGD, RMSProp, Adam and Lion optimizers. The small version using Lion showed the best validation accuracy of 0.981 in 36 epochs within 12 transfer-learned models. The top five models achieved the best validation accuracy and were evaluated with the training data for practical usage. The small version using Adam showed the best test accuracy of 0.970, but the small version using RMSProp showed the lowest repeated error. Results with high accuracy prediction in this study demonstrated the potential of muzzle patterns as an identification key for individual cattle. © 2023 by the authors.","cattle identification; deep learning; Efficientnet; Hanwoo; muzzle pattern; transfer learning","accuracy; animal experiment; Article; bovine; dairy cattle; data loader; deep learning; identification key; image quality; lion optimizer; livestock; loss function; nonhuman; optimizer algorithm; prediction; RMSProp; snout; training; transfer of learning; validation process","Korea Smart Farm R&D Foundation; Konkuk University, KU; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421014-04); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (421014-04) and this paper was supported by the KU Research Professor Program of Konkuk University. ","Y. Choi; Department of Animal Science, Konkuk University, Seoul, 05029, South Korea; email: cyj2114@gmail.com","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172482146"
"Guo Q.; Sun Y.; Orsini C.; Bolhuis J.E.; de Vlieg J.; Bijma P.; de With P.H.N.","Guo, Qinghua (58045429200); Sun, Yue (57202294123); Orsini, Clémence (58477428700); Bolhuis, J. Elizabeth (7003926611); de Vlieg, Jakob (58112751500); Bijma, Piter (6603847266); de With, Peter H.N. (7003945229)","58045429200; 57202294123; 58477428700; 7003926611; 58112751500; 6603847266; 7003945229","Enhanced camera-based individual pig detection and tracking for smart pig farms","2023","Computers and Electronics in Agriculture","211","","108009","","","","29","10.1016/j.compag.2023.108009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164215656&doi=10.1016%2fj.compag.2023.108009&partnerID=40&md5=5a81c9813f91fdba61daa472932bf32b","Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands; Department of Animal Sciences, Wageningen University & Research, Wageningen, 6708 PB, Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands; Faculty of Applied Science, Macao Polytechnic University, 999078, China","Guo Q., Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands; Sun Y., Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands, Faculty of Applied Science, Macao Polytechnic University, 999078, China; Orsini C., Department of Animal Sciences, Wageningen University & Research, Wageningen, 6708 PB, Netherlands; Bolhuis J.E., Department of Animal Sciences, Wageningen University & Research, Wageningen, 6708 PB, Netherlands; de Vlieg J., Department of Mathematics and Computer Science, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands; Bijma P., Department of Animal Sciences, Wageningen University & Research, Wageningen, 6708 PB, Netherlands; de With P.H.N., Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands","Negative social interactions are harmful for animal health and welfare. It is increasingly important to employ a continuous and effective monitoring system for detecting and tracking individual animals in large-scale farms. Such a system can provide timely alarms for farmers to intervene when damaging behavior occurs. Deep learning combined with camera-based monitoring is currently arising in agriculture. In this work, deep neural networks are employed to assist individual pig detection and tracking, which enables further analyzing behavior at the individual pig level. First, three state-of-the-art deep learning-based Multi-Object Tracking (MOT) methods are investigated, namely Joint Detection and Embedding (JDE), FairMOT, and YOLOv5s with DeepSORT. All models facilitate automated and continuous individual detection and tracking. Second, weighted-association algorithms are proposed for each MOT method, in order to optimize the object re-identification (re-ID), and improve the individual animal-tracking performance, especially for reducing the number of identity switches. The proposed weighted-association methods are evaluated on a large manually annotated pig dataset, and compared with the state-of-the-art methods. FairMOT with the proposed weighted association achieves the highest IDF1, the least number of identity switches, and the fastest execution rate. YOLOv5s with DeepSORT results in the highest MOTA and MOTP tracking metrics. These methods show high accuracy and robustness for individual pig tracking, and are promising candidates for continuous multi-object tracking for real use in commercial farms. © 2023 Elsevier B.V.","Animal detection; Animal tracking; Camera-based detection and tracking; Multi-object tracking models","Cameras; Large dataset; Mammals; Object detection; Tracking (position); Animal detection; Animal tracking; Camera-based; Camera-based detection and tracking; Detection and tracking; Multi-object tracking; Multi-object tracking model; Smart pigs; Tracking method; Tracking models; detection method; livestock farming; monitoring system; pig; tracking; Deep neural networks","Topigs Norsvin in Helvoirt; Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO","Funding text 1: This work is supported by the Dutch NWO project IMAGEN [ P18-19 Project 1 ] of the research program Perspectief. The Volmer facility in Germany was offered by Topigs Norsvin in Helvoirt, the Netherlands. The authors would like to thank all researchers and student assistants from Wageningen University & Research and Eindhoven University of Technology for assistance with the tracking ground-truth annotations.; Funding text 2: This work is supported by the Dutch NWO project IMAGEN [P18-19 Project 1] of the research program Perspectief. The Volmer facility in Germany was offered by Topigs Norsvin in Helvoirt, the Netherlands. The authors would like to thank all researchers and student assistants from Wageningen University & Research and Eindhoven University of Technology for assistance with the tracking ground-truth annotations.","Y. Sun; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, 5612 AP, Netherlands; email: joyyuesun@gmail.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85164215656"
"Wang Y.; Ma L.; Wang Q.; Wang N.; Wang D.; Wang X.; Zheng Q.; Hou X.; Ouyang G.","Wang, Yuhang (59118334300); Ma, Lingling (35216102100); Wang, Qi (58741719100); Wang, Ning (56420870400); Wang, Dongliang (55713405600); Wang, Xinhong (36629779000); Zheng, Qingchuan (57338104400); Hou, Xiaoxin (58175609800); Ouyang, Guangzhou (58174674500)","59118334300; 35216102100; 58741719100; 56420870400; 55713405600; 36629779000; 57338104400; 58175609800; 58174674500","A Lightweight and High-Accuracy Deep Learning Method for Grassland Grazing Livestock Detection Using UAV Imagery","2023","Remote Sensing","15","6","1593","","","","13","10.3390/rs15061593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151929122&doi=10.3390%2frs15061593&partnerID=40&md5=81da490a1518a83237782f879f1229bd","Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Land Surface Pattern and Simulation, Institute of Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Inner Mongolia North Heavy Industries Group Co., Ltd, Baotou, 014033, China","Wang Y., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Ma L., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Wang Q., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Wang N., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Wang D., Key Laboratory of Land Surface Pattern and Simulation, Institute of Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Wang X., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Zheng Q., Inner Mongolia North Heavy Industries Group Co., Ltd, Baotou, 014033, China; Hou X., Inner Mongolia North Heavy Industries Group Co., Ltd, Baotou, 014033, China; Ouyang G., Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China","Unregulated livestock breeding and grazing can degrade grasslands and damage the ecological environment. The combination of remote sensing and artificial intelligence techniques is a more convenient and powerful means to acquire livestock information in a large area than traditional manual ground investigation. As a mainstream remote sensing platform, unmanned aerial vehicles (UAVs) can obtain high-resolution optical images to detect grazing livestock in grassland. However, grazing livestock objects in UAV images usually occupy very few pixels and tend to gather together, which makes them difficult to detect and count automatically. This paper proposes the GLDM (grazing livestock detection model), a lightweight and high-accuracy deep-learning model, for detecting grazing livestock in UAV images. The enhanced CSPDarknet (ECSP) and weighted aggregate feature re-extraction pyramid modules (WAFR) are constructed to improve the performance based on the YOLOX-nano network scheme. The dataset of different grazing livestock (12,901 instances) for deep learning was made from UAV images in the Hadatu Pasture of Hulunbuir, Inner Mongolia, China. The results show that the proposed method achieves a higher comprehensive detection precision than mainstream object detection models and has an advantage in model size. The (Formula presented.) of the proposed method is 86.47%, with the model parameter 5.7 M. The average recall and average precision can be above 85% at the same time. The counting accuracy of grazing livestock in the testing dataset, when converted to a unified sheep unit, reached 99%. The scale applicability of the model is also discussed, and the GLDM could perform well with the image resolution varying from 2.5 to 10 cm. The proposed method, the GLDM, was better for detecting grassland grazing livestock in UAV images, combining remote sensing, AI, and grassland ecological applications with broad application prospects. © 2023 by the authors.","deep learning; grassland grazing livestock; object detection; remote sensing image; unmanned aerial vehicle (UAV)","Agriculture; Aircraft detection; Antennas; Deep learning; Ecology; Geometrical optics; Image enhancement; Image resolution; Learning systems; Object recognition; Optical remote sensing; Statistical tests; Unmanned aerial vehicles (UAV); Aerial vehicle; Deep learning; Detection models; Grassland grazing livestock; High-accuracy; Objects detection; Remote sensing images; Remote-sensing; Unmanned aerial vehicle; Vehicle images; Object detection","Science and Technology Major Project of Inner Mongolia Autonomous Region of China, (2021ZD0044); Chinese Academy of Sciences, CAS, (XDA26010201); Chinese Academy of Sciences, CAS","This research was funded by the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDA26010201) and the Science and Technology Major Project of Inner Mongolia Autonomous Region of China (Grant No. 2021ZD0044).","L. Ma; Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: llma@aoe.ac.cn","","MDPI","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85151929122"
"Shu H.; Bindelle J.; Guo L.; Gu X.","Shu, Hang (57222589919); Bindelle, Jérôme (16240687200); Guo, Leifeng (56542160600); Gu, Xianhong (8976627000)","57222589919; 16240687200; 56542160600; 8976627000","Determining the onset of heat stress in a dairy herd based on automated behaviour recognition","2023","Biosystems Engineering","226","","","238","251","13","13","10.1016/j.biosystemseng.2023.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147417506&doi=10.1016%2fj.biosystemseng.2023.01.009&partnerID=40&md5=214275988b5101b9022197166c2dbf81","Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium","Shu H., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China, State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China, AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Bindelle J., AgroBioChem/TERRA, Precision Livestock and Nutrition Unit, Gembloux Agro-Bio Tech, University of Liège, Gembloux, 5030, Belgium; Guo L., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; Gu X., State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","Dairy cows have various strategies for dealing with heat stress, including a change in behaviour. The aim of this study was to propose a deep learning-based model for recognising cow behaviours and to determine critical thresholds for the onset of heat stress at the herd level. A total of 1000 herd behaviour images taken in a free-stall pen were allocated with labels of five behaviours that are known to be influenced by the thermal environment. Three YOLOv5 architectures were trained by the transfer learning method. The results show the superiority of YOLOv5s with a mean average precision of 0.985 and an inference speed of 73 frames per second on the testing set. Further validation demonstrates excellent agreement in herd-level behavioural parameters between automated measurement and manual observation (intraclass correlation coefficient = 0.97). The analysis of automated behavioural measurements during a 10-day experiment with no to moderate heat stress reveals that lying and standing indices were most responding to heat stress and the test dairy herd began to change their behaviour at the earliest ambient temperature of 23.8 °C or temperature-humidity index of 68.5. Time effects were observed to alter the behavioural indicators values rather than their corresponding environmental thresholds. The proposed method enables a low-cost herd-level heat stress alert without imposing any burden on dairy cows. © 2023 IAgrE","Animal welfare; Behavioural index; Group measurement; Smart livestock farming; Thermal comfort","Automation; Deep learning; Farms; Learning systems; Thermal stress; Animal welfare; Behavioral index; Behaviour recognition; Dairy cow; Dairy herds; Group measurement; Heat stress; Learning Based Models; Livestock farming; Smart livestock farming; Behavioral research","Major Science and Technology Program of Inner Mongolia Autonomous Region, (2020ZD0004); Chinese Academy of Agricultural Sciences, CAAS, (CAAS-ASTIP-2016-AII); Agricultural Science and Technology Innovation Program, ASTIP, (ASTIP-IAS07); Key Research and Development Project of Hainan Province, (22326609D)","Funding text 1: This work was supported by the Agricultural Science and Technology Innovation Program [ ASTIP-IAS07 ], the Key Research and Development Project of Heibei Province [22326609D], the Major Science and Technology Program of Inner Mongolia Autonomous Region [2020ZD0004], and the Science and Technology Innovation Project of Chinese Academy of Agricultural Sciences [ CAAS - ASTIP -2016- AII ]. The authors are grateful to Mr. Fuyu Sun and Mr. Xiaoyang Chen, as well as Yinxiang dairy farm, for their assistance in data collection. ; Funding text 2: This work was supported by the Agricultural Science and Technology Innovation Program [ASTIP-IAS07], the Key Research and Development Project of Heibei Province [22326609D], the Major Science and Technology Program of Inner Mongolia Autonomous Region [2020ZD0004], and the Science and Technology Innovation Project of Chinese Academy of Agricultural Sciences [CAAS-ASTIP-2016-AII]. The authors are grateful to Mr. Fuyu Sun and Mr. Xiaoyang Chen, as well as Yinxiang dairy farm, for their assistance in data collection.","L. Guo; Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, 100086, China; email: guoleifeng@caas.cn; X. Gu; State Key Laboratory of Animal Nutrition, Institute of Animal Sciences, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; email: guxianhong@vip.sina.com","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85147417506"
"Huihui Y.; Daoliang L.; Yingyi C.","Huihui, Yu (55934197600); Daoliang, Li (58350191500); Yingyi, Chen (24079892700)","55934197600; 58350191500; 24079892700","A state-of-the-art review of image motion deblurring techniques in precision agriculture","2023","Heliyon","9","6","e17332","","","","13","10.1016/j.heliyon.2023.e17332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162901269&doi=10.1016%2fj.heliyon.2023.e17332&partnerID=40&md5=6a5e3d3f08f1a9724104dd0cad561f3b","School of Information Science & Technology, Beijing Forestry University, Beijing, 100083, China; National Innovation Center for Digital Fishery, Beijing, 100083, China; Key Laboratory of Smart Farming Technologies for Aquatic Animal and Livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Huihui Y., School of Information Science & Technology, Beijing Forestry University, Beijing, 100083, China, National Innovation Center for Digital Fishery, Beijing, 100083, China, Key Laboratory of Smart Farming Technologies for Aquatic Animal and Livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Daoliang L., National Innovation Center for Digital Fishery, Beijing, 100083, China, Key Laboratory of Smart Farming Technologies for Aquatic Animal and Livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China, College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Yingyi C., National Innovation Center for Digital Fishery, Beijing, 100083, China, Key Laboratory of Smart Farming Technologies for Aquatic Animal and Livestock, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Engineering and Technology Research Center for Internet of Things in Agriculture, Beijing, 100083, China, College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Image motion deblurring is a crucial technology in computer vision that has gained significant attention attracted by its outstanding ability for accurate acquisition of motion image information, processing and intelligent decision making, etc. Motion blur has recently been considered as one of the major challenges for applications of computer vision in precision agriculture. Motion blurred images seriously affect the accuracy of information acquisition in precision agriculture scene image such as testing, tracking, and behavior analysis of animals, recognition of plant phenotype, critical characteristics of pests and diseases, etc. On the other hand, the fast motion and irregular deformation of agriculture livings, and motion of image capture device all introduce great challenges for image motion deblurring. Hence, the demand of more efficient image motion deblurring method is rapidly increasing and developing in the applications with dynamic scene. Up till now, some studies have been carried out to address this challenge, e.g., spatial motion blur, multi-scale blur and other types of blur. This paper starts with categorization of causes of image blur in precision agriculture. Then, it gives detail introduction of general-purpose motion deblurring methods and their the strengthen and weakness. Furthermore, these methods are compared for the specific applications in precision agriculture e.g., detection and tracking of livestock animal, harvest sorting and grading, and plant disease detection and phenotyping identification etc. Finally, future research directions are discussed to push forward the research and application of advancing in precision agriculture image motion deblurring field. © 2023 The Authors","Blur kernel; Deep learning; Motion blurred image; Motion deblurring; Precision agriculture","","Beijing Digital Agriculture Innovation Consortium Project, (62076244, BAIC10-2022); National Natural Science Foundation of China, NSFC, (62206021); National Natural Science Foundation of China, NSFC","This work was supported by the National Natural Science Foundation of China “Intelligent identification method of underwater fish morphological characteristics based on binocular vision” (No. 62206021 ), Beijing Digital Agriculture Innovation Consortium Project (No. BAIC10-2022 ) and the National Natural Science Foundation of China “Analysis and feature recognition on feeding behavior of fish school in facility farming based on machine vision” (No. 62076244).","C. Yingyi; China Agricultural University, Beijing, 17 Tsinghua East Road, 100083, China; email: chenyingyi@cau.edu.cn","","Elsevier Ltd","24058440","","","","English","Heliyon","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85162901269"
"Mao A.; Huang E.; Wang X.; Liu K.","Mao, Axiu (57238219000); Huang, Endai (57226523976); Wang, Xiaoshuai (55823976100); Liu, Kai (57722412300)","57238219000; 57226523976; 55823976100; 57722412300","Deep learning-based animal activity recognition with wearable sensors: Overview, challenges, and future directions","2023","Computers and Electronics in Agriculture","211","","108043","","","","32","10.1016/j.compag.2023.108043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165191279&doi=10.1016%2fj.compag.2023.108043&partnerID=40&md5=56f503603ab1b75c55e31d7848d8255a","Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, 999077, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong, 999077, Hong Kong; College of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, 310058, China","Mao A., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, 999077, Hong Kong; Huang E., Department of Computer Science, City University of Hong Kong, Hong Kong, 999077, Hong Kong; Wang X., College of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, 310058, China; Liu K., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, 999077, Hong Kong","Animal behavior, as one of the most crucial indicators of animal health and welfare, provides rich insights into animal physical and mental states. Automated animal activity recognition (AAR) allows caretakers to monitor animal behavioral variations in real time, significantly reducing workloads and costs in veterinary clinics and promoting livestock management efficiency. With recent advances in sensing technologies and smart computing techniques, automated AAR has been increasingly studied, and tremendous successes have been achieved. This paper provides a comprehensive summary of recent research on AAR based on wearable sensors and deep learning algorithms. First, the commonly used sensor types and frequently studied animal species and activities are described. Then, an extensive overview of deep learning-based methods for wearable sensor-aided AAR is presented, according to the taxonomy of deep learning algorithms. We also provide a comprehensive list of publicly available datasets collected via wearable sensor-aided AAR over the past five years. This list can serve as a valuable resource for readers who wish to further explore the field of AAR. In addition, we discuss potential challenges associated with the development of deep learning models for AAR and suggest potential solutions and future research directions for these challenges. In conclusion, this review work provides rich inspiration for the future advancement of robust AAR systems based on wearable sensors and deep learning techniques. When combined with qualitative assessments of veterinary specialists, the accurate and quantitative results obtained by automated AAR systems hold the potential to significantly improve animal health and welfare. © 2023 Elsevier B.V.","Behavior classification; Domesticated animals; Machine learning; Wearable devices","Agriculture; Automation; Deep learning; Learning algorithms; Learning systems; Pattern recognition; Wearable sensors; Activity recognition; Animal activities; Animal behaviour; Animal health; Animal welfare; Behaviour classification; Domesticated animal; Machine-learning; Recognition systems; Wearable devices; domestication; equipment; future prospect; livestock farming; machine learning; recognition; sensor; Animals","City University of Hong Kong, CityU","Funding for conducting this study was provided by the new research initiatives at the City University of Hong Kong.","K. Liu; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, 999077, Hong Kong; email: kailiu@cityu.edu.hk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","","Scopus","2-s2.0-85165191279"
"Zortea M.; De Sousa Almeida J.L.; Klein L.; Nogueira Junior A.C.","Zortea, MacIel (22236154700); De Sousa Almeida, Joao Lucas (57191964711); Klein, Levente (7202622188); Nogueira Junior, Alberto Costa (56900666200)","22236154700; 57191964711; 7202622188; 56900666200","Detection of methane plumes using Sentinel-2 satellite images and deep neural networks trained on synthetically created label data","2023","Proceedings - 2023 IEEE International Conference on Big Data, BigData 2023","","","","3830","3839","9","0","10.1109/BigData59044.2023.10386482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184977547&doi=10.1109%2fBigData59044.2023.10386482&partnerID=40&md5=d7ff34a8be3019ebc0252f69519bb227","Ibm Research - Brazil, Rio de Janeiro, Brazil; Ibm Research - Brazil, São Paulo, Brazil; Ibm T.J. Watson Research Center, Yorktown Heights, United States","Zortea M., Ibm Research - Brazil, Rio de Janeiro, Brazil; De Sousa Almeida J.L., Ibm Research - Brazil, São Paulo, Brazil; Klein L., Ibm T.J. Watson Research Center, Yorktown Heights, United States; Nogueira Junior A.C., Ibm Research - Brazil, São Paulo, Brazil","Methane emissions from oil and gas infrastructure, wetlands, and livestock contribute to the greenhouse gas inventory. The analysis of satellite short-wave infrared imagery offers opportunities for screening large areas to detect methane leaks. Deep learning algorithms excel at analyzing these data, however, they require large annotated datasets for model calibration that are difficult to get. To overcome this limitation, we explore a methodology to spot methane plumes using deep binary classifiers trained on a large dataset of synthetically created methane plumes, customized for this specific task, using publicly available images of the Sentine1-2 satellites. To build the database, we simulate plume patterns using the Hybrid Single-Particle Lagrangian Integrated Trajectory model (HYSPLIT) and use a simple stochastic model to account for reflectance attenuation due to methane in band 12 centered at 2190 nm. To help distinguish methane plumes from the image background, we compute a methane signature image based on a background subtraction technique. Once calibrated, the classification model is applied to image patches centered in the local minima of the methane signature within the satellite image, scoring a value ranging from 0 to 1 associated with the presence of a methane plume. We compare experimentally the general-purpose ResNet architecture and MethaNet, a domain-specific convolutional neural network, using simulated data. Then, we evaluate the feasibility of our approach in detecting large methane leaks at two study sites located in the Hassi Messaoud oil field in Algeria and the Permian Basin in the US, each covering an area of 0.25$\times$ 0.25 degrees. We found that ResNet is effective in identifying large, known methane plumes that were set aside for testing purposes. This method could be considered as a component of a solution for planning mitigation activities.  © 2023 IEEE.","classification; convolutional neural networks; Methane; remote sensing.","Agriculture; Convolution; Convolutional neural networks; Deep neural networks; Gas emissions; Greenhouse gases; Image classification; Infrared radiation; Large datasets; Methane; Remote sensing; Satellite imagery; Stochastic models; Stochastic systems; Convolutional neural network; Greenhouse gas inventory; Methane emissions; Methane leaks; Methane plumes; Oil and gas infrastructures; Remote sensing.; Remote-sensing; Satellite images; Short wave infrared; Classification (of information)","","","M. Zortea; Ibm Research - Brazil, Rio de Janeiro, Brazil; email: mazortea@br.ibm.com","He J.; Palpanas T.; Hu X.; Cuzzocrea A.; Dou D.; Slezak D.; Wang W.; Gruca A.; Lin J.C.-W.; Agrawal R.","Institute of Electrical and Electronics Engineers Inc.","","979-835032445-7","","","English","Proc. - IEEE Int. Conf. Big Data, BigData","Conference paper","Final","","Scopus","2-s2.0-85184977547"
"Yulianingsih; Nurdiati S.; Sukoco H.; Sumantri C.","Yulianingsih (58923991100); Nurdiati, Sri (9432743600); Sukoco, Heru (43661850900); Sumantri, Cece (6601992915)","58923991100; 9432743600; 43661850900; 6601992915","Exploring Pixel Segmentation with Mask R-CNN: Implications for Predicting Cattle Weight","2023","Proceedings of the 3rd 2023 International Conference on Smart Cities, Automation and Intelligent Computing Systems, ICON-SONICS 2023","","","","32","37","5","1","10.1109/ICON-SONICS59898.2023.10435120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186962099&doi=10.1109%2fICON-SONICS59898.2023.10435120&partnerID=40&md5=b0aadf71aed96f7ee8b707ff898bd396","IPB University, Faculty of Mathematics and Natural Science, Department of Computer Science, Bogor, Indonesia; IPB University, Faculty of Mathematics and Natural Science, Department of Mathematics, Bogor, Indonesia; IPB University, Faculty of Animal Science, Department Animal Production and Technology, Bogor, Indonesia","Yulianingsih, IPB University, Faculty of Mathematics and Natural Science, Department of Computer Science, Bogor, Indonesia; Nurdiati S., IPB University, Faculty of Mathematics and Natural Science, Department of Mathematics, Bogor, Indonesia; Sukoco H., IPB University, Faculty of Mathematics and Natural Science, Department of Computer Science, Bogor, Indonesia; Sumantri C., IPB University, Faculty of Animal Science, Department Animal Production and Technology, Bogor, Indonesia","In the realm of livestock management, particularly concerning cattle, the swift and precise estimation of animal weight is of paramount significance. This research delves into the intricacies of image data preprocessing utilizing various combinations of the Mask R-CNN approach to support deep learning-driven cattle weight prediction models. The renowned efficacy of Mask R-CNN in object detection and segmentation is harnessed to emphasize features specific to cattle. Rigorous experiments were undertaken, encompassing 31 distinct datasets, aggregating a total of 223 images of Bali cattle before augmentation. The empirical findings emphasize the superiority of the Mask R-CNN R50-DC5 and R50-FPN backbone combinations, which exhibited commendable precision rates of 86,1% for R50-DC5. 85,8% for R50-FPN, and 85,1% for R50-C4. Additionally, observations were conducted to ascertain the most efficient process speed among these configurations. The manuscript aims to select the optimal image segmentation method to acquire pixels that will be explored as features in predicting the weight of livestock. The results obtained underscore the importance of ensuring meticulous evaluations of image segmentation accuracy to maintain the integrity and reliability of the model. © 2023 IEEE.","livestock; Mask R-CNN; pixel; prediction; segmentation","Agriculture; Deep learning; Feature extraction; Image segmentation; Object detection; Pixels; Animal weights; Data preprocessing; Image data; Images segmentations; Livestock; Mask R-CNN; Objects detection; Objects segmentation; Prediction modelling; Segmentation; Forecasting","","","Yulianingsih; IPB University, Faculty of Mathematics and Natural Science, Department of Computer Science, Bogor, Indonesia; email: ipb_yulianingsih@apps.ipb.ac.id","","Institute of Electrical and Electronics Engineers Inc.","","978-150906280-5","","","English","Proc. Int. Conf. Smart Cities, Autom. Intell. Comput. Syst., ICON-SONICS","Conference paper","Final","","Scopus","2-s2.0-85186962099"
"Hao W.; Zhang K.; Zhang L.; Han M.; Hao W.; Li F.; Yang G.","Hao, Wangli (55757033100); Zhang, Kai (57221087549); Zhang, Li (58436251900); Han, Meng (57194785061); Hao, Wangbao (57194782429); Li, Fuzhong (55494497700); Yang, Guoqiang (58597217400)","55757033100; 57221087549; 58436251900; 57194785061; 57194782429; 55494497700; 58597217400","TSML: A New Pig Behavior Recognition Method Based on Two-Stream Mutual Learning Network","2023","Sensors","23","11","5092","","","","6","10.3390/s23115092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161581368&doi=10.3390%2fs23115092&partnerID=40&md5=7597bd4171ccbbc1a23c297f4c5fc4a0","School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Yuncheng National Jinnan Cattle Genetic Resources and Gene Protection Center, Yongji, 044099, China","Hao W., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Zhang K., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Zhang L., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Han M., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Hao W., Yuncheng National Jinnan Cattle Genetic Resources and Gene Protection Center, Yongji, 044099, China; Li F., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Yang G., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China","Changes in pig behavior are crucial information in the livestock breeding process, and automatic pig behavior recognition is a vital method for improving pig welfare. However, most methods for pig behavior recognition rely on human observation and deep learning. Human observation is often time-consuming and labor-intensive, while deep learning models with a large number of parameters can result in slow training times and low efficiency. To address these issues, this paper proposes a novel deep mutual learning enhanced two-stream pig behavior recognition approach. The proposed model consists of two mutual learning networks, which include the red–green–blue color model (RGB) and flow streams. Additionally, each branch contains two student networks that learn collaboratively to effectively achieve robust and rich appearance or motion features, ultimately leading to improved recognition performance of pig behaviors. Finally, the results of RGB and flow branches are weighted and fused to further improve the performance of pig behavior recognition. Experimental results demonstrate the effectiveness of the proposed model, which achieves state-of-the-art recognition performance with an accuracy of 96.52%, surpassing other models by 2.71%. © 2023 by the authors.","animal welfare; behavior recognition; computer vision; pig breeding; two stream mutual learning","Animals; Breeding; Humans; Motion; Neural Networks, Computer; Swine; Agriculture; Deep learning; Learning systems; Mammals; Animal welfare; Behaviour recognition; Human observations; Learning network; Mutual learning; Performance; Pig behavior; Pig breeding; Two stream mutual learning; Two-stream; animal; artificial neural network; breeding; human; motion; pig; Computer vision","Shanxi Province Basic Research Program, (202203021212444); Shanxi Province Education Science","This work was supported by: Shanxi Province Basic Research Program (202203021212444); Shanxi Province Education Science “14th Five-Year Plan” 2021 Annual Project General Planning Project + “Industry-University-Research”-driven Smart Agricultural Talent Training Model in Agriculture and Forestry Colleges (GH-21006); Shanxi Agricultural University Teaching Reform Project (J202098572); Shanxi Province Higher Education Teaching Reform and Innovation Project (J20220274); Shanxi Postgraduate Education and Teaching Reform Project Fund (2022YJJG094); Shanxi Agricultural University doctoral research start-up project (2021BQ88); Shanxi Agricultural University Academic Restoration Research Project (2020xshf38); Shanxi Agricultural University 2021 “Neural Network” Course Ideological and Political Project (KCSZ202133).","G. Yang; School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; email: sxauwangzhang@stu.sxau.edu.cn","","MDPI","14248220","","","37299818","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85161581368"
"Ferziger S.S.; Condotta I.C.F.S.; Brown-Brandl T.M.; Shi Y.; Rohrer G.A.","Ferziger, S.S. (58852785700); Condotta, I.C.F.S. (57204022710); Brown-Brandl, T.M. (57207606316); Shi, Y. (58853195500); Rohrer, G.A. (7102792700)","58852785700; 57204022710; 57207606316; 58853195500; 7102792700","Deep-learning-based behavioral time budgets for sows with high and low piglet mortality rates","2023","2023 ASABE Annual International Meeting","","","","","","","1","10.13031/aim.202300833","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183579504&doi=10.13031%2faim.202300833&partnerID=40&md5=b21ba2039cbdb9a2185a9742d6f981dc","Department of Animal Sciences, University of Illinois Urbana-Champaign, Urbana, IL, United States; Department of Biological Systems Engineering, University of Nebraska-Lincoln, Lincoln, NE, United States; Genetics and Breeding Research Unit, USDA-ARS U.S. Meat Animal Research Center, Clay Center, NE, United States","Ferziger S.S., Department of Animal Sciences, University of Illinois Urbana-Champaign, Urbana, IL, United States; Condotta I.C.F.S., Department of Animal Sciences, University of Illinois Urbana-Champaign, Urbana, IL, United States; Brown-Brandl T.M., Department of Biological Systems Engineering, University of Nebraska-Lincoln, Lincoln, NE, United States; Shi Y., Department of Biological Systems Engineering, University of Nebraska-Lincoln, Lincoln, NE, United States; Rohrer G.A., Genetics and Breeding Research Unit, USDA-ARS U.S. Meat Animal Research Center, Clay Center, NE, United States","Piglet crushing is a leading cause of pre-weaning mortality in piglets. Piglet crushing occurs when the sow lies down or rolls over. Although the use of farrowing crates can reduce pre-weaning piglet mortality, substantial pre-weaning mortality persists. Better understanding of the postures and behaviors of sows during the pre-weaning period is needed in order to develop better approaches to reducing pre-weaning mortality due to piglet crushing. The objectives of this study were to 1) create an object detection model using YOLO (You Only Look Once) to detect postures in sows, 2) Determine time budgets for postures of sows during the pre-weaning period using the pre-trained model, and 3) correlate time-budgets with pre-weaning mortality. Digital images of sows and piglets during the first four to seven days after farrowing were labeled for postures and behaviors. A YOLOv8 model was then trained using the labeled images. Time budgets of 4 postures for 6 sows during the first three days after farrowing were determined using the pre-trained YOLOv8 model. The model was capable of detecting four sow postures (kneeling, sitting, standing, and lying) with an overall mean average percent (mAP) of 0.979 at 0.5 intersection over union (IoU). The mAP for all classes was above 0.979 at 0.5 IoU and above 0.83 at 0.5-0.95 IoU. These results are sactifactory to automatically perform behavioral analysis. There were no statistical differences between high mortality and low mortality animals for time spent at each posture, but slightly more time spent kneeling and lying and less time spent sitting and standing was observed for low mortality animals. © 2023 ASABE Annual International Meeting. All Rights Reserved.","computer vision; posture recognition; Precision livestock farming","Agriculture; Animals; Budget control; Crushing; Deep learning; Object detection; Behavioral analysis; Detection models; Digital image; Labeled images; Mortality rate; Objects detection; Posture recognition; Precision livestock farming; Time budget; Time-spent; Computer vision","U.S. Department of Agriculture, USDA; National Institute of Food and Agriculture, NIFA","The USDA is an equal opportunity employer. Mention of trade names or commercial products in this article is solely for the purpose of providing specific information and does not imply recommendation or endorsement by the authors. The authors would like to thank the ARS Meat Animal Research Center Swine Unit staff for their help in collecting data. This work is supported by Agricultural and Food Research Initiative grant no.2021-67015-34413 from the USDA National Institute of Food and Agriculture.","","","American Society of Agricultural and Biological Engineers","","978-171388588-7","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85183579504"
"Wang Y.; Li Q.; Chu M.; Kang X.; Liu G.","Wang, Yanchao (57375336600); Li, Qian (57215209821); Chu, Mengyuan (57224800553); Kang, Xi (57056967700); Liu, Gang (57374481500)","57375336600; 57215209821; 57224800553; 57056967700; 57374481500","Application of infrared thermography and machine learning techniques in cattle health assessments: A review","2023","Biosystems Engineering","230","","","361","387","26","21","10.1016/j.biosystemseng.2023.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159258577&doi=10.1016%2fj.biosystemseng.2023.05.002&partnerID=40&md5=c04e267927894c6dd89add7d35d2f7b3","Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; School of Computing and Data Engineering, NingboTech University, Zhejiang, Ningbo, 315200, China","Wang Y., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; Li Q., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; Chu M., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China; Kang X., School of Computing and Data Engineering, NingboTech University, Zhejiang, Ningbo, 315200, China; Liu G., Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Information Acquisition Technology, Ministry of Agriculture and Rural Affairs of China, China Agricultural University, Beijing, 100083, China","To maintain animal welfare and reduce the economic loss associated with pastures, it is essential to monitor the health status of livestock. Accurate health assessments are the heart of the future of high-quality and high-yielding livestock production. As a non-invasive detection method, infrared thermography (IRT) technology can be used to indicate changes in thermal biological characteristics in animal metabolism, and it is a useful tool to detect issues in animal health. In addition, the development of emerging technologies represented by machine learning (ML) technology provides an opportunity to achieve non-contact, high-precision, automated cattle health assessments. Therefore, in this review, we examine the more than 100 related research papers to summarise and analyse the application of IRT and ML technology in cattle health management. Firstly, IRT and ML technology are introduced, including the concepts of acquiring of thermal images and obtaining parameter information. Secondly, the development status and research progress of IRT technology in cattle health assessment is summarised, focusing not only on bovine disease detection, including mastitis, lameness, respiratory diseases, etc., but also involving the indicators for assessing the health status, including physiological characteristics, stress, temperament and the oestrus of cattle. Thirdly, we focus on the tasks and application potential of ML and the deep learning (DL) algorithms in thermal infrared imaging data analysis before finally discussing the challenges associated with IRT and ML technology in the field of cattle health assessments are and examining the relevant suggestions put forward in this paper. © 2023 IAgrE","Cattle; Health assessment; Infrared thermography; Machine learning; Temperature characteristics","Agriculture; Engineering education; Learning systems; Losses; Mammals; Thermography (imaging); Animal welfare; Cattle; Economic loss; Health assessments; Health status; High quality; Machine learning techniques; Machine learning technology; Machine-learning; Temperature characteristic; Deep learning","China Agricultural University, CAU; National Key Research and Development Program of China, NKRDPC, (2021YFD1300502); National Key Research and Development Program of China, NKRDPC","Funding text 1: This work was supported by the National Key R&D Program of China (no. 2021YFD1300502 ), Beijing. The authors have no conflicts of interest to declare. ; Funding text 2: We would like to thank the support of the National Key R&D Program of China and China Agricultural University .","G. Liu; College of information and electrical engineering, China agricultural university, China; email: pac@cau.edu.cn","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Review","Final","","Scopus","2-s2.0-85159258577"
"Abdulla M.; Marhoon A.","Abdulla, Marwa (57846224200); Marhoon, Ali (54944506400)","57846224200; 54944506400","Deep learning and IoT for Monitoring Tomato Plant","2023","Iraqi Journal for Electrical and Electronic Engineering","19","1","","70","78","8","1","10.37917/ijeee.19.1.9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160663606&doi=10.37917%2fijeee.19.1.9&partnerID=40&md5=f2e181f440bc03d17a3da731610e836c","Computer Science, College of Education, University of Basrah, Basrah, Iraq; Electrical Engineering Department, College of Engineering, University of Basrah, Basrah, Iraq","Abdulla M., Computer Science, College of Education, University of Basrah, Basrah, Iraq; Marhoon A., Electrical Engineering Department, College of Engineering, University of Basrah, Basrah, Iraq","Agriculture is the primary food source for humans and livestock in the world and the primary source for the economy of many countries. The majority of the country's population and the world depend on agriculture. Still, at present, farmers are facing difficulty in dealing with the requirements of agriculture. Due to many reasons, including different and extreme weather conditions, the abundance of water quality, etc. This paper applied the Internet of Things and deep learning system to establish a smart farming system to monitor the environmental conditions that affect tomato plants using a mobile phone. Through deep learning networks, trained the dataset taken from PlantVillage and collected from google images to classify tomato diseases, and obtained a test accuracy of 97%, which led to the publication of the model to the mobile application for classification for its high accuracy. Using the IoT, a monitoring system and automatic irrigation were built that were controlled through the mobile remote to monitor the environmental conditions surrounding the plant, such as air temperature and humidity, soil moisture, water quality, and carbon dioxide gas percentage. The designed system has proven its efficiency when tested in terms of disease classification, remote irrigation, and monitoring of the environmental conditions surrounding the plant. And giving alerts when the values of the sensors exceed the minimum or higher values causing damage to the plant. The farmer can take the appropriate action at the right time to prevent any damage to the plant and thus obtain a high-quality product. © 2023 The Authors. Published by Iraqi Journal for Electrical and Electronic Engineering by College of Engineering, University of Basrah.","Deep learning; IoT; mobile application; plant monitoring; Remote Irrigation; Smart farming system; tomato disease","","","","M. Abdulla; Computer Science, College of Education, University of Basrah, Basrah, Iraq; email: cepsm510006@avicenna.uobasrah.edu.iq","","College of Engineering, University of Basrah","18145892","","","","English","Iraqi. J. Electr. Electron. Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160663606"
"Shin M.; Hwang S.; Kim J.; Kim B.; Jung J.-S.","Shin, MoonSun (14007885100); Hwang, Seonmin (54895925400); Kim, Junghwan (57203324823); Kim, Byungcheol (57190305387); Jung, Jeong-Sung (57218124617)","14007885100; 54895925400; 57203324823; 57190305387; 57218124617","A Study on Analyses of the Production Data of Feed Crops and Vulnerability to Climate Impacts According to Climate Change in Republic of Korea","2023","Applied Sciences (Switzerland)","13","20","11603","","","","0","10.3390/app132011603","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192494097&doi=10.3390%2fapp132011603&partnerID=40&md5=d30fcf427dd93d0e1c570de76f0296b4","Department of Computer Engineering, Konkuk University, Chungju, 27478, South Korea; Department of Information and Communication, Baekseok University, Cheonan, 31065, South Korea; Division of Grassland & Forage, National Institute of Animal Science, Cheonan, 31000, South Korea","Shin M., Department of Computer Engineering, Konkuk University, Chungju, 27478, South Korea; Hwang S., Department of Computer Engineering, Konkuk University, Chungju, 27478, South Korea; Kim J., Department of Computer Engineering, Konkuk University, Chungju, 27478, South Korea; Kim B., Department of Information and Communication, Baekseok University, Cheonan, 31065, South Korea; Jung J.-S., Division of Grassland & Forage, National Institute of Animal Science, Cheonan, 31000, South Korea","According to the climate change scenario, climate change in the Korean Peninsula is expected to worsen due to extreme temperatures, with effects such as rising average temperatures, heat waves, and droughts. In Republic of Korea, which relies on foreign countries for the supply of forage crops, a decrease in the productivity of forage crops is expected to cause increased damage to the domestic livestock industry. In this paper, to solve the issue of climate vulnerability for forage crops, we performed a study to predict the productivity of forage crops in relation to climate change. We surveyed and compiled not only forage crop production data from various regions, but also experimental cultivation production data over several years from reports of the Korea Institute of Animal Science and Technology. Then, we crawled related climate data from the Korea Meteorological Administration. Therefore, we were able to construct a basic database for forage crop production data and related climate data. Using the database, a production prediction model was implemented, applying a multivariate regression analysis and deep learning regression. The key factors were determined as a result of analyzing the changes in forage crop production due to climate change. Using the prediction model, it could be possible to forecast the shifting locations of suitable cultivation areas. As a result of our study, we were able to construct electromagnetic climate maps for forage crops in Republic of Korea. It can be used to present region-specific agricultural insights and guidelines for cultivation technology for forage crops against climate change. © 2023 by the authors.","climate vulnerability; electromagnetic climate map; forage crop productivity; predictive model; suitable cultivation area","","Rural Development Administration, RDA","This research was funded by the Cooperative Research Program for Agriculture Science and Technology Development (No. PJ015079032023), Rural Development Administration, Republic of Korea.","J.-S. Jung; Division of Grassland & Forage, National Institute of Animal Science, Cheonan, 31000, South Korea; email: jjs3873@korea.kr","","Multidisciplinary Digital Publishing Institute (MDPI)","20763417","","","","English","Appl. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192494097"
"Pan Y.; Zhang Y.; Wang X.; Gao X.X.; Hou Z.","Pan, Yuanzhi (57963299800); Zhang, Yuzhen (58117967200); Wang, Xiaoping (58566017100); Gao, Xiang Xiang (57221328865); Hou, Zhongyu (14422520400)","57963299800; 58117967200; 58566017100; 57221328865; 14422520400","Low-cost livestock sorting information management system based on deep learning","2023","Artificial Intelligence in Agriculture","9","","","110","126","16","19","10.1016/j.aiia.2023.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170040525&doi=10.1016%2fj.aiia.2023.08.007&partnerID=40&md5=48e87bdbbce84e38f617b2586e3862e4","Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co. Ltd., Zhenjiang, 212050, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Jiangsu Province, Shanghai, 200030, China; School of Innovation and Entrepreneurship, Jiangsu University, 301 Xuefu Road, Jiangsu Province, Zhenjiang, 212013, China; Shanghai Picowave Technology Co. Ltd., Shanghai, 201802, China; Guangzhou Institute of Advanced Technology, Chinese Academy of Science (GIAT), Guangzhou, 511458, China","Pan Y., Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co. Ltd., Zhenjiang, 212050, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Jiangsu Province, Shanghai, 200030, China; Zhang Y., Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co. Ltd., Zhenjiang, 212050, China, School of Innovation and Entrepreneurship, Jiangsu University, 301 Xuefu Road, Jiangsu Province, Zhenjiang, 212013, China; Wang X., Shanghai Picowave Technology Co. Ltd., Shanghai, 201802, China; Gao X.X., School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Jiangsu Province, Shanghai, 200030, China; Hou Z., School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Jiangsu Province, Shanghai, 200030, China, Guangzhou Institute of Advanced Technology, Chinese Academy of Science (GIAT), Guangzhou, 511458, China","Modern pig farming leaves much to be desired in terms of efficiency, as these systems rely mainly on electromechanical controls and can only categorize pigs according to their weight. This method is not only inefficient but also escalates labor expenses and heightens the threat of zoonotic diseases. Furthermore, confining pigs in large groups can exacerbate the spread of infections and complicate the monitoring and care of ill pigs. This research executed an experiment to construct a deep-learning sorting mechanism, leveraging a dataset infused with pivotal metrics and breeding imagery gathered over 24 months. This research integrated a Kalman filter-based algorithm to augment the precision of the dynamic sorting operation. This research experiment unveiled a pioneering machine vision sorting system powered by deep learning, adept at handling live imagery for multifaceted recognition objectives. The Individual recognition model based on Residual Neural Network (ResNet) monitors livestock weight for sustained data forecasting, whereas the Wasserstein Generative Adversarial Nets (WGAN) image enhancement algorithm bolsters recognition in distinct settings, fortifying the model's resilience. Notably, system can pinpoint livestock exhibiting signs of potential illness via irregular body appearances and isolate them for safety. Experimental outcomes validate the superiority of this proposed system over traditional counterparts. It not only minimizes manual interventions and data upkeep expenses but also heightens the accuracy of livestock identification and optimizes data usage. This findings reflect an 89% success rate in livestock ID recognition, a 32% surge in obscured image recognition, a 95% leap in livestock categorization accuracy, and a remarkable 98% success rate in discerning images of unwell pigs. In essence, this research augments identification efficiency, curtails operational expenses, and provides enhanced tools for disease monitoring. © 2023 The Authors","Image recognition; Pig monitoring; ResNet; Sorting system; WGAN","Costs; Deep learning; Farms; Image enhancement; Image recognition; Information management; Learning systems; Mammals; Sorting; Electro-mechanical control; Information management systems; Labor expense; Low-costs; Neural-networks; Pig farming; Pig monitoring; Residual neural network; Sorting system; Wasserstein generative adversarial net; Efficiency","Zhenjiang Hongxiang AutomationTechnology Co. Ltd; Jiangsu Provincial Department of Science and Technology, JST; Science and Technology Bureau of Zhenjiang","This work was financial supported by the Zhenjiang Science and Technology Bureau, Jiangsu Science and Technology Department,financial support was received from Zhenjiang Hongxiang AutomationTechnology Co. Ltd. ","Y. Pan; Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co. Ltd., Zhenjiang, Jiangsu Province, 212050, China; email: yzpan@connect.hku.hk; Z. Hou; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, 800 Dongchuan Road, 200030, China; email: zhyhou@sjtu.edu.cn","","KeAi Communications Co.","25897217","","","","English","Artif. Intell. Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85170040525"
"Nguyen A.H.; Holt J.P.; Knauer M.T.; Abner V.A.; Lobaton E.J.; Young S.N.","Nguyen, Anh H. (58089429200); Holt, Jonathan P. (57197307470); Knauer, Mark T. (18042554400); Abner, Victoria A. (58089337700); Lobaton, Edgar J. (24450724500); Young, Sierra N. (57194775342)","58089429200; 57197307470; 18042554400; 58089337700; 24450724500; 57194775342","Towards rapid weight assessment of finishing pigs using a handheld, mobile RGB-D camera","2023","Biosystems Engineering","226","","","155","168","13","12","10.1016/j.biosystemseng.2023.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147328595&doi=10.1016%2fj.biosystemseng.2023.01.005&partnerID=40&md5=9d50ac116ea4e69def2c3071861b80c4","Electrical and Computer Engineering, North Carolina State University, Campus Box 7911, Raleigh, 27695, NC, United States; Animal Science, North Carolina State University, Campus Box 7621, Raleigh, 27695, NC, United States; Biological and Agricultural Engineering, North Carolina State University, Campus Box 7625, Raleigh, 27695, NC, United States; Civil and Environmental Engineering, Utah State University, Logan, 84322, UT, United States","Nguyen A.H., Electrical and Computer Engineering, North Carolina State University, Campus Box 7911, Raleigh, 27695, NC, United States; Holt J.P., Animal Science, North Carolina State University, Campus Box 7621, Raleigh, 27695, NC, United States; Knauer M.T., Animal Science, North Carolina State University, Campus Box 7621, Raleigh, 27695, NC, United States; Abner V.A., Animal Science, North Carolina State University, Campus Box 7621, Raleigh, 27695, NC, United States; Lobaton E.J., Electrical and Computer Engineering, North Carolina State University, Campus Box 7911, Raleigh, 27695, NC, United States; Young S.N., Biological and Agricultural Engineering, North Carolina State University, Campus Box 7625, Raleigh, 27695, NC, United States, Civil and Environmental Engineering, Utah State University, Logan, 84322, UT, United States","Pig weight measurement is essential for monitoring performance, welfare, and production value. Weight measurement using a scale provides the most accurate results; however, it is time consuming and may increase animal stress. Subjective visual evaluations, even when conducted by an experienced caretaker, lack consistency and accuracy. Optical sensing systems provide alternative methods for estimating pig weight, but studies examining these systems only focus on images taken from stationary cameras. This study fills a gap in existing technology through examining a handheld, portable RGB-D imaging system for estimating pig weight. An Intel RealSense camera collected RGB-D data from finishing pigs at various market weights. 3D point clouds were computed for each pig, and latent features from a 3D generative model were used to predict pig weights using three regression models (SVR, MLP and AdaBoost). These models were compared to two baseline models: median prediction and linear regression using body dimension measurements as predictor variables. Using 10-fold cross validation mean absolute error (MAE) and root-mean-square error (RMSE), all three latent feature models performed better than the median prediction model (MAE = 12.3 kg, RMSE = 16.0 kg) but did not outperform linear regression between weight and girth measurements (MAE = 4.06 kg, RMSE = 4.94 kg). Of the models under consideration, SVR performed best (MAE = 9.25 kg, RMSE = 12.3 kg, mean absolute percentage error = 7.54%) when tested on unseen data. This research is an important step towards developing rapid pig body weight estimation methods from a handheld, portable imaging system by leveraging deep learning feature outputs and depth imaging technology. © 2023 IAgrE","Deep learning; Depth imaging; Precision livestock farming; Weight prediction","3D modeling; Adaptive boosting; Agriculture; Cameras; Deep learning; Errors; Imaging systems; Mammals; Mean square error; Weighing; Deep learning; Depth imaging; Handhelds; Mean absolute error; Monitoring performance; Performance value; Precision livestock farming; Production value; Root mean square errors; Weight prediction; Forecasting","National Pork Board Animal Science and Welfare Committe, (19-208); National Pork Board Animal Science and Welfare Committee; National Institute of Food and Agriculture, NIFA, (1021499); National Institute of Food and Agriculture, NIFA","Funding text 1: This project was funded by the National Pork Board Animal Science and Welfare Committe e project number 19-208 , and the USDA National Institute of Food and Agriculture Hatch project number 1021499 .; Funding text 2: This project was funded by the National Pork Board Animal Science and Welfare Committee project number 19-208, and the USDA National Institute of Food and Agriculture Hatch project number 1021499.","S.N. Young; Biological and Agricultural Engineering, North Carolina State University, Raleigh, Campus Box 7625, 27695, United States; email: sierra.young@usu.edu","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85147328595"
"Sadeghi M.; Banakar A.; Minaei S.; Orooji M.; Shoushtari A.; Li G.","Sadeghi, Mohammad (57214847748); Banakar, Ahmad (16642169100); Minaei, Saeid (11241743100); Orooji, Mahdi (15843943400); Shoushtari, Abdolhamid (36026028300); Li, Guoming (57200960751)","57214847748; 16642169100; 11241743100; 15843943400; 36026028300; 57200960751","Early Detection of Avian Diseases Based on Thermography and Artificial Intelligence","2023","Animals","13","14","2348","","","","14","10.3390/ani13142348","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165953057&doi=10.3390%2fani13142348&partnerID=40&md5=e1e57504d34ba7a7ec415b44c3c78641","Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran; Department of Medical Engineering, Tarbiat Modares University, Tehran, 14117-13116, Iran; Department of Poultry Disease, Razi Vaccine and Serum Research Institute, Karaj, 31976-19751, Iran; Department of Poultry Science, Institute for Artificial Intelligence, University of Georgia, Athens, 30602, GA, United States","Sadeghi M., Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran; Banakar A., Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran; Minaei S., Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran; Orooji M., Department of Medical Engineering, Tarbiat Modares University, Tehran, 14117-13116, Iran; Shoushtari A., Department of Poultry Disease, Razi Vaccine and Serum Research Institute, Karaj, 31976-19751, Iran; Li G., Department of Poultry Science, Institute for Artificial Intelligence, University of Georgia, Athens, 30602, GA, United States","Non-invasive measures have a critical role in precision livestock and poultry farming as they can reduce animal stress and provide continuous monitoring. Animal activity can reflect physical and mental states as well as health conditions. If any problems are detected, an early warning will be provided for necessary actions. The objective of this study was to identify avian diseases by using thermal-image processing and machine learning. Four groups of 14-day-old Ross 308 Broilers (20 birds per group) were used. Two groups were infected with one of the following diseases: Newcastle Disease (ND) and Avian Influenza (AI), and the other two were considered control groups. Thermal images were captured every 8 h and processed with MATLAB. After de-noising and removing the background, 23 statistical features were extracted, and the best features were selected using the improved distance evaluation method. Support vector machine (SVM) and artificial neural networks (ANN) were developed as classifiers. Results indicated that the former classifier outperformed the latter for disease classification. The Dempster–Shafer evidence theory was used as the data fusion stage if neither ANN nor SVM detected the diseases with acceptable accuracy. The final SVM-based framework achieved 97.2% and 100% accuracy for classifying AI and ND, respectively, within 24 h after virus infection. The proposed method is an innovative procedure for the timely identification of avian diseases to support early intervention. © 2023 by the authors.","avian disease; machine learning; poultry; precision livestock farming; thermography","Article; artificial intelligence; artificial neural network; avian influenza; bird disease; confusion matrix; controlled study; convolutional neural network; deep learning; diagnostic test accuracy study; disease classification; early intervention; fluid intake; food intake; health; image processing; image segmentation; influenza; livestock; machine learning; mental health; Newcastle disease; nonhuman; physiological stress; poultry farming; real time polymerase chain reaction; sensitivity and specificity; support vector machine; thermography; virus infection","University of Georgia, UGA; Iran National Science Foundation, INSF, (98012001)","Partial funding of this work was supported by federal and state funds allocated to the University of Georgia. The authors would like to thank the ‘Iranian National Science Foundation’ for financial support of PhD project with grant number 98012001.","A. Banakar; Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran; email: ah_banakar@modares.ac.ir; G. Li; Department of Poultry Science, Institute for Artificial Intelligence, University of Georgia, Athens, 30602, United States; email: gmli@uga.edu","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85165953057"
"Meng D.-Y.; Li T.; Li H.-X.; Zhang M.; Tan K.; Huang Z.-P.; Li N.; Wu R.-H.; Li X.-W.; Chen B.-H.; Ren G.-P.; Xiao W.; Yang D.-Q.","Meng, De-Yao (58537033000); Li, Tao (58737561400); Li, Hao-Xuan (57612772100); Zhang, Mei (57215731150); Tan, Kun (57211501988); Huang, Zhi-Pang (37067504200); Li, Na (59087240000); Wu, Rong-Hai (58537595000); Li, Xiao-Wei (57208131215); Chen, Ben-Hui (35179094700); Ren, Guo-Peng (24725530400); Xiao, Wen (7202456631); Yang, Deng-Qi (48362065100)","58537033000; 58737561400; 57612772100; 57215731150; 57211501988; 37067504200; 59087240000; 58537595000; 57208131215; 35179094700; 24725530400; 7202456631; 48362065100","A method for automatic identification and separation of wildlife images using ensemble learning","2023","Ecological Informatics","77","","102262","","","","5","10.1016/j.ecoinf.2023.102262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167962196&doi=10.1016%2fj.ecoinf.2023.102262&partnerID=40&md5=5397619386941c89326f828b76dfaf94","College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China; Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China; College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China; Yunling black-and-white snub-nosed monkey observation and research station of Yunnan province, Yunnan, Dali, 671003, China; Department of Mathematics and Information Technology, Lijiang Teachers College, Yunnan, Lijiang, 674100, China","Meng D.-Y., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Li T., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Li H.-X., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Zhang M., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Tan K., Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China, College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China; Huang Z.-P., Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China, College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China, Yunling black-and-white snub-nosed monkey observation and research station of Yunnan province, Yunnan, Dali, 671003, China; Li N., Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China, College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China; Wu R.-H., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Li X.-W., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China; Chen B.-H., Department of Mathematics and Information Technology, Lijiang Teachers College, Yunnan, Lijiang, 674100, China; Ren G.-P., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China, Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China, College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China; Xiao W., Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China, College of Agricultural and Biological Sciences, Dali University, Yunnan, Dali, 671003, China, Yunling black-and-white snub-nosed monkey observation and research station of Yunnan province, Yunnan, Dali, 671003, China; Yang D.-Q., College of Mathematics and Computer Science, Dali University, Yunnan, Dali, 671003, China, Institute of Eastern-Himalaya Biodiversity Research, Dali University, Yunnan, Dali, 671003, China, Collaborative Innovation Center for the Biodiversity in the Three Parallel Rivers of China, Dali, 671003, China","Camera traps have revolutionized wildlife resource surveys by enabling the acquisition of comprehensive ecosystem information. Camera traps usually produce massive images. Snapshot datasets captured in nature reserves proximate to human habitation often comprise a significant volume of human activity images containing humans or livestock. Manually identifying and labeling wildlife images from large-scale datasets is a labor-intensive task that necessitates a substantial number of professionals and incurs expensive personnel costs. By harnessing the capabilities of deep learning technology to automatically differentiate between wildlife and human activity images, ecologists can solely focus on manually labeling wildlife images that account for a minor fraction of the dataset. This strategic shift can significantly reduce personnel expenses and enhance work efficiency. Existing research usually treats human activity images as ordinary categories and utilizes species recognition methods to automatically identify and filter them. However, when human activity images overwhelmingly predominate the dataset, established species recognition methods are susceptible to misclassifying a substantial proportion of wildlife images as human activity images. This misclassification can potentially result in ecologists missing opportunities to discover or observe wildlife. To tackle this challenge, we proposed an ensemble learning method based on a conservative strategy and current mainstream deep learning frameworks to automatically identify wildlife and human activity images. We validated our method on a camera trap dataset from Lasha Mountain (LSM) in Yunnan, China. The experimental results demonstrated that our method automatically identified wildlife images from the dataset with accuracy, recall, and precision of 95.75%, 94.07%, and 83.89%, respectively. This led to an approximately 80% reduction in personnel costs. © 2023 Elsevier B.V.","Camera trap images; Deep learning; Ensemble learning; Human activity images; Wildlife image","China; Yunnan; data set; ensemble forecasting; human activity; identification method; machine learning; precision; satellite imagery","National Natural Science Foundation of China, NSFC, (31960119, 32260131, 62262001); National Natural Science Foundation of China, NSFC","This study was supported by the National Natural Science Foundation of China ( 32260131 , 31960119 , and 62262001 ). ","D.-Q. Yang; Department of Mathematics and Computer Science, Dali University, Dali, Yunnan, 671003, China; email: dqyang@dali.edu.cn","","Elsevier B.V.","15749541","","","","English","Ecol. Informatics","Article","Final","","Scopus","2-s2.0-85167962196"
"Zhang X.; Xuan C.; Ma Y.; Su H.","Zhang, Xiwen (57222511917); Xuan, Chuanzhong (36618394100); Ma, Yanhua (55570493100); Su, He (56032339000)","57222511917; 36618394100; 55570493100; 56032339000","A high-precision facial recognition method for small-tailed Han sheep based on an optimised Vision Transformer","2023","Animal","17","8","100886","","","","10","10.1016/j.animal.2023.100886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165272712&doi=10.1016%2fj.animal.2023.100886&partnerID=40&md5=8b5cd01f55abbc3133f49efa04fe0055","College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China; Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China","Zhang X., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China, Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China; Xuan C., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China, Inner Mongolia Engineering Research Center for Intelligent Facilities in Prataculture and Livestock Breeding, Inner Mongolia, Hohhot, 010018, China; Ma Y., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China; Su H., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Inner Mongolia, Hohhot, 010018, China","Accurate identification of individual animals plays a pivotal role in enhancing animal welfare and optimising farm production. Although Radio Frequency Identification technology has been widely applied in animal identification, this method still exhibits several limitations that make it difficult to meet current practical application requirements. In this study, we proposed ViT-Sheep, a sheep face recognition model based on the Vision Transformer (ViT) architecture, to facilitate precise animal management and enhance livestock welfare. Compared to Convolutional Neural Network (CNN), ViT is renowned for its competitive performance. The experimental procedure of this study consisted of three main steps. Firstly, we collected face images of 160 experimental sheep to construct the sheep face image dataset. Secondly, we developed two sets of sheep face recognition models based on CNN and ViT, respectively. To enhance the ability to learn sheep face biological features, we proposed targeted improvement strategies for the sheep face recognition model. Specifically, we introduced the LayerScale module into the encoder of the ViT-Base-16 model and employed transfer learning to improve recognition accuracy. Finally, we compared the training results of different recognition models and the ViT-Sheep model. The results demonstrated that our proposed method achieved the highest performance on the sheep face image dataset, with a recognition accuracy of 97.9%. This study demonstrates that ViT can successfully achieve sheep face recognition tasks with good robustness. Furthermore, the findings of this research will promote the practical application of artificial intelligence animal recognition technology in sheep production. © 2023 The Author(s)","Deep learning; Identity recognition; Pattern recognition; Precision livestock farming; Sheep face recognition","Animal Welfare; Animals; Artificial Intelligence; Facial Recognition; Farms; Livestock; Sheep; agricultural worker; animal; animal welfare; artificial intelligence; facial recognition; livestock; sheep","Fundamental Research Funds of Inner Mongolia Agricultural University, (BR221032, BR221314); Science and Technology Planning Project of Inner Mongolia Autonomous Region, (2021GG0111)","This work was supported by the Science and Technology Planning Project of Inner Mongolia Autonomous Region (2021GG0111) and the Fundamental Research Funds of Inner Mongolia Agricultural University (BR221032 and BR221314).","C. Xuan; College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, Inner Mongolia, 010018, China; email: xcz@imau.edu.cn","","Elsevier B.V.","17517311","","","37422932","English","Animal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165272712"
"Huang E.; Mao A.; Hou J.; Wu Y.; Xu W.; Camila Ceballos M.; Parsons T.D.; Liu K.","Huang, Endai (57226523976); Mao, Axiu (57238219000); Hou, Junhui (36989331100); Wu, Yongjian (58138117600); Xu, Weitao (57188754808); Camila Ceballos, Maria (55958638400); Parsons, Thomas D. (57210708339); Liu, Kai (55823366100)","57226523976; 57238219000; 36989331100; 58138117600; 57188754808; 55958638400; 57210708339; 55823366100","Occlusion-Resistant instance segmentation of piglets in farrowing pens using center clustering network","2023","Computers and Electronics in Agriculture","210","","107950","","","","7","10.1016/j.compag.2023.107950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160620711&doi=10.1016%2fj.compag.2023.107950&partnerID=40&md5=4eb6b515b879ce9de34f773ea059e1bd","Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Department of Production Animal Health, Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States","Huang E., Department of Computer Science, City University of Hong Kong, Hong Kong, Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Mao A., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Hou J., Department of Computer Science, City University of Hong Kong, Hong Kong; Wu Y., Department of Computer Science, City University of Hong Kong, Hong Kong; Xu W., Department of Computer Science, City University of Hong Kong, Hong Kong; Camila Ceballos M., Department of Production Animal Health, Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Parsons T.D., Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States; Liu K., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong","Computer vision enables the development of new approaches to monitor the behavior, health, and welfare of animals. Instance segmentation is a high-precision method in computer vision for detecting individual animals of interest. This method can be used for in-depth analysis of animals, such as examining their subtle interactive behaviors, from videos and images. However, existing deep-learning-based instance segmentation methods have been mostly developed based on public datasets, which largely omit heavy occlusion problems; therefore, these methods have limitations in real-world applications involving object occlusions, such as farrowing pen systems used on pig farms in which the farrowing crates often impede the sow and piglets. In this paper, we adapt a Center Clustering Network originally designed for counting to achieve instance segmentation, dubbed as CClusnet-Inseg. Specifically, CClusnet-Inseg uses each pixel to predict object centers and trace these centers to form masks based on clustering results, which consists of a network for segmentation and center offset vector map, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, Centers-to-Mask (C2M), and Remain-Centers-to-Mask (RC2M) algorithms. In all, 4,600 images were extracted from six videos collected from three closed and three half-open farrowing crates to train and validate our method. CClusnet-Inseg achieves a mean average precision (mAP) of 84.1 and outperforms all other methods compared in this study. We conduct comprehensive ablation studies to demonstrate the advantages and effectiveness of core modules of our method. In addition, we apply CClusnet-Inseg to multi-object tracking for animal monitoring, and the predicted object center that is a conjunct output could serve as an occlusion-resistant representation of the location of an object. © 2023 Elsevier B.V.","Animal monitoring; Computer vision; Deep learning; Farrowing crate; Precision livestock farming","Clustering algorithms; Deep learning; Farms; Learning systems; Mammals; Animal monitoring; Clustering networks; Deep learning; Farrowing crate; High-precision; In-depth analysis; Interactive behavior; New approaches; Precision livestock farming; Segmentation methods; computer vision; livestock farming; machine learning; network analysis; pig; tracking; Computer vision","Pennsylvania Pork Producers Council; School of Veterinary Medicine, University of Pennsylvania; City University of Hong Kong, CityU, (9610450); National Pork Board, NPB","Funding text 1: Thanks to the staff at the swine teaching and research center, School of Veterinary Medicine, University of Pennsylvania for animal care. Funding was provided in part by the National Pork Board and the Pennsylvania Pork Producers Council in the U.S. and the new research initiatives at City University of Hong Kong (Project number: 9610450).; Funding text 2: Thanks to the staff at the swine teaching and research center, School of Veterinary Medicine, University of Pennsylvania for animal care. Funding was provided in part by the National Pork Board and the Pennsylvania Pork Producers Council in the U.S., and the new research initiatives at City University of Hong Kong (Project number: 9610450). ","K. Liu; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; email: kailiu@cityu.edu.hk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85160620711"
"Kim T.-K.; Kim J.S.; Cho H.-C.","Kim, Tae-Kyeong (57444811500); Kim, Jin Soo (35754486700); Cho, Hyun-Chong (22233514800)","57444811500; 35754486700; 22233514800","Deep-learning-based gestational sac detection in ultrasound images using modified YOLOv7-E6E model","2023","Journal of Animal Science and Technology","65","3","","627","637","10","2","10.5187/jast.2023.e43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164938764&doi=10.5187%2fjast.2023.e43&partnerID=40&md5=eb4d8c73c9b89d6ac0de2b5be8c762d8","Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Dept. of Electronics Engineering and Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea","Kim T.-K., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Kim J.S., College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Cho H.-C., Dept. of Electronics Engineering and Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea","As the population and income levels rise, meat consumption steadily increases annually. However, the number of farms and farmers producing meat decrease during the same period, reducing meat sufficiency. Information and Communications Technology (ICT) has begun to be applied to reduce labor and production costs of livestock farms and improve productivity. This technology can be used for rapid pregnancy diagnosis of sows; the location and size of the gestation sacs of sows are directly related to the productivity of the farm. In this study, a system proposes to determine the number of gestation sacs of sows from ultrasound images. The system used the YOLOv7-E6E model, changing the activation function from sigmoid-weighted linear unit (SiLU) to a multi-Activation function (SiLU + Mish). Also, the upsampling method was modified from nearest to bicubic to improve performance. The model trained with the original model using the original data achieved mean average precision of 86.3%. When the proposed multi-Activation function, upsampling, and AutoAugment were applied, the performance improved by 0.3%, 0.9%, and 0.9%, respectively. When all three proposed methods were simultaneously applied, a significant performance improvement of 3.5% to 89.8% was achieved. © 2023 Korean Society of Animal Sciences and Technology. All rights reserved.","Deep learning; Object-detection algorithm; Pig sac; Sow; Ultrasound","","Ministry of Education, MOE, (2022R1I1A3053872); Ministry of Education, MOE; Rural Development Administration, RDA; National Research Foundation of Korea, NRF; National Institute of Animal Science, NIAS","Funding text 1: This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2022R1I1A3053872) and this study was supported by 2022 the RDA Fellowship Program of National Institute of Animal Science, Rural Development Administration, Korea.; Funding text 2: (NRF) funded by the Ministry of Education (No. 2022R1I1A3053872).","J.S. Kim; College of Animal Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; email: kjs896@kangwon.ac.kr; H.-C. Cho; Dept. of Electronics Engineering and Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; email: hyuncho@kangwon.ac.kr","","Korean Society of Animal Sciences and Technology","26720191","","","","English","J.  Anim.  Sci. Technol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85164938764"
"Zhang J.; Lei J.; Wu J.; Lu H.; Guo H.; Pezzuolo A.; Kolpakov V.; Ruchay A.","Zhang, Jialong (57217684405); Lei, Jie (58700383100); Wu, Jianhuan (58637245700); Lu, Hexiao (58637428100); Guo, Hao (55331600800); Pezzuolo, Andrea (56023708900); Kolpakov, Vladimir (58511094300); Ruchay, Alexey (57192592568)","57217684405; 58700383100; 58637245700; 58637428100; 55331600800; 56023708900; 58511094300; 57192592568","Automatic method for quantitatively analyzing the body condition of livestock from 3D shape","2023","Computers and Electronics in Agriculture","214","","108307","","","","7","10.1016/j.compag.2023.108307","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173609418&doi=10.1016%2fj.compag.2023.108307&partnerID=40&md5=f1b0677ea7a305378fb3c83ec53b102a","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Department of Land, Environment, Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy; Department of Agronomy, Food, Natural Resources, Animals and Environment, Legnaro, 35020, Italy; Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Chelyabinsk, 454001, Russian Federation; Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation","Zhang J., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Lei J., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Wu J., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Lu H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Guo H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Pezzuolo A., Department of Land, Environment, Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy, Department of Agronomy, Food, Natural Resources, Animals and Environment, Legnaro, 35020, Italy; Kolpakov V., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Chelyabinsk, 454001, Russian Federation; Ruchay A., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation","Body condition has always been an important indicator reflecting the health status of livestock, and new automated and contactless methods for assessing it have also been continuously proposed. In recent years, with the continuous expansion of research on quantitative analysis of three-dimensional shapes in the field of agriculture, amount of studies has shown that quantitative analysis of shapes can promote the development of smart agriculture. This article proposed a method for quantitatively analyzing the local three-dimensional shape of livestock to evaluate their body conditions. This is a universal method that can be applied to various species. It uses the method of point-to-point correspondence in the three-dimensional shape to finely calculate the difference between shapes, and then maps the difference value to the range of the body condition score. The article analyzed the shape of 198 dairy cows, 100 beef cattle, and 201 pigs using body condition scores as test parameters. The accuracy of body condition evaluation within the range of error of 0.5 for dairy cows, 1 for beef cattle, and 0.5 for pigs are 100%, 87.61%, and 92.09%, respectively. Compared to previous methods for assessing body condition, the method proposed in this paper is more accurate and versatile. The dataset used in this study, along with the code implementation have been made publicly available in https://gitee.com/kznd/lshape-analyser/tree/master/MATLAB. © 2023 Elsevier B.V.","3D shape analysis; Body Condition Score; Deep learning; Precision livestock farming","Deep learning; Farms; 3-D shape; 3D shape analysis; Automatic method; Beef cattle; Body condition; Body condition score; Dairy cow; Deep learning; Precision livestock farming; Three-dimensional shape; algorithm; body condition; health status; livestock farming; quantitative analysis; Mammals","National Natural Science Foundation of China, NSFC, (41601491, 42071449); Russian Science Foundation, RSF, (21-76-20014); National Key Research and Development Program of China, NKRDPC, (2021YFD1300502-01)","The authors wish to thank everyone who participated in the research experiment. This research is supported by the National Key Research and Development Program of China ( 2021YFD1300502-01 ) , the National Natural Science Foundation of China [grant numbers 42071449 , 41601491 ] and the Russian Science Foundation within the framework of the scientific project No. 21-76-20014..","H. Guo; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; email: guohaolys@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85173609418"
"Li X.; Xiang Y.; Li S.","Li, Xiaopeng (57271095500); Xiang, Yuyun (57921087600); Li, Shuqin (55490641400)","57271095500; 57921087600; 55490641400","Combining convolutional and vision transformer structures for sheep face recognition","2023","Computers and Electronics in Agriculture","205","","107651","","","","33","10.1016/j.compag.2023.107651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434411&doi=10.1016%2fj.compag.2023.107651&partnerID=40&md5=57b3ab3fd7c7d68de2d4591b80973cf2","College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Li X., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Xiang Y., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Li S., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Significant progress has been made in individual livestock recognition based on convolutional neural networks (CNN), however, their performance still needs improvement. Vision transformer (ViT) emerged as a cutting-edge approach, which has been successfully applied in many tasks of the computer vision field. The superior performance of ViT motivates us to study whether ViT can provide more accurate results for sheep face recognition. In this study, we propose MobileViTFace for sheep face recognition. MobileViTFace is a lightweight sheep face recognition model which combines the convolutional and transformer structures. Compared with the standard ViT model, MobileViTFace does not require too much training data and high computational complexity and is more convenient to deploy on edge devices. Extensive benchmarking tests illustrate that MobileViTFace can secure competitive performance, which achieved 97.13% recognition accuracy on 7,434 sheep face images containing 186 sheep, significantly better than lightweight models based on convolutional structures such as MobileNet, EfficientNet, etc. Parameters and floating-point operations (FLOPs) are reduced by five times compared to ResNet-50, which has similar recognition accuracy. Real-time and accurate recognition results are obtained on the Jetson Nano-based edge computing platform, which is helpful for practical production. © 2023","Convolutional neural network; Deep learning; Digital agriculture; Sheep face recognition; Vision transformer","Agriculture; Benchmarking; Convolution; Convolutional neural networks; Deep neural networks; Digital arithmetic; E-learning; Convolutional neural network; Cutting edges; Deep learning; Digital agriculture; Performance; Recognition accuracy; Recognition models; Sheep face recognition; Transformer structure; Vision transformer; artificial neural network; image analysis; instrumentation; livestock farming; Face recognition","","","S. Li; College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: lsq_cie@nwafu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85146434411"
"Huang E.; He Z.; Mao A.; Camila Ceballos M.; Parsons T.D.; Liu K.","Huang, Endai (57226523976); He, Zheng (58444762800); Mao, Axiu (57238219000); Camila Ceballos, Maria (55958638400); Parsons, Thomas D. (57210708339); Liu, Kai (55823366100)","57226523976; 58444762800; 57238219000; 55958638400; 57210708339; 55823366100","A semi-supervised generative adversarial network for amodal instance segmentation of piglets in farrowing pens","2023","Computers and Electronics in Agriculture","209","","107839","","","","6","10.1016/j.compag.2023.107839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152944229&doi=10.1016%2fj.compag.2023.107839&partnerID=40&md5=e97c1297312e2519a6d060129b355caa","Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States","Huang E., Department of Computer Science, City University of Hong Kong, Hong Kong, Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; He Z., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Mao A., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Camila Ceballos M., Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Parsons T.D., Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States; Liu K., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong","Occlusions, such as farrowing pens in piggeries, hinder computer vision applications for automated animal monitoring. Amodal instance segmentation (AIS), aiming to predict a complete mask of an occluded target, is a promising solution. However, AIS usually requires amodal datasets, which are challenging to create and limit the application of AIS. To solve this problem, we proposed a novel semi-supervised generative adversarial network (GAN) for AIS, denoted “the AISGAN”. Our AISGAN only requires a regular modal dataset and generate amodal samples by random occlusions, making the AIS method more applicable. A corresponding segmentation loss was added to overcome mode collapse of GAN. The results showed that the AISGAN achieved a mean Intersection of Union (mIoU) of 0.823 and outperformed the mIoUs of Mask RCNN, Raw, and Convex Hull (0.801, 0.780, and 0.778, respectively). As a semi-supervised method, the mIoU of our AISGAN was further enhanced (by 0.6%) when we fine-tuned it with unlabeled new data, showing its extensibility to new unseen scenarios. The visualization demonstrates that the AISGAN can produce realistic masks of piglets, including details of their noses and legs, even under heavily occluded conditions. With the AISGAN, we achieved an occlusion-resistant spatial distribution analysis of the piglets in farrowing pens. Thus, the AISGAN is a promising tool to manage occlusion problems for automated animal monitoring in complex housing environments. © 2023 Elsevier B.V.","Animal monitoring; Computer vision; De-occlusion; Deep learning; Farrowing crate; Precision livestock farming","Agriculture; Animals; Computer vision; Deep learning; Supervised learning; Animal monitoring; Computer vision applications; Convex hull; De-occlusion; Deep learning; Farrowing crate; Precision livestock farming; Segmentation methods; Semi-supervised; Semi-supervised method; biomonitoring; computer vision; livestock farming; pig; precision agriculture; segmentation; spatial distribution; supervised learning; Generative adversarial networks","School of Veterinary Medicine, University of Pennsylvania; City University of Hong Kong, (9610450)","Thanks to the staff at the swine teaching and research center, School of Veterinary Medicine, University of Pennsylvania for animal care. Thanks to Cheryl Sze and Yongjian Wu for their contributions in data labeling. Funding was provided by the new research initiatives at City University of Hong Kong (Project number: 9610450).","K. Liu; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; email: kailiu@cityu.edu.hk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85152944229"
"Xue J.; Xia S.; Li Z.; Wang X.; Huang L.; He R.; Li S.","Xue, Jingbo (57191156611); Xia, Shang (36728593100); Li, Zhaojun (55623740100); Wang, Xinyi (57221485247); Huang, Liangyu (57698620300); He, Runchao (58388297000); Li, Shizhu (26641946500)","57191156611; 36728593100; 55623740100; 57221485247; 57698620300; 58388297000; 26641946500","Intelligent identification of livestock, a source of Schistosoma japonicum in- fection, based on deep learning of unmanned aerial vehicle images","2023","Chinese Journal of Schistosomiasis Control","35","2","","121","127","6","0","10.16250/j.32.1374.2022273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160649259&doi=10.16250%2fj.32.1374.2022273&partnerID=40&md5=e1a70d74c9a8e076bb42b6a4a207d8a0","National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China; School of Global Health, Shanghai Jiao Tong University School of Medicine and Chinese Center for Tropical Diseases Research, Shanghai, 200025, China; Jiangxi Provincial Institute of Parasitic Diseases Control, Jiangxi Provincial Key Laboratory of Schistosomiasis Prevention and Control, China","Xue J., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China, School of Global Health, Shanghai Jiao Tong University School of Medicine and Chinese Center for Tropical Diseases Research, Shanghai, 200025, China; Xia S., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China, School of Global Health, Shanghai Jiao Tong University School of Medicine and Chinese Center for Tropical Diseases Research, Shanghai, 200025, China; Li Z., Jiangxi Provincial Institute of Parasitic Diseases Control, Jiangxi Provincial Key Laboratory of Schistosomiasis Prevention and Control, China; Wang X., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China; Huang L., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China; He R., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China; Li S., National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China, School of Global Health, Shanghai Jiao Tong University School of Medicine and Chinese Center for Tropical Diseases Research, Shanghai, 200025, China","[Abstract] Objective To develop an intelligent recognition model based on deep learning algorithms of unmanned aerial ve- hicle (UAV) images, and to preliminarily explore the value of this model for remote identification, monitoring and management of cattle, a source of Schistosoma japonicum infection. Methods Oncomelania hupensis snail-infested marshlands around the Poy- ang Lake area were selected as the study area. Image datasets of the study area were captured by aerial photography with UAV and subjected to augmentation. Cattle in the sample database were annotated with the annotation software VGG Image Annotator to create the morphological recognition labels for cattle. A model was created for intelligent recognition of livestock based on deep learning-based Mask R-convolutional neural network (CNN) algorithms. The performance of the model for cattle recognition was evaluated with accuracy, precision, recall, F1 score and mean precision. Results A total of 200 original UAV images were obtained, and 410 images were yielded following data augmentation. A total of 2 860 training samples of cattle recognition were labeled. The created deep learning-based Mask R-CNN model converged following 200 iterations, with an accuracy of 88.01%, precision of 92.33%, recall of 94.06%, F1 score of 93.19%, and mean precision of 92.27%, and the model was effective to detect and segment the morphological features of cattle. Conclusion The deep learning-based Mask R-CNN model is highly accurate for recognition of cattle based on UAV images, which is feasible for remote intelligent recognition, monitoring, and management of the source of S. japonicum infection. © 2023, Editorial Office of Chinese Journal of Schistosomiasis Control. All rights reserved.","Cattle Schistosomiasis; Convolutional; Deep learning; Image recognition; neural network; Source of infection; Unmanned aerial vehicle","Animals; Cattle; Deep Learning; Livestock; Neural Networks, Computer; Schistosomiasis japonica; Unmanned Aerial Devices; algorithm; Article; bovine; convolutional neural network; deep learning; feasibility study; lake; livestock; measurement accuracy; nonhuman; Oncomelania hupensis; photography; schistosomiasis japonica; unmanned aerial vehicle; animal; artificial neural network; unmanned aerial vehicle; veterinary medicine","","","S. Li; National Institute of Parasitic Diseases, Chinese Center for Disease Control and Prevention (Chinese Center for Tropical Diseases Research), National Health Commission Key Laboratory of Parasite and Vector Biology, WHO Collaborating Centre for Tropical Diseases, National Center for International Research on Tropical Diseases, Shanghai, 200025, China; email: lisz@chinacdc.cn","","Editorial Office of Chinese Journal of Schistosomiasis Control","10056661","","","37253560","Chinese","Chin. J. Schistosomiasis Control","Article","Final","","Scopus","2-s2.0-85160649259"
"Tikarya K.; Jain Y.V.; Bhise D.","Tikarya, Krishna (58916270700); Jain, Yash Vilas (57456677100); Bhise, Dhiraj (57647297400)","58916270700; 57456677100; 57647297400","A review: Cattle Breed and Skin Disease Identification Using Deep Learning","2023","Proceedings - 4th IEEE 2023 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2023","","","","835","842","7","1","10.1109/ICCCIS60361.2023.10425776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186500650&doi=10.1109%2fICCCIS60361.2023.10425776&partnerID=40&md5=9376e79fe101eb73713a159bcada633a","SVKM's NMIMS, MPSTME, Department of Information Technology, Shirpur, India","Tikarya K., SVKM's NMIMS, MPSTME, Department of Information Technology, Shirpur, India; Jain Y.V., SVKM's NMIMS, MPSTME, Department of Information Technology, Shirpur, India; Bhise D., SVKM's NMIMS, MPSTME, Department of Information Technology, Shirpur, India","In recent years, skin conditions have become a serious danger to livestock productivity, resulting in significant financial losses and health problems f or both humans and animals. Due to the complexity of this virus, it is difficult to detect, so early detection and accurate diagnosis are important. We thought a new approach for cattle skin disease detection, breed identification and weight estimation using image processing and deep learning techniques. The proposed system will use digital images taken from different angles of the cow's body to extract relevant features related to Lumpy skin disease, breed identification a nd weight prediction. So in this research paper we are providing a review on varieties of datasets, general architecture, different deep learning methods and performance metrics to be considered to implement such type of applications. Image processing techniques such as segmentation and texture analysis are used to isolate regions of interest and extract meaningful information about skin disease, body weight and breed. In this study further, we explore how to use Compared to conventional methods, convolutional neural networks which can detect and identify LSDV more quickly and precisely. We also considered some uses for this new technology and talked about the benefits of employing C NNs for this purpose. To determine the extent to which animals are affected by lumpy skin disease, we need to know the different stages of the disease that can be possible with the help of breed and weight identification. This review is concerned with Early diagnosis of the virus that causes lumpy skin condition, breed identification and weight prediction using multiple CNN methods and deep learning algorithms. These approaches have great potential to improve animal husbandry practices. © 2023 IEEE.","Breed Identification; Convolutional Neural Net-work; Deep Learning; Lumpy skin disease virus; weight prediction","Agriculture; Animals; Convolution; Convolutional neural networks; Deep learning; Diagnosis; Image segmentation; Learning algorithms; Learning systems; Losses; Textures; Viruses; Breed identifications; Convolutional neural net-work; Deep learning; Financial health; Lumpy skin disease virus; Net work; Skin conditions; Skin disease; Skin disease identifications; Weight prediction; Forecasting","","","","Nand P.; Singh M.; Kaur M.; Jain V.; Gupta K.","Institute of Electrical and Electronics Engineers Inc.","","979-835030611-8","","","English","Proc. - IEEE Int. Conf. Comput., Commun., Intell. Syst., ICCCIS","Conference paper","Final","","Scopus","2-s2.0-85186500650"
"Ruchay A.; Akulshin I.; Kolpakov V.; Dzhulamanov K.; Guo H.; Pezzuolo A.","Ruchay, Alexey (57192592568); Akulshin, Ilya (58915678900); Kolpakov, Vladimir (58511094300); Dzhulamanov, Kinispay (57203805515); Guo, Hao (55331600800); Pezzuolo, Andrea (56023708900)","57192592568; 58915678900; 58511094300; 57203805515; 55331600800; 56023708900","Cattle Face Recognition Using Deep Transfer Learning Techniques","2023","2023 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2023 - Proceedings","","","","569","574","5","1","10.1109/MetroAgriFor58484.2023.10424103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186526700&doi=10.1109%2fMetroAgriFor58484.2023.10424103&partnerID=40&md5=a643440668597d112ac8ca884cb018da","Federal Research Centre of Biological Systems and Agro-technologies of RAS, Orenburg, Russian Federation; Chelyabinsk State University, Chelyabinsk, Russian Federation; Orenburg State University, Orenburg, Russian Federation; China Agricultural University, College of Land Science and Technology, Beijing, China; University of Padova, Department of Land, Environment, Agriculture, and Forestry, Legnaro, Italy","Ruchay A., Federal Research Centre of Biological Systems and Agro-technologies of RAS, Orenburg, Russian Federation, Chelyabinsk State University, Chelyabinsk, Russian Federation; Akulshin I., Chelyabinsk State University, Chelyabinsk, Russian Federation; Kolpakov V., Federal Research Centre of Biological Systems and Agro-technologies of RAS, Orenburg, Russian Federation, Orenburg State University, Orenburg, Russian Federation; Dzhulamanov K., Federal Research Centre of Biological Systems and Agro-technologies of RAS, Orenburg, Russian Federation; Guo H., China Agricultural University, College of Land Science and Technology, Beijing, China; Pezzuolo A., University of Padova, Department of Land, Environment, Agriculture, and Forestry, Legnaro, Italy","In this research, a new approach has been proposed for cattle face recognition using RGB images based on deep convolutional neural network. Nowadays, biometric identification of animals is a major problem in computer vision and livestock sector. In this research, all RGB images were preprocessed to improve recognition reliability. A deep learning model was carried out using additional data augmentation methods and fine neural network tuning. The pre-trained neural networks chosen were VGGFACE and VGGFACE2. As a result, the VGGFACE2 pre-trained neural network was chosen to identify cattle faces with 97.1% accuracy. © 2023 IEEE.","Animal Face Identification; Animal face Recognition; Artificial Intelligence; Deep Learning; Livestock Housing; Precision Livestock Farming","Agriculture; Animals; Convolutional neural networks; Deep neural networks; Image enhancement; Learning systems; Animal face identification; Animal face recognition; Deep learning; Face identification; Learning techniques; Livestock housing; Precision livestock farming; RGB images; Trained neural networks; Transfer learning; Face recognition","Russian Science Foundation, RSF, (21-76-20014); Russian Science Foundation, RSF","ACKNOWLEDGEMENT This research was supported by the Russian Science Foundation, grant n. 21-76-20014.","A. Ruchay; Federal Research Centre of Biological Systems and Agro-technologies of RAS, Orenburg, Russian Federation; email: ran@csu.ru","","Institute of Electrical and Electronics Engineers Inc.","","979-835031272-0","","","English","IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.","Conference paper","Final","","Scopus","2-s2.0-85186526700"
"Zhao S.; Bai Z.; Wang S.; Gu Y.","Zhao, Shida (57221231831); Bai, Zongchun (54379654800); Wang, Shucai (23020397800); Gu, Yue (57578479800)","57221231831; 54379654800; 23020397800; 57578479800","Research on Automatic Classification and Detection of Mutton Multi-Parts Based on Swin-Transformer","2023","Foods","12","8","1642","","","","5","10.3390/foods12081642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156263423&doi=10.3390%2ffoods12081642&partnerID=40&md5=b1658f12222a41480e3cc72da60d8cac","Institute of Facilities and Equipment in Agriculture, Jiangsu Academy of Agricultural Sciences, Nanjing, 210014, China; College of Engineering, Huazhong Agricultural University, Wuhan, 430070, China","Zhao S., Institute of Facilities and Equipment in Agriculture, Jiangsu Academy of Agricultural Sciences, Nanjing, 210014, China; Bai Z., Institute of Facilities and Equipment in Agriculture, Jiangsu Academy of Agricultural Sciences, Nanjing, 210014, China; Wang S., College of Engineering, Huazhong Agricultural University, Wuhan, 430070, China; Gu Y., College of Engineering, Huazhong Agricultural University, Wuhan, 430070, China","In order to realize the real-time classification and detection of mutton multi-part, this paper proposes a mutton multi-part classification and detection method based on the Swin-Transformer. First, image augmentation techniques are adopted to increase the sample size of the sheep thoracic vertebrae and scapulae to overcome the problems of long-tailed distribution and non-equilibrium of the dataset. Then, the performances of three structural variants of the Swin-Transformer (Swin-T, Swin-B, and Swin-S) are compared through transfer learning, and the optimal model is obtained. On this basis, the robustness, generalization, and anti-occlusion abilities of the model are tested and analyzed using the significant multiscale features of the lumbar vertebrae and thoracic vertebrae, by simulating different lighting environments and occlusion scenarios, respectively. Furthermore, the model is compared with five methods commonly used in object detection tasks, namely Sparser-CNN, YoloV5, RetinaNet, CenterNet, and HRNet, and its real-time performance is tested under the following pixel resolutions: 576 × 576, 672 × 672, and 768 × 768. The results show that the proposed method achieves a mean average precision (mAP) of 0.943, while the mAP for the robustness, generalization, and anti-occlusion tests are 0.913, 0.857, and 0.845, respectively. Moreover, the model outperforms the five aforementioned methods, with mAP values that are higher by 0.009, 0.027, 0.041, 0.050, and 0.113, respectively. The average processing time of a single image with this model is 0.25 s, which meets the production line requirements. In summary, this study presents an efficient and intelligent mutton multi-part classification and detection method, which can provide technical support for the automatic sorting of mutton as well as for the processing of other livestock meat. © 2023 by the authors.","classification; computer vision; deep learning; detection; livestock meat; mutton processing","","Agricultural Science and Technology Independent Innovation Project of Jiangsu Province, (CX(22)1008); Thirteenth Five-Year” Plan for Science & Technology Support of China, (2018YFD0700804)","This research was funded by the National “Thirteenth Five-Year” Plan for Science & Technology Support of China (Grant No. 2018YFD0700804), the Agricultural Science and Technology Independent Innovation Project of Jiangsu Province (Grant No. CX(22)1008).","Z. Bai; Institute of Facilities and Equipment in Agriculture, Jiangsu Academy of Agricultural Sciences, Nanjing, 210014, China; email: vipmaple@126.com","","MDPI","23048158","","","","English","Foods","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85156263423"
"Lamping C.; Kootstra G.; Derks M.","Lamping, Christian (57439490500); Kootstra, Gert (24537424000); Derks, Marjolein (54908684100)","57439490500; 24537424000; 54908684100","Uncertainty estimation for deep neural networks to improve the assessment of plumage conditions of chickens","2023","Smart Agricultural Technology","5","","100308","","","","2","10.1016/j.atech.2023.100308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170106191&doi=10.1016%2fj.atech.2023.100308&partnerID=40&md5=61a95c08a756e1f9fdad3b0dfe030b3e","Agricultural Biosystems Engineering, Wageningen University & Research, Wageningen, 6700 AA, Netherlands; Big Dutchman International GmbH, Auf der Lage 2, Vechta, 49377, Germany","Lamping C., Agricultural Biosystems Engineering, Wageningen University & Research, Wageningen, 6700 AA, Netherlands, Big Dutchman International GmbH, Auf der Lage 2, Vechta, 49377, Germany; Kootstra G., Agricultural Biosystems Engineering, Wageningen University & Research, Wageningen, 6700 AA, Netherlands; Derks M., Agricultural Biosystems Engineering, Wageningen University & Research, Wageningen, 6700 AA, Netherlands","The combination of computer vision with deep learning has become a popular tool for automation of labor-intensive monitoring tasks in modern livestock farming. However, uncontrolled and varying environmental conditions, which usually prevail in farmhouses, influence the performance of vision-based applications. Image quality can be reduced, for instance by occlusions, illumination or motions of the animals, which can influence the reliability of those applications. To address this issue, this study proposes an approach for the identification of uncertain neural-network predictions to improve the overall prediction quality. It proposes the direct quantification of aleatoric and epistemic uncertainty on the one hand and indirect estimation of uncertainty through the prediction of occlusions on the other hand. Our approach simultaneously integrates the different methods into an end-to-end trainable instance segmentation and regression model. The objective of this study was to first investigate how well the different measures can quantify the uncertainty of a prediction by comparing them to human uncertainty assessments. Then, it was analyzed whether the uncertainty estimations are capable to identify and reject erroneous predictions by evaluating the correlation between the predictive error and the uncertainty estimations. Finally, individual predictions were rejected based on the estimated uncertainties to analyze the effect on the overall accuracy. As a use-case, the developed methods were applied to the prediction of plumage conditions of chickens but also examined in a separate domain. The results showed that the outputs of our approaches for the estimation of aleatoric and epistemic uncertainty correlate to the predictive error of the model, and lead to increased performance when uncertain predictions are rejected. In contrast, the indirect method to identify occluded samples did not serve as a reliable indicator for uncertainty and could therefore not be used to improve the accuracy of the model outputs. © 2023 The Author(s)","Computer vision; Deep learning; Poultry; Uncertainty","","","","C. Lamping; Agricultural Biosystems Engineering, Wageningen University & Research, Wageningen, 6700 AA, Netherlands; email: christian.lamping@wur.nl","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85170106191"
"Rančić K.; Blagojević B.; Bezdan A.; Ivošević B.; Tubić B.; Vranešević M.; Pejak B.; Crnojević V.; Marko O.","Rančić, Kristina (58175309400); Blagojević, Boško (36914805600); Bezdan, Atila (53982572700); Ivošević, Bojana (56538600400); Tubić, Bojan (57211413440); Vranešević, Milica (56156786400); Pejak, Branislav (57218825618); Crnojević, Vladimir (6506665059); Marko, Oskar (57190189311)","58175309400; 36914805600; 53982572700; 56538600400; 57211413440; 56156786400; 57218825618; 6506665059; 57190189311","Animal Detection and Counting from UAV Images Using Convolutional Neural Networks","2023","Drones","7","3","179","","","","18","10.3390/drones7030179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152007712&doi=10.3390%2fdrones7030179&partnerID=40&md5=dddfb9695bfa0c443416a2e759ef77ba","Faculty of Sciences, University of Novi Sad, Novi Sad, 21102, Serbia; Faculty of Agriculture, University of Novi Sad, Novi Sad, 21102, Serbia; BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia; JVP Vojvodinašume, Petrovaradin, 21132, Serbia","Rančić K., Faculty of Sciences, University of Novi Sad, Novi Sad, 21102, Serbia; Blagojević B., Faculty of Agriculture, University of Novi Sad, Novi Sad, 21102, Serbia; Bezdan A., Faculty of Agriculture, University of Novi Sad, Novi Sad, 21102, Serbia; Ivošević B., BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia; Tubić B., JVP Vojvodinašume, Petrovaradin, 21132, Serbia; Vranešević M., Faculty of Agriculture, University of Novi Sad, Novi Sad, 21102, Serbia; Pejak B., BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia; Crnojević V., BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia; Marko O., BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia","In the last decade, small unmanned aerial vehicles (UAVs/drones) have become increasingly popular in the airborne observation of large areas for many purposes, such as the monitoring of agricultural areas, the tracking of wild animals in their natural habitats, and the counting of livestock. Coupled with deep learning, they allow for automatic image processing and recognition. The aim of this work was to detect and count the deer population in northwestern Serbia from such images using deep neural networks, a tedious process that otherwise requires a lot of time and effort. In this paper, we present and compare the performance of several state-of-the-art network architectures, trained on a manually annotated set of images, and use it to predict the presence of objects in the rest of the dataset. We implemented three versions of the You Only Look Once (YOLO) architecture and a Single Shot Multibox Detector (SSD) to detect deer in a dense forest environment and measured their performance based on mean average precision (mAP), precision, recall, and F1 score. Moreover, we also evaluated the models based on their real-time performance. The results showed that the selected models were able to detect deer with a mean average precision of up to 70.45% and a confidence score of up to a 99%. The highest precision was achieved by the fourth version of YOLO with 86%, as well as the highest recall value of 75%. Its compressed version achieved slightly lower results, with 83% mAP in its best case, but it demonstrated four times better real-time performance. The counting function was applied on the best-performing models, providing us with the exact distribution of deer over all images. Yolov4 obtained an error of 8.3% in counting, while Yolov4-tiny mistook 12 deer, which accounted for an error of 7.1%. © 2023 by the authors.","animal counting; convolutional neural networks; deep learning; deer; SSD; YOLO","Aircraft detection; Convolutional neural networks; Deep neural networks; Fertilizers; Image annotation; Livestock; Mammals; Unmanned aerial vehicles (UAV); Airborne observations; Animal counting; Convolutional neural network; Deep learning; Deer; Real time performance; Single shot multibox detector; Single-shot; Small unmanned aerial vehicles; You only look once","Horizon 2020 Framework Programme, H2020, (664387, 739570); Horizon 2020 Framework Programme, H2020; Ministarstvo Prosvete, Nauke i Tehnološkog Razvoja, MPNTR, (451-03-47/2023-01/200358, 451-03-68/2022-14/200117); Ministarstvo Prosvete, Nauke i Tehnološkog Razvoja, MPNTR","This work is supported through ANTARES project that has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement SGA-CSA. No. 739570 under FPA No. 664387—https://doi.org/10.3030/739570. The authors acknowledge financial support from the Ministry of Education, Science and Technological Development of the Republic of Serbia (Grant No. 451-03-68/2022-14/200117 and 451-03-47/2023-01/200358).","O. Marko; BioSense Institute, University of Novi Sad, Novi Sad, 21102, Serbia; email: oskar.marko@biosense.rs","","Multidisciplinary Digital Publishing Institute (MDPI)","2504446X","","","","English","Drones","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85152007712"
"Kwon K.; Park A.; Lee H.; Mun D.","Kwon, Kiyoun (57207767644); Park, Ahram (58245862300); Lee, Hyunoh (57211034722); Mun, Duhwan (23019305700)","57207767644; 58245862300; 57211034722; 23019305700","Deep learning-based weight estimation using a fast-reconstructed mesh model from the point cloud of a pig","2023","Computers and Electronics in Agriculture","210","","107903","","","","14","10.1016/j.compag.2023.107903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159130687&doi=10.1016%2fj.compag.2023.107903&partnerID=40&md5=79e991570d81b04929bfbc30f453c3be","School of Industrial Engineering, Kumoh National Institute of Technology, 61 Daehak-ro, Gyeongbuk, Gumi, 39177, South Korea; School of Mechanical Engineering, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, 02841, South Korea","Kwon K., School of Industrial Engineering, Kumoh National Institute of Technology, 61 Daehak-ro, Gyeongbuk, Gumi, 39177, South Korea; Park A., School of Industrial Engineering, Kumoh National Institute of Technology, 61 Daehak-ro, Gyeongbuk, Gumi, 39177, South Korea; Lee H., School of Mechanical Engineering, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, 02841, South Korea; Mun D., School of Mechanical Engineering, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, 02841, South Korea","In the livestock industry, securing the quality of the livestock is as important as reducing the rearing costs. Therefore, the body type and weight of livestock should be measured periodically during the entire rearing period to maintain a history of changes in body shape and weight. This study proposed the method of estimating the weight of a pig in real-time using mesh reconstruction and deep learning. The proposed method is divided into two stages, that is, generating training data by mesh reconstruction from point clouds of pigs and developing a deep neural network (DNN) to estimate weight by utilizing the training data. After acquiring 1022 point clouds for 70 pigs with the measurement equipment, the pig weight estimation experiment proceeded according to the proposed method. As a result, the mesh model could be rapidly generated within 1 s. Additionally, 48 types of measurements were determined from the mesh model, and the weight was estimated using a fully connected DNN. The results showed very high accuracy with an error of 4.89 kg (2.11% error with respect to pig weight) for the test data set. © 2023 Elsevier B.V.","3D reconstruction; Deep learning; Pigs; Point cloud; Real-time weight estimation","Agriculture; Image reconstruction; Mammals; Mesh generation; Statistical tests; Three dimensional computer graphics; 3D reconstruction; Deep learning; Mesh modeling; Mesh reconstruction; Pig; Point-clouds; Real- time; Real-time weight estimation; Time weights; Weights estimation; body shape; data set; machine learning; real time; reconstruction; three-dimensional modeling; Deep neural networks","Ministry of Trade, Industry and Energy, MOTIE; Ministry of Land, Infrastructure and Transport, MOLIT, (20012462); Ministry of Science, ICT and Future Planning, MSIP, (RS-2022-00143652); National Research Foundation of Korea, NRF","This research was supported by the Basic Science Research Program (No. NRF-2022R1A2C2005879 and 2021R1F1A104817211 ) through the National Research Foundation of Korea (NRF) funded by the Korean Government (MSIT), by the Liquid Air Energy Storage (LAES) and Utilization System Development Program (No. RS-2022-00143652) funded by the Korean government (MOLIT), and by the Industrial Technology Innovation Program (No. 20012462) funded by the Korean government (MOTIE).","D. Mun; School of Mechanical Engineering, Korea University, Seoul, 145 Anam-ro, Seongbuk-gu, 02841, South Korea; email: dhmun@korea.ac.kr","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85159130687"
"Wu D.; Han M.; Song H.; Song L.; Duan Y.","Wu, Dihua (57210377164); Han, Mengxuan (57221916841); Song, Huaibo (17342958900); Song, Lei (57211428262); Duan, Yuanchao (57543631300)","57210377164; 57221916841; 17342958900; 57211428262; 57543631300","Monitoring the respiratory behavior of multiple cows based on computer vision and deep learning","2023","Journal of Dairy Science","106","4","","2963","2979","16","21","10.3168/jds.2022-22501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148610596&doi=10.3168%2fjds.2022-22501&partnerID=40&md5=a02248a7bc92c34f60177a88fca54542","College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China; School of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, 310058, China","Wu D., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China, School of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, 310058, China; Han M., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China; Song H., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China; Song L., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China; Duan Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Services, Yangling, 712100, China","Automatic respiration monitoring of dairy cows in modern farming not only helps to reduce manual labor but also increases the automation of health assessment. It is common for cows to congregate on farms, which poses a challenge for manual observation of cow status because they physically occlude each other. In this study, we propose a method that can monitor the respiratory behavior of multiple cows. Initially, 4,000 manually labeled images were used to fine-tune the YOLACT (You Only Look At CoefficienTs) model for recognition and segmentation of multiple cows. Respiratory behavior in the resting state could better reflect their health status. Then, the specific resting states (lying resting, standing resting) of different cows were identified by fusing the convolutional neural network and bidirectional long and short-term memory algorithms. Finally, the corresponding detection algorithms (lying and standing resting) were used for respiratory behavior monitoring. The test results of 60 videos containing different interference factors indicated that the accuracy of respiratory behavior monitoring of multiple cows in 54 videos was >90.00%, and that of 4 videos was 100.00%. The average accuracy of the proposed method was 93.56%, and the mean absolute error and root mean square error were 3.42 and 3.74, respectively. Furthermore, the effectiveness of the method was analyzed for simultaneous monitoring of respiratory behavior of multiple cows under movement, occlusion disturbance, and behavioral changes. It was feasible to monitor the respiratory behavior of multiple cows based on the proposed algorithm. This study could provide an a priori technical basis for respiratory behavior monitoring and automatic diagnosis of respiratory-related diseases of multiple dairy cows based on biomedical engineering technology. In addition, it may stimulate researchers to develop robots with health-sensing functions that are oriented toward precision livestock farming. © 2023 American Dairy Science Association","computer vision; deep learning; multiple dairy cows; respiratory behavior monitoring","Animals; Behavior, Animal; Cattle; Computers; Dairying; Deep Learning; Feeding Behavior; Female; animal; animal behavior; bovine; computer; dairying; feeding behavior; female; procedures","National Natural Science Foundation of China, NSFC, (32272931); National Key Research and Development Program of China, NKRDPC, (2017YFD0701603); Fundamental Research Funds for the Central Universities, (2452019027)","This work was supported by the National Natural Science Foundation of China (Grant No. 32272931; Beijing, China), the National Key R&D Program of China (Grant No. 2017YFD0701603; Beijing, China), and the Fundamental Research Funds for the Central Universities (Grant No. 2452019027; Yangling, China). The authors have not stated any conflicts of interest.","H. Song; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: songhuaibo@nwafu.edu.cn","","Elsevier Inc.","00220302","","","36797189","English","J. Dairy Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85148610596"
"Wang Y.; Sun W.; Cao S.; Kong F.","Wang, Yi (58930103600); Sun, Wei (57208906031); Cao, Shanshan (57217367100); Kong, Fantao (55538906800)","58930103600; 57208906031; 57217367100; 55538906800","Cow Estrus Status Identification and Individual Identification System Based on Voiceprint Data","2023","2023 IEEE International Conference on Electrical, Automation and Computer Engineering, ICEACE 2023","","","","1219","1223","4","0","10.1109/ICEACE60673.2023.10442438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187396667&doi=10.1109%2fICEACE60673.2023.10442438&partnerID=40&md5=ccbfba01cb0036788a2afee9517cfa95","Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, China","Wang Y., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China, College of Computer and Information Engineering, Xinjiang Agricultural University, Urumqi, China; Sun W., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; Cao S., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; Kong F., Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China","This paper proposes a cow estrus status identification and individual identity recognition system based on voiceprint data. By introducing deep learning and voiceprint recognition technology to replace the traditional manual monitoring of cattle, we can achieve accurate monitoring of the estrus status of cattle. It can effectively solve the problem of missed and wrong detection of estrus cows in large-scale beef cattle breeding scenarios. The method includes: building a data collection system through collars, pickups and high-definition network cameras, using the data collection system to build a cattle voiceprint database and obtaining a high-quality cattle estrus cry data set, and cascade training Conv-TasNet and EcapaTdnn two voiceprint field depths Learn the model to accurately and effectively identify the estrus state and identity of cattle. This research provides an efficient and accurate solution for cattle estrus identification and individual identification. It is expected to be widely used in the breeding industry, improve breeding efficiency, reduce labor costs, and make a positive contribution to the development of the livestock breeding industry.  © 2023 IEEE.","Deep learning; Estrus state; Individual identity; Voiceprint data; Voiceprint recognition","Agriculture; Deep learning; Speech recognition; Wages; Data collection system; Deep learning; Estrus state; Identity recognition; Individual identification; Individual identity; Recognition systems; Status identifications; Voiceprint data; Voiceprint recognition; Data acquisition","Basic scientific research business fund of the Institute of Agricultural Information of the Chinese Academy of Agricultural Sciences, (JBYW-AII-2023-10); Chinese Academy of Agricultural Sciences, CAAS, (CAAS-ASTIP-2016-AII); Chinese Academy of Agricultural Sciences, CAAS","ACKNOWLEDGMENT The work of this dissertation was financially supported by Chinese Academy of Agricultural Sciences Innovation Project ""Research on Extraction Method of Key Factors of ""IoT Ranch"" with Multi-modal Data Fusion"" (CAAS-ASTIP-2016-AII) and Basic scientific research business fund of the Institute of Agricultural Information of the Chinese Academy of Agricultural Sciences ""Research on multi-modal data fusion sensing method of estrus status of beef cattle in complex pasture environment"" (JBYW-AII-2023-10).","F. Kong; Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; email: kongfantao@caas.cn","","Institute of Electrical and Electronics Engineers Inc.","","979-835030961-4","","","English","IEEE Int. Conf. Electr., Autom. Comput. Eng., ICEACE","Conference paper","Final","","Scopus","2-s2.0-85187396667"
"Lin D.; Kenéz Á.; McArt J.A.A.; Li J.","Lin, Dan (57364856900); Kenéz, Ákos (55559647100); McArt, Jessica A.A. (36091616800); Li, Jun (56007009300)","57364856900; 55559647100; 36091616800; 56007009300","Transformer neural network to predict and interpret pregnancy loss from activity data in Holstein dairy cows","2023","Computers and Electronics in Agriculture","205","","107638","","","","7","10.1016/j.compag.2023.107638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146660032&doi=10.1016%2fj.compag.2023.107638&partnerID=40&md5=85d6e6794792b89ba0025502f8824e51","City University of Hong Kong, Shenzhen Research Institute, Shenzhen, China; Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong; Department of Population Medicine and Diagnostic Sciences, College of Veterinary Medicine, Cornell University, Ithaca, NY, United States; School of Data Science, City University of Hong Kong, Hong Kong","Lin D., City University of Hong Kong, Shenzhen Research Institute, Shenzhen, China, Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong; Kenéz Á., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong; McArt J.A.A., Department of Population Medicine and Diagnostic Sciences, College of Veterinary Medicine, Cornell University, Ithaca, NY, United States; Li J., City University of Hong Kong, Shenzhen Research Institute, Shenzhen, China, Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong, School of Data Science, City University of Hong Kong, Hong Kong","Predicting/detecting pregnancy loss of dairy cows offers the opportunity to shorten the time interval between artificial inseminations. Although several methods of pregnancy detection are being practiced, models with accurate, timely and interpretable detection of pregnancy are still lacking. This study proposed a transformer neural network to predict the probability of pregnancy loss based on continuous activity data, which were collected from activity-monitoring tags attached to 185 Holstein cows from a commercial dairy farm in Cayuga County, NY, USA. Our best model achieved an average accuracy of 0.87, F1 score of 0.87, recall of 0.87 and specificity of 0.90 using 14-day time-series activity windows (90% overlap) using 5-fold cross-validation, outperforming commonly used classic statistical learning and deep learning models for time-series data. The results indicated that our predictive model gave high probabilities of correctly detecting pregnancy loss prior to the increased activities and veterinary confirmation by transrectal ultrasound. In addition, our model interpretation aligned with the changes in the temporal activity levels, revealing that drastic fluctuations in time-series activity data contributed heavily to the final prediction. To the best of our knowledge, this is the first work on developing transformer models for the prediction of pregnancy loss in dairy cows. In addition to facilitating the development of future precision management on modern farms, our work potentiates an increase in the reproductive efficiency and profitability of dairy farms. © 2023 Elsevier B.V.","Dairy cow; Precision livestock farming; Pregnancy loss prediction; Time-series activity","Cayuga County; New York [United States]; United States; Deep learning; Farms; Forecasting; Learning systems; Obstetrics; Ultrasonic applications; Dairy cow; Dairy farms; Loss prediction; Neural-networks; Precision livestock farming; Pregnancy loss; Pregnancy loss prediction; Time interval; Time-series activity; Times series; artificial neural network; cattle; livestock farming; precision; prediction; pregnancy; profitability; time series analysis; Time series","Allflex Livestock Intelligence; City University, (7005530, 7005756, 9678247, 9680310); Guangdong Basic and Applied Research Major Program, (2019B030302005); Shenzhen Basic Research Program, (JCYJ20190808182402941)","This study was supported by Shenzhen Basic Research Program (JCYJ20190808182402941), Guangdong Basic and Applied Research Major Program (2019B030302005), Collaborative Research Fund (C7013-19GF) in Hong Kong, and City University of Hong Kong internal grant (7005530, 7005756, 9678247, 9680310). We appreciate Dr. Sabine Mann for revising and improving this paper sincerely. We thank the Allflex Livestock Intelligence for providing the activity data.","J.A.A. McArt; City University of Hong Kong, Hong Kong; email: jmcart@cornell.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85146660032"
"Cheng J.; Ding L.; Fan H.","Cheng, Jun (58681311600); Ding, Linfang (56001881400); Fan, Hongchao (55368902500)","58681311600; 56001881400; 55368902500","Automated Sheep Detection from UAV Images for the Application of Sheep Roundup; [无人机影像目标自动提取在羊群围捕中的应用研究]","2023","Journal of Geo-Information Science","25","11","","2281","2292","11","0","10.12082/dqxxkx.2023.230202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175735503&doi=10.12082%2fdqxxkx.2023.230202&partnerID=40&md5=3909c73518fe61eca1695ca689fe7a1b","The First Topographic Surveying Brigade of Ministry of Natural Resources, Xi'an, 710054, China; Norwegian University of Science and Technology, Trondheim, 7491, Norway","Cheng J., The First Topographic Surveying Brigade of Ministry of Natural Resources, Xi'an, 710054, China, Norwegian University of Science and Technology, Trondheim, 7491, Norway; Ding L., Norwegian University of Science and Technology, Trondheim, 7491, Norway; Fan H., Norwegian University of Science and Technology, Trondheim, 7491, Norway","Each year, approximately 2.1 million sheep are released to graze freely in vast, forest-covered, and mountainous areas throughout Norway. At the end of the grazing season, farmers must find and round up their sheep. This can be a time consuming and challenging process because of the large area and cluttered nature of the sheep grazing environment. Existing technologies that help farmers find their sheep, such as bells, radio bells, electronic ear tags and UAVs, are limited by the cost, signal coverage, and low degree of automation, which cannot efficiently and automatically locate sheep in the wild. This study proposes an automatic sheep detection algorithm using UAV images. A model architecture using the ResNet and ResNeXt as the backbone networks is designed to address the automatic sheep detection task from UAV RGB and infrared images. Our study evaluates how well this model meets performance and processing speed requirements of a real-world application. We also compare models using fused RGB and infrared data to models using either RGB or infrared as input, and further explore the model complexity and generalization ability. Results show that fusion of RGB and infrared data yields better average precision results than using single RGB or infrared dataset in the model. The set of optimal solutions achieve average precision scores in the range of 69.6% to 96.3% with inference times ranging from 0.1 to 0.6 seconds per image. The most accurate network achieves a grid precision of 97.9% and a recall of 90.1%, at a confidence threshold of 0.5. This corresponds to the detection of 97.5% of the sheep in the validation dataset. These satisfactory results demonstrate the great potential of the proposed automatic sheep detection method using multi-channel UAV images for improving sheep roundup. © 2023 South China University of Technology. All rights reserved.","deep learning; fusion; infrared images; object detection; ResNet; ResNeXt; RGB images; sheep roundup","Norway; Agriculture; Aircraft detection; Bells; Deep learning; Image enhancement; Infrared imaging; Unmanned aerial vehicles (UAV); Covered areas; Deep learning; Infrared data; Infrared image; Mountainous area; Objects detection; Resnet; Resnext; RGB images; Sheep roundup; aerial survey; detection method; grazing; livestock farming; sheep; unmanned vehicle; Object detection","","","J. Cheng; The First Topographic Surveying Brigade of Ministry of Natural Resources, Xi'an, 710054, China; email: chengjun-1@163.com","","Science Press","15608999","","","","Chinese","J. Geo-Inf. Sci.","Article","Final","","Scopus","2-s2.0-85175735503"
"Huang Z.; Xu A.; Zhou S.; Ye J.; Weng X.; Xiang Y.","Huang, Zhijie (58071834200); Xu, Aijun (8564267200); Zhou, Suyin (57201635408); Ye, Junhua (57449082800); Weng, Xiaoxing (36096103600); Xiang, Yun (58571211700)","58071834200; 8564267200; 57201635408; 57449082800; 36096103600; 58571211700","Key point detection method for pig face fusing reparameterization and attention mechanisms; [融合重参数化和注意力机制的猪脸关键点检测方法]","2023","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","39","12","","141","149","8","6","10.11975/j.issn.1002-6819.202303201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170517528&doi=10.11975%2fj.issn.1002-6819.202303201&partnerID=40&md5=e124380e95eb73348bdf698692109e2f","School of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, 311300, China; School of Environment and Resources, Zhejiang A & F University, Hangzhou, 311300, China; Zhejiang Academy of Agricultural Machinery, Jinhua, 321000, China","Huang Z., School of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, 311300, China; Xu A., School of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, 311300, China; Zhou S., School of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, 311300, China; Ye J., School of Environment and Resources, Zhejiang A & F University, Hangzhou, 311300, China; Weng X., Zhejiang Academy of Agricultural Machinery, Jinhua, 321000, China; Xiang Y., Zhejiang Academy of Agricultural Machinery, Jinhua, 321000, China","Agricultural production efficiency is ever increasing in recent years, particularly with the continuous development of intelligent breeding technology. The production efficiency and welfare of animals have also been enhanced significantly. It is crucial to the accurate identification and management of important livestock, such as pigs. However, the traditional individual identification on the ear tags, ear notches, and color markings can easily lead to some injuries and infections in pigs, due to the labor intensity and marking time. In contrast, non-invasive individual identification methods can be expected to more conveniently, quickly, and accurately obtain the pig information, thereby improving breeding efficiency and pig welfare. Among them, facial alignment can be one of the most essential steps in pig face recognition. The prerequisite of facial alignment is to accurately locate the facial key points. However, the inaccurate extraction of the pig face key points can be resulted from the pig's movement and varying facial poses. It is a high demand to extract accurate and efficient key points for pig face detection. In this study, a precise detection model (YOLO-MOB-DFC) was proposed for the pig facial key points. The human face key points detection model YOLOv5Face was also innovatively adapted during detection. Firstly, the reparameterized MobileOne was used as the backbone network to greatly reduce the model parameters. Then, the decoupled fully connected attention module was integrated to capture the dependency among pixels at distant spatial positions, in order to enable the model to focus more on the pig's facial region for higher detection performance. Finally, the lightweight upsampling operator CARAFE was employed to fully perceive the aggregated contextual information within the neighborhood. As such, the more accurate extraction of pig facial key points was achieved after detection. A pig face dataset was constructed using 100 sow video data and 220 images with complex backgrounds featuring multiple pigs. The SSIM structural similarity algorithm was used to filter the high-similarity images without overfitting. The Labelme was used to mark the pig's face, eyes, bilateral tips of the nose, and nose tip. Six data augmentation operations were applied to enhance the model's generalization capability for offline augmentation. The custom-built pig face dataset was used to test the improved model. The results showed that the average accuracy of pig face detection was up to 99.0%, the detection speed was 153 FPS, and the normalized mean error of key points was 2.344%. The average accuracy increased by 5.43%, the number of model parameters was reduced by 78.59%, the frame rate increased by 91.25%, and the normalized mean error was reduced by 2.774%, compared with the RetinaFace model. Meanwhile, the average accuracy was improved by 2.48%, the number of model parameters was reduced by 18.29%, and the normalized mean error was reduced by 0.567%, compared with the YOLOv5s-Face model. The YOLO-MOB-DFC model shared fewer parameters. There was a more stable Normalized Mean Error (NME) fluctuation between continuous frames. There was the reduced impact of the varying pig face poses on the accuracy of keypoint detection. The improved model can be expected to provide higher detection accuracy and efficiency, in order to quickly and accurately obtain the pig face key point data. The finding can lay the foundation to construct high-quality pig face open-set recognition datasets and non-invasive intelligent identification of pig individuals. Non-invasive intelligent identification of individual pigs can be a trend in more intelligent and sustainable animal husbandry, in order to greatly improve the welfare and production efficiency of pigs, while reducing human labor and time consumption. © 2023 Chinese Society of Agricultural Engineering. All rights reserved.","attention mechanism; deep learning; MobileOne; pig face key point detection; reparameterization; YOLOv5Face","Agriculture; Deep learning; Extraction; Mammals; Production efficiency; Statistical tests; Attention mechanisms; Deep learning; Keypoints; Mean errors; Mobileone; Pig face key point detection; Point detection; Production efficiency; Reparameterization; Yolov5face; Face recognition","","","A. Xu; School of Mathematics and Computer Science, Zhejiang A & F University, Hangzhou, 311300, China; email: xuaj1976@163.com","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85170517528"
"Pham-Duc T.; Ullah M.; Yamin M.M.; Tarekegn A.N.; Beghdadi A.; Cheikh F.A.; Ullah H.","Pham-Duc, Thinh (57201084600); Ullah, Mohib (7006278145); Yamin, Muhammad Mudassar (57205121104); Tarekegn, Adane N. (57216812153); Beghdadi, Azeddine (6603839802); Cheikh, Faouzi Alaya (42461035200); Ullah, Habib (59086654000)","57201084600; 7006278145; 57205121104; 57216812153; 6603839802; 42461035200; 59086654000","Attention Guided Deep Neural Network for Animal Ear Tag Classification in Low-Resolution Images","2023","Proceedings - 2023 International Conference on Frontiers of Information Technology, FIT 2023","","","","61","66","5","0","10.1109/FIT60620.2023.00021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185834260&doi=10.1109%2fFIT60620.2023.00021&partnerID=40&md5=fdd3fd517e13e067bb35015c4d684763","Norwegian University of Science and Technology, Norway; Sorbonne Paris Nord University, Galileo Institute, France; Nmbu, Faculty of Science and Technology, Department of Computer Science, Norway","Pham-Duc T., Norwegian University of Science and Technology, Norway, Sorbonne Paris Nord University, Galileo Institute, France; Ullah M., Norwegian University of Science and Technology, Norway; Yamin M.M., Norwegian University of Science and Technology, Norway; Tarekegn A.N., Norwegian University of Science and Technology, Norway; Beghdadi A., Norwegian University of Science and Technology, Norway, Sorbonne Paris Nord University, Galileo Institute, France; Cheikh F.A., Norwegian University of Science and Technology, Norway; Ullah H., Nmbu, Faculty of Science and Technology, Department of Computer Science, Norway","We developed a deep learning method with attention features for identifying ear tags of animals in the livestock sector. The model consists of channel and spatial attention that performs channel-wise feature calibration for improving the model representation power. We investigated the impact of ear-tag crop size from the original frame to achieve desirable efficiency and reduce the image's noise and background. To validate our approach, our model is trained on the data collected at an animal farm in Norway, which the experts manually annotated. The model results are presented according to two performance metrics, accuracy and the F-score. An improvement of nearly 10% is obtained by our model over the state-of-the-art method. We also measured the time complexity and conducted an ablation study to demonstrate the effectiveness of our model.  © 2023 IEEE.","Attention mechanism; Channel Attention; Convolution Neural Network; Ear tag classification; Spatial Attention","Agriculture; Computer vision; Deep neural networks; Image classification; Learning systems; Attention mechanisms; Channel attention; Convolution neural network; Ear tag classification; Image noise; Learning methods; Low resolution images; Model representation; Representation power; Spatial attention; Animals","Norges Forskningsråd, (282252, 321409); Norges Forskningsråd","This study was funded by the Research Council of Norway through the BIONÆR program, project numbers 282252 and 321409. We are grateful to Norsvin SA for providing data and to Rune Sagevik, Norsvin SA for capturing the image.","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039578-5","","","English","Proc. - Int. Conf. Front. Inf. Technol., FIT","Conference paper","Final","","Scopus","2-s2.0-85185834260"
"Bouwman A.C.; Hulsegge I.; Hawken R.J.; Henshall J.M.; Veerkamp R.F.; Schokker D.; Kamphuis C.","Bouwman, Aniek C (36138632700); Hulsegge, Ina (6506321760); Hawken, Rachel J (6701753022); Henshall, John M (7007062995); Veerkamp, Roel F (57207558519); Schokker, Dirkjan (28167913200); Kamphuis, Claudia (9237616800)","36138632700; 6506321760; 6701753022; 7007062995; 57207558519; 28167913200; 9237616800","Classifying aneuploidy in genotype intensity data using deep learning","2023","Journal of Animal Breeding and Genetics","140","3","","304","315","11","3","10.1111/jbg.12760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148607301&doi=10.1111%2fjbg.12760&partnerID=40&md5=f7dd0c8ed1345cc5e8b030a39a06581b","Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands; Cobb-Vantress Inc., Siloam Springs, AR, United States; Cobb Vantress B.V., Boxmeer, Netherlands","Bouwman A.C., Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands; Hulsegge I., Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands; Hawken R.J., Cobb-Vantress Inc., Siloam Springs, AR, United States; Henshall J.M., Cobb Vantress B.V., Boxmeer, Netherlands; Veerkamp R.F., Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands; Schokker D., Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands; Kamphuis C., Wageningen University & Research, Animal Breeding and Genomics, Wageningen, Netherlands","Aneuploidy is the loss or gain of one or more chromosomes. Although it is a rare phenomenon in liveborn individuals, it is observed in livestock breeding populations. These breeding populations are often routinely genotyped and the genotype intensity data from single nucleotide polymorphism (SNP) arrays can be exploited to identify aneuploidy cases. This identification is a time-consuming and costly task, because it is often performed by visual inspection of the data per chromosome, usually done in plots of the intensity data by an expert. Therefore, we wanted to explore the feasibility of automated image classification to replace (part of) the visual detection procedure for any diploid species. The aim of this study was to develop a deep learning Convolutional Neural Network (CNN) classification model based on chromosome level plots of SNP array intensity data that can classify the images into disomic, monosomic and trisomic cases. A multispecies dataset enriched for aneuploidy cases was collected containing genotype intensity data of 3321 disomic, 1759 monosomic and 164 trisomic chromosomes. The final CNN model had an accuracy of 99.9%, overall precision was 1, recall was 0.98 and the F1 score was 0.99 for classifying images from intensity data. The high precision assures that cases detected are most likely true cases, however, some trisomy cases may be missed (the recall of the class trisomic was 0.94). This supervised CNN model performed much better than an unsupervised k-means clustering, which reached an accuracy of 0.73 and had especially difficult to classify trisomic cases correctly. The developed CNN classification model provides high accuracy to classify aneuploidy cases based on images of plotted X and Y genotype intensity values. The classification model can be used as a tool for routine screening in large diploid populations that are genotyped to get a better understanding of the incidence and inheritance, and in addition, avoid anomalies in breeding candidates. © 2023 The Authors. Journal of Animal Breeding and Genetics published by John Wiley & Sons Ltd.","aneuploidy; B-allele frequency; chromosome; embryo transfer; SNP","Aneuploidy; Animals; Deep Learning; Genotype; Neural Networks, Computer; aneuploidy; animal; genotype","CAT-AgroFood; Hendrix Genetics and Topigs Norsvin; TKI Agri & Food, (16,022); Ministerie van Economische Zaken, EZ","Funding text 1: This study was financially supported by the Dutch Ministry of Economic Affairs (TKI Agri & Food project 16,022) and the Breed4Food partners Cobb Europe, CRV, Hendrix Genetics and Topigs Norsvin. The use of the HPC cluster has been made possible by CAT-AgroFood (Shared Research Facilities Wageningen UR).; Funding text 2: This study was financially supported by the Dutch Ministry of Economic Affairs (TKI Agri & Food project 16,022) and the Breed4Food partners Cobb Europe, CRV, Hendrix Genetics and Topigs Norsvin. The use of the HPC cluster has been made possible by CAT‐AgroFood (Shared Research Facilities Wageningen UR). ","A.C. Bouwman; Wageningen University & Research, Animal Breeding and Genomics, Wageningen, PO Box 338, 6700AH, Netherlands; email: aniek.bouwman@wur.nl","","John Wiley and Sons Inc","09312668","","JABAE","36806175","English","J. Anim. Breed. Gen.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85148607301"
"Ferrero M.; Vignolo L.D.; Vanrell S.R.; Martinez-Rau L.S.; Chelotti J.O.; Galli J.R.; Giovanini L.L.; Rufiner H.L.","Ferrero, Mariano (57817741700); Vignolo, Leandro D. (27568168700); Vanrell, Sebastián R. (57189594289); Martinez-Rau, Luciano S. (57216525141); Chelotti, José O. (57189589603); Galli, Julio R. (7005409490); Giovanini, Leonardo L. (6602698837); Rufiner, H. Leonardo (6602776687)","57817741700; 27568168700; 57189594289; 57216525141; 57189589603; 7005409490; 6602698837; 6602776687","A full end-to-end deep approach for detecting and classifying jaw movements from acoustic signals in grazing cattle","2023","Engineering Applications of Artificial Intelligence","121","","106016","","","","8","10.1016/j.engappai.2023.106016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149030616&doi=10.1016%2fj.engappai.2023.106016&partnerID=40&md5=d88753c2a75c4afe3fa2dcc8381d9635","Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; TERRA Teaching and Research Center, University of Liège, Gembloux Agro-Bio Tech (ULiège-GxABT), Gembloux, 5030, Belgium; Instituto de Investigaciones en Ciencias Agrarias de Rosario, IICAR, Facultad de Ciencias Agrarias, UNR-CONICET, Parque J.F. Villarino, Zavalla, S2125, Argentina; Laboratorio de Cibernética, Facultad de Ingeniería, Univ. Nacional de Entre Ríos, Oro Verde, 3100, Argentina","Ferrero M., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Vignolo L.D., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Vanrell S.R., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Martinez-Rau L.S., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Chelotti J.O., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina, TERRA Teaching and Research Center, University of Liège, Gembloux Agro-Bio Tech (ULiège-GxABT), Gembloux, 5030, Belgium; Galli J.R., Instituto de Investigaciones en Ciencias Agrarias de Rosario, IICAR, Facultad de Ciencias Agrarias, UNR-CONICET, Parque J.F. Villarino, Zavalla, S2125, Argentina; Giovanini L.L., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Rufiner H.L., Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina, Laboratorio de Cibernética, Facultad de Ingeniería, Univ. Nacional de Entre Ríos, Oro Verde, 3100, Argentina","Monitoring the foraging behaviour of ruminants is a key task to improve their productivity and welfare. During the last decades, several monitoring approaches have been proposed based on different types of sensors such as pressure-based, accelerometers and microphones. Among them, microphones have been one of the most promising options because acoustic signals provide comprehensive information about the foraging behaviour. In this work, a fully end-to-end deep architecture is proposed in order to perform both detection and classification tasks of masticatory events in one step, relying only on raw acoustic signals. The main benefit of this novel approach is the substitution of handcrafted preprocessing and feature extraction phases for a pure deep learning approach, which has shown better performance in related fields. Furthermore, different data augmentation techniques have been evaluated to address the data shortness for models development, typical in this field. The results demonstrate that the proposed architecture achieves a F1 score value of 79.82, which represents an increment close to 18% with respect to other state-of-the-art algorithms. Moreover, the proposed data augmentation techniques provide further performance enhancements, emerging as interesting alternatives in this field. © 2023 Elsevier Ltd","Acoustic monitoring; Data augmentation; Deep learning; Precision livestock farming; Ruminant foraging behaviour","Acoustic waves; Agriculture; Deep learning; Mammals; Acoustic monitoring; Acoustic signals; Augmentation techniques; Data augmentation; Deep learning; End to end; Foraging behaviours; Jaw movements; Precision livestock farming; Ruminant foraging behavior; Microphones","CAID, (50620190100080LI, 50620190100151LI); Nvidia; Universidad Nacional de Rosario, UNR, (2013-AGR216, 2016-AGR266, 80020180300053UR); Universidad Nacional de Rosario, UNR; Consejo Nacional de Investigaciones Científicas y Técnicas, CONICET, (2017-PUE); Consejo Nacional de Investigaciones Científicas y Técnicas, CONICET; Universidad Nacional del Litoral, UNL; Agencia Santafesina de Ciencia, Tecnología e Innovación, ASaCTeI, (IO–2018—00082); Agencia Santafesina de Ciencia, Tecnología e Innovación, ASaCTeI","This work has been funded by Universidad Nacional del Litoral, CAID, Argentina 50620190100080LI and 50620190100151LI , Universidad Nacional de Rosario, Argentina , projects 2013-AGR216, 2016-AGR266 and 80020180300053UR, Agencia Santafesina de Ciencia, Argentina , Tecnología e Innovación (ASACTEI), Argentina , project IO–2018—00082, Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET), Argentina , project 2017-PUE sinc(i). Authors would like to thank the dedication and perceptive help by Campo Experimental J. Villarino Dairy Farm staff for their assistance and support during the completion of this study. Authors also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan XP GPU used for this research. ","M. Ferrero; Instituto de Investigación en Señales, Sistemas e Inteligencia Computacional, sinc(i), FICH-UNL/CONICET, Santa Fe, 3000, Argentina; email: mferrero@sinc.unl.edu.ar","","Elsevier Ltd","09521976","","EAAIE","","English","Eng Appl Artif Intell","Article","Final","","Scopus","2-s2.0-85149030616"
"Dave B.; Mori M.; Bathani A.; Goel P.","Dave, Brahm (58864997600); Mori, Meet (58864997700); Bathani, Anurag (58864756000); Goel, Parth (57204771167)","58864997600; 58864997700; 58864756000; 57204771167","Wild Animal Detection using YOLOv8","2023","Procedia Computer Science","230","","","100","111","11","8","10.1016/j.procs.2023.12.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184104944&doi=10.1016%2fj.procs.2023.12.065&partnerID=40&md5=c3f85f0c227114ea0a4e20673d6c400e","Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India","Dave B., Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India; Mori M., Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India; Bathani A., Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India; Goel P., Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India","People residing on the outskirts of forests encounter numerous challenges in their daily lives due to conflicts between humans and wild animals. This conflict has resulted in the loss of lives among both the local population and farmers, as well as substantial livestock casualties. The Gujarat Forest Department has reported that 12 individuals have lost their lives, 70 have been injured, and 3927 cattle have been killed. To address this conflict, various government initiatives have been implemented to track and monitor wild animals. This paper presents a deep learning-based model to track wild animals in real-time from camera footage. In this study, we propose the utilization of the YOLOv8 architecture to detect four distinct categories: Lions, Tigers, Leopards, and Bears. The dataset is constructed from various documentaries, YouTube videos, and existing datasets from Kaggle. This dataset contains 1619 images with annotated four categories of objects. We trained three different YOLOv8 architectures (medium - YOLOv8m, large - YOLOv8l, and extra-large - YOLOv8x) on our dataset. To improve the accuracy of our model, we applied various types of augmentation to the dataset images. The trained extra-large model achieved a mAP of 94.3%, demonstrating its effectiveness in detecting wild animals in real time at 20 FPS. © 2023 Elsevier B.V.. All rights reserved.","Convolutional neural networks; Deep learning; Object detection; Wild animal detection; Yolov8","Animals; Convolutional neural networks; Deep learning; Forestry; Image enhancement; Large datasets; Network architecture; Convolutional neural network; Daily lives; Deep learning; Local populations; Loss of life; Objects detection; Real- time; Wild animal detection; Wild animals; Yolov8; Object detection","","","P. Goel; Department of Computer Science & Engieering, Devang Patel Institute of Advance Technology and Research (DEPSTAR), Charotar University of Science and Technology (CHARUSAT), Changa, 388421, India; email: parthgoel.ce@charusat.ac.in","Suma V.; Lorenz P.; Kamel K.A.","Elsevier B.V.","18770509","","","","English","Procedia Comput. Sci.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85184104944"
"Wang Y.; Han D.; Wang L.; Guo Y.; Du H.","Wang, Yaxin (58510196400); Han, Ding (57043932200); Wang, Liang (57196333971); Guo, Ying (57784913500); Du, Hongwei (58189631200)","58510196400; 57043932200; 57196333971; 57784913500; 58189631200","Contextualized Small Target Detection Network for Small Target Goat Face Detection","2023","Animals","13","14","2365","","","","7","10.3390/ani13142365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165944901&doi=10.3390%2fani13142365&partnerID=40&md5=5e5a31e7265fe64659d63a5851625eed","College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China; State Key Laboratory of Reproductive Regulation and Breeding of Grassland Livestock, Hohhot, 010020, China; Department of Electronic Engineering, College of Information Science and Engineering, Fudan University, Shanghai, 200438, China; College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China","Wang Y., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China; Han D., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China, State Key Laboratory of Reproductive Regulation and Breeding of Grassland Livestock, Hohhot, 010020, China; Wang L., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China, Department of Electronic Engineering, College of Information Science and Engineering, Fudan University, Shanghai, 200438, China; Guo Y., College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China; Du H., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China","With the advancement of deep learning technology, the importance of utilizing deep learning for livestock management is becoming increasingly evident. goat face detection provides a foundation for goat recognition and management. In this study, we proposed a novel neural network specifically designed for goat face object detection, addressing challenges such as low image resolution, small goat face targets, and indistinct features. By incorporating contextual information and feature-fusion complementation, our approach was compared with existing object detection networks using evaluation metrics such as F1-Score (F1), precision (P), recall (R), and average precision (AP). Our results show that there are 8.07%, 0.06, and 6.8% improvements in AP, P, and R, respectively. The findings confirm that the proposed object detection network effectively mitigates the impact of small targets in goat face detection, providing a solid basis for the development of intelligent management systems for modern livestock farms. © 2023 by the authors.","goat face detection; intelligent management systems; small targets","agricultural land; animal experiment; article; goat; livestock; nonhuman; recall","Major Science and Technology Project of Inner Mongolia Autonomous Region, (2021ZD0019-4); Scientific Research Project of Higher Education Institutions in Inner Mongolia Autonomous Region, (NJZY22337); National Key Research and Development Program of China, NKRDPC, (2022YFF1300604-04); National Key Research and Development Program of China, NKRDPC","This research was supported by the National Key R&D Program of China (Grant No. 2022YFF1300604-04), the Major Science and Technology Project of Inner Mongolia Autonomous Region (Grant No. 2021ZD0019-4), and the Scientific Research Project of Higher Education Institutions in Inner Mongolia Autonomous Region (Grant No. NJZY22337).","D. Han; College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China; email: handing@imu.edu.cn; L. Wang; College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010020, China; email: wangliang@imu.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85165944901"
"Koh M.E.; Fong M.W.K.; Ng E.Y.K.","Koh, Ming En (58592196200); Fong, Mark Wong Kei (58592196300); Ng, Eddie Yin Kwee (7201647536)","58592196200; 58592196300; 7201647536","Aqua3DNet: Real-time 3D pose estimation of livestock in aquaculture by monocular machine vision","2023","Aquacultural Engineering","103","","102367","","","","3","10.1016/j.aquaeng.2023.102367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171442889&doi=10.1016%2fj.aquaeng.2023.102367&partnerID=40&md5=f4e6c132bea04e7cef8d4db678da44a7","School of Computer Science, Nanyang Technological University (NTU), Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University (NTU), Singapore","Koh M.E., School of Computer Science, Nanyang Technological University (NTU), Singapore; Fong M.W.K., School of Mechanical and Aerospace Engineering, Nanyang Technological University (NTU), Singapore; Ng E.Y.K., School of Mechanical and Aerospace Engineering, Nanyang Technological University (NTU), Singapore","We present a low-cost monocular 3D position estimation method for perception in aquaculture monitoring. Video surveillance of aquaculture has many advantages but given the size of farms and the complexity of their habitats, it is not feasible for farmers to continuously monitor fish health. We formulate a novel end-to-end deep visual learning pipeline called Aqua3DNet that estimates fish pose using a bottom-up approach to detect and assign key features in one pass. In addition, a depth estimation model using Saliency Object Detection (SOD) masks is implemented to track the 3D position of the fish over time, which is used in this paper to create 3D density heat maps of the fish. The evaluation of the algorithm's performance shows that the detection accuracy reaches 80.63%, the F1 score reaches 87.34%, and the frames per second (fps) reaches 5.12. Aqua3DNet achieves comparable performance to other aquaculture-based computer vision and depth estimation models, with minimal decrease in speed despite the synthesis of the two models. © 2023 Elsevier B.V.","Aqua3DNet; Aquaculture; Monocular machine vision; Real-time 3D; Saliency object detection","Agriculture; Aquaculture; Computer vision; Deep learning; Fish; Object recognition; Security systems; 3D pose estimation; Aqua3dnet; Depth Estimation; Estimation models; Machine-vision; Monocular machine vision; Objects detection; Real- time; Real-time 3d; Saliency object detection; algorithm; aquaculture; bottom-up approach; computer vision; detection method; estimation method; livestock; perception; Object detection","","","E.Y.K. Ng; School of Mechanical and Aerospace Engineering, Nanyang Technological University (NTU), Singapore; email: mykng@ntu.edu.sg","","Elsevier B.V.","01448609","","AQEND","","English","Aquac. Eng.","Article","Final","","Scopus","2-s2.0-85171442889"
"Huang X.; Hu Z.; Qiao Y.; Sukkarieh S.","Huang, Xiaoping (57208149375); Hu, Zelin (8870386500); Qiao, Yongliang (56486770900); Sukkarieh, Salah (6602844626)","57208149375; 8870386500; 56486770900; 6602844626","Deep Learning-Based Cow Tail Detection and Tracking for Precision Livestock Farming","2023","IEEE/ASME Transactions on Mechatronics","28","3","","1213","1221","8","21","10.1109/TMECH.2022.3175377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131736158&doi=10.1109%2fTMECH.2022.3175377&partnerID=40&md5=aa3892d77e412a6bfbbfa53d19d1594c","Anhui University, National Engineering Research Center for Agro-Ecological Big Data Analysis and Application, Hefei, 230601, China; Gannan Normal University, School of Physics and Electronic Information, Ganzhou, 341000, China; The University of Sydney, Australian Centre for Field Robotics (ACFR), Faculty of Engineering, Sydney, 2006, NSW, Australia","Huang X., Anhui University, National Engineering Research Center for Agro-Ecological Big Data Analysis and Application, Hefei, 230601, China; Hu Z., Gannan Normal University, School of Physics and Electronic Information, Ganzhou, 341000, China; Qiao Y., The University of Sydney, Australian Centre for Field Robotics (ACFR), Faculty of Engineering, Sydney, 2006, NSW, Australia; Sukkarieh S., The University of Sydney, Australian Centre for Field Robotics (ACFR), Faculty of Engineering, Sydney, 2006, NSW, Australia","Cow tail detection and tracking in videos provide valuable information for individual identification, calving process, behavior analysis, and body condition monitoring. Although deep learning-based detection methods have demonstrated good performance, many of them have high complexity and still require an improvement in video object detection by reducing the computation time and false positives. To fully exploit the interframe information and achieve a real-time online detection, the optimized cow tail detection and tracking method is proposed based on an improved single shot multibox detector (SSD) and Kalman filter. Here, our improved SSD-integrated DenseNet and Inception-v4 reduce detection information loss and network parameters. Then, an improved window function-based Kalman filter and Hungarian are adopted to remove error detections and enhance the cow tail tracking accuracy. Experiments on our acquired rear-view videos show that the proposed approach achieved fast tail detection with an accuracy of 96.97%, a speed of 96 fps, a smaller model size of 25 MB, and higher position accuracy with an average 6.45 pixels deviation. The proposed approach outperformed the region-based R-CNN models and other tracking methods (e.g., particle filter), which provides a new solution to automatic cow detection and tracking in smart livestock farming. © 1996-2012 IEEE.","Deep learning; Kalman filter; object detection; precision livestock farming (PLF); visual tracking","Agriculture; Condition monitoring; Deep learning; Feature extraction; Object detection; Object recognition; Cow; Deep learning; Detection and tracking; Detection methods; Features extraction; Precision livestock farming; Tail; Video; Visual Tracking; Kalman filters","Natural Science Foundation of Department of Science and Technology of Anhui Province, (1908085QF284)","This work was supported by the Natural Science Foundation of Department of Science and Technology of Anhui Province under Grant 1908085QF284.","Y. Qiao; The University of Sydney, Australian Centre for Field Robotics (ACFR), Faculty of Engineering, Sydney, 2006, Australia; email: yongliang.qiao@sydney.edu.au","","Institute of Electrical and Electronics Engineers Inc.","10834435","","IATEF","","English","IEEE ASME Trans Mechatron","Article","Final","","Scopus","2-s2.0-85131736158"
"Larsen M.L.V.; Wang M.; Willems S.; Liu D.; Norton T.","Larsen, Mona L.V. (56526109000); Wang, Meiqing (57221783935); Willems, Sam (57536119200); Liu, Dong (57189588678); Norton, Tomas (35273348100)","56526109000; 57221783935; 57536119200; 57189588678; 35273348100","Automatic detection of locomotor play in young pigs: A proof of concept","2023","Biosystems Engineering","229","","","154","166","12","5","10.1016/j.biosystemseng.2023.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153492686&doi=10.1016%2fj.biosystemseng.2023.03.006&partnerID=40&md5=1e319007d67c7ab3d0f355ebec52a0da","Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Department of Animal and Veterinary Sciences, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark","Larsen M.L.V., Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium, Department of Animal and Veterinary Sciences, Aarhus University, Blichers Allé 20, Tjele, 8830, Denmark; Wang M., Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Willems S., Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Liu D., Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Norton T., Department of Biosystems, KU Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium","Play behaviour is considered an indicator of animal welfare in young pigs. However, as play behaviour events are short-lasting and occur sporadically, continuous monitoring is necessary. This study presents a first attempt at automatic detection of locomotor play behaviour in young pigs from video by classifying locomotor play from other solitary behaviours including standing, walking, and running. Two methods were developed, compared, and sequentially combined: (1) a less computational heavy method utilising the Gaussian Mixture Model for quantification of movement combined with the calculation of contour features and standard machine learning classifiers (FEATURES); (2) a computational heavy method utilising a deep learning classifier taking both spatial and temporal features into account (DEEP). The DEEP classifier outperformed the FEATURES classifier and obtained values of internal validation recall, precision, and specificity of 94%, 88% and 96%, respectively. When combining the two classification methods, almost similar performance was retained, whilst 44% of the other behaviours were correctly classified without the need for deep learning methods. The combination thereby decreased the computational power needed to run the algorithm. Thus, locomotor play can be automatically detected in young pigs and the combination of a less computational heavy method with a deep learning method can reduce the computational requirements for the classification and detection of complex behaviours. Future work should focus on the segmentation of single pigs during high-speed activity in order to enable the play detection algorithm to work in real-life settings. © 2023 IAgrE","Animal behaviour; Animal welfare; Computer vision; Gaussian Mixture Model; Precision Livestock Farming; Technology","Computer vision; Deep learning; Farms; Gaussian distribution; Learning systems; Mammals; Animal behaviour; Animal welfare; Automatic Detection; Continuous monitoring; Gaussian Mixture Model; Learning classifiers; Learning methods; Precision livestock farming; Proof of concept; Technology; Classification (of information)","Aarhus Universitet, AU; Horizon 2020 Framework Programme, H2020; Ministeriet for Fø devarer, Landbrug og Fiskeri; H2020 Marie Skłodowska-Curie Actions, MSCA, (842555); Horizon 2020, (862919)","This study was conducted as part of the AutoPlayPig project funded by the Marie Sk\u0142odowska-Curie grant agreement No. 842555 and the ClearFarm project funded by the European Union's Horizon 2020 research and innovation programme under grant agreement No. 862919, while the data collection was funded by the Ministry of Food, Agriculture and Fisheries of Denmark under the program \u201CVeterinary agreement III\u201D (Veterin\u00E6rforlig III, AU6). The authors send great thanks to the staff at the Pig Research Unit, Aarhus University, Denmark, and to PhD student Jeanet F. M. Winters, assisting technicians Carsten K. Christensen and Dines T. Bolt, post-doctoral colleague Guilherme Amorim Franchi, and student helper Ida H. Kristoffersen.","M.L.V. Larsen; Department of Biosystems, KU Leuven, Heverlee, Kasteelpark Arenberg 30, 3001, Belgium; email: mona@anivet.au.dk; T. Norton; Department of Biosystems, KU Leuven, Heverlee, Kasteelpark Arenberg 30, 3001, Belgium; email: tomas.norton@kuleuven.be","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85153492686"
"Toro A.P.S.G.D.D.; Bueno I.T.; Werner J.P.S.; Antunes J.F.G.; Lamparelli R.A.C.; Coutinho A.C.; Esquerdo J.C.D.M.; Magalhães P.S.G.; Figueiredo G.K.D.A.","Toro, Ana P. S. G. D. D. (57741497200); Bueno, Inacio T. (56603977800); Werner, João P. S. (57212420665); Antunes, João F. G. (15020172200); Lamparelli, Rubens A. C. (6602713722); Coutinho, Alexandre C. (55513012600); Esquerdo, Júlio C. D. M. (15020462500); Magalhães, Paulo S. G. (7003731076); Figueiredo, Gleyce K. D. A. (57185295400)","57741497200; 56603977800; 57212420665; 15020172200; 6602713722; 55513012600; 15020462500; 7003731076; 57185295400","SAR and Optical Data Applied to Early-Season Mapping of Integrated Crop–Livestock Systems Using Deep and Machine Learning Algorithms","2023","Remote Sensing","15","4","1130","","","","5","10.3390/rs15041130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149334025&doi=10.3390%2frs15041130&partnerID=40&md5=248170784bda7cfdb7a5d530815a0710","School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil; Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil","Toro A.P.S.G.D.D., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Bueno I.T., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil; Werner J.P.S., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil; Antunes J.F.G., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Lamparelli R.A.C., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil; Coutinho A.C., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Esquerdo J.C.D.M., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil, Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation, SP, Campinas, 13083-886, Brazil; Magalhães P.S.G., Interdisciplinary Center of Energy Planning, University of Campinas, SP, Campinas, 13083-896, Brazil; Figueiredo G.K.D.A., School of Agricultural Engineering, University of Campinas, SP, Campinas, 13083-875, Brazil","Regenerative agricultural practices are a suitable path to feed the global population. Integrated Crop–livestock systems (ICLSs) are key approaches once the area provides animal and crop production resources. In Brazil, the expectation is to increase the area of ICLS fields by 5 million hectares in the next five years. However, few methods have been tested regarding spatial and temporal scales to map and monitor ICLS fields, and none of these methods use SAR data. Therefore, in this work, we explored the potential of three machine and deep learning algorithms (random forest, long short-term memory, and transformer) to perform early-season (with three-time windows) mapping of ICLS fields. To explore the scalability of the proposed methods, we tested them in two regions with different latitudes, cloud cover rates, field sizes, landscapes, and crop types. Finally, the potential of SAR (Sentinel-1) and optical (Sentinel-2) data was tested. As a result, we found that all proposed algorithms and sensors could correctly map both study sites. For Study Site 1(SS1), we obtained an overall accuracy of 98% using the random forest classifier. For Study Site 2, we obtained an overall accuracy of 99% using the long short-term memory net and the random forest. Further, the early-season experiments were successful for both study sites (with an accuracy higher than 90% for all time windows), and no significant difference in accuracy was found among them. Thus, this study found that it is possible to map ICLSs in the early-season and in different latitudes by using diverse algorithms and sensors. © 2023 by the authors.","ICLS; LSTM; multisource; random forest; regenerative agriculture; transformer","Brain; Cultivation; Forestry; Learning algorithms; Long short-term memory; Mapping; Integrated crop–livestock system; Livestock systems; LSTM; Multi-Sources; Random forests; Regenerative agriculture; SAR data; Study sites; Time windows; Transformer; Crops","Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2017/50205-9, 2021/15001-9); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (305271/2020-2)","Funding text 1: This research was funded by the Sao Paulo Research Foundation—FAPESP (Grants: 2021/15001-9 and 2017/50205-9).; Funding text 2: The authors are thankful for the National Council for Scientific and Technological Development (CNPQ) by the Research Grant n. 305271/2020-2, and for the Coordination for the Improvement of Higher Education Personnel (CAPES) (Finance Code 001) for the fellowship granted to J. P. S. Werner. ","A.P.S.G.D.D. Toro; School of Agricultural Engineering, University of Campinas, Campinas, SP, 13083-875, Brazil; email: anapaolagomes@hotmail.com","","MDPI","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149334025"
"Wang Y.; Yang D.; Chen H.; Wang L.; Gao Y.","Wang, Yongsheng (57347209200); Yang, Duanli (57195313048); Chen, Hui (55273423100); Wang, Lianzeng (58499655600); Gao, Yuan (57195312816)","57347209200; 57195313048; 55273423100; 58499655600; 57195312816","Pig Counting Algorithm Based on Improved YOLOv5n Model with Multiscene and Fewer Number of Parameters","2023","Animals","13","21","3411","","","","7","10.3390/ani13213411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176430858&doi=10.3390%2fani13213411&partnerID=40&md5=3915747c3925e4c3c2676176efabc658","College of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China; College of Animal Science and Technology, Hebei Agricultural University, Baoding, 071001, China; Key Laboratory of Broiler and Layer Facilities Engineering, Ministry of Agriculture and Rural Affairs, Baoding, 071001, China; Hebei Layer Industry Technology Research Institute, Handan, 056007, China","Wang Y., College of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China, Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China; Yang D., College of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China, Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China; Chen H., College of Animal Science and Technology, Hebei Agricultural University, Baoding, 071001, China, Key Laboratory of Broiler and Layer Facilities Engineering, Ministry of Agriculture and Rural Affairs, Baoding, 071001, China; Wang L., Hebei Layer Industry Technology Research Institute, Handan, 056007, China; Gao Y., College of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China, Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China","Pig counting is an important work in the breeding process of large-scale pig farms. In order to achieve high-precision pig identification in the conditions of pigs occluding each other, illumination difference, multiscenes, and differences in the number of pigs and the imaging size, and to also reduce the number of parameters of the model, a pig counting algorithm of improved YOLOv5n was proposed. Firstly, a multiscene dataset is created by selecting images from several different pig farms to enhance the generalization performance of the model; secondly, the Backbone of YOLOv5n was replaced by the FasterNet model to reduce the number of parameters and calculations to lay the foundation for the model to be applied to Android system; thirdly, the Neck of YOLOv5n was optimized by using the E-GFPN structure to enhance the feature fusion capability of the model; Finally, Focal EIoU loss function was used to replace the CIoU loss function of YOLOv5n to improve the model’s identification accuracy. The results showed that the AP of the improved model was 97.72%, the number of parameters, the amount of calculation, and the size of the model were reduced by 50.57%, 32.20%, and 47.21% compared with YOLOv5n, and the detection speed reached 75.87 f/s. The improved algorithm has better accuracy and robustness in multiscene and complex pig house environments, which not only ensured the accuracy of the model but also reduced the number of parameters as much as possible. Meanwhile, a pig counting application for the Android system was developed based on the optimized model, which truly realized the practical application of the technology. The improved algorithm and application could be easily extended and applied to the field of livestock and poultry counting, such as cattle, sheep, geese, etc., which has a widely practical value. © 2023 by the authors.","android application; cross scene; FasterNet; pig count; YOLOv5n","accuracy; algorithm; animal experiment; Article; bovine; breeding; computer model; data collection method; deep learning; economic aspect; embedding; evaluation study; goose; image analysis; information processing; light exposure; mathematical analysis; neck; nonhuman; performance; pig counting algorithm; quantitative analysis; sheep; training; validation process; whole body counting; YOLOv5n Model","CARS-40; Innovation Ability Support Project for PostGraduates of Hebei Province, (CXZZSS2023055); MARA; Special Funds of Hebei Science and Technology R & D platform Foundation, (225676150H); National Natural Science Foundation of China, NSFC, (32172779); Agriculture Research System of China","We wish to thank the National Natural Science Foundation of China (No. 32172779), the China Agriculture Research System of MOF, MARA (CARS-40), Special Funds of Hebei Science and Technology R & D platform Foundation (No. 225676150H), and the Innovation Ability Support Project for PostGraduates of Hebei Province (No. CXZZSS2023055). Financial supports from the above funds and organizations are gratefully acknowledged.","D. Yang; College of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China; email: yangdl@hebau.edu.cn; H. Chen; College of Animal Science and Technology, Hebei Agricultural University, Baoding, 071001, China; email: chenhui@hebau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85176430858"
"Hao W.; Ren C.; Han M.; Zhang L.; Li F.; Liu Z.","Hao, Wangli (55757033100); Ren, Chao (59272094000); Han, Meng (57194785061); Zhang, Li (58436251900); Li, Fuzhong (55494497700); Liu, Zhenyu (55714953500)","55757033100; 59272094000; 57194785061; 58436251900; 55494497700; 55714953500","Cattle Body Detection Based on YOLOv5-EMA for Precision Livestock Farming","2023","Animals","13","22","3535","","","","16","10.3390/ani13223535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177697517&doi=10.3390%2fani13223535&partnerID=40&md5=7027b5ff8f322cd685e3d648969866c2","School of Software, Shanxi Agricultural University, Jinzhong, 030801, China","Hao W., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Ren C., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Han M., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Zhang L., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Li F., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; Liu Z., School of Software, Shanxi Agricultural University, Jinzhong, 030801, China","Accurate cattle body detection is crucial for precision livestock farming. However, traditional cattle body detection methods rely on manual observation, which is both time-consuming and labor-intensive. Moreover, computer-vision-based methods suffer prolonged training times and training difficulties. To address these issues, this paper proposes a novel YOLOv5-EMA model for accurate cattle body detection. By incorporating the Efficient Multi-Scale Attention (EMA) module into the backbone of YOLO series detection models, the performance of detecting smaller targets, such as heads and legs, has been significantly improved. The Efficient Multi-Scale Attention (EMA) module utilizes the large receptive fields of parallel sub-networks to gather multi-scale spatial information and establishes mutual dependencies between different spatial positions, enabling cross-spatial learning. This enhancement empowers the model to gather and integrate more comprehensive feature information, thereby improving the effectiveness of cattle body detection. The experimental results confirm the good performance of the YOLOv5-EMA model, showcasing promising results across all quantitative evaluation metrics, qualitative detection findings, and visualized Grad-CAM heatmaps. To be specific, the YOLOv5-EMA model achieves an average precision (mAP@0.5) of 95.1% in cattle body detection, 94.8% in individual cattle detection, 94.8% in leg detection, and 95.5% in head detection. Moreover, this model facilitates the efficient and precise detection of individual cattle and essential body parts in complex scenarios, especially when dealing with small targets and occlusions, significantly advancing the field of precision livestock farming. © 2023 by the authors.","cattle body detection; efficient multi-scale attention; key body parts; YOLOv5-EMA","accuracy; Article; body regions; cattle farming; computer vision; deep learning; efficient multi scale attention module; head; learning algorithm; leg; livestock; nonhuman; principal component analysis; procedures; spatial learning; training","Shanxi Province Basic Research Program, (202203021212444); Shanxi Province Education Science; Shanxi Agricultural University, SXAU, (CXGC2023045); Shanxi Agricultural University, SXAU","This work was supported by the Shanxi Province Basic Research Program (202203021212444); the Shanxi Agricultural University Science and Technology Innovation Enhancement Project (CXGC2023045); the Shanxi Province Education Science “14th Five-Year Plan” 2021 Annual Project General Planning Project and “Industry-University-Research”-driven Smart Agricultural Talent Training Model in Agriculture and Forestry Colleges (GH-21006); the Shanxi Agricultural University Teaching Reform Project (J202098572); the Shanxi Province Higher Education Teaching Reform and Innovation Project (J20220274); the Shanxi Postgraduate Education and Teaching Reform Project Fund (2022YJJG094); the Shanxi Agricultural University doctoral research start-up project (2021BQ88); the Shanxi Agricultural University Academic Restoration Research Project (2020xshf38); and the Shanxi Agricultural University 2021 “Neural Network”Course Ideological and Political Project (KCSZ202133).","F. Li; School of Software, Shanxi Agricultural University, Jinzhong, 030801, China; email: lifuzhong@sxau.edu.cn","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85177697517"
"Meena S.D.; Manichandana K.B.V.; Potlur R.S.; Dhanyasri M.; Harshith P.; Sheela J.","Meena, S. Divya (57192698658); Manichandana, Kanamarlapudi Bindu Venkata (58754555800); Potlur, Raga Siri (58754354900); Dhanyasri, Makkapati (58314515900); Harshith, Pandillapally (58754286200); Sheela, J. (59302989600)","57192698658; 58754555800; 58754354900; 58314515900; 58754286200; 59302989600","Aerial imaging based sea lion count using modified U-net architecture","2023","AIP Conference Proceedings","2869","1","050024","","","","1","10.1063/5.0168211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179173998&doi=10.1063%2f5.0168211&partnerID=40&md5=d24c557ed80d4f94d5e51bc52e0c2246","School of Computer Science and Engineering, VIT-AP University, Amaravathi, India","Meena S.D., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; Manichandana K.B.V., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; Potlur R.S., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; Dhanyasri M., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; Harshith P., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; Sheela J., School of Computer Science and Engineering, VIT-AP University, Amaravathi, India","Managing livestock in large manufacturing systems is difficult, mainly in vast areas. UAVs are used to collect images of areas of interest and are rapidly becoming a possible alternative. However, extracting relevant information using good algorithms from images are still scarce. In this paper, we present a method for counting sea lions or other animals using a deep learning model for rough location, colour space control to increase the variance between sea lions and background, mathematical morphology to identify sea lions and gather them individually in clustered groups, and image matching to account for image overlap. This paper describes how to use deep learning to automatically count sea lions from aerial photographs.  © 2023 Author(s).","","","","","J. Sheela; School of Computer Science and Engineering, VIT-AP University, Amaravathi, India; email: sheela.j@vitap.ac.in","Vemanaboina H.; Kumar K.; Ferro P.; Babu B.S.; Babu M.M.; Malasri S.P.; Reddy B.D.","American Institute of Physics Inc.","0094243X","978-073544684-7","","","English","AIP Conf. Proc.","Conference paper","Final","","Scopus","2-s2.0-85179173998"
"Voogt A.M.; Schrijver R.S.; Temürhan M.; Bongers J.H.; Sijm D.T.H.M.","Voogt, Annika M. (58283817700); Schrijver, Remco S. (6701699236); Temürhan, Mine (57189268186); Bongers, Johan H. (7004306169); Sijm, Dick T. H. M. (7007148749)","58283817700; 6701699236; 57189268186; 7004306169; 7007148749","Opportunities for Regulatory Authorities to Assess Animal-Based Measures at the Slaughterhouse Using Sensor Technology and Artificial Intelligence: A Review","2023","Animals","13","19","3028","","","","0","10.3390/ani13193028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173885781&doi=10.3390%2fani13193028&partnerID=40&md5=43a2bb1971f071ab9de72a0cab822d10","Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands","Voogt A.M., Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands; Schrijver R.S., Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands; Temürhan M., Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands; Bongers J.H., Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands; Sijm D.T.H.M., Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), P.O. Box 43006, Utrecht, 3540 AA, Netherlands","Animal-based measures (ABMs) are the preferred way to assess animal welfare. However, manual scoring of ABMs is very time-consuming during the meat inspection. Automatic scoring by using sensor technology and artificial intelligence (AI) may bring a solution. Based on review papers an overview was made of ABMs recorded at the slaughterhouse for poultry, pigs and cattle and applications of sensor technology to measure the identified ABMs. Also, relevant legislation and work instructions of the Dutch Regulatory Authority (RA) were scanned on applied ABMs. Applications of sensor technology in a research setting, on farm or at the slaughterhouse were reported for 10 of the 37 ABMs identified for poultry, 4 of 32 for cattle and 13 of 41 for pigs. Several applications are related to aspects of meat inspection. However, by European law meat inspection must be performed by an official veterinarian, although there are exceptions for the post mortem inspection of poultry. The examples in this study show that there are opportunities for using sensor technology by the RA to support the inspection and to give more insight into animal welfare risks. The lack of external validation for multiple commercially available systems is a point of attention. © 2023 by the authors.","abattoir; animal welfare; camera surveillance; innovation; machine learning; meat inspection; precision livestock farming; sensors","algorithm; anemia; animal lameness; animal welfare; artificial intelligence; Automatic scoring; autopsy; broiler; cold stress; contusion; deep learning; fatigue; fear; feeding behavior; heat stress; housing; machine learning; Medline; natural language processing; nonhuman; pig; poultry; publication; Review; scoring system; skin defect; slaughterhouse; social stress; validation process; veterinarian","","","A.M. Voogt; Office for Risk Assessment & Research (BuRO), Netherlands Food and Consumer Product Safety Authority (NVWA), Utrecht, P.O. Box 43006, 3540 AA, Netherlands; email: a.m.voogt@nvwa.nl","","Multidisciplinary Digital Publishing Institute (MDPI)","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85173885781"
"Li L.; Zhang T.; Cuo D.; Zhao Q.; Zhou L.; Jiancuo S.","Li, Lei (58443750500); Zhang, Tingting (56889974500); Cuo, Da (57729173000); Zhao, Qijun (57221157913); Zhou, Liyuan (57227125000); Jiancuo, Suonan (57209908868)","58443750500; 56889974500; 57729173000; 57221157913; 57227125000; 57209908868","Automatic identification of individual yaks in in-the-wild images using part-based convolutional networks with self-supervised learning","2023","Expert Systems with Applications","216","","119431","","","","10","10.1016/j.eswa.2022.119431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144346101&doi=10.1016%2fj.eswa.2022.119431&partnerID=40&md5=8110b4ea51fbe45eaf8277c88f8ab490","College of Computer Science, Sichuan University, Chengdu, 610065, China; School of Information Science and Technology, Tibet University, Lhasa, 850000, China","Li L., College of Computer Science, Sichuan University, Chengdu, 610065, China; Zhang T., College of Computer Science, Sichuan University, Chengdu, 610065, China; Cuo D., School of Information Science and Technology, Tibet University, Lhasa, 850000, China; Zhao Q., College of Computer Science, Sichuan University, Chengdu, 610065, China, School of Information Science and Technology, Tibet University, Lhasa, 850000, China; Zhou L., School of Information Science and Technology, Tibet University, Lhasa, 850000, China; Jiancuo S., School of Information Science and Technology, Tibet University, Lhasa, 850000, China","Yaks (Bos grunniens) are the most important domestic animals for people living at high altitudes. In order to implement precise livestock management for yaks, it is of significant importance to automatically identify, keep track of, and monitor yaks. Traditional animal identification methods such as ear tags, tattoos, and RFID based methods suffer from problems like animal infection, high maintenance cost, inefficiency or sensor failure. Existing biometric-based identification methods for livestock such as muzzle prints, iris patterns, and retinal vascular patterns mostly require that animals are under control, either technically or physically, and are thus costly to deploy especially for yaks which are loosely raised on the grassland pastures and migrate with the seasons. In this paper, we propose a novel method for identifying individual yaks in in-the-wild images captured under unconstrained conditions. We utilize the part-based convolutional network (PCN) to obtain discriminative part-level feature representations. To further enhance the feature discriminativeness and alleviate the impact of small amount of yak image data, we implement self-supervised learning strategy by proposing random erasure and region-visibility prediction (RERP) as an auxiliary learning task. Experiments performed on the YakReID-103 dataset demonstrate that (i) when left and right side views of yaks are treated separately, the Rank-1 accuracy and mAP achieved by the proposed method with SEResNet50 backbone are up to 97.57% and 76.30%, which significantly advance the state-of-the-art, and (ii) when generalizing to different views, the proposed method with ViT backbone again obtains the best results compared with the counterpart methods. © 2022 Elsevier Ltd","Animal biometrics; Convolutional neural networks; Deep learning; Precision livestock; Yak identification","Agriculture; Animals; Anthropometry; Automation; Convolution; Convolutional neural networks; Deep learning; Image enhancement; Learning systems; Radio frequency identification (RFID); Supervised learning; Animal biometric; Automatic identification; Convolutional networks; Convolutional neural network; Deep learning; Identification method; Identification of individuals; Part based; Precision livestock; Yak identification; Biometrics","National Natural Science Foundation of China, NSFC, (61971005, 62176170); Science and Technology Department of Tibet, (XZ202102YD0018C)","This research was supported by the National Natural Science Foundation of China (No. 62176170, 61971005) and the Science and Technology Department of Tibet (Grant No. XZ202102YD0018C).","Q. Zhao; College of Computer Science, Sichuan University, Chengdu, 610065, China; email: qjzhao@scu.edu.cn","","Elsevier Ltd","09574174","","ESAPE","","English","Expert Sys Appl","Article","Final","","Scopus","2-s2.0-85144346101"
"Li G.; Li B.; Shi Z.; Lu G.; Chai L.; Rasheed K.M.; Regmi P.; Banakar A.","Li, Guoming (57200960751); Li, Baoming (57202857587); Shi, Zhengxiang (57207027510); Lu, Guoyu (55872294200); Chai, Lilong (57222280822); Rasheed, Khaled M. (7003492982); Regmi, Prafulla (56849288800); Banakar, Ahmad (16642169100)","57200960751; 57202857587; 57207027510; 55872294200; 57222280822; 7003492982; 56849288800; 16642169100","Interindividual distances and orientations of laying hens under 8 stocking densities measured by integrative deep learning techniques","2023","Poultry Science","102","11","103076","","","","5","10.1016/j.psj.2023.103076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171766605&doi=10.1016%2fj.psj.2023.103076&partnerID=40&md5=d2ceacc8ef57f3230e621819cfcad9d5","Department of Poultry Science, The University of Georgia, Athens, 30602, GA, United States; Institute for Artificial Intelligence, The University of Georgia, Athens, 30602, GA, United States; College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; School of Electrical and Computer Engineering, The University of Georgia, Athens, 30602, GA, United States; School of Computing, The University of Georgia, Athens, 30602, GA, United States; Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran","Li G., Department of Poultry Science, The University of Georgia, Athens, 30602, GA, United States, Institute for Artificial Intelligence, The University of Georgia, Athens, 30602, GA, United States; Li B., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Shi Z., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Lu G., Institute for Artificial Intelligence, The University of Georgia, Athens, 30602, GA, United States, School of Electrical and Computer Engineering, The University of Georgia, Athens, 30602, GA, United States; Chai L., Department of Poultry Science, The University of Georgia, Athens, 30602, GA, United States; Rasheed K.M., Institute for Artificial Intelligence, The University of Georgia, Athens, 30602, GA, United States; Regmi P., Department of Poultry Science, The University of Georgia, Athens, 30602, GA, United States, School of Computing, The University of Georgia, Athens, 30602, GA, United States; Banakar A., Biosystems Engineering Department, Tarbiat Modares University, Tehran, 14117-13116, Iran","Interindividual distances and orientations of laying hens provide quantitative measures to calculate and optimize space allocations for bird flocks. However, these metrics were often measured manually and have not been examined for different stocking densities of laying hens. The objectives of this study were to 1) integrate and develop several deep learning techniques to detect interindividual distances and orientations of laying hens; and 2) examine the 2 metrics under 8 stocking densities via the developed techniques. Laying hens (Jingfen breed, a popular hen breed in China) at 35 wk of age were raised in experimental compartments at 8 different stocking densities of 3,840, 2,880, 2,304, 1,920, 1,646, 1,440, 1,280, and 1,152 cm2•bird−1 (3–10 hens per compartment, respectively), and cameras on the top of the compartments recorded videos for further analysis. The designed deep learning image classifier achieved over 99% accuracy to classify bird's perching status and excluded frames with bird perching to ensure that all birds analyzed were on the same horizontal plane, reducing calculation errors. The YOLOv5m oriented object detection model achieved over 90% precision, recall, and F1 score in detecting birds in compartments and can output bird centroid coordinates and angles, from which interindividual distances and orientations were calculated based on pairs of birds. Laying hens maintained smaller minimum interindividual distances in higher stocking densities. They were in an intersecting relationship with conspecifics for over 90% of the time. The developed integrative deep learning techniques and behavior metrics provide animal-based measurement of space requirement for laying hens. © 2023 The Authors","animal welfare; artificial intelligence; oriented object detection; precision livestock farming; space allocation","","University of Georgia, UGA","Partial funding for this work was supported by federal and state funds allocated to the University of Georgia . ","G. Li; Department of Poultry Science, The University of Georgia, Athens, 30602, United States; email: gmli@uga.edu","","Elsevier Inc.","00325791","","","","English","Poult. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85171766605"
"Parmiggiani A.; Liu D.; Psota E.; Fitzgerald R.; Norton T.","Parmiggiani, Andrea (58044597200); Liu, Dong (57189588678); Psota, Eric (24315220900); Fitzgerald, Robert (58502249500); Norton, Tomas (35273348100)","58044597200; 57189588678; 24315220900; 58502249500; 35273348100","Don't get lost in the crowd: Graph convolutional network for online animal tracking in dense groups","2023","Computers and Electronics in Agriculture","212","","108038","","","","10","10.1016/j.compag.2023.108038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165543177&doi=10.1016%2fj.compag.2023.108038&partnerID=40&md5=53cbbb2c8c54fac5e007865c23bf5eb2","M3-BIORES, Kasteelpark Arenberg 30, B-3001 Heverlee, KU Leuven, Belgium; PIC North America, Hendersonville, Tennessee, United States","Parmiggiani A., M3-BIORES, Kasteelpark Arenberg 30, B-3001 Heverlee, KU Leuven, Belgium; Liu D., M3-BIORES, Kasteelpark Arenberg 30, B-3001 Heverlee, KU Leuven, Belgium; Psota E., PIC North America, Hendersonville, Tennessee, United States; Fitzgerald R., PIC North America, Hendersonville, Tennessee, United States; Norton T., M3-BIORES, Kasteelpark Arenberg 30, B-3001 Heverlee, KU Leuven, Belgium","Tracking the behaviour of animals in group-housed situations is a critical area of study for precision livestock farming, but it poses challenges in diverse and crowded environments. Existing methods often struggle with false positives and false negatives due to the complexities of these scenarios. In this study, we propose a robust computer vision algorithm for long-term (>10mins) animal tracking, with a primary focus on group-housed pigs. Our method addresses the limitations of current approaches by effectively handling errors from the detector in challenging environments. Our approach integrates Graph Convolutional Networks. (GCNs) with deep learning-based object detection techniques. We represent the data as a graph structure, with nodes corresponding to multiple animal detections over several frames. Edges connect the detections that do not appear in the same frame. The model's objective is to perform edge classification, where each edge is associated with a scalar representing the probability of being the same object. To enhance the robustness of edge classification, we combine the classifier with an ensemble predictor, enabling accurate tracking of animal identities for extended periods. Notably, our graph convolutional model achieves accurate re identification of non-consecutive animal detections in real-time without the need for trajectory prediction or ID association. We evaluate our method by comparing its performance against other tracking methods, including DeepSORT—a state-of-the-art animal tracking method that relies on a Kalman filter, a deep appearance descriptor, and the Hungarian algorithm for trajectory prediction and ID association. Our comparison demonstrates the advantages of our method, as it (1) improves the IDF1 score, indicating enhanced ID association accuracy, by 1.72%; (2) mitigates 93% of errors resulting from ID-switch and deviation; (3) extends the tracking duration of each animal by approximately 66% in challenging conditions; and (4) achieves a processing speed that is 1000 times faster than DeepSORT when operating at 22fps with the detector. The potential applications of our method extend to various aspects of livestock management. By accurately tracking individual animals, we can monitor behaviours such as feeding, drinking, and aggressive interactions. This information can lead to improvements in animal welfare, resource allocation, and breeding strategies. Follow-ups of this research are available at: https://gitlab.kuleuven.be/m3-biores/public/m3pig. © 2023 Elsevier B.V.","","Agriculture; Convolutional neural networks; Deep learning; Mammals; Object detection; 'current; Animal tracking; Computer vision algorithms; Convolutional networks; Edge classification; False positive and false negatives; Objects detection; Precision livestock farming; Tracking method; Trajectory prediction; algorithm; artificial neural network; computer vision; data processing; information; livestock farming; numerical model; precision; tracking; trajectory; Convolution","Flemish Research Council; Fonds Wetenschappelijk Onderzoek, FWO, (007421N); KU Leuven","Funding text 1: This study was created within the collaboration between the company PIC North America (Hendersonville, Tennessee, USA) and M3-BIORES research group of KU Leuven. We also acknowledge support for this work under the ICT-AGRI-FOOD project TailBiteAdvice and funding supplied by the Flemish Research Council (FWO) with grant number #S007421N. ; Funding text 2: This study was created within the collaboration between the company PIC North America (Hendersonville, Tennessee, USA) and M3-BIORES research group of KU Leuven. We also acknowledge support for this work under the ICT-AGRI-FOOD project TailBiteAdvice and funding supplied by the Flemish Research Council (FWO) with grant number #S007421N.","T. Norton; M3-BIORES, KU Leuven, Kasteelpark Arenberg 30, B-3001 Heverlee, Belgium; email: tomas.norton@kuleuven.be","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85165543177"
"Shao D.; He Z.; Fan H.; Sun K.","Shao, Dangguo (58333478100); He, Zihan (58475335600); Fan, Hongbo (33767612100); Sun, Kun (58475572000)","58333478100; 58475335600; 33767612100; 58475572000","Detection of Cattle Key Parts Based on the Improved Yolov5 Algorithm","2023","Agriculture (Switzerland)","13","6","1110","","","","7","10.3390/agriculture13061110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164175339&doi=10.3390%2fagriculture13061110&partnerID=40&md5=55928098e875d636ecb4d93c9c2f7267","Faculty of Information Engineering and Automation, Yunnan Province Key Laboratory of Computer, Kunming University of Science and Technology, Kunming, 650500, China; Faculty of Modern Agricultural Engineering, Kunming University of Science and Technology, Kunming, 650300, China","Shao D., Faculty of Information Engineering and Automation, Yunnan Province Key Laboratory of Computer, Kunming University of Science and Technology, Kunming, 650500, China; He Z., Faculty of Information Engineering and Automation, Yunnan Province Key Laboratory of Computer, Kunming University of Science and Technology, Kunming, 650500, China; Fan H., Faculty of Modern Agricultural Engineering, Kunming University of Science and Technology, Kunming, 650300, China; Sun K., Faculty of Information Engineering and Automation, Yunnan Province Key Laboratory of Computer, Kunming University of Science and Technology, Kunming, 650500, China","Accurate detection of key body parts of cattle is of great significance to Precision Livestock Farming (PLF), using artificial intelligence for video analysis. As the background image in cattle livestock farms is complex and the target features of the cattle are not obvious, traditional object-detection algorithms cannot detect the key parts of the image with high precision. This paper proposes the Filter_Attention attention mechanism to detect the key parts of cattle. Since the image is unstable during training and initialization, particle noise is generated in the feature graph after convolution calculation. Therefore, this paper proposes an attentional mechanism based on bilateral filtering to reduce this interference. We also designed a Pooling_Module, based on the soft pooling algorithm, which facilitates information loss relative to the initial activation graph compared to maximum pooling. Our data set contained 1723 images of cattle, in which labels of the body, head, legs, and tail were manually entered. This dataset was divided into a training set, verification set, and test set at a ratio of 7:2:1 for training the model proposed in this paper. The detection effect of our proposed module is proven by the ablation experiment from mAP, the AP value, and the F1 value. This paper also compares other mainstream object detection algorithms. The experimental results show that our model obtained 90.74% mAP, and the F1 value and AP value of the four parts were improved. © 2023 by the authors.","deep learning; Filter_Attention; key parts detection of cattle; Softpooling; Yolo","","National Natural Science Foundation of China, NSFC, (62266025)","This work is supported by National Natural Science Foundation of China (Grant NO.62266025).","H. Fan; Faculty of Modern Agricultural Engineering, Kunming University of Science and Technology, Kunming, 650300, China; email: 20110258@kust.edu.cn","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85164175339"
"Bortoluzzi E.M.; Goering M.J.; Ochoa S.J.; Holliday A.J.; Mumm J.M.; Nelson C.E.; Wu H.; Mote B.E.; Psota E.T.; Schmidt T.B.; Jaberi-Douraki M.; Hulbert L.E.","Bortoluzzi, Eduarda M. (57205455884); Goering, Mikayla J. (57219546384); Ochoa, Sara J. (58076289600); Holliday, Aaron J. (57852337100); Mumm, Jared M. (57209714469); Nelson, Catherine E. (58075748700); Wu, Hui (58077110000); Mote, Benny E. (13807961600); Psota, Eric T. (24315220900); Schmidt, Ty B. (7402839670); Jaberi-Douraki, Majid (59481810500); Hulbert, Lindsey E. (7004564871)","57205455884; 57219546384; 58076289600; 57852337100; 57209714469; 58075748700; 58077110000; 13807961600; 24315220900; 7402839670; 59481810500; 7004564871","Evaluation of Precision Livestock Technology and Human Scoring of Nursery Pigs in a Controlled Immune Challenge Experiment","2023","Animals","13","2","246","","","","6","10.3390/ani13020246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146761224&doi=10.3390%2fani13020246&partnerID=40&md5=ff175a5f881c9058e7184ac4999b173c","Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68505, NE, United States; Department of Statistics, Kansas State University, Manhattan, 66506, KS, United States; Department of Electrical and Computer Engineering, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Department of Mathematics, Kansas State University, Manhattan, 66506, KS, United States; 1-DATA, Kansas State University Olathe, Olathe, 66061, KS, United States","Bortoluzzi E.M., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Goering M.J., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Ochoa S.J., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Holliday A.J., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68505, NE, United States; Mumm J.M., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Nelson C.E., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States; Wu H., Department of Statistics, Kansas State University, Manhattan, 66506, KS, United States; Mote B.E., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68505, NE, United States; Psota E.T., Department of Electrical and Computer Engineering, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Schmidt T.B., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68505, NE, United States; Jaberi-Douraki M., Department of Statistics, Kansas State University, Manhattan, 66506, KS, United States, Department of Mathematics, Kansas State University, Manhattan, 66506, KS, United States, 1-DATA, Kansas State University Olathe, Olathe, 66061, KS, United States; Hulbert L.E., Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, KS, United States","The objectives were to determine the sensitivity, specificity, and cutoff values of a visual-based precision livestock technology (NUtrack), and determine the sensitivity and specificity of sickness score data collected with the live observation by trained human observers. At weaning, pigs (n = 192; gilts and barrows) were randomly assigned to one of twelve pens (16/pen) and treatments were randomly assigned to pens. Sham-pen pigs all received subcutaneous saline (3 mL). For LPS-pen pigs, all pigs received subcutaneous lipopolysaccharide (LPS; 300 μg/kg BW; E. coli O111:B4; in 3 mL of saline). For the last treatment, eight pigs were randomly assigned to receive LPS, and the other eight were sham (same methods as above; half-and-half pens). Human data from the day of the challenge presented high true positive and low false positive rates (88.5% sensitivity; 85.4% specificity; 0.871 Area Under Curve, AUC), however, these values declined when half-and-half pigs were scored (75% sensitivity; 65.5% specificity; 0.703 AUC). Precision technology measures had excellent AUC, sensitivity, and specificity for the first 72 h after treatment and AUC values were >0.970, regardless of pen treatment. These results indicate that precision technology has a greater potential for identifying pigs during a natural infectious disease event than trained professionals using timepoint sampling. © 2023 by the authors.","ethology; machine learning; Suidae domesticus","dexamethasone; epinephrine; lipopolysaccharide; animal experiment; animal welfare; Article; behavior; controlled study; deep learning; diagnostic test accuracy study; diet supplementation; Escherichia coli; ethology; linear regression analysis; livestock; locomotion; machine learning; nonhuman; nursery; pig; precision agriculture; receiver operating characteristic; sensitivity and specificity; validity; veterinarian; weaning","Food Animal Residue Avoidance Databank, (2020-41480-32497, 2021-41480-35271, 2022-41480-38135, 21-002-J); National Pork Foundation’s Board, (19-122); National Institute of Food and Agriculture, NIFA; Massachusetts Agricultural Experiment Station, MAES, (1013670, 1013671); Massachusetts Agricultural Experiment Station, MAES","This work was supported by the National Pork Foundation’s Board (19-122). M.J.-D. also accepted funding from the USDA NIFA for the Food Animal Residue Avoidance Databank (FARAD) Program (Award No.: 2020-41480-32497, 2021-41480-35271, and 2022-41480-38135). In addition, L.E.H. undergraduate researchers’ stipends and some consumables were covered by contribution no. 21-002-J from the Kansas Agricultural Experiment Station (Manhattan, KS, USA) with support through Hatch projects 1013670, and 1013671 from the USDA National Institute of Food and Agriculture (Washington, DC, USA).","L.E. Hulbert; Department of Animal Sciences and Industry, Kansas State University, Manhattan, 66506, United States; email: lhulbert@ksu.edu","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85146761224"
"Sharma S.; Kadyan V.","Sharma, Sugandha (57578865000); Kadyan, Virender (56373278500)","57578865000; 56373278500","Detection of estrus through automated classification approaches using vocalization pattern in Murrah buffaloes","2023","2023 3rd International Conference on Artificial Intelligence and Signal Processing, AISP 2023","","","","","","","0","10.1109/AISP57993.2023.10134787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163193781&doi=10.1109%2fAISP57993.2023.10134787&partnerID=40&md5=5f821d4b4d7921d508b420a6ff60f354","School of Computer Science, Speech and Language Research Centre, University of Petroleum and Energy Studies (UPES), Uttarakhand, Dehradun, India","Sharma S., School of Computer Science, Speech and Language Research Centre, University of Petroleum and Energy Studies (UPES), Uttarakhand, Dehradun, India; Kadyan V., School of Computer Science, Speech and Language Research Centre, University of Petroleum and Energy Studies (UPES), Uttarakhand, Dehradun, India","The field that deals with the studying of animal sounds to detect or classify various animal conditions in an open or a closed setup. Bioacoustics has its prospective applications in speaker identification, detection of respiratory distress, estrus phases, stress induced by ecological conditions, separation and weaning and so on. In dairy cattle, especially murrah buffalo breed, inaccurate detection of estrus is a significant constraint that affects the reproductive capacity of small as well as large cattle-herds. Detecting estrus in dairy cattle at correct time assists livestock managers and farmers to augment milk production, profitability and maintaining welfare of cattle.The studies on relationship between vocal behavior and reproductive performance is still at a very primary stage. This is due to inadequate amount of research work that has been done in order to study the association between acoustic features of animal vocal sounds and estrus owing to limited publicly available dataset. Outcomes of animal vocal sounds based studies are primarily dataset-dependent, i.e., the findings are ineffective if recording conditions are unidentified. Modern developments in deep-learning and machine-learning based approaches have led to the development of high-performance non-invasive ''animal sound classification system These systems, however, are built on scarce speech-corpora which is a preferred requisite for machine learning model training, which requires hours of good-quality recordings of sound of a single speaker in a noiseless environment. Developing such a good-quality sound corpus manually requires labour and time which makes the process expensive. In the present paper, the authors have executed machine-learning models like Random Forest, Decision Tree and SVM as well as a custom-built CNN architecture on a dataset and have carried out a comparative analysis of the above-mentioned techniques. © 2023 IEEE.","Bioacoustics; CNN; Decision tree; Estrus Detectionc; Random forest; SVM","Audio recordings; Dairies; Decision trees; Deep learning; Farms; Learning systems; Speech recognition; Support vector machines; Automated classification; Classification approach; Condition; Dairy cattles; Estrus detectionc; Machine learning models; Prospective applications; Random forests; Speaker identification; SVM; Animals","","","S. Sharma; School of Computer Science, Speech and Language Research Centre, University of Petroleum and Energy Studies (UPES), Dehradun, Uttarakhand, India; email: sugandhasharma2016@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835032074-9","","","English","Int. Conf. Artif. Intell. Signal Process., AISP","Conference paper","Final","","Scopus","2-s2.0-85163193781"
"Bresolin T.; Ferreira R.; Reyes F.; Van Os J.; Dórea J.R.R.","Bresolin, T. (55618681200); Ferreira, R. (57362855900); Reyes, F. (57567531000); Van Os, J. (57195983829); Dórea, J.R.R. (37057402900)","55618681200; 57362855900; 57567531000; 57195983829; 37057402900","Assessing optimal frequency for image acquisition in computer vision systems developed to monitor feeding behavior of group-housed Holstein heifers","2023","Journal of Dairy Science","106","1","","664","675","11","5","10.3168/jds.2022-22138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141242972&doi=10.3168%2fjds.2022-22138&partnerID=40&md5=3e39cd74e0df7a398ccdcd255e0c4e3a","Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States","Bresolin T., Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States; Ferreira R., Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States; Reyes F., Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States; Van Os J., Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States; Dórea J.R.R., Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States","Computer vision systems have emerged as a potential tool to monitor the behavior of livestock animals. Such high-throughput systems can generate massive redundant data sets for training and inference, which can lead to higher computational and economic costs. The objectives of this study were (1) to develop a computer vision system to individually monitor detailed feeding behaviors of group-housed dairy heifers, and (2) to determine the optimal frequency of image acquisition to perform inference with minimal effect on feeding behavior prediction quality. Eight Holstein heifers (96 ± 6 d old) were housed in a group and a total of 25,214 images (1 image every second) were acquired using 1 RGB camera. A total of 2,209 images were selected and each animal in the image was labeled with its respective identification (1–8). The label was annotated only on animals that were at the feed bunk (head through the feed rail). From the labeled images, 1,392 were randomly selected to train a deep learning algorithm for object detection with YOLOv3 (“You Only Look Once” version 3) and 154 images were used for validation. An independent data set (testing set = 663 out of the 2,209 images) was used to test the algorithm. The average accuracy for identifying individual animals in the testing set was 96.0%, and for each individual heifer from 1 to 8 the accuracy was 99.2, 99.6, 99.2, 99.6, 99.6, 99.2, 99.4, and 99.6%, respectively. After identifying the animals at the feed bunk, we computed the following feeding behavior parameters: number of visits (NV), mean visit duration (MVD), mean interval between visits (MIBV), and feeding time (FT) for each heifer using a data set composed by 8,883 sequential images (1 image every second) from 4 time points. The coefficient of determination (R2) was 0.39, 0.78, 0.48, and 0.99, and the root mean square error (RMSE) were 12.3 (count), 0.78, 0.63, and 0.31 min for NV, MVD, MIBV, and FT, respectively, considering 1 image every second. When we moved from 1 image per second to 1 image every 5 (MIBV) or 10 (NV, MDV, and FT) s, the R2 observed were 0.55 (NV), 0.74 (MVD), 0.70 (MIBV), and 0.99 (FT); and the RMSE were 2.27 (NV, count), 0.38 min (MVD), 0.22 min (MIBV), and 0.44 min (FT). Our results indicate that computer vision systems can be used to individually identify group-housed Holstein heifers (overall accuracy = 99.4%). Based on individual identification, feeding behavior such as MVD, MIBV, and FT can be monitored with reasonable accuracy and precision. Regardless of the frequency for optimal image acquisition, our results suggested that longer time intervals of image acquisition would reduce data collecting and model inference while maintaining adequate predictive performance. However, we did not find an optimal time interval for all feeding behavior; instead, the optimal frequency of image acquisition is phenotype-specific. Overall, the best R2 and RMSE for NV, MDV, and FT were achieved using 1 image every 10 s, and for MIBV it was achieved using 1 image every 5 s, and in both cases model inference and data storage could be drastically reduced. © 2023 American Dairy Science Association","deep learning; feeding time; image labeling; machine learning; prediction accuracy; visit duration","Animal Feed; Animals; Artificial Intelligence; Cattle; Dairying; Feeding Behavior; Female; animal; animal food; artificial intelligence; bovine; dairying; feeding behavior; female; procedures","National Institute of Food and Agriculture, NIFA, (1021996, 2020-67015-30831)","The authors thank the financial support from the USDA National Institute of Food and Agriculture (Washington, DC; grant 2020-67015-30831/accession no. 1021996). The authors have not stated any conflicts of interest.","J.R.R. Dórea; Department of Animal and Dairy Sciences, University of Wisconsin, Madison, 53706, United States; email: joao.dorea@wisc.edu","","Elsevier Inc.","00220302","","","36333134","English","J. Dairy Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141242972"
"Sohi R.; Almasi F.; Nguyen H.; Carroll A.; Trompf J.; Weerasinghe M.; Bervan A.; Godoy B.I.; Ahmed A.; Stear M.J.; Desai A.; Jois M.","Sohi, Rajneet (57196943846); Almasi, Fazel (57212465273); Nguyen, Hien (55369733100); Carroll, Alexandra (58026557500); Trompf, Jason (6602321500); Weerasinghe, Maneka (57196947625); Bervan, Aidin (57196940554); Godoy, Boris I. (23569196900); Ahmed, Awais (58273508100); Stear, Michael J. (7006119516); Desai, Aniruddha (57196493155); Jois, Markandeya (6603653291)","57196943846; 57212465273; 55369733100; 58026557500; 6602321500; 57196947625; 57196940554; 23569196900; 58273508100; 7006119516; 57196493155; 6603653291","Determination of ewe behaviour around lambing time and prediction of parturition 7 days prior to lambing by tri-axial accelerometer sensors in an extensive farming system","2022","Animal Production Science","62","17","","1729","1738","9","12","10.1071/AN21460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143705652&doi=10.1071%2fAN21460&partnerID=40&md5=0f342068c76dc1bc86d22fccd0f3e9b7","School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; School of Mathematics and Physics, University of Queensland, St. Lucia, 4072, QLD, Australia; Northgate Park, Glenrowan, Wangaratta, 3675, VIC, Australia; Centre for Technology Infusion, La Trobe University, Bundoora, 3086, VIC, Australia; Department of Mechanical Engineering, Boston University, Boston, 02215, MA, United States","Sohi R., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; Almasi F., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; Nguyen H., School of Mathematics and Physics, University of Queensland, St. Lucia, 4072, QLD, Australia; Carroll A., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; Trompf J., Northgate Park, Glenrowan, Wangaratta, 3675, VIC, Australia; Weerasinghe M., Centre for Technology Infusion, La Trobe University, Bundoora, 3086, VIC, Australia; Bervan A., Centre for Technology Infusion, La Trobe University, Bundoora, 3086, VIC, Australia; Godoy B.I., Department of Mechanical Engineering, Boston University, Boston, 02215, MA, United States; Ahmed A., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; Stear M.J., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia; Desai A., Centre for Technology Infusion, La Trobe University, Bundoora, 3086, VIC, Australia; Jois M., School of Life Sciences, La Trobe University, Bundoora, 3086, VIC, Australia","Context: Lamb loss and dyctocia are two major challenges in extensive farming systems. While visual observation can be impractical due to the large sizes of paddocks, number of animals and high labour cost, wearable sensors can be used to monitor the behaviour of ewes as there might be changes in their activities prior to lambing. This provides sufficient time for the farm manager to nurse those ewes that are at risk of dyctocia. Aim: The objective of this study was to determine whether the behaviour of a pregnant ewe could predict the time of parturition. Methods: Two separate trials were conducted: the first trial (T1), with 32 ewes, included human/video observations, and the second trial (T2), with 165 ewes, conducted with no humans present, to emulate real extensive farming settings. The ewes were fitted with tri-axial accelerometer sensors by means of halters. Three-dimensional movement data were collected for a period of at least 7 and 14 days in T1 and T2 respectively. The sensor units were retrieved, and their data downloaded using ActiGraph software. Ewe behaviour was determined through support vector machine learning (SVM) algorithm, including licking, grazing, rumination, walking, and idling. The behaviours of ewes predicted by analysis of sensor data were compared with behaviours determined using visual observation (video recordings), with time synchronisation to validate the results. Deep learning and neural-network algorithms were used to predict lambing time. Key results: The concordance percentages between visual observation and sensor data were 90 ± 11, 81 ± 15, 95 ± 10, 96 ± 6, and 93 ± 8% ± s.d. for grazing, licking, rumination, idling, and walking respectively. The deep-learning model predicted the time of lambing with 90% confidence via a quantile regression method, which can be interpereted as 90% prediction intervals, and shows that the time of lambing can be predicted with reasonable confidence approximately 240 h before the actual lambing events. Conclusion: It was possible to predict the time of parturition up to 10 days before lambing. Implications: The behaviour of ewes around lambing time has a direct effect on the survival of the lambs and therefore plays an important part in animal management. This knowledge could improve the productivity of sheep and considerably decrease lamb mortality rates.  © 2022 The Author(s) (or their employer(s)). Published by CSIRO Publishing. This is an open access article distributed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","accelerometer sensors; extensive farming; lamb survival; lambing time; machine learning; parturition; quantile regression; sheep behaviour","livestock farming; machine learning; sensor; sensory system; sheep","","","R. Sohi; School of Life Sciences, La Trobe University, Bundoora, 3086, Australia; email: r.sohi@latrobe.edu.au","","CSIRO","18360939","","","","English","Ani. Prod. Sci.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85143705652"
"Mehreen S.; Goëau H.; Bonnet P.; Chau S.; Champ J.; Joly A.","Mehreen, Shamprikta (58771283200); Goëau, Hervé (24450343100); Bonnet, Pierre (57195772427); Chau, Sophie (58771256500); Champ, Julien (24528232800); Joly, Alexis (56264909800)","58771283200; 24450343100; 57195772427; 58771256500; 24528232800; 56264909800","Estimating Compositions and Nutritional Values of Seed Mixes Based on Vision Transformers","2023","Plant Phenomics","5","","0112","","","","0","10.34133/plantphenomics.0112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180134061&doi=10.34133%2fplantphenomics.0112&partnerID=40&md5=4e9cc3656c79908893d732d231cc4122","Inria, LIRMM, University Montpellier, CNRS, Montpellier, France; CIRAD, UMR AMAP, Occitanie, Montpellier, France; Chambre d’Agriculture - Haute Vienne, Limoges, Nouvelle-Aquitaine, France","Mehreen S., Inria, LIRMM, University Montpellier, CNRS, Montpellier, France; Goëau H., CIRAD, UMR AMAP, Occitanie, Montpellier, France; Bonnet P., CIRAD, UMR AMAP, Occitanie, Montpellier, France; Chau S., Chambre d’Agriculture - Haute Vienne, Limoges, Nouvelle-Aquitaine, France; Champ J., Inria, LIRMM, University Montpellier, CNRS, Montpellier, France; Joly A., Inria, LIRMM, University Montpellier, CNRS, Montpellier, France","The cultivation of seed mixtures for local pastures is a traditional mixed cropping technique of cereals and legumes for producing, at a low production cost, a balanced animal feed in energy and protein in livestock systems. By considerably improving the autonomy and safety of agricultural systems, as well as reducing their impact on the environment, it is a type of crop that responds favorably to both the evolution of the European regulations on the use of phytosanitary products and the expectations of consumers who wish to increase their consumption of organic products. However, farmers find it difficult to adopt it because cereals and legumes do not ripen synchronously and the harvested seeds are heterogeneous, making it more difficult to assess their nutritional value. Many efforts therefore remain to be made to acquire and aggregate technical and economical references to evaluate to what extent the cultivation of seed mixtures could positively contribute to securing and reducing the costs of herd feeding. The work presented in this paper proposes new Artificial Intelligence techniques that could be transferred to an online or smartphone application to automatically estimate the nutritional value of harvested seed mixes to help farmers better manage the yield and thus engage them to promote and contribute to a better knowledge of this type of cultivation. For this purpose, an original open image dataset has been built containing 4,749 images of seed mixes, covering 11 seed varieties, with which 2 types of recent deep learning models have been trained. The results highlight the potential of this method and show that the best-performing model is a recent state-of-the-art vision transformer pre-trained with self-supervision (Bidirectional Encoder representation from Image Transformer). It allows an estimation of the nutritional value of seed mixtures with a coefficient of determination R2 score of 0.91, which demonstrates the interest of this type of approach, for its possible use on a large scale. © 2023 American Association for the Advancement of Science. All Rights Reserved.","","Costs; Deep learning; Seed; Agricultural system; Animal feed; Energy; European regulation; Harvested seeds; Impact on the environment; Livestock systems; Nutritional value; Phytosanitary products; Production cost; Cultivation","Agence Nationale de la Recherche, ANR, (Pl@ntAgroEco 22-PEAE0009)","The authors would first like to thank the seed companies who shared the seeds on which this study was conducted: Sem-Partners, Agri-Obtentions, Caussade Semences, and Mr. Jouffray Drillaud. They would also like to warmly thank the following organizations, which participated in the co-production of the visual data used: CDA12, CDA09, CDA17, CDA23, CDA24, CDA31, CDA32, CDA64, CRANA, CRAO, the RedCap network, and the agricultural high school of Venours. Finally, the authors warmly thank Jean-Christophe Lombardo and Hugo Gresse for their help in implementing the experiments and deploying the results on the demonstration web interface. Funding: This research work was mainly undertaken within the framework of the Carpeso project (AAP IP 2019 n°19AIP5902), supported by the CASDAR program (The Special Affection Account for Agricultural and Rural Development). It was also partially funded by the French National Research Agency (ANR) through the grant Pl@ntAgroEco 22-PEAE0009. Author contributions: All authors contributed equally to this work. Competing interests: The authors declare that they have no competing interests.","S. Mehreen; Inria, LIRMM, University Montpellier, CNRS, Montpellier, France; email: shamprikta.mehreen@gmail.com","","American Association for the Advancement of Science","26436515","","","","English","Plant. Phenomic.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180134061"
"Nakaguchi V.M.; Ahamed T.","Nakaguchi, Victor Massaki (57853537400); Ahamed, Tofael (9269729600)","57853537400; 9269729600","Development of an Early Embryo Detection Methodology for Quail Eggs Using a Thermal Micro Camera and the YOLO Deep Learning Algorithm","2022","Sensors","22","15","5820","","","","16","10.3390/s22155820","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136341664&doi=10.3390%2fs22155820&partnerID=40&md5=90e8bec5225cdde222acca32f324cb7d","Graduate School of Science and Technology, University of Tsukuba, Tennodai 1-1-1, Tsukuba, 305-8577, Japan; Faculty of Life and Environmental Sciences, University of Tsukuba, Tennodai 1-1-1, Tsukuba, 305-8577, Japan","Nakaguchi V.M., Graduate School of Science and Technology, University of Tsukuba, Tennodai 1-1-1, Tsukuba, 305-8577, Japan; Ahamed T., Faculty of Life and Environmental Sciences, University of Tsukuba, Tennodai 1-1-1, Tsukuba, 305-8577, Japan","Poultry production utilizes many available technologies in terms of farm-industry automation and sanitary control. However, there is a lack of robust techniques and affordable equipment for avian embryo detection and sexual segregation at the early stages. In this work, we aimed to evaluate the potential use of thermal micro cameras for detecting embryos in quail eggs via thermal images during the first 168 h (7 days) of incubation. We propose a methodology to collect data during the incubation period. Additionally, to support the visual analysis, YOLO deep learning object detection algorithms were applied to detect unfertilized eggs; the results showed its potential to distinguish fertilized eggs from unfertilized eggs during the incubation period, after filtering radiometric images. We compared YOLOv4, YOLOv5 and SSD-MobileNet V2 trained models. The mAP@0.50 of the YOLOv4, YOLOv5 and SSD-MobileNet V2 was 98.62%, 99.5% and 91.8%, respectively. We also compared three testing datasets for different intervals of rotation of eggs, as our hypothesis was that fewer turning periods could improve the visualization of fertilized egg features, and applied three treatments: 1.5 h, 6 h, and 12 h. The results showed that turning eggs in different periods did not exhibit a linear relation, as the F1 Score for YOLOv4 of detection for the 12 h period was 0.569, that for the 6 h period was 0.404 and that for the 1.5 h period was 0.384. YOLOv5 F1 Scores for 12 h, 6 h and 1.5 h were 1, 0.545 and 0.386, respectively. SSD-MobileNet V2 performed F1 scores of 0.60 for 12 h, 0.22 for 6 h and 0 for 1.5 h turning periods. © 2022 by the authors.","deep learning; embryo detection; precision livestock farming; quail eggs; thermal imaging; YOLO","Animals; Deep Learning; Eggs; Quail; Agriculture; Deep learning; Learning algorithms; Object detection; Video cameras; Deep learning; Embryo detection; F1 scores; Incubation periods; Micro cameras; Precision livestock farming; Quail eggs; Thermal; Thermal-imaging; YOLO; animal; egg; quail; Infrared imaging","University of Tsukuba","","T. Ahamed; Faculty of Life and Environmental Sciences, University of Tsukuba, Tsukuba, Tennodai 1-1-1, 305-8577, Japan; email: tofael.ahamed.gp@u.tsukuba.ac.jp","","MDPI","14248220","","","35957378","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136341664"
"Yoon H.; Lee I.; Kang H.; Kim K.-S.; Lee E.","Yoon, Hachung (55498241400); Lee, Ilseob (55201241400); Kang, Hyeonjeong (58386748000); Kim, Kyung-Sook (57737027600); Lee, Eunesub (55723743900)","55498241400; 55201241400; 58386748000; 57737027600; 55723743900","Big data-based risk assessment of poultry farms during the 2020/2021 highly pathogenic avian influenza epidemic in Korea","2022","PLoS ONE","17","6 June","e0269311","","","","4","10.1371/journal.pone.0269311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131772215&doi=10.1371%2fjournal.pone.0269311&partnerID=40&md5=3cdd98d18051dc5069b5457c9ea2b86b","Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea","Yoon H., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea; Lee I., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea; Kang H., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea; Kim K.-S., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea; Lee E., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gyeongsangbuk-do, Gimcheon, South Korea","Outbreaks of H5-type highly pathogenic avian influenza (HPAI) in poultry have been reported in various parts of the world. To respond to these continuous threats, numerous surveillance programs have been applied to poultry raising facilities as well as wild birds. In Korea, a surveillance program was developed aimed at providing a preemptive response to possible outbreaks at poultry farms. The purpose of this study is to comprehensively present the risks of HPAI evaluated by this program in relation to actual outbreak farms during the epidemic of 2020/2021. A deep learning-based risk assessment program was trained based on the pattern of livestock vehicles visiting poultry farms and HPAI outbreaks to calculate the risk of HPAI for farms linked by the movement of livestock vehicles (such farms are termed “epidemiologically linked farms”). A total of 7,984 risk assessments were conducted, and the results were categorized into four groups. The proportion of the highest risk level was greater in duck farms (13.6%) than in chicken farms (8.8%). Among the duck farms, the proportion of the highest risk level was much greater in farms where breeder ducks were raised (accounting for 26.4% of the risk) than in farms where ducks were raised to obtain meat (12.8% of the risk). A higher risk level was also found in cases where the species of the outbreak farm and epidemiologically linked farms were the same (proportion of the highest risk level = 13.2%) compared to that when the species between the two farms were different (7.9%). The overall proportion of farms with HPAI outbreaks among epidemiologically linked farms (attack rate, AR) was 1.7% as HPAI was confirmed on 67 of the 3,883 epidemiologically linked farms. The AR was highest for breeder ducks (15.3%) among duck farms and laying hens (4.8%) among chicken farms. The AR of the pairs where livestock vehicles entered the inner farm area was 1.3 times (95% confidence interval: 1.4–2.9) higher than that of all pairs. With the risk information provided, customized preventive measures can be implemented for each epidemiologically linked farm. The use of this risk assessment program would be a good example of information-based surveillance and support decision-making for controlling animal diseases. Copyright: © 2022 Yoon et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","","Animals; Big Data; Chickens; Disease Outbreaks; Ducks; Farms; Female; Influenza in Birds; Poultry; Poultry Diseases; Risk Assessment; agricultural land; Article; chicken; deep learning; duck; epidemic; highly pathogenic avian influenza; infection risk; Korea; nonhuman; poultry farming; risk assessment; animal; avian influenza; bird disease; epidemic; female; poultry; risk assessment; veterinary medicine","","","H. Yoon; Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, Gyeongsangbuk-do, South Korea; email: heleney@korea.kr","","Public Library of Science","19326203","","POLNC","35671297","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131772215"
"Mielke F.; Van Ginneken C.; Aerts P.","Mielke, Falk (57188948917); Van Ginneken, Chris (26656407800); Aerts, Peter (7005208299)","57188948917; 26656407800; 7005208299","A workflow for automatic, high precision livestock diagnostic screening of locomotor kinematics","2023","Frontiers in Veterinary Science","10","","1111140","","","","4","10.3389/fvets.2023.1111140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150503066&doi=10.3389%2ffvets.2023.1111140&partnerID=40&md5=e98226c8b7716e17ae339da5037c41b1","Functional Morphology, Department of Biology, Faculty of Science, University of Antwerp, Antwerp, Belgium; Comparative Perinatal Development, Department of Veterinary Sciences, University of Antwerp, Antwerp, Belgium","Mielke F., Functional Morphology, Department of Biology, Faculty of Science, University of Antwerp, Antwerp, Belgium, Comparative Perinatal Development, Department of Veterinary Sciences, University of Antwerp, Antwerp, Belgium; Van Ginneken C., Comparative Perinatal Development, Department of Veterinary Sciences, University of Antwerp, Antwerp, Belgium; Aerts P., Functional Morphology, Department of Biology, Faculty of Science, University of Antwerp, Antwerp, Belgium","Locomotor kinematics have been challenging inputs for automated diagnostic screening of livestock. Locomotion is a highly variable behavior, and influenced by subject characteristics (e.g., body mass, size, age, disease). We assemble a set of methods from different scientific disciplines, composing an automatic, high through-put workflow which can disentangle behavioral complexity and generate precise individual indicators of non-normal behavior for application in diagnostics and research. For this study, piglets (Sus domesticus) were filmed from lateral perspective during their first 10 h of life, an age at which maturation is quick and body mass and size have major consequences for survival. We then apply deep learning methods for point digitization, calculate joint angle profiles, and apply information-preserving transformations to retrieve a multivariate kinematic data set. We train probabilistic models to infer subject characteristics from kinematics. Model accuracy was validated for strides from piglets of normal birth weight (i.e., the category it was trained on), but the models infer the body mass and size of low birth weight (LBW) piglets (which were left out of training, out-of-sample inference) to be “normal.” The age of some (but not all) low birth weight individuals was underestimated, indicating developmental delay. Such individuals could be identified automatically, inspected, and treated accordingly. This workflow has potential for automatic, precise screening in livestock management. Copyright © 2023 Mielke, Van Ginneken and Aerts.","diagnostics; Fourier Series; kinematics; locomotion; low birth weight; piglets; precision livestock farming; probabilistic modeling","animal experiment; Article; birth weight; body mass; body size; classifier; controlled study; deep learning; developmental delay; digitization; domestic pig; female; joint angle; kinematics; livestock; locomotion; low birth weight; machine learning; male; mathematical transformation; maturation; newborn; nonhuman; piglet; screening; survival; videorecording; walking; workflow","Universiteit Antwerpen, (BOF-GOA 2016 33927)","This study was funded by a grant from the special research fund of the University of Antwerp (BOF-GOA 2016 33927). ","F. Mielke; Functional Morphology, Department of Biology, Faculty of Science, University of Antwerp, Antwerp, Belgium; email: falkmielke.biology@mailbox.org","","Frontiers Media S.A.","22971769","","","","English","Front. Vet. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85150503066"
"Katiyar S.; Kumar K.; Ramanujam E.; Suganya Devi K.; Naidu V.N.","Katiyar, Shikhar (58750797200); Kumar, Krishna (57219219573); Ramanujam, E. (55603273600); Suganya Devi, K. (57190939432); Naidu, Vadagana Nagendra (58750772600)","58750797200; 57219219573; 55603273600; 57190939432; 58750772600","Comparative analysis of lumpy skin disease detection using deep learning models","2023","Deep Learning in Medical Image Processing and Analysis","","","","79","96","17","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178987765&partnerID=40&md5=939ef396a74284282f0af0b280143108","Department of Computer Science and Engineering, National Institute of Technology Silchar, India","Katiyar S., Department of Computer Science and Engineering, National Institute of Technology Silchar, India; Kumar K., Department of Computer Science and Engineering, National Institute of Technology Silchar, India; Ramanujam E., Department of Computer Science and Engineering, National Institute of Technology Silchar, India; Suganya Devi K., Department of Computer Science and Engineering, National Institute of Technology Silchar, India; Naidu V.N., Department of Computer Science and Engineering, National Institute of Technology Silchar, India","Lumpy skin disease (LSD) is an infectious disease caused by a Poxviridae family of viruses to the cattle, and it is a transboundary disease affecting the cattle industry worldwide. Asia has the highest number of LSD reports from the year 2019 till today, as per the LSD outbreak report data generated by the World Organization of Animal Health. In India, it started in 2022 and resulted in the death of over 97,000 cattle in three months between July and September 2022. LSD transmission is mainly due to blood-feeding insects. The other water feed troughs and contaminated environments are minor cases. According to Cattle India Foundation (CIF) analysis, more than 16.42 lakh cattle have been infected by LSD, and 75,000 have died since July 2022. Thus, the reduction of the livestock mortality rate of cattle is significant today either by analyzing skin diseases or through early detection mechanisms. The LSD research is evolving and attracts various Artificial Intelligence (AI) experts to analyze the problem through image processing, machine learning, and deep learning. This chapter compares the performance of deep and hybrid deep learning models for detecting LSD. The challenges and limitations of this study have been extended into future scope for enhancements in LSD detection. © The Institution of Engineering and Technology 2023. All rights reserved.","","","","","","","Institution of Engineering and Technology","","978-183953794-3; 978-183953793-6","","","English","Deep Learn. in Med. Image Process. and Anal.","Book chapter","Final","","Scopus","2-s2.0-85178987765"
"","","","1st international conference on Machine Intelligence and Computer Science Applications, ICMICSA 2022","2023","Lecture Notes in Networks and Systems","656 LNNS","","","","","372","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152571063&partnerID=40&md5=94ff6f0475f7468ac0760bb8203455f8","","","The proceedings contain 32 papers. The special focus in this conference is on Machine Intelligence and Computer Science Applications. The topics include: A Deep Learning Approach for Hand Gestures Recognition; a Review on Video-Based Heart Rate, Respiratory Rate and Blood Pressure Estimation; Arabic Handwritten Characters Recognition by Combining PHOG Descriptor with Ensemble Methods; emotion Recognition Techniques; a Deep Convolutional Neural Networks Approach for Word-Level Handwritten Script Identification Using a Large Dataset; arabic Sign Language Analysis and Recognition; masked Facial Recognition Using Deep Metric Learning; Hardware Software Co-design Approch for ECG Signal Analysis; dynamic Output Feedback Controller Design for a Class of Takagi-Sugeno Models: Application to One-Link Flexible Joint Robot; A Solution Based on Faster R-CNN for Augmented Reality Markers’ Detection: Drawing Courses Case Study; solution Based on Mobile Web Application to Detect and Treat Patients with Mental Disorders; an Unsupervised Voice Activity Detection Using Time-Frequency Features; Dimensionality Reduction for Predicting Students Dropout in MOOC; remote Heart Rate Measurement Using Plethysmographic Wave Analysis; an Online Model for Detecting Attacks in Wireless Sensor Networks; an IoT Ecosystem-Based Architecture of a Smart Livestock Farm; energy-efficient Next Hop Selection for Topology Creation in Wireless Sensor Networks; Physical Layer Parameters for Jamming Attack Detection in VANETs: A Long Short Term Memory Approach; an Improved Active Machine Learning Query Strategy for Entity Matching Problem; Study of COVID19 Impact on Moroccan Financial Market Based on ARDL; long-Term Average Temperature Forecast Using Machine Learning and Deep Learning in the Region of Beni Mellal; Augmented Analytics Big Data Warehouse Based on Big Data Architecture and LOD System; analyzing Instagram Images to Predict Personality Traits; heuristic Algorithm for Extracting Frequent Patterns in Transactional Databases; unsupervising Denoising Model Based Generative Adversarial Networks; meta-heuristic Algorithms for Text Feature Selection Problems.","","","","","","Aboutabit N.; Hafidi I.; Lazaar M.","Springer Science and Business Media Deutschland GmbH","23673370","978-303128845-6","","","English","Lect. Notes Networks Syst.","Conference review","Final","","Scopus","2-s2.0-85152571063"
"Molapo M.; Tu C.; Du Plessis D.; Du S.","Molapo, Makhabane (58614270800); Tu, Chunling (26436160600); Du Plessis, Deao (57206669891); Du, Shengzhi (8918963700)","58614270800; 26436160600; 57206669891; 8918963700","Management and Monitoring of Livestock in the Farm Using Deep Learning","2023","6th International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems, icABCD 2023 - Proceedings","","","","","","","3","10.1109/icABCD59051.2023.10220556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171992910&doi=10.1109%2ficABCD59051.2023.10220556&partnerID=40&md5=e9efb88aaa468244245b9eced87ad2c4","Tshwane University of Technology, Department of Computer Systems Engineering, Pretoria, South Africa","Molapo M., Tshwane University of Technology, Department of Computer Systems Engineering, Pretoria, South Africa; Tu C., Tshwane University of Technology, Department of Computer Systems Engineering, Pretoria, South Africa; Du Plessis D., Tshwane University of Technology, Department of Computer Systems Engineering, Pretoria, South Africa; Du S., Tshwane University of Technology, Department of Computer Systems Engineering, Pretoria, South Africa","Livestock management and monitoring system play a crucial role in farm operations. This paper proposes a system for the management and monitoring of livestock on a farm using deep learning techniques. Traditional methods of monitoring livestock involve manual observation, which can be time-consuming and unreliable. Various systems have been developed, however, there are still challenges existing in present livestock classification and counting, including occlusion, animal overlapping, shadow, etc. To improve all these challenges, this paper presents a monitoring system of livestock under different conditions by the end-to-end deep learning model of You Only Look Once version 5 (YOLOv5). The suggested model conducts feature extraction on the original image with the original YOLOv5 backbone network and detects livestock of different sizes for counting on each anchor frame. Additionally, this model identifies and tracks individual animals The Kaggle dataset collected in real-time containing different animals is used as YOLOv5 relies heavily on data augmentation to improve its detection and tracking performance and validate the proposed system. The scaling, resizing, and manipulation of the splitting dataset are done by the Roboflow application. Additionally, this paper seeks to demonstrate the latest research in utilizing Faster Regional convolutional neural networks (R-CNN) and compare its backbones with the original YOLOv5 backbone. The tensor board graphs from Colab show that this proposed system outperformed other R-CNN, achieving an accuracy of 93% on mAP@_0.5%, making it a promising option for intelligent farm monitoring and managing.  © 2023 IEEE.","Convolutional Neural Network (CNN); Deep Learning (DL); Faster Convolutional Neural Network (FCNN); Regional Convolutional Neural Network (R-CNN); You Only Look Once version 5 (YOLOv5)","Agriculture; Animals; Convolution; Deep learning; Feature extraction; Learning systems; Convolutional neural network; Deep learning; Fast convolutional neural network; Management systems; Monitoring system; Regional convolutional neural network; You only look once version 5; Convolutional neural networks","","","","Pudaruth S.; Singh U.","Institute of Electrical and Electronics Engineers Inc.","","979-835031480-9","","","English","Int. Conf. Artif. Intell., Big Data, Comput. Data Commun. Syst., icABCD - Proc.","Conference paper","Final","","Scopus","2-s2.0-85171992910"
"Kalinaki K.; Shafik W.; Gutu T.J.L.; Malik O.A.","Kalinaki, Kassim (58174759100); Shafik, Wasswa (57210203704); Gutu, Tar J. L. (57222494503); Malik, Owais Ahmed (57301482300)","58174759100; 57210203704; 57222494503; 57301482300","Computer Vision and Machine Learning for Smart Farming and Agriculture Practices","2023","Artificial Intelligence Tools and Technologies for Smart Farming and Agriculture Practices","","","","79","100","21","22","10.4018/978-1-6684-8516-3.ch005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166667032&doi=10.4018%2f978-1-6684-8516-3.ch005&partnerID=40&md5=f9a64a1a3248a5222f0296555ac05e6b","Islamic University in Uganda, Uganda; Ndejje University, Uganda; Department, Electronics and Computer Engineering, School of Engineering and Technology, Soroti University, Uganda; Universiti Brunei Darussalam, Brunei Darussalam","Kalinaki K., Islamic University in Uganda, Uganda; Shafik W., Ndejje University, Uganda; Gutu T.J.L., Department, Electronics and Computer Engineering, School of Engineering and Technology, Soroti University, Uganda; Malik O.A., Universiti Brunei Darussalam, Brunei Darussalam","The advent of cutting-edge techniques such as Computer Vision (CV) and Artificial Intelligence (AI) have sparked a revolution in the agricultural industry, with applications ranging from crop and livestock monitoring to yield optimization, crop grading and sorting, pest and disease identification, and pesticide spraying among others. By leveraging these innovative techniques, sustainable farming practices are being adopted to ensure future food security. With the help of CV, AI, and related methods, such as Machine Learning (ML) together with Deep Learning (DL), key stakeholders can gain invaluable insights into the performance of agricultural and farm initiatives, enabling them to make data-driven decisions without the need for direct interaction. This chapter presents a comprehensive overview of the requirements, techniques, applications, and future directions for smart farming and agriculture. Different vital stakeholders, researchers, and students who have a keen interest in this field would find the discussions in this chapter insightful. © 2023 by IGI Global. All rights reserved.","","","","","","","IGI Global","","978-166848518-7; 978-166848516-3","","","English","Artificial Intelligence Tools and Technologies for Smart Farming and Agriculture Practices","Book chapter","Final","","Scopus","2-s2.0-85166667032"
"Kim H.S.; Muallifah N.; Cho Y.; Lee B.; Yi M.Y.","Kim, Hyun Soo (57191717782); Muallifah, Nabilah (57950278900); Cho, Yesung (57950524200); Lee, Bumho (57230460900); Yi, Mun Yong (7102961073)","57191717782; 57950278900; 57950524200; 57230460900; 7102961073","Deep learning-based defect detection on livestock operations","2022","ACM International Conference Proceeding Series","","","","21","27","6","0","10.1145/3538641.3561483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141068448&doi=10.1145%2f3538641.3561483&partnerID=40&md5=a1c874dfff5ebeba9ffeaae9fba1f25b","Department of Civil Environmental Engineering, Kaist, Daejeon, South Korea; Graduate School of Data Science, Kaist, Daejeon, South Korea; Department of Industrial and Systems Engineering, Kaist, Daejeon, South Korea","Kim H.S., Department of Civil Environmental Engineering, Kaist, Daejeon, South Korea; Muallifah N., Graduate School of Data Science, Kaist, Daejeon, South Korea; Cho Y., Graduate School of Data Science, Kaist, Daejeon, South Korea; Lee B., Graduate School of Data Science, Kaist, Daejeon, South Korea; Yi M.Y., Department of Industrial and Systems Engineering, Kaist, Daejeon, South Korea","The recent growth of computer vision system research has contributed to the advancement of livestock operations including meat processing. However, the defect detection of livestock products has not been actively studied due to the scarcity of dataset despite its significance. In the real world, the ratio of the defects in the livestock dataset is extremely low, compared to the normal products, resulting in insufficient model training. To address this problem, this study propose a deep learning-based anomaly detection method for an extremely unbalanced dataset. Adopting an anomaly detection framework from prior research, we suggest an adversarial autoencoder, which includes loss inversion to optimize reconstruction error for training skewed data, and a linear perturbation method called the Fast Gradient Sign Method (FGSM) to generate noises as anomaly cases. The model was then evaluated on the real-world dataset acquired from a livestock factory to detect defective chicken carcass (e.g., broken leg, injured skin, bent wing). The experiment results show that the proposed model outperforms existing models used for anomaly detection problems (i.e. DevNet and DeepSAD) and loss inversion and FGSM successfully improve the model performance when detecting livestock defects, even if the dataset contains extremely few defects.  © 2022 ACM.","anomaly detection; autoencoder; image classification; livestock quality control; neural networks","Agriculture; Anomaly detection; Deep learning; Image classification; Learning systems; Perturbation techniques; Quality control; Anomaly detection; Auto encoders; Computer vision system; Defect detection; Images classification; Livestock operations; Livestock quality control; Meat processing; Neural-networks; Systems research; Defects","Korea Smart Farm RD Foundation; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421043-04); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm RD Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (421043-04)","M.Y. Yi; Department of Industrial and Systems Engineering, Kaist, Daejeon, South Korea; email: munyi@kaist.ac.kr","","Association for Computing Machinery","","978-145039398-0","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85141068448"
"Mao A.; Giraudet C.S.E.; Liu K.; De Almeida Nolasco I.; Xie Z.; Xie Z.; Gao Y.; Theobald J.; Bhatta D.; Stewart R.; McElligott A.G.","Mao, Axiu (57238219000); Giraudet, Claire S. E. (57771037100); Liu, Kai (55823366100); De Almeida Nolasco, Inês (57209888053); Xie, Zhiqin (8960603600); Xie, Zhixun (7402266956); Gao, Yue (57206592088); Theobald, James (59265278000); Bhatta, Devaki (22833533700); Stewart, Rebecca (35800305300); McElligott, Alan G. (6701399660)","57238219000; 57771037100; 55823366100; 57209888053; 8960603600; 7402266956; 57206592088; 59265278000; 22833533700; 35800305300; 6701399660","Automated identification of chicken distress vocalizations using deep learning models","2022","Journal of the Royal Society Interface","19","191","20210921","","","","21","10.1098/rsif.2021.0921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133106709&doi=10.1098%2frsif.2021.0921&partnerID=40&md5=678b92c831a3cec0051fb3d40c02d9b4","Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; Centre For Animal Health And Welfare, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; Animal Health Research Centre, Chengdu Research Institute, City University Of Hong Kong, Chengdu, China; School Of Electronic Engineering And Computer Science, Queen Mary University Of London, London, United Kingdom; Guangxi Key Laboratory Of Veterinary Biotechnology, Guangxi Veterinary Research Institute, 51 North Road You Ai, Nanning, Guangxi, 530001, China; School Of Computer Science And Electronic Engineering, University Of Surrey, Guildford, United Kingdom; Agsenze, Parc House, Kingston Upon Thames, London, United Kingdom; Dyson School Of Design Engineering, Imperial College London, London, United Kingdom","Mao A., Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; Giraudet C.S.E., Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong, Centre For Animal Health And Welfare, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; Liu K., Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong, Animal Health Research Centre, Chengdu Research Institute, City University Of Hong Kong, Chengdu, China; De Almeida Nolasco I., School Of Electronic Engineering And Computer Science, Queen Mary University Of London, London, United Kingdom; Xie Z., Guangxi Key Laboratory Of Veterinary Biotechnology, Guangxi Veterinary Research Institute, 51 North Road You Ai, Nanning, Guangxi, 530001, China; Xie Z., Guangxi Key Laboratory Of Veterinary Biotechnology, Guangxi Veterinary Research Institute, 51 North Road You Ai, Nanning, Guangxi, 530001, China; Gao Y., School Of Computer Science And Electronic Engineering, University Of Surrey, Guildford, United Kingdom; Theobald J., Agsenze, Parc House, Kingston Upon Thames, London, United Kingdom; Bhatta D., Agsenze, Parc House, Kingston Upon Thames, London, United Kingdom; Stewart R., Dyson School Of Design Engineering, Imperial College London, London, United Kingdom; McElligott A.G., Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong, Centre For Animal Health And Welfare, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong","The annual global production of chickens exceeds 25 billion birds, which are often housed in very large groups, numbering thousands. Distress calling triggered by various sources of stress has been suggested as an 'iceberg indicator' of chicken welfare. However, to date, the identification of distress calls largely relies on manual annotation, which is very labour-intensive and time-consuming. Thus, a novel convolutional neural network-based model, light-VGG11, was developed to automatically identify chicken distress calls using recordings (3363 distress calls and 1973 natural barn sounds) collected on an intensive farm. The light-VGG11 was modified from VGG11 with significantly fewer parameters (9.3 million versus 128 million) and 55.88% faster detection speed while displaying comparable performance, i.e. precision (94.58%), recall (94.89%), F1-score (94.73%) and accuracy (95.07%), therefore more useful for model deployment in practice. To additionally improve light-VGG11's performance, we investigated the impacts of different data augmentation techniques (i.e. time masking, frequency masking, mixed spectrograms of the same class and Gaussian noise) and found that they could improve distress calls detection by up to 1.52%. Our distress call detection demonstration on continuous audio recordings, shows the potential for developing technologies to monitor the output of this call type in large, commercial chicken flocks.  © 2022 The Author(s).","Animal welfare; Bioacoustics; Convolutional neural networks; Data augmentation; Precision livestock farming","Animals; Chickens; Deep Learning; Neural Networks, Computer; Noise; Vocalization, Animal; Agriculture; Animals; Convolution; Deep learning; Gaussian noise (electronic); Animal welfare; Automated identification; Convolutional neural network; Data augmentation; Distress call; Global production; Large groups; Learning models; Performance; Precision livestock farming; animal welfare; Article; audio recording; Bayesian learning; chicken; computer assisted diagnosis; convolutional neural network; cross validation; decision tree; deep learning; diagnostic accuracy; diagnostic test accuracy study; distress syndrome; feature learning (machine learning); gradient boosting; intermethod comparison; machine learning; masking; measurement accuracy; measurement precision; noise; nonhuman; poultry farming; prediction; preliminary data; random forest; residual neural network; short time Fourier transform; simultaneous masking; sound; support vector machine; time masking; vocalization; waveform; animal; vocalization; Convolutional neural networks","Biotechnology and Biological Sciences Research Council, BBSRC, (2016YFE01242200); Biotechnology and Biological Sciences Research Council, BBSRC; Innovate UK","The research was carried out as part of the LIVEQuest project supported by InnovateUK and BBSRC grant no. 2016YFE01242200. Acknowledgements ","K. Liu; Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; email: kailiu@cityu.edu.hk; A.G. Mcelligott; Department Of Infectious Diseases And Public Health, Jockey Club College Of Veterinary Medicine And Life Sciences, City University Of Hong Kong, Hong Kong, Hong Kong; email: alan.mcelligott@cityu.edu.hk","","Royal Society Publishing","17425689","","","35765806","English","J. R. Soc. Interface","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133106709"
"Qiao Y.; Guo Y.; He D.","Qiao, Yongliang (56486770900); Guo, Yangyang (57200132879); He, Dongjian (19933691800)","56486770900; 57200132879; 19933691800","Cattle body detection based on YOLOv5-ASFF for precision livestock farming","2023","Computers and Electronics in Agriculture","204","","107579","","","","53","10.1016/j.compag.2022.107579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144400659&doi=10.1016%2fj.compag.2022.107579&partnerID=40&md5=140ac1333f6654bdda64338fdb2b7634","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; School of internet, Anhui University, Anhui, Hefei, 230039, China; College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Guo Y., School of internet, Anhui University, Anhui, Hefei, 230039, China; He D., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Precision livestock farming is a hot topic in the field of agriculture at present. However, due to the diversity of breeding environments, the current intelligent monitoring of animal information still faces challenges. In this study, a YOLOv5-ASFF object detection model was proposed to detect cattle body parts (e.g. individual, head, legs) in complex scenes. The proposed YOLOv5-ASFF consists of two components: YOLOv5 responsible for extracting multi-scale features from sample images, while ASFF was used to adaptively learn fused spatial weights for each scale feature map and fully acquire the features. In this way, the cattle area detection was realized and the generalization of detection model was improved. To verify the applicability and robustness of YOLOv5-ASFF, a challenging dataset consisting of cattle (cow and beef) with complex environments (e.g. different lighting, occlusion, different depths of field, multiple targets and small targets) was constructed for experimental testing. The proposed method based on YOLOv5-ASFFachieved a precision of 96.2%, a recall of 92%, an F1 score of 94.1%, and an mAP@0.5 of 94.7% on this dataset, which outperformed Faster R-CNN, Cascade R-CNN, SSD, YOLOv3 and YOLOv5s. Experimental results showed that the YOLOv5-ASFF method could fully learn more animal biometric visual features, thereby improving the performance of cattle detection model, especially the detection of key parts. Overall, the proposed deep learning-based cattle detection method is favorable for long-term autonomous cattle monitoring and management in intelligent livestock farming. © 2022 Elsevier B.V.","Attention mechanism; Cattle; Intelligent monitoring; YOLOv5 model","Deep learning; Farms; Object detection; Statistical tests; 'current; Attention mechanisms; Breeding environments; Cattle; Detection models; Hot topics; Intelligent monitoring; Learn+; Precision livestock farming; YOLOv5 model; cattle; livestock farming; monitoring; Animals","National Natural Science Foundation of China, NSFC, (61473235); National Key Research and Development Program of China, NKRDPC, (2017YFD0701603)","The authors particular thanks to other members of the team for their involvement and efforts in the whole experiment organization and information collection. The authors also acknowledge the support by the project: National Natural Science Foundation of China (grant number 61473235) and the National Key Technology R&D Program of China (grant number 2017YFD0701603) .","Y. Guo; School of internet, Anhui University, Hefei, Anhui, 230039, China; email: guoyangyang113529@ahu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85144400659"
"Valente J.; Hiremath S.; Ariza-Sentís M.; Doldersum M.; Kooistra L.","Valente, João (42062501000); Hiremath, Santosh (57221341244); Ariza-Sentís, Mar (56964491300); Doldersum, Marty (57209345550); Kooistra, Lammert (6603894270)","42062501000; 57221341244; 56964491300; 57209345550; 6603894270","Mapping of Rumex obtusifolius in nature conservation areas using very high resolution UAV imagery and deep learning","2022","International Journal of Applied Earth Observation and Geoinformation","112","","102864","","","","15","10.1016/j.jag.2022.102864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132755643&doi=10.1016%2fj.jag.2022.102864&partnerID=40&md5=3c47c23dededb36bf400c34c11ec31ba","Wageningen University & Research, Information Technology Group, Hollandseweg 1, Wageningen, Netherlands; Aalto University School of Science, Espoo, Finland; Wageningen University & Research, Laboratory of Geo-information Science and Remote Sensing, Droevendaalsesteeg 3, Wageningen, Netherlands","Valente J., Wageningen University & Research, Information Technology Group, Hollandseweg 1, Wageningen, Netherlands; Hiremath S., Aalto University School of Science, Espoo, Finland; Ariza-Sentís M., Wageningen University & Research, Information Technology Group, Hollandseweg 1, Wageningen, Netherlands; Doldersum M., Wageningen University & Research, Laboratory of Geo-information Science and Remote Sensing, Droevendaalsesteeg 3, Wageningen, Netherlands; Kooistra L., Wageningen University & Research, Laboratory of Geo-information Science and Remote Sensing, Droevendaalsesteeg 3, Wageningen, Netherlands","Rumex obtusifolius (Rumex or broad leaved dock) is one of the most common weeds in grasslands. It spreads quickly, lowers the nutritional value of the grass, and is poisonous for livestock due to its oxalic acid content. Mapping it is important before any control treatment is applied. Current methods for mapping Rumex either involve manual work or the utilization of ground robots, which are not efficient in large fields. This study investigated the feasibility of using aerial images from unmanned aerial vehicles (UAV) and deep learning to map Rumex in grasslands. Seven pre-trained CNN models were tested using transfer learning on UAV images acquired at 10 m, 15 m, and 30 m height. Based on Cross Validation results, MobileNet performed the best in detecting Rumex, with an F1-Score of 78.36% and an AUROC of 93.74%, at 10 m height. At 15 m, the detection performance was relatively lower (F1-score = 72.00%, AUROC = 88.67%), but the results showed that the performance can increase with more data. Experiments also showed that Rumex detection was dependent on the flight height since the algorithm was unable to detect the plants at 30 m height. The code and the datasets used in this work were released in an open access repository to contribute to the advances in grassland management using UAV technology. © 2022 The Author(s)","Deep learning; Rumex; Transfer learning; UAV; Weed detection","aerial photography; feasibility study; grassland; imagery; machine learning; mapping; nature conservation; protected area; unmanned vehicle; weed","INTERREG Deutschland-Nederland; Naturschutzzentrum in Kreis Kleve; Academy of Finland, AKA, (315896); Aalto-Yliopisto, (316172)","This study was supported by the SPECTORS project (143081), which is funded by the European cooperation program INTERREG Deutschland-Nederland. It was also partly funded by the AIPSE programme of Academy of Finland through the AI-CropPro project; decision number 315896 (Aalto University, Finland) and 316172 (Luke, Finland). We also would like to acknowledge the computational resources provided by the Aalto Science-IT project and Corinna Roers from Naturschutzzentrum in Kreis Kleve e.V. for labelling the data.","J. Valente; Wageningen University & Research, Information Technology Group, Wageningen, Hollandseweg 1, Netherlands; email: joao.valente@wur.nl","","Elsevier B.V.","15698432","","","","English","Int. J. Appl. Earth Obs. Geoinformation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132755643"
"Pan Y.; Jin H.; Gao J.; Rauf H.T.","Pan, Yuanzhi (57963299800); Jin, Hua (37013421500); Gao, Jiechao (57215610933); Rauf, Hafiz Tayyab (57204077842)","57963299800; 37013421500; 57215610933; 57204077842","Identification of Buffalo Breeds Using Self-Activated-Based Improved Convolutional Neural Networks","2022","Agriculture (Switzerland)","12","9","1386","","","","10","10.3390/agriculture12091386","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141780140&doi=10.3390%2fagriculture12091386&partnerID=40&md5=0a191b97096de0adb2631bcd62011b31","Faculty of Business and Economics, The University of Hong Kong, 999077, Hong Kong; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, 200030, China; Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co., Ltd, Zhenjiang, 212050, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China; Department of Computer Science, University of Virginia, Charlottesville, 22903, VA, United States; Department of Electrical Engineering, Columbia University, New York City, 10027, NY, United States; Independent Researcher, Bradford BD8 0HS, United Kingdom","Pan Y., Faculty of Business and Economics, The University of Hong Kong, 999077, Hong Kong, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, 200030, China, Artificial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co., Ltd, Zhenjiang, 212050, China; Jin H., School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China; Gao J., Department of Computer Science, University of Virginia, Charlottesville, 22903, VA, United States, Department of Electrical Engineering, Columbia University, New York City, 10027, NY, United States; Rauf H.T., Independent Researcher, Bradford BD8 0HS, United Kingdom","The livestock of Pakistan includes different animal breeds utilized for milk farming and exporting worldwide. Buffalo have a high milk production rate, and Pakistan is the third-largest milk-producing country, and its production is increasing over time. Hence, it is essential to recognize the best Buffalo breed for a high milk- and meat yield to meet the world’s demands and breed production. Pakistan has the second-largest number of buffalos among countries worldwide, where the Neli-Ravi breed is the most common. The extensive demand for Neli and Ravi breeds resulted in the new cross-breed “Neli-Ravi” in the 1960s. Identifying and segregating the Neli-Ravi breed from other buffalo breeds is the most crucial concern for Pakistan’s dairy-production centers. Therefore, the automatic detection and classification of buffalo breeds are required. In this research, a computer-vision-based recognition framework is proposed to identify and classify the Neli-Ravi breed from other buffalo breeds. The proposed framework employs self-activated-based improved convolutional neural networks (CNN) combined with self-transfer learning. Moreover, feature maps extracted from CNN are further transferred to obtain rich feature vectors. Different machine learning (Ml) classifiers are adopted to classify the feature vectors. The proposed framework is evaluated on two buffalo breeds, namely, Neli-Ravi and Khundi, and one additional target class contains different buffalo breeds collectively called Mix. The proposed research achieves a maximum of 93% accuracy using SVM and more than 85% accuracy employing recent variants. © 2022 by the authors.","buffalo breeds; deep learning; Neural Networks; Self Activated CNN","","Technology Co., Ltd.; Zhenjiang Finance Bureau; National Natural Science Foundation of China, NSFC, (61902156); National Natural Science Foundation of China, NSFC; Jiangsu Science and Technology Department; Jiangsu Provincial Department of Finance; Science and Technology Bureau of Zhenjiang","This research was funded by the National Natural Science Foundation of China [61902156], Zhenjiang Science and Technology Bureau; Zhenjiang Finance Bureau high-tech enterprise storage cultivation funds; Jiangsu Science and Technology Department, Jiangsu Provincial Finance Department high-tech enterprise storage cultivation funds; and Zhenjiang Hongxiang Automation Financial support from Technology Co., Ltd.","Y. Pan; Faculty of Business and Economics, The University of Hong Kong, 999077, Hong Kong; email: yzpan@connect.hku.hk","","Multidisciplinary Digital Publishing Institute (MDPI)","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141780140"
"Illa P.K.; Ravuri S.; Vuppala R.; Megha Reddy B.","Illa, Pavan Kumar (57207917967); Ravuri, Shandeepa (58512648500); Vuppala, Rohit (58556372800); Megha Reddy, B. (58512409000)","57207917967; 58512648500; 58556372800; 58512409000","Deep Learning Methods for Plant Disease Identification using Leaf Images: A Survey","2023","International Conference on Sustainable Computing and Smart Systems, ICSCSS 2023 - Proceedings","","","","307","316","9","0","10.1109/ICSCSS57650.2023.10169260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166256922&doi=10.1109%2fICSCSS57650.2023.10169260&partnerID=40&md5=f5ab5d0595a59babca12a6805379803c","Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India","Illa P.K., Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India; Ravuri S., Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India; Vuppala R., Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India; Megha Reddy B., Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India","Food is a basic human need that is indispensable. The population of the world is increasing daily, making it crucial to produce sufficient food to sustain such a large populace. However, as time goes by, various plant diseases infect the crops, causing a negative impact on agricultural plant productivity. A country additionally needs to accomplish agricultural productivity of fundamental agricultural products to ensure the well-being of its residents because Many countries' economies are greatly reliant on the cultivation of crops and livestock for commercial purposes. One industry that significantly affects human existence and economic condition is agriculture. Agriculture-related items are lost because of poor management. Diseases are very injurious to the condition of the plant's overall health, which will have an influence on its growth. It is crucial to keep track of the development of agricultural plants to ensure the slightest loss. Plant disorders may be identified spontaneously, that can assist farmers manage their crops more efficiently and increase production. An imprecise diagnosis of plant diseases can lead to substantial losses in production, time, expenses, and product quality. To ensure successful cultivation, it is necessary to monitor the plant's condition at every stage of growth. Employing an automated technique for detecting plant diseases has several advantages, such as decreasing the amount of labor needed to oversee vast crop farms and recognizing disease indications at a very early stage, such as when it initially appear on plant leaves. Various Machine learning and Deep Learning techniques have been studied and presented in this research work. After tremendous survey it has been concluded in this study that deep learning algorithms are more efficient and suitable for implementation and detection of ailments present in various categories of plant leaves. The potential to come across and recognize things from images is made feasible by thorough advancements in the field of Deep Learning (DL) techniques.  © 2023 IEEE.","Convolutional Neural Network; Deep Learning; Plant Diseases; Plant Village Dataset","Agricultural products; Agricultural technology; Convolutional neural networks; Cultivation; Deep learning; Diagnosis; Learning algorithms; Learning systems; Plants (botany); Agricultural plants; Condition; Convolutional neural network; Deep learning; Leaf images; Learning methods; Learning techniques; Plant disease; Plant leaves; Plant village dataset; Crops","","","P.K. Illa; Vallurupalli Nageswara Rao Vignana Jyothi Institute of Engineering and Technology, Department of Information Technology, Hyderabad, India; email: illa.pavankumar@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835033360-2","","","English","Int. Conf. Sustain. Comput. Smart Syst., ICSCSS - Proc.","Conference paper","Final","","Scopus","2-s2.0-85166256922"
"Ko E.; Jeong K.; Oh H.; Park Y.; Choi J.; Lee E.","Ko, Eunyoung (57203058555); Jeong, Kyungchang (58164622300); Oh, Hongseok (58165068100); Park, Yunhwan (57226300531); Choi, Jungseok (56488733000); Lee, Euijong (57028745100)","57203058555; 58164622300; 58165068100; 57226300531; 56488733000; 57028745100","A deep learning-based framework for predicting pork preference","2023","Current Research in Food Science","6","","100495","","","","7","10.1016/j.crfs.2023.100495","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151297311&doi=10.1016%2fj.crfs.2023.100495&partnerID=40&md5=578c7c165e136b49cf7d73e05c1415e0","Dodram Pig Farmers Cooperative Company, Icheon, 17405, South Korea; Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Department of Animal Science, Chungbuk National University, Cheongju, 28644, South Korea","Ko E., Dodram Pig Farmers Cooperative Company, Icheon, 17405, South Korea; Jeong K., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Oh H., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Park Y., Department of Animal Science, Chungbuk National University, Cheongju, 28644, South Korea; Choi J., Department of Animal Science, Chungbuk National University, Cheongju, 28644, South Korea; Lee E., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea","Meat consumption per capita in South Korea has steadily increased over the last several years and is predicted to continue increasing. Up to 69.5% of Koreans eat pork at least once a week. Considering pork-related products produced and imported in Korea, Korean consumers have a high preference for high-fat parts, such as pork belly. Managing the high-fat portions of domestically produced and imported meat according to consumer needs has become a competitive factor. Therefore, this study presents a deep learning-based framework for predicting the flavor and appearance preference scores of the customers based on the characteristic information of pork using ultrasound equipment. The characteristic information is collected using ultrasound equipment (AutoFom III). Subsequently, according to the measured information, consumers’ preferences for flavor and appearance were directly investigated for a long period and predicted using a deep learning methodology. For the first time, we have applied a deep neural network-based ensemble technique to predict consumer preference scores according to the measured pork carcasses. To demonstrate the efficiency of the proposed framework, an empirical evaluation was conducted using a survey and data on pork belly preference. Experimental results indicate a strong relationship between the predicted preference scores and characteristics of pork belly. © 2023 The Authors","Artificial intelligence; Consumer preference prediction; Deep learning; Pork quality; Precision livestock farming (PLF)","agricultural worker; article; artificial intelligence; carcass; consumer; deep learning; deep neural network; flavor; human; livestock; nonhuman; pork; prediction; ultrasound","Dodram Quality Control Management Team of Dodram Pig Farmers Cooperative; Ministry of Agriculture, Food and Rural Affairs, MAFRA, (321028–5); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) through High Value-added Food Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) ( 321028–5 ). This work was supported by the Dodram Quality Control Management Team of Dodram Pig Farmers Cooperative.","E. Lee; Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; email: kongjjagae@cbnu.ac.kr; J. Choi; Department of Animal Science, Chungbuk National University, Cheongju, 28644, South Korea; email: jchoi@cbnu.ac.kr","","Elsevier B.V.","26659271","","","","English","Curr. Res.Food Sci.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85151297311"
"Pech-May F.; Sánchez-Hernández J.V.; López-Gómez L.A.; Magaña-Govea J.; Mil-Chontal E.M.","Pech-May, Fernando (35759140900); Sánchez-Hernández, Julio Víctor (58524531000); López-Gómez, Luis Antonio (58525859000); Magaña-Govea, Jorge (58525191500); Mil-Chontal, Edna Mariel (58525191600)","35759140900; 58524531000; 58525859000; 58525191500; 58525191600","Flooded Areas Detection through SAR Images and U-NET Deep Learning Model","2023","Computacion y Sistemas","27","2","","449","458","9","2","10.13053/CyS-27-2-4624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166908663&doi=10.13053%2fCyS-27-2-4624&partnerID=40&md5=451c6354751cc4bae4ea0a540708e241","Tecnológico Nacional de México, Campus de Los Ríos, Mexico","Pech-May F., Tecnológico Nacional de México, Campus de Los Ríos, Mexico; Sánchez-Hernández J.V., Tecnológico Nacional de México, Campus de Los Ríos, Mexico; López-Gómez L.A., Tecnológico Nacional de México, Campus de Los Ríos, Mexico; Magaña-Govea J., Tecnológico Nacional de México, Campus de Los Ríos, Mexico; Mil-Chontal E.M., Tecnológico Nacional de México, Campus de Los Ríos, Mexico","Floods are common in much of the world, this is due to different factors among which climate change and land use stand out. In Mexico they happen every year in different entities. Tabasco is an entity that is periodically flooded, causing losses and negative consequences for the rural, urban, livestock, agricultural and service industries. Consequently, it is necessary to create strategies to intervene effectively in the affected areas. Therefore, different strategies and techniques have been developed to mitigate the damage caused by this phenomenon. Satellite programs provide a large amount of data on the earth’s surface as well as geospatial information processing tools that are useful for environmental and forest monitoring, climate change impacts, risk analysis, natural disasters, among others. This paper presents a strategy for the classification of flooded areas using satellite images radar of synthetic aperture and the U-NET neural network. The study area is centered on Los Ríos, region of Tabasco, Mexico. The partial results show that U-NET performs well despite the limited amount in the training samples. As training data and epochs increased, its accuracy increased. © 2023 Instituto Politecnico Nacional. All rights reserved.","Deep learning and SAR; flood detection; sentinel-1 SAR","","","","F. Pech-May; Tecnológico Nacional de México, Campus de Los Ríos, Mexico; email: fernando.pech@cinvestav.mx","","Instituto Politecnico Nacional","14055546","","","","English","Comput. Sist.","Article","Final","","Scopus","2-s2.0-85166908663"
"Bouchekara H.R.E.H.; Sadiq B.O.; O Zakariyya S.; Sha’aban Y.A.; Shahriar M.S.; Isah M.M.","Bouchekara, Houssem R. E. H. (24066258400); Sadiq, Bashir O (57194234226); O Zakariyya, Sikiru (57202750022); Sha’aban, Yusuf A. (57220063346); Shahriar, Mohammad S. (56073493100); Isah, Musab M. (57188995108)","24066258400; 57194234226; 57202750022; 57220063346; 56073493100; 57188995108","SIFT-CNN Pipeline in Livestock Management: A Drone Image Stitching Algorithm","2023","Drones","7","1","17","","","","7","10.3390/drones7010017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146774244&doi=10.3390%2fdrones7010017&partnerID=40&md5=c3b1e844907f82c2184842220302b5df","Department of Electrical Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia; Department Electrical and Computer Engineering, Kampala International University, Kampala, 20000, Uganda; Department of Computer Engineering, Ahmadu Bello University, Zaria, 610001, Nigeria; Department of Electrical and Electronics Engineering, University of Ilorin, Ilorin, 240003, Nigeria; Department of Computer Science and Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia","Bouchekara H.R.E.H., Department of Electrical Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia; Sadiq B.O., Department Electrical and Computer Engineering, Kampala International University, Kampala, 20000, Uganda, Department of Computer Engineering, Ahmadu Bello University, Zaria, 610001, Nigeria; O Zakariyya S., Department of Electrical and Electronics Engineering, University of Ilorin, Ilorin, 240003, Nigeria; Sha’aban Y.A., Department of Electrical Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia, Department of Computer Engineering, Ahmadu Bello University, Zaria, 610001, Nigeria; Shahriar M.S., Department of Electrical Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia; Isah M.M., Department of Computer Science and Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia","Images taken by drones often must be preprocessed and stitched together due to the inherent noise, narrow imaging breadth, flying height, and angle of view. Conventional UAV feature-based image stitching techniques significantly rely on the quality of feature identification, made possible by image pixels, which frequently fail to stitch together images with few features or low resolution. Furthermore, later approaches were developed to eliminate the issues with conventional methods by using the deep learning-based stitching technique to collect the general attributes of remote sensing images before they were stitched. However, since the images have empty backgrounds classified as stitched points, it is challenging to distinguish livestock in a grazing area. Consequently, less information can be inferred from the surveillance data. This study provides a four-stage object-based image stitching technique that, before stitching, removes the background’s space and classifies images in the grazing field. In the first stage, the drone-based image sequence of the livestock on the grazing field is preprocessed. In the second stage, the images of the cattle on the grazing field are classified to eliminate the empty spaces or backgrounds. The third stage uses the improved SIFT to detect the feature points of the classified images to o8btain the feature point descriptor. Lastly, the stitching area is computed using the image projection transformation. © 2022 by the authors.","cattle monitoring; drone image processing; drones; image stitching algorithms; livestock management","Fertilizers; Cattle monitoring; Classifieds; Drone image processing; Flying heights; Image stitching; Image stitching algorithm; Images processing; Inherent noise; Livestock management; Stitching techniques; Image enhancement","ministry of education and University of Hafr Al Batin","This research work was funded by institutional fund projects under no (IFP-A-01-2-1-2022). Therefore, authors gratefully acknowledge technical and financial support from the ministry of education and University of Hafr Al Batin, Saudi Arabia.","H.R.E.H. Bouchekara; Department of Electrical Engineering, University of Hafr Al Batin, Hafr Al Batin, 31991, Saudi Arabia; email: rbouchekara@uhb.edu.sa","","Multidisciplinary Digital Publishing Institute (MDPI)","2504446X","","","","English","Drones","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146774244"
"Falque R.; Vidal-Calleja T.; Alempijevic A.","Falque, Raphael (57118257100); Vidal-Calleja, Teresa (6507021929); Alempijevic, Alen (8324650300)","57118257100; 6507021929; 8324650300","Semantic Keypoint Extraction for Scanned Animals using Multi-Depth-Camera Systems","2023","Proceedings - IEEE International Conference on Robotics and Automation","2023-May","","","11794","11801","7","2","10.1109/ICRA48891.2023.10160307","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168661466&doi=10.1109%2fICRA48891.2023.10160307&partnerID=40&md5=ef40f4c76996931f199aae7ddf64bd55","Robotics Institute, University of Technology, Sidney, Australia","Falque R., Robotics Institute, University of Technology, Sidney, Australia; Vidal-Calleja T., Robotics Institute, University of Technology, Sidney, Australia; Alempijevic A., Robotics Institute, University of Technology, Sidney, Australia","Keypoint annotation in pointclouds is an important task for 3D reconstruction, object tracking and alignment, in particular in deformable or moving scenes. In the context of agriculture robotics, it is a critical task for livestock automation to work toward condition assessment or behaviour recognition. In this work, we propose a novel approach for semantic keypoint annotation in pointclouds, by reformulating the keypoint extraction as a regression problem of the distance between the keypoints and the rest of the pointcloud. We use the distance on the pointcloud manifold mapped into a radial basis function (RBF), which is then learned using an encoder-decoder architecture. Special consideration is given to the data augmentation specific to multi-depth-camera systems by considering noise over the extrinsic calibration and camera frame dropout. Additionally, we investigate computationally efficient non-rigid deformation methods that can be applied to animal pointclouds. Our method is tested on data collected in the field, on moving beef cattle, with a calibrated system of multiple hardware-synchronised RGB-D cameras. © 2023 IEEE.","3D deep learning; keypoints annotation; livestock; multi-depth-camera systems","Agriculture; Animals; Behavioral research; Cameras; Computer vision; Deep learning; Radial basis function networks; Semantics; 3d deep learning; 3D reconstruction; Camera systems; Depth camera; Keypoint annotation; Keypoints; Livestock; Multi-depth-camera system; Object Tracking; Point-clouds; Extraction","Department of Agriculture and Water Resources, Australian Government, DAWR, (V.RDP.2005)","ACKNOWLEDGMENT This paper is supported by funding from the Australian Government Department of Agriculture and Water Resources as part of its Rural R&D for Profit program, MLA grant number V.RDP.2005.","R. Falque; Robotics Institute, University of Technology, Sidney, Australia; email: raphael.guenot-falque@uts.edu.au","","Institute of Electrical and Electronics Engineers Inc.","10504729","979-835032365-8","PIIAE","","English","Proc IEEE Int Conf Rob Autom","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85168661466"
"Gupta H.; Jindal P.; Verma O.P.; Arya R.K.; Ateya A.A.; Soliman N.F.; Mohan V.","Gupta, Himanshu (57221291542); Jindal, Parul (57976891400); Verma, Om Prakash (58100586900); Arya, Raj Kumar (56023616700); Ateya, Abdelhamied A. (56521370300); Soliman, Naglaa. F. (7003625335); Mohan, Vijay (56005389800)","57221291542; 57976891400; 58100586900; 56023616700; 56521370300; 7003625335; 56005389800","Computer Vision-Based Approach for Automatic Detection of Dairy Cow Breed","2022","Electronics (Switzerland)","11","22","3791","","","","10","10.3390/electronics11223791","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142447478&doi=10.3390%2felectronics11223791&partnerID=40&md5=f983e65f96ccd479787de002ef492f24","Department of Instrumentation and Control Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Department of Chemical Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Department of Electronics and Communications Engineering, Zagazig University, Zagazig, 44519, Egypt; Department of Information Technology, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, P.O. Box 84428, Riyadh, 11671, Saudi Arabia; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104, India","Gupta H., Department of Instrumentation and Control Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Jindal P., Department of Instrumentation and Control Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Verma O.P., Department of Instrumentation and Control Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Arya R.K., Department of Chemical Engineering, Dr. B. R. Ambedkar National Institute of Technology Jalandhar, Jalandhar, 144027, India; Ateya A.A., Department of Electronics and Communications Engineering, Zagazig University, Zagazig, 44519, Egypt; Soliman N.F., Department of Information Technology, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, P.O. Box 84428, Riyadh, 11671, Saudi Arabia; Mohan V., Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104, India","Purpose: Identification of individual cow breeds may offer various farming opportunities for disease detection, disease prevention and treatment, fertility and feeding, and welfare monitoring. However, due to the large population of cows with hundreds of breeds and almost identical visible appearance, their exact identification and detection become a tedious task. Therefore, the automatic detection of cow breeds would benefit the dairy industry. This study presents a computer-vision-based approach for identifying the breed of individual cattle. Methods: In this study, eight breeds of cows are considered to verify the classification process: Afrikaner, Brown Swiss, Gyr, Holstein Friesian, Limousin, Marchigiana, White Park, and Simmental cattle. A custom dataset is developed using web-mining techniques, comprising 1835 images grouped into 238, 223, 220, 212, 253, 185, 257, and 247 images for individual breeds. YOLOv4, a deep learning approach, is employed for breed classification and localization. The performance of the YOLOv4 algorithm is evaluated by training the model on different sets of training parameters. Results: Comprehensive analysis of the experimental results reveal that the proposed approach achieves an accuracy of 81.07%, with maximum kappa of 0.78 obtained at an image size of 608 × 608 and an intersection over union (IoU) threshold of 0.75 on the test dataset. Conclusions: The model performed better with YOLOv4 relative to other compared models. This places the proposed model among the top-ranked cow breed detection models. For future recommendations, it would be beneficial to incorporate simple tracking techniques between video frames to check the efficiency of this work. © 2022 by the authors.","automatic livestock farming; cow breed classification; deep learning; object detection; YOLOv4","","Abdulrahman University, (PNURSP2022R66); Princess Nourah Bint Abdulrahman University, PNU","Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2022R66), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.","N.F. Soliman; Department of Information Technology, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, Riyadh, P.O. Box 84428, 11671, Saudi Arabia; email: nfsoliman@pnu.edu.sa; V. Mohan; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104, India; email: vijay.mohan@manipal.edu","","MDPI","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142447478"
"Wang M.; Li W.; Zhang W.; Li G.; Dong K.; Zheng L.; He Y.; Chen Y.","Wang, Minjuan (56434879900); Li, Wanwan (58693154300); Zhang, Wenxin (57210605447); Li, Guixin (57576200300); Dong, Kun (58693154400); Zheng, Lihua (10139458500); He, Yifan (58692986300); Chen, Yifei (58693126800)","56434879900; 58693154300; 57210605447; 57576200300; 58693154400; 10139458500; 58692986300; 58693126800","Single Pig Pose Estimation Using Cross-stage Stacked Hourglass Network","2023","Proceedings of SPIE - The International Society for Optical Engineering","12709","","127090F","","","","0","10.1117/12.2685014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176498235&doi=10.1117%2f12.2685014&partnerID=40&md5=ea9abeebd5f86cbc7449b5f7e349d5f6","Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Institute of cosmetic regulatory science, Beijing Technology and Business University, Beijing, 100048, China","Wang M., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Li W., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Zhang W., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Li G., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Dong K., Institute of cosmetic regulatory science, Beijing Technology and Business University, Beijing, 100048, China; Zheng L., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; He Y., Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; Chen Y., Institute of cosmetic regulatory science, Beijing Technology and Business University, Beijing, 100048, China","Posture changes in pig behaviors are a helpful way of detecting early signs of precision livestock farming. In commercial settings, computer vision-based approaches have been widely used to obtain individual pig health and welfare information such as body condition score, live weight, and activity behaviors. For this, precisely estimating pig posture is a prerequisite, which is an important step in obtaining real-time pig information. In this paper, a cross-stage stacked hourglass network is conducted to solve pig posture estimation in a real feedlot environment. In addition, a cross-stage connection method that increases the flow of information to mitigate possible information loss is proposed, which obtains 2.1% improvement of PCKh than that of the original network. We trained and tested the proposed approach on a challenging pig dataset, which includes not only real pig images but also images from different websites. The results indicate that it is possible to recognize pig posture from images without any manual interference automatically, and it has great potential for application in the early detection of health and welfare challenges of commercial pigs. © 2023 SPIE.","Cross-stage connection; Deep learning; Pose estimation; Stacked hourglass network","Agriculture; Computer vision; Mammals; Activity behavior; Body condition score; Commercial settings; Cross-stage connection; Deep learning; Pig behavior; Pose-estimation; Precision livestock farming; Stacked hourglass network; Vision-based approaches; Deep learning","","","M. Wang; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, 100083, China; email: minjuan@cau.edu.cn","Wen F.; Zhao C.; Chen Y.","SPIE","0277786X","978-151066641-2","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85176498235"
"van Klompenburg T.; Kassahun A.","van Klompenburg, Thomas (57218658520); Kassahun, Ayalew (13410716700)","57218658520; 13410716700","Data-driven decision making in pig farming: A review of the literature","2022","Livestock Science","261","","104961","","","","14","10.1016/j.livsci.2022.104961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130346354&doi=10.1016%2fj.livsci.2022.104961&partnerID=40&md5=85695af53580edf44ec465c122a0d240","Information Technology Group, Wageningen University & Research, Wageningen, Netherlands","van Klompenburg T., Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; Kassahun A., Information Technology Group, Wageningen University & Research, Wageningen, Netherlands","Applications of data analytics, and recently machine learning, in pig farming have been investigated in literature and the results indicate great potential for data-driven decision support at various scales of the sector—from farm to the management of entire supply chains. However, there is insufficient overview of the studies conducted so far. Particularly, there is little insight into the extent of studies conducted in the context of actual business cases. In this study we conducted a systematic literature review to shed light on the state-of-the-art knowledge about data-driven decision making in the pig sector. In order to cover both classical data analysis techniques and machine learning, we used two separate search strings to search the literature. The results show that the various attributes of live pigs and slaughter data are used in analytics. Most studies focus on the occurrence and prevention of diseases, followed by DNA-related analysis and the effect of feeding strategies on growth. Among the studies we analysed, there was a large variation in herd size under study. Most studies used a selected group of pigs in an experimental environment; fewer studies used a larger number of pigs. Notably, all studies except two focussed on real-life business contexts where real-time data is used. The application of machine learning, mainly the use of random forest and neural network algorithms, took off since 2018. Current studies focus on isolated and one-off problems, and we suggest future research to consider the complexity encountered in real-life business circumstances and routine decision making through the integration of data analytics within farm information management systems. © 2022 The Author(s)","Data analysis; Data processing; Decision support; Machine learning; Pig farming","volatile organic compound; algorithm; animal lameness; artificial neural network; behavioral science; coccidiosis; convolutional neural network; data analysis; data processing; decision making; decision support system; decision tree; deep learning; DNA determination; electric conductivity; feeding; leg movement; livestock; machine learning; mastitis; nonhuman; pig; pig farming; principal component analysis; random forest; Review; slaughtering; support vector machine; systematic review","","","A. Kassahun; Information Technology Group, Wageningen University & Research, Wageningen, Netherlands; email: ayalew.kassahun@wur.nl","","Elsevier B.V.","18711413","","","","English","Livest. Sci.","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85130346354"
"Mekruksavanich S.; Jantawong P.; Tancharoen D.; Jitpattanakul A.","Mekruksavanich, Sakorn (35174894700); Jantawong, Ponnipa (57302885000); Tancharoen, Datchakorn (56635002100); Jitpattanakul, Anuchit (36185528200)","35174894700; 57302885000; 56635002100; 36185528200","Sensor-Based Cattle Behavior Classification Using Deep Learning Approaches","2023","2023 International Technical Conference on Circuits/Systems, Computers, and Communications, ITC-CSCC 2023","","","","","","","0","10.1109/ITC-CSCC58803.2023.10212958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169786823&doi=10.1109%2fITC-CSCC58803.2023.10212958&partnerID=40&md5=2c4e345c906c8fb554c34ecd0cbeca80","School of Information and Communication Technology, University of Phayao, Department of Computer Engineering, Phayao, Thailand; Panyapiwat Institute of Management, Faculty of Engineering and Technology, Nonthaburi, Thailand; King Mongkut's University of Technology North Bangkok, Intelligent and Nonlinear Dynamic Innovations Research Center, Faculty of Applied Science, Department of Mathematics, Bangkok, Thailand","Mekruksavanich S., School of Information and Communication Technology, University of Phayao, Department of Computer Engineering, Phayao, Thailand; Jantawong P., School of Information and Communication Technology, University of Phayao, Department of Computer Engineering, Phayao, Thailand; Tancharoen D., Panyapiwat Institute of Management, Faculty of Engineering and Technology, Nonthaburi, Thailand; Jitpattanakul A., King Mongkut's University of Technology North Bangkok, Intelligent and Nonlinear Dynamic Innovations Research Center, Faculty of Applied Science, Department of Mathematics, Bangkok, Thailand","The usage of precision livestock has grown due to the need for higher efficiency and productivity in response to the high demand for food. To ensure sustainable development and quality control of the inputs required by the industry, it is essential to monitor and classify the behavior of cattle. Sensor-based monitoring systems provide accurate information by capturing raw data and identifying behavior through machine learning and deep learning algorithms. This approach has allowed farmers to better understand the individual needs of their animals. This study presents a deep residual neural network for the classification of cattle behavior. The performance of the ResNeXt model was evaluated using a public real-world dataset collected from sensors attached to the neck of six different Japanese black beef cows. The experimental results showed that the presented ResNeXt model achieved the highest average accuracy of 94.96% and the highest average F1-score of 93.66%. Compared to other baseline deep learning models and the current state-of-the-art model for cattle behavior classification, the presented model outperformed them and achieved better performance. © 2023 IEEE.","animal activity recognition; cattle behavior classification; deep learning; livestock; wearable sensor","Agriculture; Animals; Behavioral research; Deep learning; Learning algorithms; Learning systems; Wearable sensors; Activity recognition; Animal activities; Animal activity recognition; Behaviour classification; Cattle behavior classification; Deep learning; High productivity; Learning approach; Livestock; Performance; Quality control","NSRF; National Science, Research and Innovation Fund; King Mongkut's University of Technology North Bangkok, KMUTNB, (KMUTNB-FF- 67-B-10); University of Phayao, UP, (FF66-UoE001); Thailand Science Research and Innovation, TSRI","This research project was supported by Thailand Science Research and Innovation Fund; University of Phayao (Grant No. FF66-UoE001); National Science, Research and Innovation Fund (NSRF); and King Mongkut s University of Technology North Bangkok with Contract no. KMUTNB-FF- 67-B-10.","A. Jitpattanakul; King Mongkut's University of Technology North Bangkok, Intelligent and Nonlinear Dynamic Innovations Research Center, Faculty of Applied Science, Department of Mathematics, Bangkok, Thailand; email: anuchit.j@sci.kmutnb.ac.th","","Institute of Electrical and Electronics Engineers Inc.","","979-835032641-3","","","English","Int. Tech. Conf. Circuits/Syst., Comput., Commun., ITC-CSCC","Conference paper","Final","","Scopus","2-s2.0-85169786823"
"Li J.; Green-Miller A.R.; Hu X.; Lucic A.; Mahesh Mohan M.R.; Dilger R.N.; Condotta I.C.F.S.; Aldridge B.; Hart J.M.; Ahuja N.","Li, Jiangong (57216664214); Green-Miller, Angela R. (57198352792); Hu, Xiaodan (57221044307); Lucic, Ana (56763962100); Mahesh Mohan, M.R. (57197314551); Dilger, Ryan N. (6602508522); Condotta, Isabella C.F.S. (57204022710); Aldridge, Brian (7004887050); Hart, John M. (56485125500); Ahuja, Narendra (35515078200)","57216664214; 57198352792; 57221044307; 56763962100; 57197314551; 6602508522; 57204022710; 7004887050; 56485125500; 35515078200","Barriers to computer vision applications in pig production facilities","2022","Computers and Electronics in Agriculture","200","","107227","","","","19","10.1016/j.compag.2022.107227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135145463&doi=10.1016%2fj.compag.2022.107227&partnerID=40&md5=d7f7a30ae217420def23138383fd5d8d","Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, 1304 W Pennsylvania Ave, Urbana, 61801, IL, United States; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, 61801, IL, United States; Department of Animal Sciences, University of Illinois at Urbana-Champaign, 1207 W. Gregory Dr., Urbana, 61801, IL, United States; Applied Research Institute, University of Illinois at Urbana-Champaign, 2100 S Oak St, Champaign, 61820, IL, United States; Department of Veterinary Clinical Medicine, College of Veterinary Medicine, University of Illinois at Urbana-Champaign, Urbana, 61802, IL, United States; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, 1308 W. Main St., Urbana, 61801, IL, United States; College of Animal Science and Technology, China Agricultural University, Beijing, China","Li J., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, 1304 W Pennsylvania Ave, Urbana, 61801, IL, United States, College of Animal Science and Technology, China Agricultural University, Beijing, China; Green-Miller A.R., Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, 1304 W Pennsylvania Ave, Urbana, 61801, IL, United States; Hu X., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, 61801, IL, United States; Lucic A., Applied Research Institute, University of Illinois at Urbana-Champaign, 2100 S Oak St, Champaign, 61820, IL, United States; Mahesh Mohan M.R., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, 61801, IL, United States; Dilger R.N., Department of Animal Sciences, University of Illinois at Urbana-Champaign, 1207 W. Gregory Dr., Urbana, 61801, IL, United States; Condotta I.C.F.S., Department of Animal Sciences, University of Illinois at Urbana-Champaign, 1207 W. Gregory Dr., Urbana, 61801, IL, United States; Aldridge B., Department of Veterinary Clinical Medicine, College of Veterinary Medicine, University of Illinois at Urbana-Champaign, Urbana, 61802, IL, United States; Hart J.M., Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, 1308 W. Main St., Urbana, 61801, IL, United States; Ahuja N., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, 306 N Wright St, Urbana, 61801, IL, United States","Surveillance and analysis of behavior can be used to detect and characterize health disruption and welfare status in animals. The accurate identification of changes in behavior is a time-consuming task for caretakers in large, commercial pig production systems and requires strong observational skills and a working knowledge of animal husbandry and livestock systems operations. In recent years, many studies have explored the use of various technologies and sensors to assist animal caretakers in monitoring animal activity and behavior. Of these technologies, computer vision offers the most consistent promise as an effective aid in animal care, and yet, a systematic review of the state of application of this technology indicates that there are many significant barriers to its widespread adoption and successful utilization in commercial production system settings. One of the most important of these barriers is the recognition of the sources of errors from objective behavior labeling that are not measurable by current algorithm performance evaluations. Additionally, there is a significant disconnect between the remarkable advances in computer vision research interests and the integration of advances and practical needs being instituted by scientific experts working in commercial animal production partnerships. This lack of synergy between experts in the computer vision and animal health and production sectors means that existing and emerging datasets tend to have a very particular focus that cannot be easily pivoted or extended for use in other contexts, resulting in a generality versus particularity conundrum. This goal of this paper is to help catalogue and consider the major obstacles and impediments to the effective use of computer vision associated technologies in the swine industry by offering a systematic analysis of computer vision applications specific to commercial pig management by reviewing and summarizing the following: (i) the purpose and associated challenges of computer vision applications in pig behavior analysis; (ii) the use of computer vision algorithms and datasets for pig husbandry and management tasks; (iii) the process of dataset construction for computer vision algorithm development. In this appraisal, we outline common difficulties and challenges associated with each of these themes and suggest possible solutions. Finally, we highlight the opportunities for future research in computer vision applications that can build upon existing knowledge of pig management by extending our capability to interpret pig behaviors and thereby overcome the current barriers to applying computer vision technologies to pig production systems. In conclusion, we believe productive collaboration between animal-based scientists and computer-based scientists may accelerate animal behavior studies and lead the computer vision technologies to commercial applications in pig production facilities. © 2022 Elsevier B.V.","Behavior; Computer vision; Dataset; Deep learning; Precision livestock farming; Swine","Agriculture; Deep learning; Mammals; Veterinary medicine; Animal behaviour; Behavior; Computer vision applications; Dataset; Deep learning; Pig production; Pig production systems; Precision livestock farming; Production facility; Swine; behavior; computer vision; data set; pig; precision agriculture; surveillance and enforcement; Computer vision","AFRI, (1024178, 2020-67021-32799); Agriculture and Food Research Initiative; National Institute of Food and Agriculture, NIFA","This work is supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.","J. Li; Department of Agricultural and Biological Engineering, University of Illinois at Urbana-Champaign, Urbana, 1304 W Pennsylvania Ave, 61801, United States; email: jli153@illinois.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85135145463"
"Muriga V.W.; Rich B.; Mauro F.; Sebastianelli A.; Ullo S.L.","Muriga, Veronica Wairimu (58078265400); Rich, Benjamin (58078774300); Mauro, Francesco (57221325220); Sebastianelli, Alessandro (57202958220); Ullo, Silvia Liberata (6503914067)","58078265400; 58078774300; 57221325220; 57202958220; 6503914067","A Machine Learning Approach to Long-Term Drought Prediction Using Normalized Difference Indices Computed on a Spatiotemporal Dataset","2023","International Geoscience and Remote Sensing Symposium (IGARSS)","2023-July","","","4927","4930","3","1","10.1109/IGARSS52108.2023.10282592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178345634&doi=10.1109%2fIGARSS52108.2023.10282592&partnerID=40&md5=8addbdc43ad969051ee558bf6e9c5c89","Massachusetts Institute of Technology, Boston, United States; University of Sannio, Engineering Department, Benevento, Italy; European Space Agency, φ-Lab, Frascati, Italy","Muriga V.W., Massachusetts Institute of Technology, Boston, United States; Rich B., Massachusetts Institute of Technology, Boston, United States; Mauro F., University of Sannio, Engineering Department, Benevento, Italy; Sebastianelli A., European Space Agency, φ-Lab, Frascati, Italy; Ullo S.L., University of Sannio, Engineering Department, Benevento, Italy","Climate change and increases in drought conditions affect the lives of many and are closely tied to global agricultural output and livestock production. This research presents a novel approach utilizing machine learning frameworks for drought prediction around water basins. Our method focuses on the next-frame prediction of the Normalized Difference Drought Index (NDDI) by leveraging the recently developed SEN2DWATER database. We propose and compare two prediction methods for estimating NDDI values over a specific land area. Our work makes possible proactive measures that can ensure adequate water access for drought-affected communities and sustainable agriculture practices by implementing a proof-of-concept of short and long-term drought prediction of changes in water resources. © 2023 IEEE.","Climate change; Deep Learning; Drought; Late/Early Stage Computation; Sentinel-2; Water Indices","","MIT s MISTI; University of Sannio; Massachusetts Institute of Technology, MIT","We would like to acknowledge MIT s MISTI research exchange program for making this scientific collaboration possible. The SEN2DWATER dataset was created by students at the University of Sannio, and the works presented here are contributions from Veronica Muriga and Benjamin Rich from MIT, and Francesco Mauro, Alessandro Sebastianelli, and Silvia Liberata Ullo from University of Sannio.","V.W. Muriga; Massachusetts Institute of Technology, Boston, United States; email: wmuriga@mit.edu","","Institute of Electrical and Electronics Engineers Inc.","","979-835032010-7","IGRSE","","English","Dig Int Geosci Remote Sens Symp (IGARSS)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85178345634"
"Galkin V.; Makarenko A.","Galkin, Vsevolod (57224356974); Makarenko, Andrey (48561467400)","57224356974; 48561467400","Methods and Algorithms for Intelligent Video Analytics in the Context of Solving Problems of Precision Pig Farming","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14389 LNCS","","","223","238","15","0","10.1007/978-3-031-49435-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182588197&doi=10.1007%2f978-3-031-49435-2_16&partnerID=40&md5=cadcb5416e30623eae77614a3dee7dcb","Institute of Control Sciences, Russian Academy of Sciences, ul. Profsoyuznaya 65, Moscow, 117977, Russian Federation","Galkin V., Institute of Control Sciences, Russian Academy of Sciences, ul. Profsoyuznaya 65, Moscow, 117977, Russian Federation; Makarenko A., Institute of Control Sciences, Russian Academy of Sciences, ul. Profsoyuznaya 65, Moscow, 117977, Russian Federation","The paper proposes an approach to developing a video data pipeline that addresses the basic tasks of precision pig farming. The pipeline performs both low-level tasks, such as data pre-processing, object detection, instance segmentation, tracking, object density estimation, etc., and high-level tasks, e.g. livestock counting, feeders and drinkers condition assessment, behavioral patterns analysis, estimation of livestock activity and weight, etc. The proposed solution is based on neural network algorithms and can be flexibly adjusted to specific conditions and tasks, including while sending emergency notifications. Furthermore, the system is architecturally capable of integrating additional sensor and input data. The approach is demonstrated by solving several problems in the fattening phase. The system has proven to have a number of competitive advantages, including stable operation in a high animal density environment. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Animal activity analysis; Animal weight estimation; Deep learning; Instance segmentation; Object density; Precision livestock farming; Video monitoring","Agriculture; Competition; Data handling; Deep learning; Mammals; Object detection; Activity analysis; Animal activities; Animal activity analyse; Animal weight estimation; Animal weights; Deep learning; Instance segmentation; Object density; Precision livestock farming; Video monitoring; Weights estimation; Pipelines","","","V. Galkin; Institute of Control Sciences, Russian Academy of Sciences, Moscow, ul. Profsoyuznaya 65, 117977, Russian Federation; email: galckin.vsevolod@gmail.com","Voevodin V.; Sobolev S.; Yakobovskiy M.; Shagaliev R.","Springer Science and Business Media Deutschland GmbH","03029743","978-303149434-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85182588197"
"Martha G.W.; Waweru Mwangi R.; Aramvith S.; Rimiru R.","Martha, Gichuki Wambui (58759140000); Waweru Mwangi, Ronald (57209738091); Aramvith, Supavadee (8943602000); Rimiru, Richard (55027374700)","58759140000; 57209738091; 8943602000; 55027374700","Comparing Deep Learning Object detection Methods for Real Time Cow Detection","2023","IEEE Region 10 Annual International Conference, Proceedings/TENCON","","","","1187","1192","5","1","10.1109/TENCON58879.2023.10322403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179501572&doi=10.1109%2fTENCON58879.2023.10322403&partnerID=40&md5=3dacf933428268858b6b9764918ee96f","Jomo Kenyatta University of Agriculture & Technology, Computing Department, School of Computing & Information Technology, Nairobi, Kenya; Chulalongkorn University, Department of Electrical Engineering, Faculty of Engineering, Bangkok, Thailand","Martha G.W., Jomo Kenyatta University of Agriculture & Technology, Computing Department, School of Computing & Information Technology, Nairobi, Kenya; Waweru Mwangi R., Jomo Kenyatta University of Agriculture & Technology, Computing Department, School of Computing & Information Technology, Nairobi, Kenya; Aramvith S., Chulalongkorn University, Department of Electrical Engineering, Faculty of Engineering, Bangkok, Thailand; Rimiru R., Jomo Kenyatta University of Agriculture & Technology, Computing Department, School of Computing & Information Technology, Nairobi, Kenya","Deep learning algorithms particularly Convolutional Neural Networks (CNNs) are the state-of-the-art techniques for object detection, classification, segmentation and behaviour classification. These algorithms have extensive application across various domains including agriculture. However, cow identification in dairy farming still relies on methods like direct visual monitoring which are time consuming, costly and inaccurate; or use of invasive contact devices such as sensors which can cause discomfort during attachment or removal. This research compared three deep learning object detection models i.e. YOLOv5, YOLOv7 and YOLOv8, which were selected based on their performance in object detection tasks. We generated cow images from videos captured from a housed dairy cattle barn. The dataset had 11,828 cow images, augmented to depict different illumination conditions and using makesense AI tool, we annotated the images in YOLO format, trained and validated the three models to visualize cow bounding boxes. Our approach demonstrates efficiency of the YOLOv8 model, achieving an accuracy of 94.7% and 93 % before and after data augmentation respectively. YOLOv8 baseline model was finetuned using the Ray Tune library achieving a mAP@O.50 score of 92. 9%. This research makes a significant contribution in the future research direction of the YOLO algorithm and highlights the practical implementation of deep learning models for cow detection, applicable in livestock management.  © 2023 IEEE.","Convolutional Neural Networks; Object Detection; YOLO","Agricultural robots; Convolution; Convolutional neural networks; Deep learning; Farms; Learning algorithms; Learning systems; Object recognition; Behaviour classification; Convolutional neural network; Learning objects; Object behavior; Object classification; Object detection method; Objects detection; Real- time; State-of-the-art techniques; YOLO; Object detection","Japan International Cooperation Agency, JICA","This work is supported by JICA Project for AUN/SEED-Net CR-X project titled AI-Based Video Analytics for Smart Farming.","","","Institute of Electrical and Electronics Engineers Inc.","21593442","979-835030219-6","85QXA","","English","IEEE Reg 10 Annu Int Conf Proc TENCON","Conference paper","Final","","Scopus","2-s2.0-85179501572"
"Feng W.; Wang K.; Zhou S.","Feng, Wei (58498972600); Wang, Kaining (58497611900); Zhou, Shangbo (7404166053)","58498972600; 58497611900; 7404166053","An Efficient Neural Network for Pig Counting and Localization by Density Map Estimation","2023","IEEE Access","11","","","81079","81091","12","5","10.1109/ACCESS.2023.3297141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165367027&doi=10.1109%2fACCESS.2023.3297141&partnerID=40&md5=20805e7cfc2d3bc35fe18aad36d15525","Chongqing University, College of Computer Science, Chongqing, 400044, China; SimpleCredit Micro-Lending Company Ltd., Chongqing, 401147, China","Feng W., Chongqing University, College of Computer Science, Chongqing, 400044, China, SimpleCredit Micro-Lending Company Ltd., Chongqing, 401147, China; Wang K., SimpleCredit Micro-Lending Company Ltd., Chongqing, 401147, China; Zhou S., Chongqing University, College of Computer Science, Chongqing, 400044, China","Automatic pig counting and locating from camera images is one of the most important tasks in modern pig farming industry, which helps farmers to improve the efficiency of the livestock management in pig feeding, welfare estimation, unexpected events monitoring and etc. Due to the complex and diverse pigpen environment, complicated distributions of pig population and various motions of live pigs, traditional image processing techniques are not effective in counting and locating pigs in crowds. Thus, this task relies on manual labor heavily which is time-consuming and error-prone. In this paper, we propose an efficient and accurate pig counting method for top-view surveillance images in large-scale, crowded feeding scenes. The proposed method is composed of a novel density map generator and a density map estimation network architecture. The pigs in images are expressed as ellipses and their group density is generated with elliptical 2D Gaussian distribution. The proposed network is designed with efficient hybrid blocks including selective kernel convolution and vision transformer for feature extraction and density map regression. The total number of pigs in one image can be calculated by summing entire values in the density map. We also apply a modified K-means clustering algorithm on the density map to locate pig targets. To verify the effectiveness and precision of the proposed method, we evaluate our proposed method on our testing dataset. The Mean Absolute Error of counting numbers on testing images is 0.726. Due to lightweight design by using depth-wise separable convolutions and hybrid-vit blocks, our proposed method has very fast inference speed and will reduce dependency on computing resources substantially when deployed in pig farms. Based on the accurate estimated density maps of the testing images, pig locating can also achieve pretty good results. The modified K-means clustering algorithm proposed in this paper obtain target locating with 88.22% precision and 86.02% recall respectively. These results indicate our proposed method can accurately count pigs in piggery by density map estimation and locate pig targets even in crowded situations. © 2013 IEEE.","density map estimation; Pig counting; pig locating; selective kernel convolution; vision transformer","Agriculture; Clustering algorithms; Computer vision; Convolution; Deep learning; Extraction; Gaussian distribution; Image enhancement; Location; Mammals; Network architecture; Statistical tests; Animal behaviour; Deep learning; Density map estimation; Density maps; Density measurement; Farming; Features extraction; Image captures; Kernel; Kernel convolution; MAP estimation; Pig counting; Pig locating; Selective kernel convolution; Transformer; Vision transformer; Feature extraction","","","S. Zhou; Chongqing University, College of Computer Science, Chongqing, 400044, China; email: shbzhou@cqu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165367027"
"Qiao Y.; Guo Y.; He D.","Qiao, Yongliang (56486770900); Guo, Yangyang (57200132879); He, Dongjian (19933691800)","56486770900; 57200132879; 19933691800","Deep Learning-Based Autonomous Cow Detection for Smart Livestock Farming","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13744 LNCS","","","246","258","12","0","10.1007/978-3-031-26118-3_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151049588&doi=10.1007%2f978-3-031-26118-3_19&partnerID=40&md5=e47393f8e0af39d9e2c3e10d68f9ed3b","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; School of Internet, Anhui University, Anhui, Hefei, 230039, China; College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Guo Y., School of Internet, Anhui University, Anhui, Hefei, 230039, China; He D., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China","Animal sourced protein is increasing rapidly due to the growing of population and incomes. The big data, robots and smart sensing technologies have brought the autonomous robotic system to the smart farming that enhance productivity and efficiency. Therefore, a YOLOv4-SAM was proposed to achieve high detection precision of cow body parts in long-term complex scenes. The proposed YOLOv4-SAM consists of two components: YOLOv4 is for multi-scale feature extraction, while the Spatial Attention Mechanisms (SAM) highlights the key cow biometric-related features. By doing this, visual biometric feature representation ability is enhanced for improving cow detection performance. To verify the performance of YOLOv4-SAM, a challenging dataset consisting of adult cows and calves with complex environments (e.g., day and night, occlusion, multiple target) was constructed for experimental testing. The precision, recall, mIoU and of the proposed YOLOv4-CBAM were 92.29%, 96.51%, 77.22% and 93.13%, respectively. The data shows that its overall performance was better than that of the comparison algorithm (Faster R-CNN, RetinaNet and YOLOv4). In addition, object detection based on the YOLOv4-SAM model can capture the key biometric-related features for cow visual representation and improve the performance of cow detection. In addition, the detected height difference between head and leg proved the capability in automatic identification of lame cows. The proposed deep learning-based cow detection approach provides a basis for developing an automated system for animal monitoring and management on commercial dairy farms. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Attention mechanism; Autonomous detection; Deep learning; Precision livestock farming; YOLOv4","Agricultural robots; Biometrics; Deep learning; Farms; Feature extraction; Object detection; Statistical tests; Attention mechanisms; Autonomous detection; Data sensing; Deep learning; Livestock farming; Performance; Precision livestock farming; Robot sensing; Spatial attention; YOLOv4; Animals","","","Y. Guo; School of Internet, Anhui University, Hefei, Anhui, 230039, China; email: guoyangyang113529@ahu.edu.cn","Yu C.; Zhou J.; Song X.; Lu Z.","Springer Science and Business Media Deutschland GmbH","03029743","978-303126117-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85151049588"
"Siriani A.L.R.; Kodaira V.; Mehdizadeh S.A.; de Alencar Nääs I.; de Moura D.J.; Pereira D.F.","Siriani, Allan Lincoln Rodrigues (57192177832); Kodaira, Vanessa (57016827100); Mehdizadeh, Saman Abdanan (55696253900); de Alencar Nääs, Irenilza (57211115618); de Moura, Daniella Jorge (22133963900); Pereira, Danilo Florentino (56187746200)","57192177832; 57016827100; 55696253900; 57211115618; 22133963900; 56187746200","Detection and tracking of chickens in low-light images using YOLO network and Kalman filter","2022","Neural Computing and Applications","34","24","","21987","21997","10","30","10.1007/s00521-022-07664-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136810255&doi=10.1007%2fs00521-022-07664-w&partnerID=40&md5=26441caa7b1c86abbc51584f6872224e","Graduate Program in Agribusiness and Development, School of Sciences and Engineering, São Paulo State University, SP, Tupã, Brazil; Graduate Program in Agricultural Engineering, School of Agricultural Engineering, State University of Campinas, SP, Campinas, Brazil; Department of Mechanics of Biosystems Engineering, Faculty of Agricultural Engineering, Agricultural Sciences and Natural Resources University of Khuzestan, Ahvaz, Iran; Graduate Program in Production Engineering, Universidade Paulista, SP, São Paulo, Brazil; School of Agricultural Engineering, Campinas State University, SP, Campinas, Brazil; Department of Management, Development and Technology, School of Sciences and Engineering, São Paulo State University, SP, Tupã, Brazil","Siriani A.L.R., Graduate Program in Agribusiness and Development, School of Sciences and Engineering, São Paulo State University, SP, Tupã, Brazil; Kodaira V., Graduate Program in Agricultural Engineering, School of Agricultural Engineering, State University of Campinas, SP, Campinas, Brazil; Mehdizadeh S.A., Department of Mechanics of Biosystems Engineering, Faculty of Agricultural Engineering, Agricultural Sciences and Natural Resources University of Khuzestan, Ahvaz, Iran; de Alencar Nääs I., Graduate Program in Production Engineering, Universidade Paulista, SP, São Paulo, Brazil; de Moura D.J., School of Agricultural Engineering, Campinas State University, SP, Campinas, Brazil; Pereira D.F., Department of Management, Development and Technology, School of Sciences and Engineering, São Paulo State University, SP, Tupã, Brazil","Continuous monitoring of chickens’ movement on-farm is a challenge. The present study aimed to associate the modified YOLO v4 model with a bird tracking algorithm based on a Kalman filter to identify a chicken’s movement using low-resolution video. The videos were captured in grayscale using a top-view camera with a low resolution of 702 × 480 pixels, preventing the application of usual image processing techniques. We used YOLO to extract the characteristics of the image and classification automatically. A dataset with images of tagged chickens was used to detect chickens, being 1000 frames tagged in different videos. The generated model was applied in a video that returned the bounding box of the location of the chicken in the frame. With the limits of the box, the centroid was calculated and exported in a CSV file for tracking processing. The Kalman filter was implemented to track chickens in low light intensity. Results indicated that YOLO presented a 99.9% accuracy in detecting chickens in low-quality videos. Using the Kalman filter, the algorithm tracks the chickens and gives them a particular identification number until they leave the compartment. Furthermore, each moving chicken is located in different colors along with the maps below the image, making chicken detection more convenient. The tracking results of chickens show that the proposed method can correctly handle the new entry and exit moving targets in crowded conditions. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Convolutional neural network; Deep learning; Precision livestock farming; YOLO v4","Agriculture; Animals; Cameras; Convolutional neural networks; Deep learning; Bird tracking; Continuous monitoring; Convolutional neural network; Deep learning; Detection and tracking; Low resolution video; Low-light images; Precision livestock farming; Tracking algorithm; YOLO v4; Kalman filters","Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (304085/2021-9, 308177/2021-7)","Funding was provided by National Council for Scientific and Technological Development - CNPq (Grant # 304085/2021-9 and # 308177/2021-7). ","D.F. Pereira; Department of Management, Development and Technology, School of Sciences and Engineering, São Paulo State University, Tupã, SP, Brazil; email: danilo.florentino@unesp.br","","Springer Science and Business Media Deutschland GmbH","09410643","","","","English","Neural Comput. Appl.","Article","Final","","Scopus","2-s2.0-85136810255"
"Zhang H.; Sun Y.; Zhao C.; Wang B.; Li B.; Wang B.","Zhang, Hongming (55685512300); Sun, Yang (57712913300); Zhao, Chunping (55476848100); Wang, Bowen (57203341879); Li, Bin (58259110000); Wang, Bingke (57559700700)","55685512300; 57712913300; 55476848100; 57203341879; 58259110000; 57559700700","Review on Typical Behavior Monitoring and Physiological Condition Identification Methods for Ruminant Livestock; [反刍家畜典型行为监测与生理状况识别方法研究综述]","2023","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","54","3","","1","21","20","13","10.6041/j.issn.1000-1298.2023.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159622549&doi=10.6041%2fj.issn.1000-1298.2023.03.001&partnerID=40&md5=35965c12fa702945bf0fda531b2ca4d3","College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Research Center of Shaanxi Agricultural Information Intelligent Perception and Analysis Engineering Technology, Shaanxi, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; College of Animal Science and Technology, Northwest A&F University, Shaanxi, Yangling, 712100, China; College of Economics and Management, Northwest A&F University, Shaanxi, Yangling, 712100, China; Research Center of Beijing Academy of Agriculture and Forestry Sciences Intelligent Equipment Technology, Beijing, 100097, China; Yangling Nongfu Agriculture and Animal Husbandry Technology Co., Ltd., Shaanxi, Yangling, 712100, China","Zhang H., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Research Center of Shaanxi Agricultural Information Intelligent Perception and Analysis Engineering Technology, Shaanxi, Yangling, 712100, China; Sun Y., College of Information Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Shaanxi, Yangling, 712100, China; Zhao C., College of Animal Science and Technology, Northwest A&F University, Shaanxi, Yangling, 712100, China; Wang B., College of Economics and Management, Northwest A&F University, Shaanxi, Yangling, 712100, China; Li B., Research Center of Beijing Academy of Agriculture and Forestry Sciences Intelligent Equipment Technology, Beijing, 100097, China; Wang B., Yangling Nongfu Agriculture and Animal Husbandry Technology Co., Ltd., Shaanxi, Yangling, 712100, China","Ruminant livestock is an important source of meat, milk and other food for human beings. With the improvement of people's requirements for the output and quality of ruminant livestock products, the traditional manual supervision mode, which is time-consuming, labor-intensive and high labor cost, has been difficult to meet the needs of large-scale ruminant livestock breeding. Ruminant livestock behavior contains a lot of body condition information. The intelligent monitoring of ruminant livestock behavior is helpful to identify abnormal behavior of ruminant livestock earlier, evaluate the health level of ruminant livestock, early warning of abnormal physiological state of ruminant livestock, and assist farmers to adjust breeding strategies in a timely manner to achieve low cost-effective, efficient and profitable production process. Firstly, the monitoring methods for basic movements (lying, walking and standing), rumination, eating and drinking, lameness of ruminant livestock were overally described. Secondly, the different characteristic indicators to identify the condition of ruminant livestock in estrus, parturition, disease and pain were analyzed in detail and the physiological condition identification method was introduced based on the characteristic indicators. Thirdly, the problems and challenges of ruminant livestock behavior monitoring methods were summarized. Finally, the future development directions of relevant key technologies were prospected, including optimizing sensor power consumption, fusion of multi-sensor data, reducing data transmission delay, reducing large-scale data annotation, lightweight deep learning models and deep analysis and application data. © 2023 Chinese Society of Agricultural Machinery. All rights reserved.","behavior monitoring; physiological condition characteristics; physiological condition identification; ruminant livestock","Agriculture; Cost effectiveness; Physiology; Sensor data fusion; Wages; Behaviour monitoring; Condition identification; Human being; Identification method; Monitoring methods; Physiological condition; Physiological condition characteristic; Physiological condition identification; Ruminant livestock; Deep learning","","","","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85159622549"
"Ali AlZubi A.; Al-Zu’bi M.","Ali AlZubi, Ahmad (59338287500); Al-Zu’bi, Maha (57189997147)","59338287500; 57189997147","Application of Artificial Intelligence in Monitoring of Animal Health and Welfare","2023","Indian Journal of Animal Research","57","11","","1550","1555","5","6","10.18805/IJAR.BF-1698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178338018&doi=10.18805%2fIJAR.BF-1698&partnerID=40&md5=f3a60e86c43ceb8649f43b9dd9ac0035","Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia; Department of Planning and Landscape, School of Architecture, University of Calgary, Calgary, AB, Canada","Ali AlZubi A., Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia; Al-Zu’bi M., Department of Planning and Landscape, School of Architecture, University of Calgary, Calgary, AB, Canada","Background: With the advent of AI technology, great strides have been made in the realm of animal healthcare. This article delves into the numerous uses of AI in veterinary medicine and demonstrates its revolutionary potential in the field. Artificial intelligence (AI) algorithms have shown impressive skills in illness detection, using medical imaging analysis to help veterinarians discover and categorise diseases with greater accuracy and efficiency. Furthermore, predictive analytics algorithms use various data sources, such as electronic health records and genetic profiles, to recognise trends and forecast illness outbreaks, allowing veterinarians to remotely monitor vital signs and act swiftly paving the way for preventative measures and individualised treatment. Methods: The purpose of this article is to offer a synopsis of the many ways in which artificial intelligence (AI) is being used to improve the health and well-being of animals. Understanding the effects of AI in animal healthcare and setting the stage for its further development will be accomplished via an examination of the present state of the subject. Result: It is evident that through mountains of data from studies and clinical trials, AI is helping to speed up the discovery of novel treatments and improve the understanding of animal health. A responsible and useful application of AI in animal healthcare requires the establishment of ethical concerns, data protection and regulatory frameworks. © 2023 Agricultural Research Communication Centre. All rights reserved.","Animal healthcare; Artificial intelligence (AI); Precision livestock; Remote monitoring","accuracy; algorithm; animal health; animal welfare; Article; artificial intelligence; computer vision; convolutional neural network; data protection; deep learning; early diagnosis; feature extraction; machine learning; monitoring; nonhuman; prediction accuracy; recurrent neural network; supervised machine learning; training; unsupervised machine learning; wellbeing","King Saud University, KSU, (RSP2023R395); King Saud University, KSU","The authors would like to thank the editors and reviewers for their review and recommendations and also to extend their thanks to King Saud University for funding this work through the Researchers Supporting Project (RSP2023R395), King Saud University, Riyadh, Saudi Arabia.","A. Ali AlZubi; Department of Computer Science, Community College, King Saud University, Riyadh, Saudi Arabia; email: aalzubi@ksu.edu.sa","","Agricultural Research Communication Centre","03676722","","","","English","Ind. J. Ani. Res","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85178338018"
"Schauer M.; Hohl R.; Vaupel D.; Bienhaus D.; Ghobadi S.E.","Schauer, Moritz (57871694000); Hohl, Renke (58834498200); Vaupel, Dennis (58836158100); Bienhaus, Diethelm (55533342600); Ghobadi, Seyed Eghbal (23466639200)","57871694000; 58834498200; 58836158100; 55533342600; 23466639200","Towards Automated Regulation of Jacobaea Vulgaris in Grassland using Deep Neural Networks","2023","Proceedings - 2023 IEEE/CVF International Conference on Computer Vision Workshops, ICCVW 2023","","","","702","711","9","0","10.1109/ICCVW60793.2023.00078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182929330&doi=10.1109%2fICCVW60793.2023.00078&partnerID=40&md5=5134484454f487ce57135ab62b552944","University of Applied Sciences Mittelhessen, Germany","Schauer M., University of Applied Sciences Mittelhessen, Germany; Hohl R., University of Applied Sciences Mittelhessen, Germany; Vaupel D., University of Applied Sciences Mittelhessen, Germany; Bienhaus D., University of Applied Sciences Mittelhessen, Germany; Ghobadi S.E., University of Applied Sciences Mittelhessen, Germany","The highly poisonous ragwort (Jacobaea Vulgaris) is increasingly spreading, posing significant risks to agriculture, livestock, and nature conservation due to the production of toxic pyrrolizidine alkaloids (PAs). The current manual control methods, such as plucking weed, are labor-intensive and time-consuming. This paper introduces a workflow towards automated regulation of J. Vulgaris, which consists of the two independent tasks of deep learning-based monitoring and controlling. We aim to detect and control J. Vulgaris in an early growth stage before the plant can reseed, which challenges the data collection and the training of deep neural networks. Primarily we need to detect the green leaf rosettes on a green meadow. The main focus lies on the monitoring part with synthetic training data generation and a deep neural network-based labeling assistant. © 2023 IEEE.","Agriculture; Annotations; Computer Vision; Deep Learning; Jacobaea Vulgaris; Ragwort; Robotics; Weed Detection","Agricultural robots; Conservation; Deep neural networks; Weed control; 'current; Annotation; Control methods; Deep learning; Jacobaeum vulgari; Labor time; Pyrrolizidine alkaloid; Ragwort; Vulgaris; Weed detection; Computer vision","University of Applied Sciences Mittelhessen","This work’s writing was partly enabled from within the context of the project “KIhUG - AI assisted highly automated weed control in grasslands”, funded by the strategic research fund of the University of Applied Sciences Mittelhessen.","","","Institute of Electrical and Electronics Engineers Inc.","","979-835030744-3","","","English","Proc. - IEEE/CVF Int. Conf. Comput. Vis. Workshops, ICCVW","Conference paper","Final","","Scopus","2-s2.0-85182929330"
"Gong S.-H.; Jung H.-S.; Lee M.-J.; Lee K.-J.; Oh K.-Y.; Chang J.-Y.","Gong, Sung-Hyun (58097386500); Jung, Hyung-Sup (36124919500); Lee, Moung-Jin (7409120842); Lee, Kwang-Jae (56101125400); Oh, Kwan-Young (55635856400); Chang, Jae-Young (57218994889)","58097386500; 36124919500; 7409120842; 56101125400; 55635856400; 57218994889","Semantic Segmentation of Hazardous Facilities in Rural Area Using U-Net from KOMPSAT Ortho Mosaic Imagery","2023","Korean Journal of Remote Sensing","39","6-3","","1693","1705","12","4","10.7780/kjrs.2023.39.6.3.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182873687&doi=10.7780%2fkjrs.2023.39.6.3.3&partnerID=40&md5=87563afaf80e3d8f3e4372fc663faf5b","Department of Geoinformatics, University of Seoul, Seoul, South Korea; Department of Smart Cities, University of Seoul, Seoul, South Korea; Department of Earth Sciences, Southern Methodist University, Dallas, TX, United States; Division for Environmental Planning, Water and Land Research Group, Korea Environment Institute, Sejong, South Korea; Satellite Application Division, Korea Aerospace Research Institute, Daejeon, South Korea","Gong S.-H., Department of Geoinformatics, University of Seoul, Seoul, South Korea, Department of Smart Cities, University of Seoul, Seoul, South Korea; Jung H.-S., Department of Geoinformatics, University of Seoul, Seoul, South Korea, Department of Smart Cities, University of Seoul, Seoul, South Korea, Department of Earth Sciences, Southern Methodist University, Dallas, TX, United States; Lee M.-J., Division for Environmental Planning, Water and Land Research Group, Korea Environment Institute, Sejong, South Korea; Lee K.-J., Satellite Application Division, Korea Aerospace Research Institute, Daejeon, South Korea; Oh K.-Y., Satellite Application Division, Korea Aerospace Research Institute, Daejeon, South Korea; Chang J.-Y., Satellite Application Division, Korea Aerospace Research Institute, Daejeon, South Korea","Rural areas, which account for about 90% of the country’s land area, are increasing in importance and value as a space that performs various public functions. However, facilities that adversely affect residents’ lives, such as livestock facilities, factories, and solar panels, are being built indiscriminately near residential areas, damaging the rural environment and landscape and lowering the quality of residents’ lives. In order to prevent disorderly development in rural areas and manage rural space in a planned manner, detection and monitoring of hazardous facilities in rural areas is necessary. Data can be acquired through satellite imagery, which can be acquired periodically and provide information on the entire region. Effective detection is possible by utilizing image-based deep learning techniques using convolutional neural networks. Therefore, U-Net model, which shows high performance in semantic segmentation, was used to classify potentially hazardous facilities in rural areas. In this study, KOMPSAT ortho-mosaic optical imagery provided by the Korea Aerospace Research Institute in 2020 with a spatial resolution of 0.7 meters was used, and AI training data for livestock facilities, factories, and solar panels were produced by hand for training and inference. After training with U-Net, pixel accuracy of 0.9739 and mean Intersection over Union (mIoU) of 0.7025 were achieved. The results of this study can be used for monitoring hazardous facilities in rural areas and are expected to be used as basis for rural planning. Copyright © 2023 by The Korean Society of Remote Sensing.","Convolutional nerual network; Deep learning; Rural; Semantic segmentation; U-Net","","","","H.-S. Jung; Department of Geoinformatics, University of Seoul, Seoul, South Korea; email: hsjung@uos.ac.kr","","Korean Society of Remote Sensing","12256161","","","","Korean","Kor. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85182873687"
"Badgujar C.; Flippo D.; Gunturu S.; Baldwin C.","Badgujar, Chetan (57546734700); Flippo, Daniel (56074894100); Gunturu, Sujith (58560217500); Baldwin, Carolyn (57213816273)","57546734700; 56074894100; 58560217500; 57213816273","Tree Trunk Detection of Eastern Red Cedar in Rangeland Environment with Deep Learning Technique","2023","Croatian Journal of Forest Engineering","44","2","","357","368","11","4","10.5552/crojfe.2023.2012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169468067&doi=10.5552%2fcrojfe.2023.2012&partnerID=40&md5=d5aac46b987d295838e13f5babac99ec","Kansas State University Biological and Agricultural Engineering, Seaton 1037A, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Kansas State University Biological and Agricultural Engineering, Seaton Hall 1033, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Kansas State University Computer Science, Seaton 1037A, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Kansas State University Agriculture, Natural Resources and Community Vitality, Umberger 103, 1612 Claflin, Manhattan, 66506, KS, United States","Badgujar C., Kansas State University Biological and Agricultural Engineering, Seaton 1037A, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Flippo D., Kansas State University Biological and Agricultural Engineering, Seaton Hall 1033, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Gunturu S., Kansas State University Computer Science, Seaton 1037A, 920 N. Martin Luther King Jr. Drive, Manhattan, 66506, KS, United States; Baldwin C., Kansas State University Agriculture, Natural Resources and Community Vitality, Umberger 103, 1612 Claflin, Manhattan, 66506, KS, United States","Uncontrolled spread of eastern red cedar invades the United States Great Plains prairie ecosystems and lowers biodiversity across native grasslands. The eastern red cedar (ERC) infes-tations cause significant challenges for ranchers and landowners, including the high costs of removing mature red cedars, reduced livestock forage feed, and reduced revenue from hunting leases. Therefore, a fleet of autonomous ground vehicles (AGV) is proposed to address the ERC infestation. However, detecting the target tree or trunk in a rangeland environment is critical in automating an ERC cutting operation. A tree trunk detection method was developed in this study for ERC trees trained in natural rangeland environments using a deep learning-based YOLOv5 model. An action camera acquired RGB images in a natural rangeland environment. A transfer learning method was adopted, and the YOLOv5 was trained to detect the varying size of the ERC tree trunk. A trained model precision, recall, and average precision were 87.8%, 84.3%, and 88.9%. The model accurately predicted the varying tree trunk sizes and differen-tiated between trunk and branches. This study demonstrated the potential for using pretrained deep learning models for tree trunk detection with RGB images. The developed machine vision system could be effectively integrated with a fleet of AGVs for ERC cutting. The proposed ERC tree trunk detection models would serve as a fundamental element for the AGV fleet, which would assist in effective rangeland management to maintain the ecological balance of grassland systems. © 2023 by the authors.","autonomous ground vehicle; eastern red cedar; invasive species; object detection; rangeland management; transfer learning; YOLO","Agriculture; Biodiversity; Cutting; Detection; Juniperus Virginiana; Trees; Great Plains; Agriculture; Autonomous vehicles; Computer vision; Deep learning; Fleet operations; Learning systems; Object detection; Autonomous ground vehicles; Eastern red cedars; High costs; Invasive species; Learning techniques; Objects detection; Rangeland management; RGB images; Transfer learning; YOLO; coniferous tree; detection method; grassland; hunting; invasive species; land management; machine learning; rangeland; unmanned vehicle; Biodiversity","","","C. Badgujar; Kansas State University Biological and Agricultural Engineering, Manhattan, Seaton 1037A, 920 N. Martin Luther King Jr. Drive, 66506, United States; email: chetan19@ksu.edu","","University of Zagreb, Faculty of Mining, Geology and Petroleum Engineering","18455719","","","","English","Croat. J. For. Eng.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85169468067"
"Ahmad M.; Zhang W.; Smith M.; Brilot B.; Bell M.","Ahmad, Misbah (57208081360); Zhang, Wenhao (57188662601); Smith, Melvyn (55495905800); Brilot, Ben (6506985377); Bell, Matt (15922235000)","57208081360; 57188662601; 55495905800; 6506985377; 15922235000","Real-Time Livestock Activity Monitoring via Fine-Tuned Faster R-CNN for Multiclass Cattle Behaviour Detection","2023","2023 IEEE 14th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2023","","","","805","811","6","2","10.1109/UEMCON59035.2023.10316066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179754385&doi=10.1109%2fUEMCON59035.2023.10316066&partnerID=40&md5=9e09ff4f6b0df153e0923db9b65283be","Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom","Ahmad M., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom, University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Zhang W., University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Smith M., University of the West of England, Centre for Machine Vision, Bristol Robotics Laboratory, Bristol, United Kingdom; Brilot B., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom; Bell M., Hartpury University, Animal and Agriculture Department, Gloucester, United Kingdom","Automated cattle activity detection plays a pivotal role in modern livestock management, significantly impacting animal welfare and operational efficiency. This paper introduces an automated approach for cattle activity detection using advanced deep learning-based architecture named Faster-RCNN. The deep learning model addresses the simultaneous detection and precise localization of three primary cattle activities in the input image: standing, lying, and walking. The methodology involves fine-tuning a pre-trained model using a dataset collected from a real-time barn environment at Hartpury University Farm. Overall, the proposed approach is based on data pre-processing and fine-tuning steps. Data augmentation techniques, such as random cropping, flipping, and rotation, ensure dataset diversity. This enriches the model's generalization ability across various lighting conditions and cattle orientations. The fine-tuning process adapts a pre-trained model, initially trained on a general object detection dataset. We adjust the model's architecture to the subtleties inherent in different cattle activities through training on our custom cattle activity dataset. This process ensures the model's significance in accurately detecting and classifying the distinct behaviours of cattle. Experimental results demonstrate the model's effectiveness in identifying and localizing cattle activities. The model correctly predicted of standing, lying, and walking events with accuracy rate of 0.94, 0.92 and 0.89 respectively. © 2023 IEEE.","Behaviour Detection; Cattle Activities; Computer Vision; Deep Learning; Image processing; Livestock Monitoring","Agriculture; Data handling; Deep learning; Object detection; Activity detection; Activity monitoring; Animal welfare; Behavior detection; Cattle activity; Deep learning; Fine tuning; Images processing; Livestock monitoring; Real- time; Computer vision","Hartpury University","ACKNOWLEDGMENT We gratefully acknowledge the support and funding provided by Hartpury University, UK, under project No.HK009687 for this research endeavour. Their commitment to advancing scientific knowledge and innovative solutions in livestock management has been instrumental in the successful execution of this work.","","Chakrabarti S.; Paul R.","Institute of Electrical and Electronics Engineers Inc.","","979-835030413-8","","","English","IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON","Conference paper","Final","","Scopus","2-s2.0-85179754385"
"Keshavamurthy R.; Dixon S.; Pazdernik K.T.; Charles L.E.","Keshavamurthy, Ravikiran (57207696583); Dixon, Samuel (57439324800); Pazdernik, Karl T. (56078896600); Charles, Lauren E. (35298645500)","57207696583; 57439324800; 56078896600; 35298645500","Predicting infectious disease for biopreparedness and response: A systematic review of machine learning and deep learning approaches","2022","One Health","15","","100439","","","","28","10.1016/j.onehlt.2022.100439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139311988&doi=10.1016%2fj.onehlt.2022.100439&partnerID=40&md5=d959998873da16e58beb45f1ef4db50f","Pacific Northwest National Laboratory, Richland, 99354, WA, United States; Paul G. Allen School for Global Health, Washington State University, Pullman, 99164, WA, United States; Department of Statistics, North Carolina State University, Raleigh, 27695, NC, United States","Keshavamurthy R., Pacific Northwest National Laboratory, Richland, 99354, WA, United States, Paul G. Allen School for Global Health, Washington State University, Pullman, 99164, WA, United States; Dixon S., Pacific Northwest National Laboratory, Richland, 99354, WA, United States; Pazdernik K.T., Pacific Northwest National Laboratory, Richland, 99354, WA, United States, Department of Statistics, North Carolina State University, Raleigh, 27695, NC, United States; Charles L.E., Pacific Northwest National Laboratory, Richland, 99354, WA, United States, Paul G. Allen School for Global Health, Washington State University, Pullman, 99164, WA, United States","The complex, unpredictable nature of pathogen occurrence has required substantial efforts to accurately predict infectious diseases (IDs). With rising popularity of Machine Learning (ML) and Deep Learning (DL) techniques combined with their unique ability to uncover connections between large amounts of diverse data, we conducted a PRISMA systematic review to investigate advances in ID prediction for human and animal diseases using ML and DL. This review included the type of IDs modeled, ML and DL techniques utilized, geographical distribution, prediction tasks performed, input features utilized, spatial and temporal scales, error metrics used, computational efficiency, uncertainty quantification, and missing data handling methods. Among 237 relevant articles published between January 2001 and May 2021, highly contagious diseases in humans were most often represented, including COVID-19 (37.1%), influenza/influenza-like illnesses (9.3%), dengue (8.9%), and malaria (5.1%). Out of 37 diseases identified, 51.4% were zoonotic, 37.8% were human-only, and 8.1% were animal-only, with only 1.6% economically significant, non-zoonotic livestock diseases. Despite the number of zoonoses, 86.5% of articles modeled humans whereas only a few articles (5.1%) contained more than one host species. Eastern Asia (32.5%), North America (17.7%), and Southern Asia (13.1%) were the most represented locations. Frequent approaches included tree-based ML (38.4%) and feed-forward neural networks (26.6%). Articles predicted temporal incidence (66.7%), disease risk (38.0%), and/or spatial movement (31.2%). Less than 10% of studies addressed uncertainty quantification, computational efficiency, and missing data, which are essential to operational use and deployment. This study highlights trends and gaps in ML and DL for ID prediction, providing guidelines for future works to better support biopreparedness and response. To fully utilize ML and DL for improved ID forecasting, models should include the full disease ecology in a One-Health context, important food and agricultural diseases, underrepresented hotspots, and important metrics required for operational deployment. © 2022 The Authors","Deep learning; Disease forecast; Disease prediction; Infectious diseases; Machine learning; Systematic review","agricultural management; Asia; biopreparedness; coronavirus disease 2019; deep learning; dengue; feed forward neural network; flu like syndrome; foodborne transmission; forecasting; geographic distribution; host; human; incidence; infection; infection risk; influenza; information processing; livestock; machine learning; malaria; nonhuman; North America; One Health; practice guideline; prediction; quantitative analysis; Review; risk management; South Asia; spatial analysis; species difference; systematic review; uncertainty; zoonotic transmission","Defense Threat Reduction Agency, DTRA, (CB11029); Defense Threat Reduction Agency, DTRA; Washington State University, WSU; Pacific Northwest National Laboratory, PNNL","Funding text 1: This work was funded by the Defense Threat Reduction Agency (project number CB11029 ). ; Funding text 2: The authors wish to thank Nakita Pradhan, Samuel Ortega, and Jaidyn Bryant for their contribution in the initial review process. The authors wish to thank Samantha Erwin for reviewing the manuscript and providing general feedback. R.K. acknowledges the support from the Pacific Northwest National Laboratory (PNNL)-Washington State University (WSU) Distinguished Graduate Research Program (DGRP) Fellowship for facilitating this research collaboration. ","L.E. Charles; Pacific Northwest National Laboratory, Richland, 99354, United States; email: lauren.charles@pnnl.gov","","Elsevier B.V.","23527714","","","","English","One Health","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139311988"
"Ferreira R.E.P.; Bresolin T.; Rosa G.J.M.; Dórea J.R.R.","Ferreira, Rafael E.P. (57362855900); Bresolin, Tiago (55618681200); Rosa, Guilherme J.M. (57219659850); Dórea, João R.R. (37057402900)","57362855900; 55618681200; 57219659850; 37057402900","Using dorsal surface for individual identification of dairy calves through 3D deep learning algorithms","2022","Computers and Electronics in Agriculture","201","","107272","","","","15","10.1016/j.compag.2022.107272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136725514&doi=10.1016%2fj.compag.2022.107272&partnerID=40&md5=db44cf6f61af8a10fcd1db54d792cc12","Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, Madison-WI, United States; Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 53706, Madison-WI, United States; Department of Biological Systems Engineering, University of Wisconsin-Madison, 53706, Madison-WI, United States","Ferreira R.E.P., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, Madison-WI, United States; Bresolin T., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, Madison-WI, United States; Rosa G.J.M., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, Madison-WI, United States, Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison, 53706, Madison-WI, United States; Dórea J.R.R., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, Madison-WI, United States, Department of Biological Systems Engineering, University of Wisconsin-Madison, 53706, Madison-WI, United States","Advances in machine learning techniques have allowed the development of computer vision systems (CVS) that can accurately predict several phenotypes of interest for livestock operations. In this context, 3D images taken from a top-down view are particularly useful for estimating body condition score, growth development, and body biometrics in cattle. Frequently, such CVS rely on identification (ID) systems, such as electronic tags, as a way to match animal ID and the predicted phenotype. However, the same 3D images used to predict body weight and other animal biometrics could be adopted for animal recognition as well. Such alternative would optimize CVS to recognize animal ID and monitor growth development simultaneously while leveraging the same hardware infrastructure. Furthermore, this strategy could be used to recognize animals with similar color patterns. Nonetheless, growing animals are continuously changing body shape, which could limit its use as an invariant feature for pattern recognition. Thus, the objectives of this study were: (1) to compare algorithms for different 3D object representations to identify individual animals; and (2) to evaluate how short-term changes in body shape due to animal growth affect the predictive performance of these algorithms. For objective 1, the algorithms were trained (n = 4,558) and tested (n = 1,139) using images from 38 Holstein calves. For objective 2, we designed three different experiments using images (n = 2,347) from five Holstein calves taken over six weeks during their growing period, always training and testing on different weeks. Each experiment evaluated how changing a different parameter of the image capturing procedure affected the predictive ability of the trained algorithms. In the first experiment, we varied the total number of images per animal in the training set; in the second experiment, we varied the number of weeks while keeping a fixed number of images in the training set; and in the third experiment, we skipped weeks between images in the training and test sets. The F1 score for objective (1) was up to 0.804 when testing with the last frames of each video, and up to 0.959 when using random frames for testing. For objective (2), the F1 score was up to 0.947 for the first experiment when using 130 images per animal; up to 0.979 for the second experiment when using all five weeks; and up to 0.917 when not skipping weeks between training and testing. These results show that deep learning algorithms can be used to identify individual animals through their dorsal area 3D surfaces, and, from our experiments using calves in their growing period, that they are robust enough to account for changes in body shape and size, making them a promising tool for animal recognition during growth. © 2022 Elsevier B.V.","3D neural networks; Animal identification; Animal traceability; Calves; Deep learning; Growth","Agriculture; Anthropometry; Biometrics; Deep learning; Forecasting; Image recognition; Learning algorithms; Learning systems; Neural networks; 3d neural network; 3D-images; Animal identification; Animal traceability; Body shapes; Calf; Computer vision system; Deep learning; Neural-networks; Training sets; algorithm; Animals","Advanced Computing Initiative; Wisconsin Institutes for Discovery; National Science Foundation, NSF; U.S. Department of Energy, USDOE; U.S. Department of Agriculture, USDA, (WIS03085); U.S. Department of Agriculture, USDA; Wisconsin Alumni Research Foundation, WARF; National Institute of Food and Agriculture, NIFA, (2020-67015-30831); National Institute of Food and Agriculture, NIFA; Office of Science, SC; University of Wisconsin-Madison, UW; Department of Computer Science, Saarland University","Funding text 1: This research was performed using the computational resources and assistance of the University of Wisconsin-Madison Center for High Throughput Computing (CHTC) in the Department of Computer Sciences. The CHTC is supported by University of Wisconsin-Madison, the Advanced Computing Initiative, the Wisconsin Alumni Research Foundation, the Wisconsin Institutes for Discovery, and the National Science Foundation, and is an active member of the Open Science Grid, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science. The authors would like to thank the financial support from the USDA National Institute of Food and Agriculture (Washington, DC; grant 2020-67015-30831) and USDA Hatch (WIS03085).; Funding text 2: This research was performed using the computational resources and assistance of the University of Wisconsin-Madison Center for High Throughput Computing (CHTC) in the Department of Computer Sciences. The CHTC is supported by University of Wisconsin-Madison , the Advanced Computing Initiative , the Wisconsin Alumni Research Foundation , the Wisconsin Institutes for Discovery , and the National Science Foundation , and is an active member of the Open Science Grid, which is supported by the National Science Foundation and the U.S. Department of Energy’s Office of Science . The authors would like to thank the financial support from the USDA National Institute of Food and Agriculture (Washington, DC; grant 2020-67015-30831 ) and USDA Hatch ( WIS03085 ).","J.R.R. Dórea; Department of Animal and Dairy Sciences, University of Wisconsin-Madison, 53706, United States; email: joao.dorea@wisc.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85136725514"
"","","","Proceedings of the 2022 Research in Adaptive and Convergent Systems, RACS 2022","2022","ACM International Conference Proceeding Series","","","","","","203","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141056776&partnerID=40&md5=5faa9cf64cb8a180de888eb854a6984c","","","The proceedings contain 30 papers. The topics discussed include: a software framework of roadside units for traffic condition perception and broadcast; Easta : energy-aware scheduling for timely applications on intermittent systems; knowledge distillation-based privacy-preserving data analysis; deep learning-based defect detection on livestock operations; deep reinforcement learning based secondary user transmit power control for underlay cognitive radio networks; a dynamic mutation particle swarm optimization algorithm; architectural languages in the microservice era: a systematic mapping study; Lore: a learning-based approach for workflow scheduling in clouds; sentence embedding based emotion recognition from text data; on-device federated learning with fuzzy logic based client selection; end-to-end image captioning based on reduced feature maps of deep learners pre-trained for object detection; and visualizing architectural evolution via provenance tracking: a systematic review.","","","","","","","Association for Computing Machinery","","978-145039398-0","","","English","ACM Int. Conf. Proc. Ser.","Conference review","Final","","Scopus","2-s2.0-85141056776"
"Du A.; Guo H.; Lu J.; Su Y.; Ma Q.; Ruchay A.; Marinello F.; Pezzuolo A.","Du, Ao (57393576900); Guo, Hao (55331600800); Lu, Jie (57393576800); Su, Yang (57204974615); Ma, Qin (35332370300); Ruchay, Alexey (57192592568); Marinello, Francesco (16230574000); Pezzuolo, Andrea (56023708900)","57393576900; 55331600800; 57393576800; 57204974615; 35332370300; 57192592568; 16230574000; 56023708900","Automatic livestock body measurement based on keypoint detection with multiple depth cameras","2022","Computers and Electronics in Agriculture","198","","107059","","","","40","10.1016/j.compag.2022.107059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130718824&doi=10.1016%2fj.compag.2022.107059&partnerID=40&md5=037262d692c6fae822044e5f9a78ee83","College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Federal Research Centre of Biological Systems and Agro-technologies of the Russian Academy of Sciences, 9 Yanvarya 29, Orenburg, 460000, Russian Federation; Department of Land, Environment, Agriculture and Forestry, University of Padova, Viale dell'Universitá 16, PD, Legnaro, 35020, Italy","Du A., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China, College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Guo H., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China, College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Lu J., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Su Y., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Ma Q., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Ruchay A., Federal Research Centre of Biological Systems and Agro-technologies of the Russian Academy of Sciences, 9 Yanvarya 29, Orenburg, 460000, Russian Federation; Marinello F., Department of Land, Environment, Agriculture and Forestry, University of Padova, Viale dell'Universitá 16, PD, Legnaro, 35020, Italy; Pezzuolo A., Department of Land, Environment, Agriculture and Forestry, University of Padova, Viale dell'Universitá 16, PD, Legnaro, 35020, Italy","The body measurement of livestock is an important task in precision livestock farming. To reduce the cost of manual measurement, an increasing number of studies have proposed non-contact body measurement methods using depth cameras. However, these methods only use 3D data to construct geometric features for body measurements, which is prone to error on incomplete and noisy point clouds. This paper introduces a 2D-3D fusion body measurement method, developed in order to exploit the potential of raw scanned data including high-resolution RGB images and 3D spatial information. The keypoints for body measurement are detected on RGB images with a deep learning model. Then these keypoints are projected onto the surface of livestock point clouds by utilizing the intrinsic parameters of the camera. Combining the process of interpolation and the pose normalization method, 9 body measurements of cattle and 5 body measurements of pig (including body lengths, body widths, body heights, and heart girth) are measured. To verify the feasibility of this method, the experiments are performed on 103 cattle data and 13 pig data. Compared with manual measurements, the MAPEs (mean absolute percentage errors) of 5 cattle body measurements and 1 pig body measurement are reduced to less than 10%. Body widths are more susceptible to non-standard posture. The MAPEs of 2 cattle body widths are larger than 20% and the MAPE of 1 pig body width reaches 30%. In comparison with a previous girth measurement method, the presented method is more accurate and robust for the cattle dataset. The same approach can be adapted and implemented for non-contact body measurement for different livestock species. © 2022 Elsevier B.V.","2D-3D fusion; Deep learning; Point cloud; Precision livestock farming","Cameras; Deep learning; Mammals; 2d-3d fusion; Body measurements; Deep learning; Manual measurements; Measurement methods; Measurements of; Non-contact; Percentage error; Point-clouds; Precision livestock farming; cattle; detection method; interpolation; learning; livestock; pig; Agriculture","National Natural Science Foundation of China, NSFC, (41601491, 42071449); National Natural Science Foundation of China, NSFC; Russian Foundation for Basic Research, РФФИ, (20–416-740003); Russian Foundation for Basic Research, РФФИ","The authors wish to thank everyone who participated in the method assessment. This research was supported by the National Natural Science Foundation of China [Grant Nos. 42071449, 41601491] and RFBR research project No 20–416-740003. ","H. Guo; College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; email: guohaolys@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85130718824"
"de Freitas Curti P.; Selli A.; Pinto D.L.; Merlos-Ruiz A.; de Carvalho Balieiro J.C.; Ventura R.V.","de Freitas Curti, Paula (58623788000); Selli, Alana (57259229700); Pinto, Diógenes Lodi (58055091100); Merlos-Ruiz, Alexandre (58691236000); de Carvalho Balieiro, Julio Cesar (24079946300); Ventura, Ricardo Vieira (59396406900)","58623788000; 57259229700; 58055091100; 58691236000; 24079946300; 59396406900","Applications of livestock monitoring devices and machine learning algorithms in animal production and reproduction: an overview","2023","Animal Reproduction","20","2","e20230077","","","","7","10.1590/1984-3143-AR2023-0077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176456870&doi=10.1590%2f1984-3143-AR2023-0077&partnerID=40&md5=ea8b20582dbbaf97608042427e173999","Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil","de Freitas Curti P., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil; Selli A., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil; Pinto D.L., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil; Merlos-Ruiz A., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil; de Carvalho Balieiro J.C., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil; Ventura R.V., Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, SP, Pirassununga, Brazil","Some sectors of animal production and reproduction have shown great technological advances due to the development of research areas such as Precision Livestock Farming (PLF). PLF is an innovative approach that allows animals to be monitored, through the adoption of cutting-edge technologies that continuously collect real-time data by combining the use of sensors with advanced algorithms to provide decision tools for farmers. Artificial Intelligence (AI) is a field that merges computer science and large datasets to create expert systems that are able to generate predictions and classifications similarly to human intelligence. In a simplified manner, Machine Learning (ML) is a branch of AI, and can be considered as a broader field that encompasses Deep Learning (DL, a Neural Network formed by at least three layers), generating a hierarchy of subsets formed by AI, ML and DL, respectively. Both ML and DL provide innovative methods for analyzing data, especially beneficial for large datasets commonly found in livestock-related activities. These approaches enable the extraction of valuable insights to address issues related to behavior, health, reproduction, production, and the environment, facilitating informed decision-making. In order to create the referred technologies, studies generally go through five steps involving data processing: acquisition, transferring, storage, analysis and delivery of results. Although the data collection and analysis steps are usually thoroughly reported by the scientific community, a good execution of each step is essential to achieve good and credible results, which impacts the degree of acceptance of the proposed technologies in real life practical circumstances. In this context, the present work aims to describe an overview of the current implementations of ML/DL in livestock reproduction and production, as well to identify potential challenges and critical points in each of the five steps mentioned, which can affect results and application of AI techniques by farmers in practical situations. © This is an Open Access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted nd reproduction in any medium, provided the original work is properly cited.","computer vision; machine learning; precision livestock farming; sensors","","São Paulo Research Foundation-FAPESP, (16/19514-2, 21/03101-9, 21/11156-8); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES","Funding text 1: The authors would like to acknowledge the São Paulo Research Foundation-FAPESP (Grants 16/19514-2, 21/03101-9. and 21/11156-8), and CAPES for the financial support.; Funding text 2: *Corresponding author: paula.curti@usp.br Received: May 22, 2023. Accepted: July 10, 2023. Financial support: São Paulo Research Foundation-FAPESP (Grants 16/19514-2, 21/03101-9 and 21/11156-8), and CAPES for the financial support. Conflict of interest: The authors have no conflict of interest to declare.","P. de Freitas Curti; Departamento de Nutrição e Produção Animal, Faculdade de Medicina Veterinária e Zootecnia, Universidade de São Paulo, Pirassununga, SP, Brazil; email: paula.curti@usp.br","","Colegio Brasileiro de Reproducao Animal","18069614","","","","English","Anim. Reprod.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85176456870"
"Evangelista I.R.S.; Catajay L.T.; Bandala A.A.; Concepcion Ii R.S.; Sybingco E.; Dadios E.P.","Evangelista, Ivan Roy S. (57207911065); Catajay, Lenmar T. (58070631800); Bandala, Argel A. (55599317400); Concepcion Ii, Ronnie S. (57208041010); Sybingco, Edwin (55497208700); Dadios, Elmer P. (6602629924)","57207911065; 58070631800; 55599317400; 57208041010; 55497208700; 6602629924","Exploring Deep Learning for Detection of Poultry Activities-Towards an Autonomous Health and Welfare Monitoring in Poultry Farms","2023","Proceedings of the 2023 17th International Conference on Ubiquitous Information Management and Communication, IMCOM 2023","","","","","","","1","10.1109/IMCOM56909.2023.10035656","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148637236&doi=10.1109%2fIMCOM56909.2023.10035656&partnerID=40&md5=61fddbf5574f5945e386356a633ddd39","De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Sultan Kudarat State University, Computer Engineering Department, Sultan Kudarat, Isulan, Philippines; De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines","Evangelista I.R.S., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Catajay L.T., Sultan Kudarat State University, Computer Engineering Department, Sultan Kudarat, Isulan, Philippines; Bandala A.A., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Concepcion Ii R.S., De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines; Sybingco E., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Dadios E.P., De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines","The health condition of poultry significantly affects egg production, meat quality, and reproduction. Behavioral activities such as feeding patterns can be indicators of their current welfare. However, assessing through on-site observation is tedious, time-consuming, possibly biased, and can induce stress to the birds. Hence, employment of an autonomous surveillance system that can continuously and noninvasively monitor the poultry behaviors is the most viable approach. In this study, detection of quail activities: eating, drinking, and roaming, is administered using computer vision (CV) and deep learning (DL). Four DL models, YOLOv5, YOLOX, Faster R-CNN, and EfficientDet, were explored to detect quail activities in cages. The three models YOLOv5, YOLOX, and Faster R-CNN, achieved an average precision (AP) of 85.52, 79.31, and 74.28, respectively. For the EfficientDet model, the training was evaluated using total loss. A total loss of 0.1616 was achieved at 10,000 iterations. All the DL models performed impressively in detecting quail activities in cages. This study contributes to the development of an intelligent health assessment system for poultry. © 2023 IEEE.","computer vision; deep learning; object detection; precision livestock farming; quail farming; smart poultry","Agriculture; Animals; Cell proliferation; Computer vision; Deep learning; Deep learning; Egg production; Health condition; Learning models; Objects detection; Poultry farms; Precision livestock farming; Quail farming; Smart poultry; Total loss; Object detection","De La Salle University, DLSU; Department of Science and Technology, Philippines, DOST; Department of Science and Technology, Republic of the Philippines, DOST","ACKNOWLEDGMENTS The authors are grateful to the support provided by the Intelligent System Laboratory of De La Salle University and the Department of Science and Technology – Engineering Research and Development for Technology (DOST-ERDT) of the Philippines.","","Lee S.; Choo H.; Ismail R.","Institute of Electrical and Electronics Engineers Inc.","","978-166545348-6","","","English","Proc. Int. Conf. Ubiquitous Inf. Manag. Commun., IMCOM","Conference paper","Final","","Scopus","2-s2.0-85148637236"
"Yang L.; Xiong B.; Wang H.; Chen R.; Zhao Y.","Yang, Liang (35207494600); Xiong, Benhai (24537909900); Wang, Hui (59138744600); Chen, Ruipeng (59042318400); Zhao, Yiguang (57188702711)","35207494600; 24537909900; 59138744600; 59042318400; 57188702711","Research Progress and Outlook of Livestock Feeding Robot; [家畜饲喂机器人研究进展与发展展望]","2022","Smart Agriculture","4","2","","86","98","12","3","10.12133/j.smartag.SA202204001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151278638&doi=10.12133%2fj.smartag.SA202204001&partnerID=40&md5=932cfa6bc36573c6a1b6f77f218b4630","State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","Yang L., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; Xiong B., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; Wang H., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; Chen R., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; Zhao Y., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","The production mode of livestock breeding has changed from extensive to intensive, and the production level is improved. However, low labor productivity and labor shortage have seriously restricted the rapid development of China's livestock breeding industry. As a new intelligent agricultural machinery equipment, agricultural robot integrates advanced technologies, such as intelligent monitoring, automatic control, image recognition technology, environmental modeling algorithm, sensors, flexible execution, etc. Using modern information and artificial intelligence technology, developing livestock feeding and pushing robots, realizing digital and intelligent livestock breeding, improving livestock breeding productivity are the main ways to solve the above contradiction. In order to deeply analyze the research status of robot technology in livestock breeding, products and literature were collected worldwide. This paper mainly introduced the research progress of livestock feeding robot from three aspects: Rail feeding robot, self-propelled feeding robot and pushing robot, and analyzed the technical characteristics and practical application of feeding robot.The rail feeding robot runs automatically along the fixed track, identifies the target animal, positions itself, and accurately completes feed delivery through preset programs to achieve accurate feeding of livestock. The self-propelled feeding robot can walk freely in the farm and has automatic navigation and positioning functions. The system takes single chip microcomputer as the control core and realizes automatic walking by sensor and communication module. Compared with the rail feeding robot, the feeding process is more flexible, convenient and technical, which is more conducive to the promotion and application of livestock farms. The pushing robot will automatically push the feed to the feeding area, promote the increase of feed intake of livestock, and effectively reduce the labor demand of the farm. By comparing the feeding robots of developed countries and China from two aspects of technology and application, it is found that China has achieved some innovation in technology, while developted countries do better in product application. The development of livestock robot was prospected. In terms of strategic planning, it is necessary to keep up with the international situation and the trend of technological development, and formulate the agricultural robot development strategic planning in line with China's national conditions. In terms of the development of core technologies, more attention should be paid to the integration of information perception, intelligent sensors and deep learning algorithms to realize human-computer interaction. In terms of future development trends, it is urgent to strengthen innovation, improve the friendliness and intelligence of the robot, and improve the learning ability of the robot. To sum up, intelligent livestock feeding and pushing machine operation has become a cutting-edge technology in the field of intelligent agriculture, which will surely lead to a new round of agricultural production technology reform, promote the transformation and upgrading of China's livestock industry. . © 2022 Agricultural Information Institute, Chinese Academy of Agricultural Sciences. All rights reserved.","artificial intellignce; digital breeding; feeding robot; livestock feeding; pushing robot","","","","B. Xiong; State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; email: xiongbenhai@caas.cn","","Agricultural Information Institute, Chinese Academy of Agricultural Sciences","20968094","","","","Chinese","Smart. Agric.","Article","Final","","Scopus","2-s2.0-85151278638"
"MacPhillamy C.; Alinejad-Rokny H.; Pitchford W.S.; Low W.Y.","MacPhillamy, Callum (57240581200); Alinejad-Rokny, Hamid (53871090400); Pitchford, Wayne S. (7004473724); Low, Wai Yee (57195761219)","57240581200; 53871090400; 7004473724; 57195761219","Cross-species enhancer prediction using machine learning","2022","Genomics","114","5","110454","","","","8","10.1016/j.ygeno.2022.110454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136588095&doi=10.1016%2fj.ygeno.2022.110454&partnerID=40&md5=57fe9f2450b96525130acdcb0079c6d2","The Davies Livestock Research Centre, School of Animal and Veterinary Sciences, University of Adelaide, Roseworthy, 5371, SA, Australia; BioMedical Machine Learning Lab, The Graduate School of Biomedical Engineering, UNSW, Sydney, 2052, NSW, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney, 2052, NSW, Australia","MacPhillamy C., The Davies Livestock Research Centre, School of Animal and Veterinary Sciences, University of Adelaide, Roseworthy, 5371, SA, Australia; Alinejad-Rokny H., BioMedical Machine Learning Lab, The Graduate School of Biomedical Engineering, UNSW, Sydney, 2052, NSW, Australia, School of Computer Science and Engineering, University of New South Wales, Sydney, 2052, NSW, Australia; Pitchford W.S., The Davies Livestock Research Centre, School of Animal and Veterinary Sciences, University of Adelaide, Roseworthy, 5371, SA, Australia; Low W.Y., The Davies Livestock Research Centre, School of Animal and Veterinary Sciences, University of Adelaide, Roseworthy, 5371, SA, Australia","Cis-regulatory elements (CREs) are non-coding parts of the genome that play a critical role in gene expression regulation. Enhancers, as an important example of CREs, interact with genes to influence complex traits like disease, heat tolerance and growth rate. Much of what is known about enhancers come from studies of humans and a few model organisms like mouse, with little known about other mammalian species. Previous studies have attempted to identify enhancers in less studied mammals using comparative genomics but with limited success. Recently, Machine Learning (ML) techniques have shown promising results to predict enhancer regions. Here, we investigated the ability of ML methods to identify enhancers in three non-model mammalian species (cattle, pig and dog) using human and mouse enhancer data from VISTA and publicly available ChIP-seq. We tested nine models, using four different representations of the DNA sequences in cross-species prediction using both the VISTA dataset and species-specific ChIP-seq data. We identified between 809,399 and 877,278 enhancer-like regions (ELRs) in the study species (11.6–13.7% of each genome). These predictions were close to the ~8% proportion of ELRs that covered the human genome. We propose that our ML methods have predictive ability for identifying enhancers in non-model mammalian species. We have provided a list of high confidence enhancers at https://github.com/DaviesCentreInformatics/Cross-species-enhancer-prediction and believe these enhancers will be of great use to the community. © 2022","ChIP-seq; Cross-species enhancer prediction; Deep learning; Livestock; Machine learning","Animals; Base Sequence; Cattle; Dogs; Enhancer Elements, Genetic; Genome, Human; Genomics; Humans; Machine Learning; Mammals; Mice; Swine; Article; bovine; convolutional neural network; deep learning; DNA sequence; dog; enhancer region; genome; genome-wide association study; human; machine learning; nonhuman; pig; prediction; support vector machine; animal; genetics; genomics; human genome; machine learning; mammal; mouse; nucleotide sequence; procedures","Biomedical Machine Learning Lab; University of New South Wales, UNSW","The authors would like to thank members of the Biomedical Machine Learning Lab (BML) at the University of New South Wales for their valuable input and discussion relating to this work.","W.Y. Low; The Davies Livestock Research Centre, School of Animal and Veterinary Sciences, University of Adelaide, Roseworthy, 5371, Australia; email: wai.low@adelaide.edu.au","","Academic Press Inc.","08887543","","GNMCE","36030022","English","Genomics","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136588095"
"Gokul Krishna R.; Periyasamy S.V.; Roshan Khan S.B.; Mohan Raj T.","Gokul Krishna, R. (57222246591); Periyasamy, S.V. (58713975700); Roshan Khan, S.B. (58617680500); Mohan Raj, T. (57214448606)","57222246591; 58713975700; 58617680500; 57214448606","Exploring the Potential of Machine Learning for Early Cattle Disease Diagnosis","2023","Proceedings of the 5th International Conference on Inventive Research in Computing Applications, ICIRCA 2023","","","","853","857","4","2","10.1109/ICIRCA57980.2023.10220827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172282924&doi=10.1109%2fICIRCA57980.2023.10220827&partnerID=40&md5=d45995db4c1ff3f0ee5b895ff068a5f7","Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India","Gokul Krishna R., Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India; Periyasamy S.V., Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India; Roshan Khan S.B., Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India; Mohan Raj T., Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India","Dairy farming has long been a common occupation in regions where industrial agriculture is not prevalent, such as India. The dairy industry has seen a rise in productivity as a result of dairy farming modernization, but it is still afflicted by various ailments that can affect cow product output, and poor-quality dairy products not only hamper long-term national economic success. Due to the large number of dairy cattle housed in various dairies, dairy owners and local governments struggle to maintain and monitor their health. A health management method must include continuous monitoring of each cow's health as well as rapid identification and treatment of sick animals. To do this, sensor technology is employed to measure essential animal properties such as heart rate and temperature. This data is then gathered and put into a data mining system, which forecasts any disease-related events. As a result, even routine veterinary exams and animal care may become more expensive. As a result, in this study, the costs of livestock monitoring are avoided. © 2023 IEEE.","Cattle Farming; Data Mining; Deep Learning; IoT; Machine Learning; Smart Contract","Animals; Dairies; Dairy products; Deep learning; Diagnosis; Farms; Internet of things; Learning systems; Cattle disease; Cattle farming; Dairy cattles; Dairy farming; Dairy industry; Deep learning; Disease diagnosis; Economic success; IoT; Machine-learning; Data mining","","","T. Mohan Raj; Karpagam Academy of Higher Education, Department of Computer Science and Engineering, Coimbatore, India; email: mohanraj.t@kahedu.edu.in","","Institute of Electrical and Electronics Engineers Inc.","","979-835032142-5","","","English","Proc. Int. Conf. Inventive Res. Comput. Appl., ICIRCA","Conference paper","Final","","Scopus","2-s2.0-85172282924"
"Teng G.; Ji H.; Zhuang Y.; Liu M.","Teng, Guanghui (10140843100); Ji, Hengyi (57223181066); Zhuang, Yanrong (57204034561); Liu, Mulin (57204040225)","10140843100; 57223181066; 57204034561; 57204040225","Research progress of deep learning in the process of pig feeding; [深度学习在猪只饲养过程的应用研究进展]","2022","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","38","14","","235","249","14","11","10.11975/j.issn.1002-6819.2022.14.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141922339&doi=10.11975%2fj.issn.1002-6819.2022.14.027&partnerID=40&md5=48f61eb67c180dc5b88d445661ffa847","College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Beijing Research Center for Livestock and Poultry Healthy Environmental Engineering Technology, Beijing, 100083, China","Teng G., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China, Beijing Research Center for Livestock and Poultry Healthy Environmental Engineering Technology, Beijing, 100083, China; Ji H., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Zhuang Y., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Liu M., College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Agricultural Engineering in Structure and Environment, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China","China is the major producer and consumer of pork all around the world. In China, pork production has accounted for more than 60% of meat production for a long time. Nowadays, the pig industry has gradually shifted from decentralized and extensive breeding to intensive large-scale breeding, promoting production efficiency. At present, as people's demand for the quantity and quality of pork is increasing, China's pig industry faces the problem that the output and quality of pork can not meet people's daily needs. With the rise of artificial intelligence technology in recent years, deep learning technology has been developed rapidly and widely used in image and audio recognition, natural language processing, robotics, bioinformatics, chemistry, finance, and other fields. It is also an essential tool in developing precision livestock farming. The body condition, behavior and health status of pigs could directly affect the income level of pig farms, so through using deep learning technology, we can quickly and accurately acquire the relevant information about pigs, and carry out precise management to improve the feeding efficiency and welfare levels of pigs. This paper expounds on the research progress and application status of deep learning technology used in target pig detection, pig image segmentation, pig body condition and abnormal monitoring, as well as pig behavior recognition. Then we put forward the improvement strategy of deep learning technology used in the process of pig feeding, which make it easier for researchers to understand. At the same time, we summarize and analyze the data sources and datasets, application scope, and model optimization of previous works using deep learning technology in pig breeding. In the field of data sources and datasets, mobile computer vision systems are currently more suitable than systems with many fixed cameras when they are applied in pig houses, therefore, further research could focus on how to use deep learning technology to mobile computer vision systems. Deep learning technology requires a large amount of data to learn data features, which is a massive disadvantage for pig applications. To get pre-training weights suitable for agricultural data sets, a large number of public datasets and a unified dataset standard suited to the pig field should be established. In terms of application scope, due to the short application time of deep learning in the process of raising pigs, many critical occasions are not involved or seldom involved in existing research, so the application scope of deep learning should further expand. In model optimization, optimizing the model to meet the needs of practical application scenarios is the direction of future research. Optimizing the model to achieve a better balance between size and model performance requires further study. Optimizing the model to locate the start and end time of the behavior in unclipped video and recognize the behavior of target pigs is the challenge in identifying the behavior of pigs in future. China's research on deep learning is at the top level, so the application prospect of deep learning in pig farming is highly expected. Combined with the actual production scene of pigs, deep learning technology will significantly contribute to the development of precision livestock farming. © 2022 Chinese Society of Agricultural Engineering. All rights reserved.","computer vision; convolutional neural network; deep learning; pigs; precision livestock farming","Agriculture; Behavioral research; Computer vision; Convolutional neural networks; Deep learning; Engineering education; Image segmentation; Large dataset; Learning algorithms; Learning systems; Mammals; Meats; Natural language processing systems; Body condition; Convolutional neural network; Data-source; Deep learning; Learning technology; Mobile computers; Model optimization; Pig; Pig industry; Precision livestock farming; Production efficiency","","","","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85141922339"
"Lima M.L.F.; de Souza S.M.F.; de Sá I.V.; Santana O.A.","Lima, Mayara Lopes de Freitas (57218094792); de Souza, Samara Maria Farias (58145256900); de Sá, Isabelle Ventura (58145257000); Santana, Otacilio Antunes (25031652500)","57218094792; 58145256900; 58145257000; 25031652500","Deep learning with aerial surveys for extensive livestock hotspot recognition in the Brazilian Semi-arid Region; [Deep learning no levantamento aéreo de hotspots para pecuária extensiva no Semiárido Brasileiro]","2023","Ciencia e Agrotecnologia","47","","e010922","","","","1","10.1590/1413-7054202347010922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150266379&doi=10.1590%2f1413-7054202347010922&partnerID=40&md5=906ad920171c078d192b35eba57c0688","Universidade Federal de Pernambuco/UFPE, Departamento de Biofísica e Radiobiologia /DBR, PE, Recife, Brazil; Universidade Federal de Pernambuco/UFPE, Centro de Ciências Médicas/CCM, PE, Recife, Brazil; Colégio Militar do Recife/CMR, PE, Recife, Brazil","Lima M.L.F., Universidade Federal de Pernambuco/UFPE, Departamento de Biofísica e Radiobiologia /DBR, PE, Recife, Brazil; de Souza S.M.F., Universidade Federal de Pernambuco/UFPE, Centro de Ciências Médicas/CCM, PE, Recife, Brazil; de Sá I.V., Colégio Militar do Recife/CMR, PE, Recife, Brazil; Santana O.A., Universidade Federal de Pernambuco/UFPE, Departamento de Biofísica e Radiobiologia /DBR, PE, Recife, Brazil","In the Brazilian Semi-arid Region, extensive livestock farming with ecoproductive management is the most efficient way to maintain and increase the production of goat products (e.g., meat) with of not depleting environmental resources. This set of actions (induced goat migration and pasture closure) is part of Livestock 4.0, in which Industry 4.0 feed areas are efficiently managed using artificial intelligence and deep learning properly monitored by the producer and the consumer. The objective of this work was to identify pasture areas with Opuntia ficus-indica (Mill, Cactaceae) forage palm species for breeding and production of Capra aegagrus-hircus goats (Lineu, Bovidae) using aerial survey images captured by drones classified using deep learning techniques. The methodological steps of the Industry Architecture Reference Model 4.0 were adapted to the field situation (Semi-arid Region) including (A) study area delimitation, (B) image collection (by drones), (C) deep learning training, convolutional neural network (CNN) training, (D) training accuracy analysis, and (E) automatic goat production evaluation and validation. The area classification based on the forage palm density allowed us to measure the environmental degradation caused by livestock. Stimulated goat migration reduced this degradation as well as increased goat biomass and volume production. © 2023, Federal University of Lavras. All rights reserved.","convolutional neural network; Industry 4.0; smart factory; sustainable farming","","Agência Nacional de Águas e Saneamento; CMR; Colégio Militar do Recife; PROEXC; PROPESQI; Pró-Reitoria de Extensão e Cultura; Pró-Reitoria de Pesquisa e Inovação; Agência Nacional de Águas, ANA; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq; Universidade Federal de Pernambuco, UFPE","Authors are grateful for the institutional support of ‘Pró-Reitoria de Pós-Graduação (PROPG), ‘Pró-Reitoria de Extensão e Cultura (PROEXC)’ and ‘Pró-Reitoria de Pesquisa e Inovação (PROPESQI)’ of Federal University of Pernambuco (UFPE); the Graduate Program in Teaching for Environmental Science (ProfCiAmb); the ‘Colégio Militar do Recife (CMR)’; ‘Agência Nacional de Águas e Saneamento (ANA)’; the ‘Coordenação de Aperfeiçoamento de Pessoal Nível Superior (CAPES)’; and the Educometry Research Group (UFPE/CNPq) for the discussion and data collection.","O.A. Santana; Universidade Federal de Pernambuco/UFPE, Departamento de Biofísica e Radiobiologia /DBR, Recife, PE, Brazil; email: otacilio.santana@ufpe.br","","Federal University of Lavras","14137054","","","","English","Ciencia e Agrotecnologia","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85150266379"
"Dac H.H.; Gonzalez Viejo C.; Lipovetzky N.; Tongson E.; Dunshea F.R.; Fuentes S.","Dac, Hai Ho (57890392300); Gonzalez Viejo, Claudia (57192378497); Lipovetzky, Nir (36701605000); Tongson, Eden (57202582254); Dunshea, Frank R. (7005947650); Fuentes, Sigfredo (23982386600)","57890392300; 57192378497; 36701605000; 57202582254; 7005947650; 23982386600","Livestock Identification Using Deep Learning for Traceability","2022","Sensors","22","21","8256","","","","12","10.3390/s22218256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141591961&doi=10.3390%2fs22218256&partnerID=40&md5=24d63b0778f3876537aa74b713b97387","Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia; School of Computing and Information Systems, Faculty of Engineering and Information Technology, The University of Melbourne, Parkville, 3010, VIC, Australia; Faculty of Biological Sciences, The University of Leeds, Leeds, LS2 9JT, United Kingdom","Dac H.H., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia; Gonzalez Viejo C., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia; Lipovetzky N., School of Computing and Information Systems, Faculty of Engineering and Information Technology, The University of Melbourne, Parkville, 3010, VIC, Australia; Tongson E., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia; Dunshea F.R., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia, Faculty of Biological Sciences, The University of Leeds, Leeds, LS2 9JT, United Kingdom; Fuentes S., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, VIC, Australia","Farm livestock identification and welfare assessment using non-invasive digital technology have gained interest in agriculture in the last decade, especially for accurate traceability. This study aimed to develop a face recognition system for dairy farm cows using advanced deep-learning models and computer vision techniques. This approach is non-invasive and potentially applicable to other farm animals of importance for identification and welfare assessment. The video analysis pipeline follows standard human face recognition systems made of four significant steps: (i) face detection, (ii) face cropping, (iii) face encoding, and (iv) face lookup. Three deep learning (DL) models were used within the analysis pipeline: (i) face detector, (ii) landmark predictor, and (iii) face encoder. All DL models were finetuned through transfer learning on a dairy cow dataset collected from a robotic dairy farm located in the Dookie campus at The University of Melbourne, Australia. Results showed that the accuracy across videos from 89 different dairy cows achieved an overall accuracy of 84%. The computer program developed may be deployed on edge devices, and it was tested on NVIDIA Jetson Nano board with a camera stream. Furthermore, it could be integrated into welfare assessment previously developed by our research group. © 2022 by the authors.","computer vision; cow identification system; deep learning in agriculture; edge computing","Agriculture; Animals; Cattle; Dairying; Deep Learning; Farms; Female; Humans; Livestock; Deep learning; Edge computing; Face recognition; Learning systems; Pipelines; Signal encoding; Software testing; Computer vision techniques; Cow identification system; Dairy cow; Dairy farms; Deep learning in agriculture; Digital technologies; Edge computing; Face recognition systems; Farm animals; Learning models; agricultural land; agriculture; animal; bovine; dairying; female; human; livestock; procedures; Computer vision","Food Fibre Trace Global Pty Ltd; Food Fibre Trace P/L","Funding text 1: The project was funded by Food Fibre Trace P/L.; Funding text 2: This research was funded by Food Fibre Trace Global Pty Ltd. Victoria, Australia. The authors would like to acknowledge Nathan Anderson, Geoff Wilhelms, Louie Minoza and Stephanie Habib from the Faculty of Veterinary and Agricultural Sciences (FVAS) Robotic Dairy Farm located at The University of Melbourne (UoM) Dookie College, Victoria, Australia and Pragna Prathap (FVAS-UoM) for their support in the access to facilities and animal handling. ","S. Fuentes; Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, University of Melbourne, Melbourne, 3010, Australia; email: sfuentes@unimelb.edu.au","","MDPI","14248220","","","36365954","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141591961"
"Zang J.; Ye S.; Xu Z.; Wang J.; Liu W.; Bai Y.; Yong C.; Zou X.; Zhang W.","Zang, Jianjun (13407289200); Ye, Shuqin (57874723400); Xu, Zeying (57218844937); Wang, Junjun (58768889000); Liu, Wenchao (57835194100); Bai, Yungang (57282831100); Yong, Cheng (57204563324); Zou, Xiuguo (55551790900); Zhang, Wentian (57201317830)","13407289200; 57874723400; 57218844937; 58768889000; 57835194100; 57282831100; 57204563324; 55551790900; 57201317830","Prediction Model of Carbon Dioxide Concentration in Pig House Based on Deep Learning","2022","Atmosphere","13","7","1130","","","","5","10.3390/atmos13071130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137266452&doi=10.3390%2fatmos13071130&partnerID=40&md5=d3ffa9d835bbad654017fcc8bd3d97ca","College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China; National Key Laboratory of Animal Nutrition, Beijing, 100193, China; College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, 2007, NSW, Australia","Zang J., College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China, National Key Laboratory of Animal Nutrition, Beijing, 100193, China; Ye S., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; Xu Z., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; Wang J., College of Animal Science and Technology, China Agricultural University, Beijing, 100193, China, National Key Laboratory of Animal Nutrition, Beijing, 100193, China; Liu W., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; Bai Y., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Yong C., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; Zou X., College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; Zhang W., Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, 2007, NSW, Australia","The air environment (e.g., high concentration of carbon dioxide) in a pig house will affect the health conditions and growth performance of the pigs, and the quality of pork as well. In order to reduce the cumulative concentration of carbon dioxide in the pig house, the prediction model was established by the deep learning method to predict the changes of the carbon dioxide cumulative concentration in a pig house. This model will also be used for the real-time monitoring and adjustment of the concentration of carbon dioxide of the pig house. The experiment was designed to collect environmental parameters (e.g., temperature, humidity, wind speed, and carbon dioxide concentration) data in the pig house for several months. The ensemble empirical mode decomposition–gated recurrent unit (EEMD–GRU) prediction model was established in the prediction of carbon dioxide concentration in the pig house. The results show that compared with the other models, the prediction accuracy of the EEMD–GRU model is the highest, and the root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and r-squared (R2) of carbon dioxide concentration in autumn and winter are 123.2 ppm, 88.3 ppm, 3.2%, and 0.99, respectively. The RMSE, MAE, MAPE, and R2 for carbon dioxide concentration are 129.1 ppm, 93.2 ppm, 5.9%, and 0.76 in spring and summer. The prediction model proposed in this paper can effectively predict the concentration of carbon dioxide in the pig house and provide effective help for the precise control of the pig house environment. © 2022 by the authors.","carbon dioxide concentration; deep learning; pig house environment; prediction model","Deep learning; Empirical mode decomposition; Errors; Forecasting; Houses; Learning systems; Mammals; Mean square error; Wind; Air environment; Carbon dioxide concentrations; Deep learning; Empirical Mode Decomposition; Mean absolute error; Percentage error; Pig house; Pig house environment; Prediction modelling; Root mean square errors; atmospheric modeling; carbon dioxide; concentration (composition); livestock farming; machine learning; prediction; Carbon dioxide","Key Technology Research and Development Program of Shandong, (2019JZZY020308); Key Technology Research and Development Program of Shandong; National Key Research and Development Program of China, NKRDPC, (199A7310H, 2016YFD0500506, 2021YFD1300202); National Key Research and Development Program of China, NKRDPC","This research was funded by National Key R&D Program of China (2021YFD1300202), National Key R&D Program of China (2016YFD0500506), S and T Program of Hebei (199A7310H), and Key Research and Developmental Program of Shandong Province (2019JZZY020308).","X. Zou; College of Artificial Intelligence, Nanjing Agricultural University, Nanjing, 210031, China; email: zouxiuguo@njau.edu.cn","","MDPI","20734433","","","","English","Atmosphere","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137266452"
"Low B.E.; Cho Y.; Lee B.; Yi M.Y.","Low, Beng Ern (58000971200); Cho, Yesung (57950524200); Lee, Bumho (57230460900); Yi, Mun Yong (7102961073)","58000971200; 57950524200; 57230460900; 7102961073","Playing Behavior Classification of Group-Housed Pigs Using a Deep CNN-LSTM Network","2022","Sustainability (Switzerland)","14","23","16181","","","","5","10.3390/su142316181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143846165&doi=10.3390%2fsu142316181&partnerID=40&md5=d9a45b522a6a523c0f5cb2dd58ee8248","Graduate School of Data Science, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea","Low B.E., Graduate School of Data Science, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea; Cho Y., Graduate School of Data Science, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea; Lee B., Graduate School of Data Science, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea; Yi M.Y., Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea","The swine industry is one of the industries that progressively incorporates smart livestock farming (SLF) to monitor the grouped-housed pigs’ welfare. In recent years, pigs’ positive welfare has gained much attention. One of the evident behavioral indicators of positive welfare is playing behaviors. However, playing behavior is spontaneous and temporary, which makes the detection of playing behaviors difficult. The most direct method to monitor the pigs’ behaviors is a video surveillance system, for which no comprehensive classification framework exists. In this work, we develop a comprehensive pig playing behavior classification framework and build a new video-based classification model of pig playing behaviors using deep learning. We base our deep learning framework on an end-to-end trainable CNN-LSTM network, with ResNet34 as the CNN backbone model. With its high classification accuracy of over 92% and superior performances over the existing models, our proposed model highlights the importance of applying the global maximum pooling method on the CNN final layer’s feature map and leveraging a temporal attention layer as an input to the fully connected layer for final prediction. Our work has direct implications on advancing the welfare assessment of group-housed pigs and the current practice of SLF. © 2022 by the authors.","animal positive welfare; convolutional neural network; long short-term memory network; pig play behavior; video classification","accuracy assessment; animal welfare; artificial neural network; classification; livestock farming; performance assessment; pig","Korea Smart Farm R&D Foundation; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421043-04-2-HD020); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This research was supported by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (421043-04-2-HD020).","M.Y. Yi; Department of Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 34141, South Korea; email: munyi@kaist.ac.kr","","MDPI","20711050","","","","English","Sustainability","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143846165"
"Alameer A.; Buijs S.; O'Connell N.; Dalton L.; Larsen M.; Pedersen L.; Kyriazakis I.","Alameer, Ali (57190429643); Buijs, Stephanie (25635437300); O'Connell, Niamh (7006123507); Dalton, Luke (57936163200); Larsen, Mona (56526109000); Pedersen, Lene (7201717901); Kyriazakis, Ilias (7006474425)","57190429643; 25635437300; 7006123507; 57936163200; 56526109000; 7201717901; 7006474425","Automated detection and quantification of contact behaviour in pigs using deep learning","2022","Biosystems Engineering","224","","","118","130","12","20","10.1016/j.biosystemseng.2022.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140294286&doi=10.1016%2fj.biosystemseng.2022.10.002&partnerID=40&md5=547d5928ae4f57ccf575e5f24e96dfc3","School of Science, Engineering and Environment, University of Salford, Manchester, M5 4WT, United Kingdom; Institute for Global Food Security, Queen's University, Belfast, BT9 5DL, United Kingdom; Agri-Food and Biosciences Institute, Large Park, Hillsborough, BT26 6DR, United Kingdom; Zoetis International, Cherrywood, Loughlinstown, Dublin, Ireland; Research Group of Animal Welfare, Department of Animal Science, Aarhus University, Tjele, Denmark; M3-BIORES, Department of Biosystems, KU Leuven, Leuven, Belgium","Alameer A., School of Science, Engineering and Environment, University of Salford, Manchester, M5 4WT, United Kingdom, Institute for Global Food Security, Queen's University, Belfast, BT9 5DL, United Kingdom; Buijs S., Agri-Food and Biosciences Institute, Large Park, Hillsborough, BT26 6DR, United Kingdom; O'Connell N., Institute for Global Food Security, Queen's University, Belfast, BT9 5DL, United Kingdom; Dalton L., Zoetis International, Cherrywood, Loughlinstown, Dublin, Ireland; Larsen M., Research Group of Animal Welfare, Department of Animal Science, Aarhus University, Tjele, Denmark, M3-BIORES, Department of Biosystems, KU Leuven, Leuven, Belgium; Pedersen L., M3-BIORES, Department of Biosystems, KU Leuven, Leuven, Belgium; Kyriazakis I., Institute for Global Food Security, Queen's University, Belfast, BT9 5DL, United Kingdom","Change in the frequency of contact between pigs within a group may be indicative of a change in the physiological or health status of one or more pigs within a group, or indicative of the occurrence of abnormal behaviour, e.g. tail-biting. Here, we developed a novel framework that detects and quantifies the frequency of interaction, i.e., a pig head to another pig rear, between pigs in groups. The method does not require individual pig tracking/identification and uses only inexpensive camera-based data capturing infrastructure. We modified the architecture of well-established deep learning models and further developed a lightweight processing stage that scans over pigs to score said interactions. This included the addition of a detection subnetwork to a selected layer of the base residual network. We first validated the automated system to score the interactions between individual pigs within a group, and determined an average accuracy of 92.65% ± 3.74%, under a variety of settings, e.g., management set-ups and data capturing. We then applied the method to a significant welfare challenge in pigs, that of the detection of tail-biting outbreaks in pigs and quantified the changes that happen in contact behaviour during such an outbreak. Our study shows that the system is able to accurately monitor pig interactions under challenging farming conditions, without the need for additional sensors or a pig tracking stage. The method has a number of potential applications to the field of precision livestock farming of pigs that may transform the industry. © 2022 The Author(s)","Automated detection; Deep learning; Pig behaviour; Pig social interactions; Tail-biting","Agriculture; Automation; Mammals; Automated detection; Contact behavior; Data capturing; Deep learning; Detection and quantifications; Physiological status; Pig behavior; Pig social interaction; Social interactions; Tail-biting; Deep learning","Commission; EU-China; European Union H2020 research and innovation program; European Commission, EC; UK Centre for Innovation Excellence in Livestock; Zoetis Inc.; CIEL; Horizon 2020 Framework Programme, H2020, (773436); Ministeriet for Fø devarer, Landbrug og Fiskeri, (34,009-13-0743)","Funding text 1: We are grateful to Dr Katarina Buckova, Melanie McAuley, Zoe Tey and Joy McMillen for help with the collection and annotation of the datasets. This research was part of the EU-China HealthyLivestock project https://healthylivestock.net/. The authors wish to acknowledge that HealthyLivestock is funded by the European Union H2020 research and innovation program under grant agreement number 773436. The European Commission's support for the production of this publication does not constitute an endorsement of the contents, which reflect the views only of the authors, and the Commission cannot be held responsible for any use which may be made of the information contained therein. The automated detection work was supported in part by the UK Centre for Innovation Excellence in Livestock (CIEL) and Zoetis Inc. Zoetis did not influence the data selection, interpretation, conclusions drawn or the decision on how or what to publish. The study was made possible by a grant from the Green Development and Demonstration Programme under the Ministry of Food, Agriculture and Fisheries, Denmark (project IntactTails j. nr. 34,009-13-0743).; Funding text 2: We are grateful to Dr Katarina Buckova, Melanie McAuley, Zoe Tey and Joy McMillen for help with the collection and annotation of the datasets. This research was part of the EU-China HealthyLivestock project https://healthylivestock.net/ . The authors wish to acknowledge that HealthyLivestock is funded by the European Union H2020 research and innovation program under grant agreement number 773436. The European Commission\u2019s support for the production of this publication does not constitute an endorsement of the contents, which reflect the views only of the authors, and the Commission cannot be held responsible for any use which may be made of the information contained therein. ; Funding text 3: The study was made possible by a grant from the Green Development and Demonstration Programme under the Ministry of Food, Agriculture and Fisheries, Denmark (project IntactTails j. nr. 34,009-13-0743). ","A. Alameer; School of Science, Engineering and Environment, University of Salford, Manchester, M5 4WT, United Kingdom; email: A.Alameer1@salford.ac.uk","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85140294286"
"Gunawan T.S.; Kartiwi M.; Saifudin A.; Nur L.O.; Nugroho B.S.","Gunawan, Teddy Surya (8286407700); Kartiwi, Mira (35310925800); Saifudin, Ali (58305479100); Nur, Levy Olivia (56085320300); Nugroho, Bambang Setia (55575749100)","8286407700; 35310925800; 58305479100; 56085320300; 55575749100","Optimizing Livestock Productivity with Computer Vision-Based Cow Estrus Detection in Free Stall Barns using Various YOLOv8 Models","2023","ICSIMA 2023 - 9th IEEE International Conference on Smart Instrumentation, Measurement and Applications","","","","105","110","5","1","10.1109/ICSIMA59853.2023.10373431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183469141&doi=10.1109%2fICSIMA59853.2023.10373431&partnerID=40&md5=60317d59a9e0b6242cd06c79e1fe71ef","International Islamic University Malaysia, Electrical and Computer Engineering Department, Kuala Lumpur, 53100, Malaysia; International Islamic University Malaysia, Information Systems Department, Kuala Lumpur, 53100, Malaysia; Airlangga University, Department of Veterinary Sciences, Surabaya, Indonesia; Telkom University, School of Electrical Engineering, Bandung, Indonesia","Gunawan T.S., International Islamic University Malaysia, Electrical and Computer Engineering Department, Kuala Lumpur, 53100, Malaysia; Kartiwi M., International Islamic University Malaysia, Information Systems Department, Kuala Lumpur, 53100, Malaysia; Saifudin A., Airlangga University, Department of Veterinary Sciences, Surabaya, Indonesia; Nur L.O., Telkom University, School of Electrical Engineering, Bandung, Indonesia; Nugroho B.S., Telkom University, School of Electrical Engineering, Bandung, Indonesia","In the domain of livestock management, the precise detection of estrus in cows is crucial for reproductive efficiency and enhanced livestock production. Traditional methods, primarily based on human observation, are labor-intensive and can be error-prone. This study leverages YOLOv8, a cutting-edge computer vision technology, for cow estrus detection. Our evaluation reveals that YOLOv8 achieved a remarkable accuracy rate, outperforming conventional methods in speed and reliability. Specifically, the model demonstrated a precision of 96%, a recall of 96.1%, and a mean average precision (mAP) of 98.35% for the 50% intersection over union (IoU) threshold. By integrating YOLOv8, we highlight the potential for substantial improvements in reproductive efficiency, labor cost savings, and increased profitability in the cattle sector. This work emphasizes the transformative impact of advanced technology in agriculture and paves the way for future innovations in livestock management.  © 2023 IEEE.","computer vision; cow estrus detection; deep learning; Livestock management; YOLOv8","Agriculture; Deep learning; Efficiency; Wages; Cow estrus detection; Deep learning; Error prones; Free-stalls; Human observations; Labour-intensive; Livestock management; Livestock production; Vision based; YOLOv8; Computer vision","International Islamic University Malaysia, IIUM; Universitas Airlangga, UNAIR; PETRONAS Research Sdn Bhd, PRSB; Universitas Telkom, Tel-U","Funding text 1: ACKNOWLEDGMENT The authors would like to acknowledge the support from the International Islamic University Malaysia, Airlangga University, and Telkom University for the research funding and facilities. The authors acknowledge that Petronas Research Sdn Bhd supported the hardware used.; Funding text 2: The authors would like to acknowledge the support from the International Islamic University Malaysia, Airlangga University, and Telkom University for the research funding and facilities. The authors acknowledge that Petronas Research Sdn Bhd supported the hardware used.","T.S. Gunawan; International Islamic University Malaysia, Electrical and Computer Engineering Department, Kuala Lumpur, 53100, Malaysia; email: tsgunawan@iium.edu.my","","Institute of Electrical and Electronics Engineers Inc.","","979-835034338-0","","","English","ICSIMA - IEEE Int. Conf. Smart Instrum., Meas. Appl.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85183469141"
"Wang H.; Jiang Y.; Li X.; Ma Y.; Liu X.","Wang, Haiyan (57191848968); Jiang, Yehao (58648387100); Li, Xuan (26638743500); Ma, Yunlong (58736279000); Liu, Xiaolei (56540112900)","57191848968; 58648387100; 26638743500; 58736279000; 56540112900","Pig Image Instance Segmentation Based on Weakly Supervised Dataset; [基于弱监督数据集的猪只图像实例分割]","2023","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","54","10","","255","265","10","0","10.6041/j.issn.1000-1298.2023.10.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175796438&doi=10.6041%2fj.issn.1000-1298.2023.10.025&partnerID=40&md5=27021c0db90c3823f3425ea060ba381d","Shenzhen Institute of Nutrition and Health, Huazhong Agricultural University, Shenzhen, 518000, China; Agricultural Genomics Institute at Shenzhen (AGIS), Chinese Academy of Agricultural Sciences, Shenzhen, 518000, China; College of Informatics, Huazhong Agricultural University, Wuhan, 430070, China; Shenzhen Branch of Guangdong Laboratory of Lingnan Modern Agricultural Science and Technology, Shenzhen, 518000, China; Key Laboratory of Smart Farming for Agricultural Animals, Ministry of Agriculture and Rural Affairs, Wuhan, 430070, China; Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Wuhan, 430070, China","Wang H., Shenzhen Institute of Nutrition and Health, Huazhong Agricultural University, Shenzhen, 518000, China, Agricultural Genomics Institute at Shenzhen (AGIS), Chinese Academy of Agricultural Sciences, Shenzhen, 518000, China; Jiang Y., College of Informatics, Huazhong Agricultural University, Wuhan, 430070, China, Shenzhen Branch of Guangdong Laboratory of Lingnan Modern Agricultural Science and Technology, Shenzhen, 518000, China; Li X., Shenzhen Institute of Nutrition and Health, Huazhong Agricultural University, Shenzhen, 518000, China, Key Laboratory of Smart Farming for Agricultural Animals, Ministry of Agriculture and Rural Affairs, Wuhan, 430070, China; Ma Y., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Wuhan, 430070, China; Liu X., Shenzhen Institute of Nutrition and Health, Huazhong Agricultural University, Shenzhen, 518000, China, Agricultural Genomics Institute at Shenzhen (AGIS), Chinese Academy of Agricultural Sciences, Shenzhen, 518000, China","In smart livestock farming research, deep learning-based method for pig image instance segmentation is crucial for downstream tasks such as individual pig recognition, weight estimation, and behavior recognition. However, the model often requires a large number of pixel-wise annotated images for training, which imposes significant manpower and time costs. To address this issue, a weakly supervised pig segmentation strategy was proposed, creating a weakly supervised dataset, and introducing afeature extraction backbone network called RdsiNet. Firstly, the second-generation deformable convolution was incorporated into the ResNet - 50 residual module to expand the network's receptive field. Secondly, spatial attention mechanisms were used to strengthen the network's weight values for important features. Finally, the involution operator was introduced to enhance deep spatial information and connect feature maps with semantic information by using its spatial specificity and channel sharing mechanism. The efficacy of RdsiNet for weakly supervised datasets was demonstrated through ablation experiments and comparative experiments. The experiments showed that the mean value of mask AP under the Mask R - CNN reached 88. 6%, which was higher than a series of backbone networks such as ResNet - 50 and GCNet. Meanwhile, the mean value of mask AP under the Boxlnst reached 95.2%, which was also higher than that of ResNet - 50 which reached only 76. 7% . Furthermore, the display of image segmentation results of the test set showd RdsiNet also had better segmentation effect than ResNet - 50. In the case of pig stacking, RdsiNet can better distinguish each pig. When using the Boxlnst for training, RdsiNet can perfectly segment the outline of pigs, which was more conducive to downstream analysis. © 2023 Chinese Society of Agricultural Machinery. All rights reserved.","involution operator; pig; spatial attention mechanisms; weakly supervised instance segmentation","Behavioral research; Deep learning; Farms; Mammals; Semantic Segmentation; Attention mechanisms; Back-bone network; Down-stream; Involution operator; Livestock farming; Mean values; Pig; Spatial attention; Spatial attention mechanism; Weakly supervised instance segmentation; Semantics","","","","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85175796438"
"Jiang K.; Xie T.; Yan R.; Wen X.; Li D.; Jiang H.; Jiang N.; Feng L.; Duan X.; Wang J.","Jiang, Kailin (57226395602); Xie, Tianyu (57967823800); Yan, Rui (57204761634); Wen, Xi (57967976100); Li, Danyang (57281018200); Jiang, Hongbo (57459792500); Jiang, Ning (57968439600); Feng, Ling (57939686300); Duan, Xuliang (57191651500); Wang, Jianjun (36626733000)","57226395602; 57967823800; 57204761634; 57967976100; 57281018200; 57459792500; 57968439600; 57939686300; 57191651500; 36626733000","An Attention Mechanism-Improved YOLOv7 Object Detection Algorithm for Hemp Duck Count Estimation","2022","Agriculture (Switzerland)","12","10","1659","","","","127","10.3390/agriculture12101659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142011546&doi=10.3390%2fagriculture12101659&partnerID=40&md5=a3e2c4551c93f4c69aadee37d9dc4ce6","College of Science, Sichuan Agricultural University, Ya’an, 625000, China; College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; College of Electrical Engineering, Anhui Polytechnic University, Wuhu, 241000, China","Jiang K., College of Science, Sichuan Agricultural University, Ya’an, 625000, China; Xie T., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Yan R., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Wen X., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Li D., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Jiang H., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Jiang N., College of Electrical Engineering, Anhui Polytechnic University, Wuhu, 241000, China; Feng L., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Duan X., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Wang J., College of Science, Sichuan Agricultural University, Ya’an, 625000, China","Stocking density presents a key factor affecting livestock and poultry production on a large scale as well as animal welfare. However, the current manual counting method used in the hemp duck breeding industry is inefficient, costly in labor, less accurate, and prone to double counting and omission. In this regard, this paper uses deep learning algorithms to achieve real-time monitoring of the number of dense hemp duck flocks and to promote the development of the intelligent farming industry. We constructed a new large-scale hemp duck object detection image dataset, which contains 1500 hemp duck object detection full-body frame labeling and head-only frame labeling. In addition, this paper proposes an improved attention mechanism YOLOv7 algorithm, CBAM-YOLOv7, adding three CBAM modules to the backbone network of YOLOv7 to improve the network’s ability to extract features and introducing SE-YOLOv7 and ECA-YOLOv7 for comparison experiments. The experimental results show that CBAM-YOLOv7 had higher precision, and the recall, mAP@0.5, and mAP@0.5:0.95 were slightly improved. The evaluation index value of CBAM-YOLOv7 improved more than those of SE-YOLOv7 and ECA-YOLOv7. In addition, we also conducted a comparison test between the two labeling methods and found that the head-only labeling method led to the loss of a high volume of feature information, and the full-body frame labeling method demonstrated a better detection effect. The results of the algorithm performance evaluation show that the intelligent hemp duck counting method proposed in this paper is feasible and can promote the development of smart reliable automated duck counting. © 2022 by the authors.","attention mechanism; deep learning; hemp duck count; object detection; YOLOv7","","","","J. Wang; College of Science, Sichuan Agricultural University, Ya’an, 625000, China; email: jianjunw@sicau.edu.cn","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142011546"
"Delplanque A.; Lamprey R.; Foucher S.; Théau J.; Lejeune P.","Delplanque, Alexandre (57226592039); Lamprey, Richard (8744656800); Foucher, Samuel (6701728686); Théau, Jérôme (6507809197); Lejeune, Philippe (8700431500)","57226592039; 8744656800; 6701728686; 6507809197; 8700431500","Surveying wildlife and livestock in Uganda with aerial cameras: Deep Learning reduces the workload of human interpretation by over 70%","2023","Frontiers in Ecology and Evolution","11","","1270857","","","","2","10.3389/fevo.2023.1270857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178968026&doi=10.3389%2ffevo.2023.1270857&partnerID=40&md5=8224e98ba2742796bfaf620d965d381e","TERRA Teaching and Research Centre – Forest Is Life, Gembloux Agro-Bio Tech, University of Liège (ULiège), Gembloux, Belgium; Department of Natural Resources, Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands; Department of Applied Geomatics, Université de Sherbrooke, Sherbrooke, QC, Canada; Quebec Centre for Biodiversity Science (QCBS), Stewart Biology, McGill University, Montréal, QC, Canada","Delplanque A., TERRA Teaching and Research Centre – Forest Is Life, Gembloux Agro-Bio Tech, University of Liège (ULiège), Gembloux, Belgium; Lamprey R., Department of Natural Resources, Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands; Foucher S., Department of Applied Geomatics, Université de Sherbrooke, Sherbrooke, QC, Canada; Théau J., Department of Applied Geomatics, Université de Sherbrooke, Sherbrooke, QC, Canada, Quebec Centre for Biodiversity Science (QCBS), Stewart Biology, McGill University, Montréal, QC, Canada; Lejeune P., TERRA Teaching and Research Centre – Forest Is Life, Gembloux Agro-Bio Tech, University of Liège (ULiège), Gembloux, Belgium","As the need to accurately monitor key-species populations grows amid increasing pressures on global biodiversity, the counting of large mammals in savannas has traditionally relied on the Systematic-Reconnaissance-Flight (SRF) technique using light aircrafts and human observers. However, this method has limitations, including non-systematic human errors. In recent years, the Oblique-Camera-Count (OCC) approach developed in East Africa has utilized cameras to capture high-resolution imagery replicating aircraft observers’ oblique view. Whilst demonstrating that human observers have missed many animals, OCC relies on labor-intensive human interpretation of thousands of images. This study explores the potential of Deep Learning (DL) to reduce the interpretation workload associated with OCC surveys. Using oblique aerial imagery of 2.1 hectares footprint collected during an SRF-OCC survey of Queen Elizabeth Protected Area in Uganda, a DL model (HerdNet) was trained and evaluated to detect and count 12 wildlife and livestock mammal species. The model’s performance was assessed both at the animal instance-based and image-based levels, achieving accurate detection performance (F1 score of 85%) in positive images (i.e. containing animals) and reducing manual interpretation workload by 74% on a realistic dataset showing less than 10% of positive images. However, it struggled to differentiate visually related species and overestimated animal counts due to false positives generated by landscape items resembling animals. These challenges may be addressed through improved training and verification processes. The results highlight DL’s potential to semi-automate processing of aerial survey wildlife imagery, reducing manual interpretation burden. By incorporating DL models into existing counting standards, future surveys may increase sampling efforts, improve accuracy, and enhance aerial survey safety. Copyright © 2023 Delplanque, Lamprey, Foucher, Théau and Lejeune.","aerial survey; animal conservation; convolutional neural networks; Deep Learning; livestock; object detection; remote sensing; wildlife","","Fund for Research Training in Industry and Agriculture; Uganda Conservation Foundation; Uganda Wildlife Authority; Vulcan Inc.; WildSpace-Image-Analytics team of Uganda; University of Central Florida, UCF; Fonds pour la Formation à la Recherche dans l’Industrie et dans l’Agriculture, FRIA; Université de Liège, ULg","Funding text 1: The author(s) declare financial support was received for the research, authorship, and/or publication of this article. The work of A. Delplanque was supported under a grant from the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS). The annotation work was supported under a grant from Global Conservation of California, USA to R. Lamprey and the WildSpace-Image-Analytics team of Uganda ( www.wildspace-image-analytics.com ). Sharing of annotation data was conducted under a Memorandum of Understanding between Uganda Conservation Foundation (UCF) and the University of Liege. The original aerial 2018 survey of QENP, from which this experimental imagery was collected, was funded by UCF with support from Global Conservation, Vulcan Inc., Save the Elephants and the Uganda Wildlife Authority. Acknowledgments ; Funding text 2: The author(s) declare financial support was received for the research, authorship, and/or publication of this article. The work of A. Delplanque was supported under a grant from the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS). The annotation work was supported under a grant from Global Conservation of California, USA to R. Lamprey and the WildSpace-Image-Analytics team of Uganda (www.wildspace-image-analytics.com). Sharing of annotation data was conducted under a Memorandum of Understanding between Uganda Conservation Foundation (UCF) and the University of Liege. The original aerial 2018 survey of QENP, from which this experimental imagery was collected, was funded by UCF with support from Global Conservation, Vulcan Inc., Save the Elephants and the Uganda Wildlife Authority.; Funding text 3: The authors declare that this study received funding from the not-for-profit organizations Uganda Conservation Foundation, Global Conservation, Save the Elephants, Vulcan Inc. and Uganda Wildlife Authority. The funders were not involved in the study design, collection, analysis, interpretation of data, the writing of this article or the decision to submit it for publication. ","A. Delplanque; TERRA Teaching and Research Centre – Forest Is Life, Gembloux Agro-Bio Tech, University of Liège (ULiège), Gembloux, Belgium; email: alexandre.delplanque@uliege.be; R. Lamprey; Department of Natural Resources, Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands; email: lamprey.richard@gmail.com","","Frontiers Media SA","2296701X","","","","English","Front. ecol. evol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85178968026"
"Eryigit R.; AR Y.; Tugrul B.","Eryigit, Recep (6602419330); AR, Yilmaz (35730956900); Tugrul, Bulent (55258504400)","6602419330; 35730956900; 55258504400","Classification of Trifolium Seeds by Computer Vision Methods","2023","WSEAS Transactions on Systems","22","","","313","320","7","3","10.37394/23202.2023.22.34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153952841&doi=10.37394%2f23202.2023.22.34&partnerID=40&md5=7455ead74361e6333236a3c4f2df1e0b","Department of Computer Engineering, Ankara University, Golbasi, Ankara, Turkey","Eryigit R., Department of Computer Engineering, Ankara University, Golbasi, Ankara, Turkey; AR Y., Department of Computer Engineering, Ankara University, Golbasi, Ankara, Turkey; Tugrul B., Department of Computer Engineering, Ankara University, Golbasi, Ankara, Turkey","Traditional machine learning methods have been extensively used in computer vision applications. However, recent improvements in computer technology have changed this trend. The dominance of deep learning methods in the field is observed when state-of-the-art studies are examined. This study employs traditional computer vision methods and deep learning to classify five different types of Trifolium seeds. Trifolium, the leading food for nutritious dairy products, plays an essential role in livestock in some parts of the world. First, an image data set consisting of 1903 images belonging to five different species of Trifolium was created. Descriptive and quantitative morphological features of each species are extracted using image-processing techniques. Then a feature matrix was created using eight different features. After feature selection and transformation, unnecessary and irrelevant features were removed from the data set to build more accurate and robust classification models. Four common and frequently applied classification algorithms created a prediction model in the seed data set. In addition, the same dataset was trained using VGG19, a convolutional neural network. Finally, the performance metrics of each classifier were computed and evaluated. The decision tree has the worst accuracy among the four traditional methods, 92.07%. On the other hand, Artificial Neural Network has the highest accuracy with 94.59%. As expected, VGG19 outperforms all traditional methods with 96.29% accuracy. However, as the results show, traditional methods can also produce results close to the deep learning methods. © 2023 PAGEPress Publications. All rights reserved.","Classification; Computer Vision; Deep Learning; Machine Learning; Trifolium","","","","","","World Scientific and Engineering Academy and Society","11092777","","","","English","WSEAS Trans. Syst.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85153952841"
"Xiong R.; Zheng Y.; Chen N.; Tian Q.; Liu W.; Han F.; Jiang S.; Lu M.; Zheng Y.","Xiong, Rui (57211660086); Zheng, Yi (56409084200); Chen, Nengwang (55961165600); Tian, Qing (57218352947); Liu, Wei (57207219516); Han, Feng (36664584200); Jiang, Shijie (57188877493); Lu, Mengqian (55872663900); Zheng, Yan (7404837907)","57211660086; 56409084200; 55961165600; 57218352947; 57207219516; 36664584200; 57188877493; 55872663900; 7404837907","Predicting Dynamic Riverine Nitrogen Export in Unmonitored Watersheds: Leveraging Insights of AI from Data-Rich Regions","2022","Environmental Science and Technology","56","14","","10530","10542","12","33","10.1021/acs.est.2c02232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134720781&doi=10.1021%2facs.est.2c02232&partnerID=40&md5=f54c06dcf88bbf28e25db1718582c072","School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Hong Kong, 999077, Hong Kong; Shenzhen Municipal Engineering Lab of Environmental IoT Technologies, Southern University of Science and Technology, Guangdong Province, Shenzhen, 518055, China; Fujian Provincial Key Laboratory for Coastal Ecology and Environmental Studies, College of the Environment and Ecology, Xiamen University, Xiamen, 361102, China; Department of Computational Hydrosystems, Helmholtz Centre for Environmental Research, Leipzig, 04318, Germany","Xiong R., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Hong Kong, 999077, Hong Kong; Zheng Y., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China, Shenzhen Municipal Engineering Lab of Environmental IoT Technologies, Southern University of Science and Technology, Guangdong Province, Shenzhen, 518055, China; Chen N., Fujian Provincial Key Laboratory for Coastal Ecology and Environmental Studies, College of the Environment and Ecology, Xiamen University, Xiamen, 361102, China; Tian Q., Fujian Provincial Key Laboratory for Coastal Ecology and Environmental Studies, College of the Environment and Ecology, Xiamen University, Xiamen, 361102, China; Liu W., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Han F., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Jiang S., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Computational Hydrosystems, Helmholtz Centre for Environmental Research, Leipzig, 04318, Germany; Lu M., Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Hong Kong, 999077, Hong Kong; Zheng Y., School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China","Terrestrial export of nitrogen is a critical Earth system process, but its global dynamics remain difficult to predict at a high spatiotemporal resolution. Here, we use deep learning (DL) to model daily riverine nitrogen export in response to hydrometeorological and anthropogenic drivers. Long short-term memory (LSTM) models for the daily concentration and flux of dissolved inorganic nitrogen (DIN) were built in a coastal watershed in southeastern China with a typical subtropical monsoon climate. The DL models exhibited excellent accuracy for both DIN concentration and flux, with Nash-Sutcliffe efficiency coefficients (NSEs) up to 0.67 and 0.92, respectively, a performance unlikely to be achieved by generic process-based models with comparable data quality. The flux model ensemble, without retraining, performed well (mean NSE = 0.32-0.84) in seven distinct watersheds in Asia, Europe, and North America, and retraining with multi-watershed data further improved the lowest NSE from 0.32 to 0.68. DL interpretation confirmed that interbasin consistency of riverine nitrogen export exists across different continents, which stems from the similarities in rainfall-runoff relationships. The multi-watershed flux model projects 0.60-12.4% increases in the nitrogen export to oceans from the studied watersheds under a 20% increase in fertilizer consumption, which rises to 6.7-20.1% with a 10% increase in runoff, indicating the synergistic effect of human activities and climate change. The DL-based method represents a successful case of explainable artificial intelligence in environmental science, providing a potential shortcut to a consistent understanding of the global daily-resolution dynamics of riverine nitrogen export under the currently limited data conditions. © 2022 American Chemical Society.","artificial intelligence; deep learning; LSTM; nitrogen; nonpoint sources; riverine export; transfer learning","Artificial Intelligence; China; Environmental Monitoring; Fertilizers; Humans; Nitrogen; Rivers; Asia; China; Europe; North America; Climate change; Climate models; Earth system models; Learning systems; Runoff; Watersheds; fertilizer; ground water; inorganic compound; nitrogen; rain; nitrogen; Deep learning; Dissolved inorganic nitrogens; Earth systems; Efficiency coefficient; Flux modeling; Nitrogen export; Nonpoint sources; Riverine export; Riverine nitrogens; Transfer learning; artificial intelligence; human activity; machine learning; nitrogen; nonpoint source pollution; pollutant transport; prediction; river pollution; watershed; Article; artificial intelligence; Asia; China; climate change; coastal waters; concentration (parameter); deep learning; dry season; environmental science; Europe; fertilizer application; growing season; human activities; human impact (environment); livestock; long short term memory network; meteorology; monsoon climate; nitrogen concentration; North America; ocean environment; rainy season; river basin; runoff; runoff model (hydrology); seasonal variation; transfer of learning; water monitoring; watershed; watershed management; environmental monitoring; human; river; Long short-term memory","National Natural Science Foundation of China, NSFC, (51961125203, 92047302); National Natural Science Foundation of China, NSFC; Science, Technology and Innovation Commission of Shenzhen Municipality, (KCXFZ202002011006491); Science, Technology and Innovation Commission of Shenzhen Municipality","This work was financially supported by the National Natural Science Foundation of China (nos. 51961125203 and 92047302) and the Shenzhen Science and Technology Innovation Commission (no. KCXFZ202002011006491).","Y. Zheng; School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; email: zhengy@sustech.edu.cn","","American Chemical Society","0013936X","","ESTHA","35772808","English","Environ. Sci. Technol.","Article","Final","","Scopus","2-s2.0-85134720781"
"Li G.; Erickson G.E.; Xiong Y.","Li, Guoming (57200960751); Erickson, Galen E. (7102189233); Xiong, Yijie (56421066000)","57200960751; 7102189233; 56421066000","Individual Beef Cattle Identification Using Muzzle Images and Deep Learning Techniques","2022","Animals","12","11","1453","","","","37","10.3390/ani12111453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131457096&doi=10.3390%2fani12111453&partnerID=40&md5=e72100b8e6752235b9ca0a225b25a138","Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, 50011, IA, United States; Department of Animal Science, University of Nebraska‐Lincoln, Lincoln, 68583, NE, United States; Department of Biological Systems Engineering, University of Nebraska‐Lincoln, Lincoln, 68583, NE, United States","Li G., Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, 50011, IA, United States; Erickson G.E., Department of Animal Science, University of Nebraska‐Lincoln, Lincoln, 68583, NE, United States; Xiong Y., Department of Animal Science, University of Nebraska‐Lincoln, Lincoln, 68583, NE, United States, Department of Biological Systems Engineering, University of Nebraska‐Lincoln, Lincoln, 68583, NE, United States","Individual feedlot beef cattle identification represents a critical component in cattle trace-ability in the supply food chain. It also provides insights into tracking disease trajectories, ascertain-ing ownership, and managing cattle production and distribution. Animal biometric solutions, e.g., identifying cattle muzzle patterns (unique features comparable to human fingerprints), may offer noninvasive and unique methods for cattle identification and tracking, but need validation with advancement in machine learning modeling. The objectives of this research were to (1) collect and publish a high‐quality dataset for beef cattle muzzle images, and (2) evaluate and benchmark the performance of recognizing individual beef cattle with a variety of deep learning models. A total of 4923 muzzle images for 268 US feedlot finishing cattle (>12 images per animal on average) were taken with a mirrorless digital camera and processed to form the dataset. A total of 59 deep learning image classification models were comparatively evaluated for identifying individual cattle. The best accuracy for identifying the 268 cattle was 98.7%, and the fastest processing speed was 28.3 ms/im-age. Weighted cross‐entropy loss function and data augmentation can increase the identification accuracy of individual cattle with fewer muzzle images for model development. In conclusion, this study demonstrates the great potential of deep learning applications for individual cattle identification and is favorable for precision livestock management. Scholars are encouraged to utilize the published dataset to develop better models tailored for the beef cattle industry. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","animal biometrics; cognitive science; computer vision; machine learning; pattern recognition; precision livestock management","Article; beef cattle; biometry; computer vision; convolutional neural network; deep learning; diagnostic accuracy; entropy; environmental exposure; facial recognition; image analysis; image processing; learning algorithm; livestock; machine learning; nonhuman; pattern recognition; processing speed; snout","Nebraska Agricultural Experiment Station; Ne‐ braska Agricultural Experiment Station, (29448); University of Nebraska‐Lincoln; Iowa State University, ISU; College of Agriculture and Life Sciences, Iowa State University, CALS","Funding text 1: Funding: This work was partially supported by faculty start‐up funds provided internally by the Institution of Agriculture and Natural Resources at University of Nebraska‐Lincoln and the College of Agriculture and Life Sciences, Iowa State University. This work was also a product of the Ne‐ braska Agricultural Experiment Station (NEAES) Project Number 29448, sponsored by the Agricul‐ ture and Natural Resources Hatch Multistate Enhanced Program.; Funding text 2: This work was partially supported by faculty start‐up funds provided internally by the Institution of Agriculture and Natural Resources at University of Nebraska‐Lincoln and the College of Agriculture and Life Sciences, Iowa State University. This work was also a product of the Nebraska Agricultural Experiment Station (NEAES) Project Number 29448, sponsored by the Agriculture and Natural Resources Hatch Multistate Enhanced Program.","Y. Xiong; Department of Animal Science, University of Nebraska‐Lincoln, Lincoln, 68583, United States; email: yijie.xiong@unl.edu","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131457096"
"Aye C.C.; Zin T.T.; Kobayashi I.","Aye, Cho Cho (57956570000); Zin, Thi Thi (6506258245); Kobayashi, Ikuo (24174822700)","57956570000; 6506258245; 24174822700","BLACK COW TRACKING BY USING DEEP LEARNING-BASED ALGORITHMS","2022","ICIC Express Letters, Part B: Applications","13","12","","1313","1319","6","6","10.24507/icicelb.13.12.1313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141415571&doi=10.24507%2ficicelb.13.12.1313&partnerID=40&md5=8d7e3e531eaace27e9c72be6e6b5006d","Graduate School of Engineering University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan; Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan","Aye C.C., Graduate School of Engineering University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan; Zin T.T., Graduate School of Engineering University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan; Kobayashi I., Graduate School of Engineering University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan, Sumiyoshi Livestock Science Station, Field Science Center, Faculty of Agriculture, University of Miyazaki, 1-1, Gakuen Kibanadai-Nishi, Miyazaki, 889-2192, Japan","Raising livestock is essential in the farming industry to meet the consumer’s requirement. Livestock monitoring system is useful to monitor their health without need-ing too much manpower. Thus, livestock tracking becomes one of the vital parts of livestock monitoring system. The objective of this proposed system is to track black cows based on detected features. Here, the YOLOv5 (You Only Look Once) model was used in detection phase to detect cow regions and Deep SORT (Simple Online Real-time Tracking) was applied to tracking the target cows in every consecutive frame. In Deep SORT, it includes appearance feature model to recognize cow’s visual appearance such as shape, size, and pose. The proposed system was best trained by adopting transfer learning method. The detection model achieves an accuracy of 0.995 mAP@0.5 whereas the tracking model gets the performance results in video-1 and video-2 with 99.4% and 98.9%, respectively. © 2022 ICIC International.","Black cow; Cow detection and tracking; Deep SORT; Transfer learning; YOLOv5","","","","T.T. Zin; Graduate School of Engineering University of Miyazaki, Miyazaki, 1-1, Gakuen Kibanadai-Nishi, 889-2192, Japan; email: thithi@cc.miyazaki-u.ac.jp","","ICIC International","21852766","","","","English","ICIC Express Lett Part B Appl.","Article","Final","","Scopus","2-s2.0-85141415571"
"Montenegro S.; Pusdá-Chulde M.; Caranqui-Sánchez V.; Herrera-Tapia J.; Ortega-Bustamante C.; García-Santillán I.","Montenegro, Sebastián (58305133700); Pusdá-Chulde, Marco (57200384175); Caranqui-Sánchez, Víctor (57200381252); Herrera-Tapia, Jorge (56888806700); Ortega-Bustamante, Cosme (57204808907); García-Santillán, Iván (57192817984)","58305133700; 57200384175; 57200381252; 56888806700; 57204808907; 57192817984","Android Mobile Application for Cattle Body Condition Score Using Convolutional Neural Networks","2023","Communications in Computer and Information Science","1705 CCIS","","","91","105","14","0","10.1007/978-3-031-32213-6_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161120974&doi=10.1007%2f978-3-031-32213-6_7&partnerID=40&md5=4f4ab3179484d32129b114d6a80d1887","Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; Facultad de Ciencias Informáticas, Universidad Laica Eloy Alfaro de Manabí, Manta, Ecuador","Montenegro S., Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; Pusdá-Chulde M., Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; Caranqui-Sánchez V., Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; Herrera-Tapia J., Facultad de Ciencias Informáticas, Universidad Laica Eloy Alfaro de Manabí, Manta, Ecuador; Ortega-Bustamante C., Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; García-Santillán I., Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador","The livestock sector is the set of activities related to raising cattle to take advantage of reproduction, dairy production, and beef benefits. In this sector, a vital factor is a cattle’s body condition, considered a nutritional indicator since the subcutaneous body fat level found in certain anatomical points determines the animal’s thinness or fatness levels. Therefore, it is a clue in defining nutritional deficiencies, a common problem in the livestock industry. Providing a timely cattle body condition assessment may prevent nutritional issues that improve cattle’s health, reproduction processes, and dairy production. This study aims to develop an Android mobile app assessing Bos Taurus cattle body condition through computer-vision techniques and Deep Learning. The app was developed following the XP agile methodology and the Flutter, TensorFlow, and Keras frameworks. For this end, three CNN models were trained: Yolo, MobileNet, and VGG-16, for different tasks within the App. Models were evaluated using quantitative metrics such as Confusion Matrix, ROC Curve, CED Curve, and AUC. The ISO/IEC 25022 standard and USE questionnaire were used to assess the mobile app quality in use. The mobile app achieved an accuracy of 0.88 between the manual body condition score (BCS) and the one predicted. Results proved that this application enables anyone to adequately assess cattle’s body condition using a conventional mobile device, contributing to the innovation of the livestock sector. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Android mobile app; Body condition score; CNN; Deep Learning; ISO 25022; MobileNet; VGG-16; Yolo","Agriculture; Cell proliferation; Convolutional neural networks; Deep learning; E-learning; ISO Standards; Learning systems; Android mobile app; Body condition; Body condition score; Deep learning; ISO 25022; Mobile app; Mobile applications; Mobilenet; VGG-16; Yolo; Android (operating system)","","","I. García-Santillán; Facultad de Ingeniería en Ciencias Aplicadas, Universidad Técnica del Norte, Ibarra, Ecuador; email: idgarcia@utn.edu.ec","Narváez F.R.; Urgilés F.; Salgado-Guerrero J.P.; Bastos-Filho T.F.","Springer Science and Business Media Deutschland GmbH","18650929","978-303132212-9","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85161120974"
"Kumar Ghosh K.; Ul Islam M.F.; Efaz A.A.; Chakrabarty A.; Hossain S.","Kumar Ghosh, Kawshik (58413369500); Ul Islam, Md. Fahim (58930069100); Efaz, Abrar Ahsan (58415669200); Chakrabarty, Amitabha (35108854200); Hossain, Shahriar (57221953105)","58413369500; 58930069100; 58415669200; 35108854200; 57221953105","Real-Time Mastitis Detection in Livestock using Deep Learning and Machine Learning Leveraging Edge Devices","2023","International Symposium on Medical Information and Communication Technology, ISMICT","2023-May","","","","","","2","10.1109/ISMICT58261.2023.10152110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163724255&doi=10.1109%2fISMICT58261.2023.10152110&partnerID=40&md5=87e5ea8de74d995e3688f786e8a828d2","Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh","Kumar Ghosh K., Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh; Ul Islam M.F., Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh; Efaz A.A., Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh; Chakrabarty A., Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh; Hossain S., Brac University, Department of Computer Science and Engineering, Dhaka, Bangladesh","Livestock production is a crucial part of the global economy with a worth of estimated 1.4 trillion. It provides livelihoods for 1.3 billion people and supports 600 million poor rural household farmers in developing countries. In Bangladesh, it contributes 6.5% to the country's GDP. However, this industry faces substantial financial setbacks when contagious diseases transmit among their livestock. One of the most common and expensive diseases affecting the livestock industry is Bovine Mastitis. This paper presents a real-time system for detecting bovine mastitis in livestock using deep learning (dl) and machine learning (ml) techniques. The system aims to provide a timely and accurate diagnosis of mastitis, ultimately reducing costs and improving the efficiency of treatment. By utilizing dl and ml techniques, the system is able to analyze data collected from edge devices and make accurate predictions about the presence of mastitis. The dataset that has been used for the classification contains both an Image dataset consisting of 1341 images and a Numerical dataset that had been taken from 1100 cows over a period of six days. The edge device utilizes sensors and cameras to collect data from the cow, which is then processed through ml and dl algorithms using Raspberry Pi and cloud computing respectively, and then displays if the cow is infected with mastitis or not. Inception V3 and RandomForest algorithms were used for dl and ml, respectively, and had an accuracy of 99.34% and 99% respectively. The proposed system has the potential to significantly reduce the economic impact of this disease in the dairy industry of Bangladesh and other developing countries by providing timely and accurate diagnosis and helping to improve treatment efficiency and protect the health and productivity of livestock animals. © 2023 IEEE.","deep learning; edge devices; livestock; machine learning; mastitis","Agriculture; Classification (of information); Deep learning; Diagnosis; Diseases; Display devices; Economics; Efficiency; Interactive computer systems; Learning systems; Mammals; Real time systems; Bangladesh; Deep learning; Edge device; Global economies; Livestock; Livestock production; Machine learning techniques; Machine-learning; Mastiti; Real- time; Developing countries","","","","","IEEE Computer Society","2326828X","979-835030417-6","","","English","Int. Symp. Med. Inf. Commun. Technol., ISMICT","Conference paper","Final","","Scopus","2-s2.0-85163724255"
"Kawada H.; Higuchi Y.; Nishiyama M.; Iwai Y.","Kawada, Hirohumi (57226716721); Higuchi, Yuito (58763867600); Nishiyama, Masashi (57191888628); Iwai, Yoshio (57201788506)","57226716721; 58763867600; 57191888628; 57201788506","Estimation of Beef Marbling Standard for Live Cattle Using Multi-input CNN with Ultrasound Images and Individual Cattle Information","2023","GCCE 2023 - 2023 IEEE 12th Global Conference on Consumer Electronics","","","","107","110","3","1","10.1109/GCCE59613.2023.10315670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179760063&doi=10.1109%2fGCCE59613.2023.10315670&partnerID=40&md5=d36b05c573b6d0e84343c1326f2838a0","Tottori University, Graduate School of Sustainability Science, Tottori, Japan","Kawada H., Tottori University, Graduate School of Sustainability Science, Tottori, Japan; Higuchi Y., Tottori University, Graduate School of Sustainability Science, Tottori, Japan; Nishiyama M., Tottori University, Graduate School of Sustainability Science, Tottori, Japan; Iwai Y., Tottori University, Graduate School of Sustainability Science, Tottori, Japan","There is great interest among livestock farmers in estimating the value of live beef cattle prior to slaughter. In this paper, we attempt to design a network structure to estimate the Beef Marbling Standard (BMS) from ultrasonic images of live beef cattle and individual information such as producer and father cattle. We demonstrated that our multi-input neural network with ultrasound images and individual information obtained a high accuracy in BMS estimation.  © 2023 IEEE.","beef marbling standard; deep learning; individual information; ultrasound images","Agriculture; Deep learning; Beef cattle; Beef marbling standard; Deep learning; High-accuracy; Individual information; Livestock farmers; Multiinput; Network structures; Neural-networks; Ultrasound images; Beef","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835034018-1","","","English","GCCE - IEEE Glob. Conf. Consum. Electron.","Conference paper","Final","","Scopus","2-s2.0-85179760063"
"Duan Q.; Zhao Z.; Jiang T.; Gui X.; Zhang Y.","Duan, Qingling (24536645900); Zhao, Zhiqing (58679305300); Jiang, Tao (58593560300); Gui, Xiaofei (58678929900); Zhang, Yuhang (57839569900)","24536645900; 58679305300; 58593560300; 58678929900; 57839569900","Behavior Recognition Method of Beef Cattle Based on SNSS-YOLO v7; [基于SNSS YOLO v7的肉牛行为识别方法]","2023","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","54","10","","266","274and347","274081","4","10.6041/j.issn.1000-1298.2023.10.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175624136&doi=10.6041%2fj.issn.1000-1298.2023.10.026&partnerID=40&md5=09ddc0e10557cc99964eb1c31457aad3","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Smart Breeding Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Beijing Futong Internet Technology Group Co., Ltd., Beijing, 101300, China","Duan Q., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Smart Breeding Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Zhao Z., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Smart Breeding Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; Jiang T., Beijing Futong Internet Technology Group Co., Ltd., Beijing, 101300, China; Gui X., Beijing Futong Internet Technology Group Co., Ltd., Beijing, 101300, China; Zhang Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, Key Laboratory of Smart Breeding Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China","The behavior of beef cattle in the process of activity is the comprehensive embodiment of the health status of beef cattle. The rapid and accurate recognition of beef cattle behavior plays an important role in the prevention and control of beef cattle diseases, their own development assessment and estrus monitoring. Behavior recognition technology based on machine vision has been applied to behavior recognition of livestock and poultry breeding because of its lossless and fast characteristics. However, the existing behavior recognition methods of beef cattle based on machine vision were usually studied for a single cow or a single behavior, and there were problems such as large amount of calculation. In view of the above problems, a method based on Slim - Neck & Separated and enhancement attention module & Simplified spatial pyramid pooling-fast - YOLO v7 (SNSS - YOLO v7) was proposed. Firstly, seven common behavior images of beef cattle, such as mounting, lying, searching, standing, walking, licking and fighting, were collected in the complex environment to construct a beef cattle behavior dataset. Secondly, the Slim - Neck structure was used in the neck of YOLO v7 to reduce the amount of calculation and parameters of the model. Then, separated and enhancement attention module (SEAM) was introduced into the head to enhance the detection effect after the output of the Neck layer. Finally, the simplified spatial pyramid pooling-fast (SimSPPF) module was used to replace the spatial pyramid pooling cross stage partial conv (SPPCSPC) module of the original YOLO v7, which further reduced the number of parameters while increased the receptive field. Tested on the self-built dataset, the mean average precision (mAP&0 5) of the beef cattle behavior recognition method proposed was 95. 2%, the model size was 39 MB, and the number of parameters was 1. 926 x 107. Compared with YOLO v7, YOLO v6m, YOLO v5m, YOLOX-S, TPH - YOLO v5 and Faster R - CNN, the model size was reduced by 47. 9%, 45. 4%, 7. 6%, 43. 1 %, 57. 8% and 92. 5%, respectively. The mean average precision (mAP@0 5) was improved by 1. 4 percentage points, 2. 2 percentage points, 3. 1 percentage points, 13. 7 percentage points, 1.9 percentage points, and 4.5 percentage points, respectively. The experimental results showed that the proposed method can achieve accurate recognition of beef cattle behavior, and can be deployed on devices with limited computing resources to provide support for intelligent livestock breeding. © 2023 Chinese Society of Agricultural Machinery. All rights reserved.","beef cattle; behavior recognition; deep learning; multi-object recognition; YOLO v7","Agriculture; Behavioral research; Deep learning; Disease control; Object recognition; Social networking (online); Beef cattle; Behaviour recognition; Deep learning; Multi-object recognition; Multiobject; Objects recognition; Percentage points; Recognition methods; Spatial pyramids; YOLO v7; Computer vision","","","","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85175624136"
"Yang M.; Zhao J.","Yang, Mei (57875186900); Zhao, Jianmin (57446644300)","57875186900; 57446644300","Design of cow face recognition system for insurance business based on three-dimensional loss algorithm; [基于三元损失的保险业务牛脸识别系统的设计]","2022","Guangdianzi Jiguang/Journal of Optoelectronics Laser","33","8","","831","839","8","3","10.16136/j.joel.2022.08.0795","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137284290&doi=10.16136%2fj.joel.2022.08.0795&partnerID=40&md5=dca1e1fdb711ccc25e734feab925298d","College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China","Yang M., College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China; Zhao J., College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China","Aiming at the current problem of difficult cattle identity recognition in the survey and loss determination of livestock insurance industry, this paper combines deep learning framework with internet of things (IOT) technology to build cow face recognition system.Inception_Resnet_v1 is used as the deep learning network framework in the system, combined with Triplet Loss loss function, to complete the extraction of cow face features, and the identification of features by calculating the cosine distance. Based on flask and vue framework, we deploy cow face detection and feature extraction and verification model, develop cow face information database, and provide identity registration and verification web service. To verify the feasibility of the system, the cow face recognition dataset CFID200 is collected and produced. Under the condition that 20% of cow faces are not seen, the accuracy and verification rate of the cow face recognition system reaches more than 95%, which meets the need of cow face identification in insurance business. © 2022, The Editorial Department of Journal of Optoelectronics•Laser. All right reserved.","Convolutional network; Cow face recognition; Deep learning; Triplet loss","","","","J. Zhao; College of Information Engineering, Inner Mongolia University of Science and Technology, Baotou, 014010, China; email: zhao_jm@imust.edu.cn","","Tianjin University","10050086","","GUJIE","","Chinese","Guangdianzi Jiguang","Article","Final","","Scopus","2-s2.0-85137284290"
"Özden C.; Bulut M.; Çanga Boğa D.; Boğa M.","Özden, Cevher (57402916500); Bulut, Mutlu (58075900600); Çanga Boğa, Demet (57210442412); Boğa, Mustafa (24279437800)","57402916500; 58075900600; 57210442412; 24279437800","Determination of Non-Digestible Parts in Dairy Cattle Feces Using U-NET and F-CRN Architectures","2023","Veterinary Sciences","10","1","32","","","","0","10.3390/vetsci10010032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146741030&doi=10.3390%2fvetsci10010032&partnerID=40&md5=726048f6f74901c3b44ba8a2fb21b2d3","Computer Engineering, Akdeniz University, Antalya, 07040, Turkey; Department of Agricultural Engineering, Çukurova University, Adana, 01120, Turkey; Department of Chemistry and Chemical Processing, Osmaniye Korkut Ata University, Osmaniye, 80050, Turkey; Bor Vocational School, Niğde Ömer Halisdemir University, Niğde, 51700, Turkey","Özden C., Computer Engineering, Akdeniz University, Antalya, 07040, Turkey; Bulut M., Department of Agricultural Engineering, Çukurova University, Adana, 01120, Turkey; Çanga Boğa D., Department of Chemistry and Chemical Processing, Osmaniye Korkut Ata University, Osmaniye, 80050, Turkey; Boğa M., Bor Vocational School, Niğde Ömer Halisdemir University, Niğde, 51700, Turkey","Deep learning algorithms can now be used to identify, locate, and count items in an image thanks to advancements in image processing technology. The successful application of image processing technology in different fields has attracted much attention in the field of agriculture in recent years. This research was done to ascertain the number of indigestible cereal grains in animal feces using an image processing method. In this study, a regression-based way of object counting was used to predict the number of cereal grains in the feces. For this purpose, we have developed two different neural network architectures based upon Fully Convolutional Regression Networks (FCRN) and U-Net. The images used in the study were obtained from three different dairy cows enterprises operating in Nigde Province. The dataset consists of the 277 distinct dropping images of dairy cows in the farm. According to findings of the study, both models yielded quite acceptable prediction accuracy with U-Net providing slightly better prediction with a MAE value of 16.69 in the best case, compared to 23.65 MAE value of FCRN with the same batch. © 2023 by the authors.","deep learning; image processing; images of feces; indigestible parts; livestock","article; cereal; dairy cattle; deep learning; feces; image processing; livestock; nonhuman; prediction","","","M. Bulut; Department of Agricultural Engineering, Çukurova University, Adana, 01120, Turkey; email: mtlbulut@gmail.com","","MDPI","23067381","","","","English","Vet. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85146741030"
"Zhao H.; Li P.; Chen Z.; Guo K.; Chen Y.; Su C.","Zhao, Hongyu (58806499600); Li, Pengfeng (58806499700); Chen, Zhenling (58806828300); Guo, Kai (58806548700); Chen, Yiwen (57247550400); Su, Chao (58796353500)","58806499600; 58806499700; 58806828300; 58806548700; 57247550400; 58796353500","Cow Face Recognition with Noise Labels Using Vision Transformer","2023","2023 IEEE 4th International Conference on Pattern Recognition and Machine Learning, PRML 2023","","","","93","98","5","0","10.1109/PRML59573.2023.10348318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182024337&doi=10.1109%2fPRML59573.2023.10348318&partnerID=40&md5=38d725d4ade1f4d7f330552a56a56df8","Sichuan University, College of Computer Science, Chengdu, China","Zhao H., Sichuan University, College of Computer Science, Chengdu, China; Li P., Sichuan University, College of Computer Science, Chengdu, China; Chen Z., Sichuan University, College of Computer Science, Chengdu, China; Guo K., Sichuan University, College of Computer Science, Chengdu, China; Chen Y., Sichuan University, College of Computer Science, Chengdu, China; Su C., Sichuan University, College of Computer Science, Chengdu, China","Animal face recognition is a crucial step for people to monitor and manage livestock more accurately and conveniently, playing an important role in intelligent breeding. Traditionally, the method of labeling animals with a different type of marks such as tags, tattoos and brands is generally applied to uniquely identify individual livestock. However, the traditional way of identification is high cost, low efficiency and unreliable. In this paper, we propose a deep learning approach based on a framework of semi-supervised noise-label learning to realize noisy cow face recognition, which adopts the Vision Transformer model that has recently become popular in the filed of computer vision. Our network is trained on a dataset that consists of 6 individual cows with a total of 3556 facial images. Due to the lack of adequate datasets, this is a challenging task for achieving high performance. To evaluate the robustness of our approach to noise, we compared it with several other noisy learning methods. Our results demonstrate that Vision Transformer can be used effectively for cow face recognition, and can achieve good performance even in the presence of noise. © 2023 IEEE.","Cow face recognition; Learning with noise labels; Semi-supervised learning; Vision Transformer","Agriculture; Animals; Computer vision; Deep learning; Learning systems; Cow face recognition; High costs; Labelings; Learning approach; Learning with noise label; Performance; Semi-supervised; Semi-supervised learning; Transformer modeling; Vision transformer; Face recognition","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835032430-3","","","English","IEEE Int. Conf. Pattern Recognit. Mach. Learn., PRML","Conference paper","Final","","Scopus","2-s2.0-85182024337"
"Jayakumar N.; Nithish Kumar R.; Ruthrakumar A.; Saikavya K.","Jayakumar, Nithya (58360210900); Nithish Kumar, R. (58403878300); Ruthrakumar, A. (58399752600); Saikavya, K. (58401810900)","58360210900; 58403878300; 58399752600; 58401810900","Biometric Muzzle Pattern Recognition for Cattle Identification using Deep Learning","2023","6th International Conference on Inventive Computation Technologies, ICICT 2023 - Proceedings","","","","34","40","6","2","10.1109/ICICT57646.2023.10134082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163536975&doi=10.1109%2fICICT57646.2023.10134082&partnerID=40&md5=fdfde13a87d356baf4aeae2c97a5866d","Department of Information Technology, K. S. Rangasamy College of Technology, Tiruchengode, India","Jayakumar N., Department of Information Technology, K. S. Rangasamy College of Technology, Tiruchengode, India; Nithish Kumar R., Department of Information Technology, K. S. Rangasamy College of Technology, Tiruchengode, India; Ruthrakumar A., Department of Information Technology, K. S. Rangasamy College of Technology, Tiruchengode, India; Saikavya K., Department of Information Technology, K. S. Rangasamy College of Technology, Tiruchengode, India","The ability to identify specific cows is a critical aspect of keeping cattle. Registration is essential for the production, distribution, and breeding of cows. It is dangerous to brand cows in the ear in the traditional manner. To solve this issue, we recommend using a biometric scanner for identification. Numerous studies have shown that the muzzle pattern can be used to distinguish animals and eliminate inconsistent patterns. This strategy might be effective in reducing spurious matching muzzle patterns. On the tip of their nose or muzzle, the majority of animals have a distinctive pattern. An animal's characteristic pattern is obvious to the unaided eye once it is born. Animals can be recognized by human muzzle patterns, just like with fingerprints. Systems for identifying animals are needed, as they would be useful for submitting loan and insurance applications. Based on a biometric muzzle scanner, the approach of machine learning, image processing, and computer vision has been assessed for identification purposes. © 2023 IEEE.","Animal Identification; Biometric Scanner; Computer Vision; Livestock Management; Muzzle Pattern","Agriculture; Animals; Computer vision; Deep learning; Animal identification; Biometric scanner; Image processing and computer vision; Livestock management; Machine-learning; Matchings; Muzzle pattern; Production distribution; Biometrics","Tamilnadu State Council For Science And Technology, TNSCST, (2022-2023)","We would also like to acknowledge , Tamil Nādu State Council for Science and Technology for approving grant to our project under Student Project Scheme 2022-2023.","","","Institute of Electrical and Electronics Engineers Inc.","","979-835039849-6","","","English","Int. Conf. Inven. Comput. Technol., ICICT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85163536975"
"Siniosoglou I.; Xouveroudis K.; Argyriou V.; Lagkas T.; Margounakis D.; Boulogeorgos A.-A.A.; Sarigiannidis P.","Siniosoglou, Ilias (57211407735); Xouveroudis, Konstantinos (57302252900); Argyriou, Vasileios (13806485100); Lagkas, Thomas (6507945237); Margounakis, Dimitrios (8383235700); Boulogeorgos, Alexandros-Apostolos A. (56072371700); Sarigiannidis, Panagiotis (12445587500)","57211407735; 57302252900; 13806485100; 6507945237; 8383235700; 56072371700; 12445587500","Applying Federated Learning on Decentralized Smart Farming: A Case Study","2023","2023 IEEE International Conference on Communications Workshops: Sustainable Communications for Renaissance, ICC Workshops 2023","","","","1295","1300","5","1","10.1109/ICCWorkshops57953.2023.10283681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177864591&doi=10.1109%2fICCWorkshops57953.2023.10283681&partnerID=40&md5=9d3ec00b85d00abf86fe2e1f66e5f3cf","University of Western Macedonia, Department of Electrical and Computer Engineering, Kozani, Greece; MetaMind Innovations P.C., R&d Department, Kozani, Greece; Kingston University, Department of Networks and Digital Media, Kingston upon Thames, United Kingdom; International Hellenic University, Department of Computer Science, Kavala Campus, Greece; Sidroco Holdings Ltd., Nicosia, Cyprus","Siniosoglou I., University of Western Macedonia, Department of Electrical and Computer Engineering, Kozani, Greece; Xouveroudis K., MetaMind Innovations P.C., R&d Department, Kozani, Greece; Argyriou V., Kingston University, Department of Networks and Digital Media, Kingston upon Thames, United Kingdom; Lagkas T., International Hellenic University, Department of Computer Science, Kavala Campus, Greece; Margounakis D., Sidroco Holdings Ltd., Nicosia, Cyprus; Boulogeorgos A.-A.A., University of Western Macedonia, Department of Electrical and Computer Engineering, Kozani, Greece; Sarigiannidis P., University of Western Macedonia, Department of Electrical and Computer Engineering, Kozani, Greece","In the field of Smart Agriculture, accurate time series forecasting is essential for farmers to gather and evaluate relevant information about various aspects of their work, such as the management of harvests, livestock, crops, water and soil. One commonly used method for trend forecasting in time series is the Long Short Term Memory (LSTM) Recurrent Neural Network (RNN) model, due to its ability to retain context for longer periods and enhance performance in context-intensive tasks. To further improve the results, the use of Federated Learning (FL) can be implemented, allowing multiple data providers to simultaneously train on a shared model while preserving data privacy. In this study, a Centralised Federated Learning System (CFLS) is leveraged, that implements and evaluates the efficacy of FL in smart agriculture through the use of datasets produced by such infrastructures. The system receives data from multiple clients and creates an optimised global model through model federation. Consequently, the federated approach is compared with the conventional local training to explore the potential of FL in real-time forecasting for the Smart Farming sector. © 2023 IEEE.","Animal Welfare; Crop Optimisation; Dataset; Deep Learning; Federated Learning; Forecasting; LSTM; Smart farming; Synthetic Data","Data privacy; Farms; Forecasting; Learning systems; Long short-term memory; Time series; Animal welfare; Crop optimization; Dataset; Decentralised; Deep learning; Federated learning; Optimisations; Smart agricultures; Smart farming; Synthetic data; Crops","Horizon 2020 Framework Programme, H2020, (957406)","ACKNOWLEDGEMENT This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 957406 (TERMINET).","","","Institute of Electrical and Electronics Engineers Inc.","","979-835033307-7","","","English","IEEE Int. Conf. Commun. Workshops: Sustain. Commun. Renaiss., ICC Workshops","Conference paper","Final","","Scopus","2-s2.0-85177864591"
"Siche R.; Siche N.","Siche, Raúl (22987015000); Siche, Nikol (58202021400)","22987015000; 58202021400","The language model based on sensitive artificial intelligence - ChatGPT: Bibliometric analysis and possible uses in agriculture and livestock; [El modelo de lenguaje basado en inteligencia artificial sensible - ChatGPT: Análisis bibliométrico y posibles usos en la agricultura y pecuaria]","2023","Scientia Agropecuaria","14","1","","111","116","5","12","10.17268/sci.agropecu.2023.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153855335&doi=10.17268%2fsci.agropecu.2023.010&partnerID=40&md5=5645f47b8aed2c556c62f24ed8a8fff7","Escuela de Ingeniería Agroindustrial, Facultad de Ciencias Agropecuarias, Universidad Nacional de Trujillo, Trujillo, Peru","Siche R., Escuela de Ingeniería Agroindustrial, Facultad de Ciencias Agropecuarias, Universidad Nacional de Trujillo, Trujillo, Peru; Siche N., Escuela de Ingeniería Agroindustrial, Facultad de Ciencias Agropecuarias, Universidad Nacional de Trujillo, Trujillo, Peru","ChatGPT adds to the list of artificial intelligence-based systems designed to perform specific tasks and answer questions by interacting with users (Apple's Siri, Amazon's Alexa, Google's Assistant and Bard, Microsoft's Cortana, IBM's Watson, Bixby from Samsung, among others). ChatGPT works using OpenAI's GPT (Generative Pretrained Transformer) language model and is capable of learning from users' preferences and behavior patterns to customize its response. ChatGPT has the potential to be applied in different fields, including education, journalism, scientific writing, communication, cell biology, and biotechnology, where there is already evidence. The aim of this work was to analyze the possible applications of ChatGPT in the agricultural and livestock industry. First, a scientometric analysis was performed with VosViewer and Bibliometrix (Bliblioshiny). 3 clusters were identified: (a) Main characteristics; (b) learning systems you use; and (c) applications. To the question: What are the main applications in which ChatGTP will revolutionize agriculture (or livestock) in the world? ChatGPT responded: (a) in the agricultural field: improvement of agricultural decision-making, optimization of agricultural production, detection and prevention of plant diseases, climate management, and supply chain management; and (b) in the livestock field: improvement of animal health and welfare, optimization of animal production, supply chain management, detection and prevention of zoonotic diseases, and climate management for animal production. ChatGPT does not scientifically support its answer, but from the analysis carried out, we find that there is enough scientific evidence to conclude, in this case, that its answers were correct. While ChatGPT does not necessarily scientifically substantiate its answers, users should. There is a lack of studies on the use of Artificial Intelligence and its relationship with ethics. © FFC 2023.","artificial intelligence; autoregressive language model; chatbot; data mining; deep learning; text mining; text production","","","","R. Siche; Escuela de Ingeniería Agroindustrial, Facultad de Ciencias Agropecuarias, Universidad Nacional de Trujillo, Trujillo, Peru; email: rsiche@unitru.edu.pe","","Universidad Nacional de Trujillo","20779917","","","","Spanish","Scientia Agropecu.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85153855335"
"Uladzislau S.; Feng X.","Uladzislau, Starasotnikau (58397471800); Feng, Xin (34869355900)","58397471800; 34869355900","Modified Omni-Scale Net architecture for cattle identification on their muzzle point image pattern characteristics","2023","Proceedings of SPIE - The International Society for Optical Engineering","12645","","1264520","","","","1","10.1117/12.2681201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163356699&doi=10.1117%2f12.2681201&partnerID=40&md5=75d3fdaf40177146a50264ca0bc1f7ce","College of Computer Science and Engineering, Chongqing University of Technology, Chongqing, China","Uladzislau S., College of Computer Science and Engineering, Chongqing University of Technology, Chongqing, China; Feng X., College of Computer Science and Engineering, Chongqing University of Technology, Chongqing, China","Animal biometrics is a frontier field of computer vision, pattern recognition and cognitive science that plays a vital role in the registration, unique identification and verification of livestock (cattle). In this study, we propose a deep learning approach to cattle identification based on the characteristics of the muzzle point image (nose pattern) to solve the problem of missed or replaced animals and false insurance claims. Inspired on the state-of-the-art OSNet architecture, which was developed for Person Re-Identification, we introduces significant modifications in order to improve accuracy in the cattle recognition problem. First, each Convolution layer is replaced by a Depth-wise Separable Convolution layer, and Parametric Rectified Linear Unit is used as a non-linear activation function. Next we add two Convolutional Block Attention Module. Under the same experimental conditions, improved OSNet achieves significantly superior accuracy than the original OSNet, maintaining the same speed and compact storage. © 2023 SPIE.","cattle identification; muzzle point image pattern characteristics","Agriculture; Animals; Deep learning; Image processing; Insurance; Pattern recognition; Cattle identification; Cognitive science; Image patterns; Insurance claims; Learning approach; Muzzle point image pattern characteristic; NET architecture; Pattern characteristic; Unique identifications; Vision pattern; Convolution","","","S. Uladzislau; College of Computer Science and Engineering, Chongqing University of Technology, Chongqing, China; email: xfeng@cqut.edu.cn","Feng X.; Bhattacharjya A.","SPIE","0277786X","978-151066504-0","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85163356699"
"","","","6th International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems, icABCD 2023 - Proceedings","2023","6th International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems, icABCD 2023 - Proceedings","","","","","","263","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172019338&partnerID=40&md5=ce89eef6695248bf1666fa8c3648df7e","","","The proceedings contain 39 papers. The topics discussed include: cybersecurity practices of rural underserved communities in Africa: a case study from northern Namibia; a deep learning model for predicting under-five mortality in Zimbabwe; development of a sign language recognition system using machine learning; an underwater network for mini-submarine underwater observatory; artificial intelligence and state power; 1: development of a sign language recognition system using machine learning; blockchain electoral vote counting solutions: a comparative analysis of methods, constraints, and approaches; management and monitoring of livestock in the farm using deep learning; design of a 45 nm complementary metal oxide semiconductor low noise amplifier for a 30 GHz millimeter-wave wireless transceiver in radar sensor applications; realizing the potential of stratosphere utilization via stratosphere data centers; and plant disease detection using vision transformers on multispectral natural environment images.","","","","","","Pudaruth S.; Singh U.","Institute of Electrical and Electronics Engineers Inc.","","979-835031480-9","","","English","Int. Conf. Artif. Intell., Big Data, Comput. Data Commun. Syst., icABCD - Proc.","Conference review","Final","","Scopus","2-s2.0-85172019338"
"Symeonidis G.; Kiourt C.; Kazakis N.A.; Nerantzis E.; Nestor T.","Symeonidis, Georgios (57203263227); Kiourt, Chairi (55669286500); Kazakis, Nikolaos A (6504016442); Nerantzis, Evangelos (57407085400); Nestor, Tsirliganis (12763399100)","57203263227; 55669286500; 6504016442; 57407085400; 12763399100","Fat calculation from raw-beef-steak images through machine learning approaches: An end-to-end pipeline","2022","ACM International Conference Proceeding Series","","","","110","115","5","0","10.1145/3575879.3575975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152121066&doi=10.1145%2f3575879.3575975&partnerID=40&md5=e83fbe81eefdf4af1225c2e3e9b55894","Athena Research Centre, Xanthi, GR-67100, Greece","Symeonidis G., Athena Research Centre, Xanthi, GR-67100, Greece; Kiourt C., Athena Research Centre, Xanthi, GR-67100, Greece; Kazakis N.A., Athena Research Centre, Xanthi, GR-67100, Greece; Nerantzis E., Athena Research Centre, Xanthi, GR-67100, Greece; Nestor T., Athena Research Centre, Xanthi, GR-67100, Greece","The livestock meat and its nutrition quality is considered to be an important factor in our daily eating habits giving particular emphasis to health issues. The quality and the nutrition value of a raw-beef-steak, is highly connected with the fat percentage of it. Consequently, the determination of the fat percentage of a raw-beef-steak is crucial for meat producers and consumers as well. In this work, we present a fat mass estimation approach based on a state-of-the-art deep learning pipeline by utilizing a single colored image presenting raw-beef-steak. In order to produce more accurate outcomes, our pipeline combines two U-Nets, one for the background removal and one for the fat extraction. By following popular computational approaches we estimate the fat amount based on the pixels presenting it. To enhance the outcomes of this work, we introduce a new data-set annotated based on the needs of the experiment. The main goal of this work is to provide accurate nutritional information to end-users through novel technologies by exploiting a single image through a mobile application. © 2022 ACM.","Fat calculation; Image segmentation; Machine learning pipelines","Agriculture; Beef; Deep learning; Image segmentation; Learning systems; Nutrition; Eating habits; End to end; Fat calculation; Health issues; Images segmentations; Machine learning approaches; Machine learning pipeline; Machine-learning; Nutrition quality; Nutrition value; Pipelines","European Commission, EC; European Regional Development Fund, ERDF","We acknowledge the partial support of this work by the project “AGRO4+"" - Holistic approach to Agriculture 4.0 for new farmers” (MIS 5046239) which is implemented under the Action “Reinforcement of the Research and Innovation Infrastructure”, funded by the Operational Programme ""Competitiveness, Entrepreneurship and Innovation"" (NSRF 2014-2020) and co-financed by Greece and the European Union (European Regional Development Fund).","","Karanikolas N.N.; Troussas C.; Vassilakopoulos M.Gr.","Association for Computing Machinery","","978-145039854-1","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85152121066"
"Myat Noe S.; Zin T.T.; Tin P.; Kobayashi I.","Myat Noe, Su (57221527571); Zin, Thi Thi (6506258245); Tin, Pyke (24923729700); Kobayashi, Ikuo (24174822700)","57221527571; 6506258245; 24923729700; 24174822700","Comparing State-of-the-Art Deep Learning Algorithms for the Automated Detection and Tracking of Black Cattle","2023","Sensors","23","1","532","","","","29","10.3390/s23010532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145977038&doi=10.3390%2fs23010532&partnerID=40&md5=af0a347c1f430c154a2ceca28d3e58b4","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, 889-2192, Japan","Myat Noe S., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Zin T.T., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Tin P., Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; Kobayashi I., Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, 889-2192, Japan","Effective livestock management is critical for cattle farms in today’s competitive era of smart modern farming. To ensure farm management solutions are efficient, affordable, and scalable, the manual identification and detection of cattle are not feasible in today’s farming systems. Fortunately, automatic tracking and identification systems have greatly improved in recent years. Moreover, correctly identifying individual cows is an integral part of predicting behavior during estrus. By doing so, we can monitor a cow’s behavior, and pinpoint the right time for artificial insemination. However, most previous techniques have relied on direct observation, increasing the human workload. To overcome this problem, this paper proposes the use of state-of-the-art deep learning-based Multi-Object Tracking (MOT) algorithms for a complete system that can automatically and continuously detect and track cattle using an RGB camera. This study compares state-of-the-art MOTs, such as Deep-SORT, Strong-SORT, and customized light-weight tracking algorithms. To improve the tracking accuracy of these deep learning methods, this paper presents an enhanced re-identification approach for a black cattle dataset in Strong-SORT. For evaluating MOT by detection, the system used the YOLO v5 and v7, as a comparison with the instance segmentation model Detectron-2, to detect and classify the cattle. The high cattle-tracking accuracy with a Multi-Object Tracking Accuracy (MOTA) was 96.88%. Using these methods, the findings demonstrate a highly accurate and robust cattle tracking system, which can be applied to innovative monitoring systems for agricultural applications. The effectiveness and efficiency of the proposed system were demonstrated by analyzing a sample of video footage. The proposed method was developed to balance the trade-off between costs and management, thereby improving the productivity and profitability of dairy farms; however, this method can be adapted to other domestic species. © 2023 by the authors.","cattle detection; cattle tracking; deep learning; multi-object tracking; precision livestock farming; re-identification","Agriculture; Algorithms; Animals; Cattle; Dairying; Deep Learning; Farms; Female; Humans; Deep learning; Economic and social effects; Learning algorithms; Learning systems; Object detection; Tracking (position); Automated detection; Automated tracking; Cattle detection; Cattle tracking; Deep learning; Multi-object tracking; Precision livestock farming; Re identifications; State of the art; Tracking accuracy; agricultural land; agriculture; algorithm; animal; bovine; dairying; female; human; procedures; Farms","UK Research and Innovation, UKRI, (105300); Japan Science and Technology Agency, JST, (WN922001)","This work was supported in part by JST SPRING, Grant Number WN922001.","T.T. Zin; Graduate School of Engineering, University of Miyazaki, Miyazaki, 889-2192, Japan; email: thithi@cc.miyazaki-u.ac.jp","","MDPI","14248220","","","36617130","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145977038"
"Chae J.-W.; Choi Y.-H.; Lee J.-N.; Park H.-J.; Jeong Y.-D.; Cho E.-S.; Kim Y.-S.; Kim T.-K.; Sa S.-J.; Cho H.-C.","Chae, Jung-Woo (57215821871); Choi, Yo-Han (35298577000); Lee, Jeong-Nam (57219243531); Park, Hyun-Ju (58532273000); Jeong, Yong-Dae (57192102384); Cho, Eun-Seok (56401493800); Kim, Young-Sin (56324700900); Kim, Tae-Kyeong (57444811500); Sa, Soo-Jin (55695134700); Cho, Hyun-Chong (22233514800)","57215821871; 35298577000; 57219243531; 58532273000; 57192102384; 56401493800; 56324700900; 57444811500; 55695134700; 22233514800","An intelligent method for pregnancy diagnosis in breeding sows according to ultrasonography algorithms","2023","Journal of Animal Science and Technology","65","2","","365","376","11","2","10.5187/jast.2022.e107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167695789&doi=10.5187%2fjast.2022.e107&partnerID=40&md5=9149f366560331cd0a4d24092c23d45d","Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Department of Electronics Engineering, Kangwon National University, Chuncheon, 24341, South Korea; Department of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea","Chae J.-W., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Choi Y.-H., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Lee J.-N., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea; Park H.-J., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Jeong Y.-D., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Cho E.-S., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Kim Y.-S., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Kim T.-K., Department of Electronics Engineering, Kangwon National University, Chuncheon, 24341, South Korea; Sa S.-J., Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; Cho H.-C., Department of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon, 24341, South Korea","Pig breeding management directly contributes to the profitability of pig farms, and pregnancy diagnosis is an important factor in breeding management. Therefore, the need to diagnose pregnancy in sows is emphasized, and various studies have been conducted in this area. We propose a computer-aided diagnosis system to assist livestock farmers to diagnose sow pregnancy through ultrasound. Methods for diagnosing pregnancy in sows through ultrasound include the Doppler method, which measures the heart rate and pulse status, and the echo method, which diagnoses by amplitude depth technique. We propose a method that uses deep learning algorithms on ultrasonography, which is part of the echo method. As deep learning-based classification algorithms, Inception-v4, Xception, and EfficientNetV2 were used and compared to find the optimal algorithm for pregnancy diagnosis in sows. Gaussian and speckle noises were added to the ultrasound images according to the characteristics of the ultrasonography, which is easily affected by noise from the surrounding environments. Both the original and noise added ultrasound images of sows were tested together to determine the suitability of the proposed method on farms. The pregnancy diagnosis performance on the original ultrasound images achieved 0.99 in accuracy in the highest case and on the ultrasound images with noises, the performance achieved 0.98 in accuracy. The diagnosis performance achieved 0.96 in accuracy even when the intensity of noise was strong, proving its robustness against noise. Copyright © 2023 Korean Society of Animal Sciences and Technology. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/bync/4.0/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.","Classification algorithm; Deep learning; Pregnancy diagnosis; Sow; Ultrasound","","Rural Development Administration, RDA; National Institute of Animal Science, NIAS","This study was supported by 2022 the RDA Fellowship Program of National Institute of Animal Science, Rural Development Administration, Korea.","S.-J. Sa; Swine Science Division, National Institute of Animal Science, Rural Development Administration, Cheonan, 31000, South Korea; email: soojinsa@korea.kr","","Korean Society of Animal Sciences and Technology","26720191","","","","English","J.  Anim.  Sci. Technol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85167695789"
"Jung H.; Kwon B.; Kim Y.; Lee Y.; Park J.; Pegg G.; Wang Y.M.; Smith A.H.","Jung, Heesun (57702799200); Kwon, Bokyung (58175179200); Kim, Youngbin (55653511500); Lee, Yejin (58175653600); Park, Jihyeon (58175179300); Pegg, Griffin (58176122900); Wang, Yaqin Mia (58175410400); Smith, Anthony H. (55599443800)","57702799200; 58175179200; 55653511500; 58175653600; 58175179300; 58176122900; 58175410400; 55599443800","A Deep Learning-Based Coyote Detection System Using Audio Data","2023","5th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2023","","","","170","175","5","1","10.1109/ICAIIC57133.2023.10067023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151996107&doi=10.1109%2fICAIIC57133.2023.10067023&partnerID=40&md5=74f0979ebb3acccbe9d16173084ca4e9","Hallym University, Dept. Computer Science, Chuncheon, South Korea; Kwangwoon University, Dept. Computer Information Engineering, Seoul, South Korea; Jeju National University, Dept. Computer Science and Statistics, Jeju, South Korea; Dept. Computer and Information Technology, Purdue University, West Lafayette, IN, United States","Jung H., Hallym University, Dept. Computer Science, Chuncheon, South Korea; Kwon B., Kwangwoon University, Dept. Computer Information Engineering, Seoul, South Korea; Kim Y., Kwangwoon University, Dept. Computer Information Engineering, Seoul, South Korea; Lee Y., Hallym University, Dept. Computer Science, Chuncheon, South Korea; Park J., Jeju National University, Dept. Computer Science and Statistics, Jeju, South Korea; Pegg G., Dept. Computer and Information Technology, Purdue University, West Lafayette, IN, United States; Wang Y.M., Dept. Computer and Information Technology, Purdue University, West Lafayette, IN, United States; Smith A.H., Dept. Computer and Information Technology, Purdue University, West Lafayette, IN, United States","The attacks on livestock, human, and crops by coyotes are occurring over the United States, while traditional simple management such as public education about the method of avoiding coyotes and coyote hunting contests to reduce their numbers are executed. There are not sufficient cases of technical approaches or research about the preventing coyotes. The method of a coyote howling sound classification using machine learning and feature extraction to reduce the damage from the coyotes is needed. This paper applies the optimal model created by comparing and analyzing various combinations of machine learning and feature extraction methods to the coyote detection platform. It is expected that an additional technical approach to current coyote damage prevention can improve the accuracy and make the previous management more practical.  © 2023 IEEE.","audio classification; feature extraction; image classification; machito learning; spectrogram features","Agriculture; Audio acoustics; Classification (of information); Deep learning; Extraction; Image classification; Learning systems; Audio classification; Audio data; Detection system; Features extraction; Images classification; Machine-learning; Machito learning; Simple++; Spectrogram feature; Spectrograms; Feature extraction","Jeju National University, JNU, (2018-0-01863); Hallym University, Hallym, (20180002160302001); Kwangwoon University, KW University, (2017-0-00096); Ministry of Science, ICT and Future Planning, MSIP","VIII. ACKNOWLEDGMENT This research was supported by the MSIT(Ministry of Science, ICT), Korea, under the National Program for Excellence in SW), Kwangwoon University under Grant 2017-0-00096, Hallym University under Grant 20180002160302001 and Jeju National University under Grant 2018-0-01863, supervised by the IITP(Institute of Information & communications Technology Planing & Evaluation).","","","Institute of Electrical and Electronics Engineers Inc.","","978-166545645-6","","","English","Int. Conf. Artif. Intell. Inf. Commun., ICAIIC","Conference paper","Final","","Scopus","2-s2.0-85151996107"
"Chu C.-T.; Lin Z.-X.","Chu, Chao-Ting (55332293600); Lin, Zhi-Xuan (57486067300)","55332293600; 57486067300","Deep Learning for Pig Size Detection in Smart Farm Imaging","2023","Proceedings of Technical Papers - International Microsystems, Packaging, Assembly, and Circuits Technology Conference, IMPACT","","","","283","286","3","1","10.1109/IMPACT59481.2023.10348741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182744187&doi=10.1109%2fIMPACT59481.2023.10348741&partnerID=40&md5=aa78248f17aef41d68470d701e1c7cd8","Chunghwa Telecom Laboratories, No.99, Dianyan Rd., Yangmei Distric, Taoyuan City, 32661, Taiwan; Industrial Technology Research Institute, No. 2, Wenxian Road, Nantou County, Nantou City, Taiwan","Chu C.-T., Chunghwa Telecom Laboratories, No.99, Dianyan Rd., Yangmei Distric, Taoyuan City, 32661, Taiwan; Lin Z.-X., Industrial Technology Research Institute, No. 2, Wenxian Road, Nantou County, Nantou City, Taiwan","This paper presented the deep learning for pig size detection in farm imaging. The livestock industry in Taiwan is currently grappling with a critical shortage of manpower, necessitating prompt technological advancements. Compounded by the challenges of rural youth outmigration and declining birth rates, the industry faces a significant labor gap, resulting in reduced productivity. To address this issue, this study proposes the implementation of deep learning techniques in camera systems to enable real-time detection of pig sizes. By accurately assessing the condition of each pig, this innovative approach helps bridge the manpower shortage and enhances operational efficiency in the livestock sector, ultimately contributing to its sustainable growth.  © 2023 IEEE.","","Agriculture; Deep learning; Mammals; Birth rates; Camera systems; Condition; Innovative approaches; Learning techniques; Operational efficiencies; Real-time detection; Size detection; Sustainable growth; Technological advancement; Real time systems","","","","","IEEE Computer Society","21505934","979-835038412-3","","","English","Proc. Tech. Pap. - Int. Microsystems, Pack., Assem., Circuits Technol. Conf., IMPACT","Conference paper","Final","","Scopus","2-s2.0-85182744187"
"Hao W.; Hao W.; Wang J.; Yang H.; Li F.","Hao, Wangli (55757033100); Hao, Wangbao (57194782429); Wang, Jing (58463749900); Yang, Hua (57198825489); Li, Fuzhong (55494497700)","55757033100; 57194782429; 58463749900; 57198825489; 55494497700","A novel method for Jinnan cattle individual classification based on deep mutual learning","2023","Systems Science and Control Engineering","11","1","2207587","","","","2","10.1080/21642583.2023.2207587","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162937221&doi=10.1080%2f21642583.2023.2207587&partnerID=40&md5=8a90661ae6b2f497ba074026ea4dc4c9","School of Software, Shanxi Agricultural University, Jinzhong City, Shanxi, China; Yuncheng National Jinnan Cattle Genetic Resources and Gene Protection Center, Shanxi, Yuncheng City, China; School of Information Science and Engineering, Shanxi Agricultural University, Jinzhong City, Shanxi, China","Hao W., School of Software, Shanxi Agricultural University, Jinzhong City, Shanxi, China; Hao W., Yuncheng National Jinnan Cattle Genetic Resources and Gene Protection Center, Shanxi, Yuncheng City, China; Wang J., School of Software, Shanxi Agricultural University, Jinzhong City, Shanxi, China; Yang H., School of Information Science and Engineering, Shanxi Agricultural University, Jinzhong City, Shanxi, China; Li F., School of Software, Shanxi Agricultural University, Jinzhong City, Shanxi, China","As the core technology of precision animal husbandry, efficient and rapid identification of Jinnan cattle individuals can promote the scale, informatization and refinement of breeding, which is very necessary for the development of animal husbandry at this stage. However, the traditional livestock individual recognition method based on ear tag is labour-consuming, time-consuming, inefficient, easy to wear and limited by the recognition distance, and the accuracy is also very low. In order to solve this problem, a new method of Jinnan cattle individual recognition based on deep mutual learning is proposed by using the non-contact image recognition method. Two student networks are designed. They supervise each other and complete the task together. Their efficiency can be higher than that of a strong teacher network. Through this method, the individual recognition performance of Jinnan cattle is also enhanced. The experimental results verify the effectiveness of the method of deep mutual learning. Consult and learn from the peer network are used to improve generalization, so as to improve model recognition performance. Finally, the accuracy is 99.3% on the Jinnan cattle individual dataset established in this paper. The application of the contactless cattle individual recognition method in the farm is of great significance. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","deep mutual learning; individual classification; Jinnan cattle; precision animal husbandry","Agriculture; Animals; Deep learning; Image recognition; Learning systems; Animal husbandry; Deep mutual learning; Individual classification; Individual recognition; Jinnan cattle; Mutual learning; Novel methods; Performance; Precision animal husbandry; Recognition methods; Informatization","Shanxi Province Basic Research Program, (202203021212444); Shanxi Province Education Science","This work was supported by the Shanxi Province Basic Research Program (202203021212444); Shanxi Province Education Science “14th Five-Year Plan” 2021 Annual Project General Planning Project + “Industry-University-Research”-driven Smart Agricultural Talent Training Model in Agriculture and Forestry Colleges(GH-21006); Shanxi Province Higher Education Teaching Reform and Innovation Project (J20220274); Shanxi Agricultural University doctoral research start-up project (2021BQ88); Shanxi Agricultural University 2021 (Neural Network) Course Ideological and Political Project (KCSZ202133) and Shanxi Postgraduate Education and Teaching Reform Project Fund (2022YJJG094).","F. Li; School of Software, Shanxi Agricultural University, Jinzhong City, Shanxi, China; email: hualimengyu@163.com","","Taylor and Francis Ltd.","21642583","","","","English","Syst. Sci. Control Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85162937221"
"Wang X.; Dai B.; Wei X.; Shen W.; Zhang Y.; Xiong B.","Wang, Xinjie (58112052900); Dai, Baisheng (57225754325); Wei, Xiaoli (36070226800); Shen, Weizheng (23390072800); Zhang, Yonggen (7601315132); Xiong, Benhai (24537909900)","58112052900; 57225754325; 36070226800; 23390072800; 7601315132; 24537909900","Vision-based measuring method for individual cow feed intake using depth images and a Siamese network","2023","International Journal of Agricultural and Biological Engineering","16","3","","233","239","6","5","10.25165/j.ijabe.20231603.7985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168944268&doi=10.25165%2fj.ijabe.20231603.7985&partnerID=40&md5=495001cb1b6f53321ac536b675c79bc5","College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China; State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","Wang X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Dai B., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Wei X., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Shen W., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Zhang Y., College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China; Xiong B., State Key Laboratory of Animal Nutrition, Institute of Animal Science, Chinese Academy of Agricultural Sciences, Beijing, 100193, China","Feed intake is an important indicator to reflect the production performance and disease risk of dairy cows, which can also evaluate the utilization rate of pasture feed. To achieve an automatic and non-contact measurement of feed intake, this paper proposes a method for measuring the feed intake of cows based on computer vision technology with a Siamese network and depth images. An automated data acquisition system was first designed to collect depth images of feed piles and constructed a dataset with 24 150 samples. A deep learning model based on the Siamese network was then constructed to implement non-contact measurement of feed intake for dairy cows by training with collected data. The experimental results show that the mean absolute error (MAE) and the root mean square error (RMSE) of this method are 0.100 kg and 0.128 kg in the range of 0-8.2 kg respectively, which outperformed existing works. This work provides a new idea and technology for the intelligent measuring of dairy cow feed intake. © 2023, Chinese Society of Agricultural Engineering. All rights reserved.","computer vision; cow feed intake; depth image; precision livestock farming; Siamese network","","Postdoctoral Research Start-up Fund of Heilongjiang Province, (LBH-Q21062); National Natural Science Foundation of China, NSFC, (31902210, 32072788); National Key Research and Development Program of China, NKRDPC, (2019YFE0125600)","This work was supported in part by the National Natural Science Foundation of China (Grant No. 32072788; 31902210), the National Key Research and Development Program of China (Grant No. 2019YFE0125600), the Postdoctoral Research Start-up Fund of Heilongjiang Province (Grant No. LBH-Q21062) and the Earmarked Fund for CARS36.","X. Wei; College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; email: wxllsz@neau.edu.cn; W. Shen; College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; email: wzshen@neau.edu.cn","","Chinese Society of Agricultural Engineering","19346344","","","","English","Int. J. Agric. Biol. Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85168944268"
"Pathak R.K.; Kim J.-M.","Pathak, Rajesh Kumar (57200749609); Kim, Jun-Mo (57197706730)","57200749609; 57197706730","Vetinformatics from functional genomics to drug discovery: Insights into decoding complex molecular mechanisms of livestock systems in veterinary science","2022","Frontiers in Veterinary Science","9","","1008728","","","","7","10.3389/fvets.2022.1008728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142540962&doi=10.3389%2ffvets.2022.1008728&partnerID=40&md5=a173a401a9f54f2a7ee740eafe3b9688","Department of Animal Science and Technology, Chung-Ang University, Anseong-si, South Korea","Pathak R.K., Department of Animal Science and Technology, Chung-Ang University, Anseong-si, South Korea; Kim J.-M., Department of Animal Science and Technology, Chung-Ang University, Anseong-si, South Korea","Having played important roles in human growth and development, livestock animals are regarded as integral parts of society. However, industrialization has depleted natural resources and exacerbated climate change worldwide, spurring the emergence of various diseases that reduce livestock productivity. Meanwhile, a growing human population demands sufficient food to meet their needs, necessitating innovations in veterinary sciences that increase productivity both quantitatively and qualitatively. We have been able to address various challenges facing veterinary and farm systems with new scientific and technological advances, which might open new opportunities for research. Recent breakthroughs in multi-omics platforms have produced a wealth of genetic and genomic data for livestock that must be converted into knowledge for breeding, disease prevention and management, productivity, and sustainability. Vetinformatics is regarded as a new bioinformatics research concept or approach that is revolutionizing the field of veterinary science. It employs an interdisciplinary approach to understand the complex molecular mechanisms of animal systems in order to expedite veterinary research, ensuring food and nutritional security. This review article highlights the background, recent advances, challenges, opportunities, and application of vetinformatics for quality veterinary services. Copyright © 2022 Pathak and Kim.","drug discovery; functional genomics; livestock systems; veterinary science; vetinformatics","ab initio calculation; amino acid sequence; animal behavior; basic science; binding site; bioinformatics; climate change; data analysis; deep learning; domestic cattle; functional genomics; gene expression; genetic resource; genome-wide association study; growth, development and aging; high throughput sequencing; human; human development; industrialization; livestock; molecular docking; molecular phylogeny; muscle contraction; natural resource; nonhuman; oocyte maturation; prophylaxis; protein family; protein structure; Review; RNA sequencing; veterinary medicine; whole genome sequencing","Chung-Ang University, CAU; Ministry of Science, ICT and Future Planning, MSIP, (NRF-2022R1A2C1005830); Ministry of Science, ICT and Future Planning, MSIP; National Research Foundation of Korea, NRF","Funding text 1: The authors thank Chung-Ang University, Anseong-si, Republic of Korea for providing high-performance computing and other necessary facilities.; Funding text 2: This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (NRF-2022R1A2C1005830). ","J.-M. Kim; Department of Animal Science and Technology, Chung-Ang University, Anseong-si, South Korea; email: junmokim@cau.ac.kr","","Frontiers Media S.A.","22971769","","","","English","Front. Vet. Sci.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142540962"
"Fuentes S.; Gonzalez Viejo C.; Tongson E.; Dunshea F.R.","Fuentes, Sigfredo (23982386600); Gonzalez Viejo, Claudia (57192378497); Tongson, Eden (57202582254); Dunshea, Frank R. (7005947650)","23982386600; 57192378497; 57202582254; 7005947650","The livestock farming digital transformation: implementation of new and emerging technologies using artificial intelligence","2022","Animal Health Research Reviews","23","1","","59","71","12","47","10.1017/S1466252321000177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134432715&doi=10.1017%2fS1466252321000177&partnerID=40&md5=1ee191b6f2a440ca7c86d877b73aa900","Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, VIC, Australia; Faculty of Biological Sciences, The University of Leeds, Leeds, LS2 9JT, United Kingdom","Fuentes S., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, VIC, Australia; Gonzalez Viejo C., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, VIC, Australia; Tongson E., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, VIC, Australia; Dunshea F.R., Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, VIC, Australia, Faculty of Biological Sciences, The University of Leeds, Leeds, LS2 9JT, United Kingdom","Livestock welfare assessment helps monitor animal health status to maintain productivity, identify injuries and stress, and avoid deterioration. It has also become an important marketing strategy since it increases consumer pressure for a more humane transformation in animal treatment. Common visual welfare practices by professionals and veterinarians may be subjective and cost-prohibitive, requiring trained personnel. Recent advances in remote sensing, computer vision, and artificial intelligence (AI) have helped developing new and emerging technologies for livestock biometrics to extract key physiological parameters associated with animal welfare. This review discusses the livestock farming digital transformation by describing (i) biometric techniques for health and welfare assessment, (ii) livestock identification for traceability and (iii) machine and deep learning application in livestock to address complex problems. This review also includes a critical assessment of these topics and research done so far, proposing future steps for the deployment of AI models in commercial farms. Most studies focused on model development without applications or deployment for the industry. Furthermore, reported biometric methods, accuracy, and machine learning approaches presented some inconsistencies that hinder validation. Therefore, it is required to develop more efficient, non-contact and reliable methods based on AI to assess livestock health, welfare, and productivity.  Copyright © The Author(s), 2022. Published by Cambridge University Press.","Animal welfare; biometrics; computer vision; deep learning; machine learning","Agriculture; Animal Welfare; Animals; Artificial Intelligence; Farms; Livestock; accuracy; animal health; animal lameness; animal welfare; artificial intelligence; behavior; behavior assessment; biometry; body mass; body movement; body temperature; breathing rate; computer vision; consumer; contactless; dairy cattle; dairy sheep; deep learning; digitalization; facial recognition; health impact assessment; health status; heart rate; image analysis; infrared thermal images; injury; livestock; machine learning; mastication; nonhuman; oxygen saturation; physiological stress; pig; pressure; productivity; radiofrequency identification; remote sensing; review; Review; skin temperature; temperature; veterinarian; agricultural land; agriculture; animal","","","S. Fuentes; Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and Food, Faculty of Veterinary and Agricultural Sciences, The University of Melbourne, Parkville, 3010, Australia; email: sigfredo.fuentes@unimelb.edu.au","","Cambridge University Press","14662523","","","35676797","English","Anim. health Res. Rev.","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85134432715"
"Du Q.; Lu G.; Tian L.; Sun Z.; Wang Z.","Du, Qiliang (21233408300); Lu, Guozhen (58685051500); Tian, Lianfang (7202296393); Sun, Zhengzheng (57316637400); Wang, Zhaolin (57317359900)","21233408300; 58685051500; 7202296393; 57316637400; 57317359900","Pig Face Recognition Based on Multi Supervising Forms","2023","2023 4th International Conference on Intelligent Computing and Human-Computer Interaction, ICHCI 2023","","","","55","58","3","0","10.1109/ICHCI58871.2023.10277942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175985218&doi=10.1109%2fICHCI58871.2023.10277942&partnerID=40&md5=2420fbda1a6e36f2c0cc22ad40113ec7","South China University of Technology School of Automation Science and Engineering, Guangzhou, China","Du Q., South China University of Technology School of Automation Science and Engineering, Guangzhou, China; Lu G., South China University of Technology School of Automation Science and Engineering, Guangzhou, China; Tian L., South China University of Technology School of Automation Science and Engineering, Guangzhou, China; Sun Z., South China University of Technology School of Automation Science and Engineering, Guangzhou, China; Wang Z., South China University of Technology School of Automation Science and Engineering, Guangzhou, China","With the development of intelligent breeding, non-contact pig face recognition based on deep learning has been widely studied, enabling accurate and intensive management. Existing face recognition network training in limited categories cannot meet the requirements for breeding short-growth period edible livestock. Moreover, using a single supervising function is hard to distinguish pig faces that are more similar to humans. Therefore, this paper improves Inner origin loss, which can adaptively adjust repulsive circles, allowing for fast and efficient pig face feature extraction. In addition, we propose the TACO loss which can learn facial features to separate pig faces. The experiments conducted on our dataset indicate that in the 1200 pairs of pig face verification pairs composed of 15 pigs, the accuracy of open-set is 96.08%. This study provides a technical reference for pig face recognition research. © 2023 IEEE.","Center loss; loss function; Origin loss; pig face recognition; Triplet loss","Agriculture; Computer vision; Deep learning; Mammals; Center loss; Face feature extraction; Growth period; Intensive management; Loss functions; Network training; Non-contact; Origin loss; Pig face recognition; Triplet loss; Face recognition","Key-Area Research and Devel -opment Program of Guangdong Province, (2018B010109001, 2019B020214001, 2020B1111010002); South China University of Technology, SCUT, (C9212040)","ACKNOWLEDGMENT This work was supported by Key-Area Research and Devel -opment Program of Guangdong Province (2020B1111010002, 2018B010109001, 2019B020214001), the 8th batch of exploratory experimental projects of SCUT (C9212040).","","","Institute of Electrical and Electronics Engineers Inc.","","979-835034311-3","","","English","Int. Conf. Intell. Comput. Human-Comput. Interact., ICHCI","Conference paper","Final","","Scopus","2-s2.0-85175985218"
"Patel N.; Jain H.; Lonkar V.S.; Singh D.","Patel, Neel (58243424000); Jain, Harshal (58243424100); Lonkar, Vaibhav Sadashiv (57202334530); Singh, Dineshkumar (56046410400)","58243424000; 58243424100; 57202334530; 56046410400","Biometric-based Unique Identification for Bovine Animals - Comparative Study of Various Machine and Deep Learning Computer Vision Methods","2023","2023 Somaiya International Conference on Technology and Information Management, SICTIM 2023","","","","1","5","4","2","10.1109/SICTIM56495.2023.10105004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159005129&doi=10.1109%2fSICTIM56495.2023.10105004&partnerID=40&md5=65e28634d5d09038d822b3a269ff72b5","Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Ahmedabad, India; Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Indore, India; Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Pune, India; Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Mumbai, India","Patel N., Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Ahmedabad, India; Jain H., Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Indore, India; Lonkar V.S., Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Pune, India; Singh D., Digital Farming Initiative (DFI), Tata Consultancy Services Ltd. (TCS), Mumbai, India","Animal recognition and identification is an expanding area of inquiry in computer vision, feature extraction, cognitive science, and pattern recognition. In the context of zoonotic diseases, cattle recognition has become an unfolding research field in modern times for registration, distinctive identification, verification of livestock (cattle), controlling outbreaks of diseases, production management, vaccination, assignment of ownership, settlement of insurance claims, and traceability of livestock. Out of the existing noninvasive methods using computer vision, this study illustrates the fundamental implementation of a cattle biometric method to distinguish them using its muzzle (snout) point. Cattle muzzle point characteristics may be recognized in a manner how a human fingerprint can be recognized down to the tiniest of details, and how we can generate unique muzzle signatures to compare with each other. We have reviewed various methods like SIFT algorithm, LBP matcher approach, and image classification. Our study concludes that when the need is to distinctly identify between two cattle using muzzle images-based key points descriptor, then SIFT and LBP approaches are more suitable. For other scenarios, the image classification method gives better results. © 2023 IEEE.","Cattle Biometric; Cattle Recognition; Computer Vision; Deep Learning; Image Classification; Key point Descriptor; LBP; Livestock Identification; Local Binary Pattern; Muzzle Recognition; Muzzle Signature; Scale-Invariant Feature Transform; SIFT","Agriculture; Biometrics; Deep learning; Insurance; Local binary pattern; Mammals; Noninvasive medical procedures; Cattle biometric; Cattle recognition; Deep learning; Images classification; Invariant feature transforms; Key point descriptor; Keypoints; Livestock identification; Local binary patterns; Muzzle recognition; Muzzle signature; Point descriptors; Scale invariant features; Scale-invariant feature transform; SIFT; Image classification","","","","","Institute of Electrical and Electronics Engineers Inc.","","979-835033329-9","","","English","Somaiya Int. Conf. Technol. Inf. Manag., SICTIM","Conference paper","Final","","Scopus","2-s2.0-85159005129"
"Pu J.; Yu C.; Chen X.; Zhang Y.; Yang X.; Li J.","Pu, Jingyu (57218863406); Yu, Chengjun (57783413600); Chen, Xiaoyan (57192479753); Zhang, Yu (57225166799); Yang, Xiao (57211636142); Li, Jun (57211877674)","57218863406; 57783413600; 57192479753; 57225166799; 57211636142; 57211877674","Research on Chengdu Ma Goat Recognition Based on Computer Vison","2022","Animals","12","14","1746","","","","7","10.3390/ani12141746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133552095&doi=10.3390%2fani12141746&partnerID=40&md5=eb374eadfeeb4a5ef4da537397427d8a","College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Sichuan Key Laboratory of Agricultural Information Engineering, Ya’an, 625000, China","Pu J., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Yu C., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Chen X., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China, Sichuan Key Laboratory of Agricultural Information Engineering, Ya’an, 625000, China; Zhang Y., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Yang X., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; Li J., College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China, Sichuan Key Laboratory of Agricultural Information Engineering, Ya’an, 625000, China","The Chengdu ma goat is an excellent local breed in China. As one of the breeds listed in the National List of Livestock and Poultry Genetic Resources Protection, the protection of its germplasm resources is particularly important. However, the existing breeding and protection methods for them are relatively simple, due to the weak technical force and lack of intelligent means to assist. Most livestock farmers still conduct small-scale breeding in primitive ways, which is not conducive to the breeding and protection of Chengdu ma goats. In this paper, an automatic individual recognition method for Chengdu ma goats is proposed, which saves labor costs and does not depend on largescale mechanized facilities. The main contributions of our work are as follows: (1) a new Chengdu ma goat dataset is built, which forms the basis for object detection and classification tasks; (2) an improved detection algorithm for Chengdu ma goats based on TPH-YOLOv5 is proposed, which is able to accurately localize goats in high-density scenes with severe scale variance of targets; (3) a classifier incorporating a self-supervised learning module is implemented to improve the classification performance without increasing the labeled data and inference computation overhead. Experiments show that our method is able to accurately recognize Chengdu ma goats in the actual indoor barn breeding environment, which lays the foundation for precision feeding based on sex and age. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Chengdu ma goat; computer vision; deep learning; object detection; precision livestock farming; self-supervised learning","animal experiment; animal model; Article; artificial neural network; breeding; chengdu ma; classifier; computer vision; controlled study; deep learning; detection algorithm; feeding; goat; learning; livestock; nonhuman; supervised machine learning","National College Students Innovation and Entrepreneurship Training Program, (202110626048)","Funding: This research was supported by Innovation and Entrepreneurship Training Program for College Students (Grant No. 202110626048).","X. Chen; College of Information Engineering, Sichuan Agricultural University, Ya’an, 625000, China; email: chenxy@sicau.edu.cn","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85133552095"
"Wang Z.; Song H.; Wang Y.; Hua Z.; Li R.; Xu X.","Wang, Zheng (55352644900); Song, Huaibo (17342958900); Wang, Yunfei (57669107600); Hua, Zhixin (57724662500); Li, Rong (57577247800); Xu, Xingshi (57812538800)","55352644900; 17342958900; 57669107600; 57724662500; 57577247800; 57812538800","Research Progress and Technology Trend of Intelligent Morning of Dairy Cow Motion Behavior; [奶牛运动行为智能监测研究进展与技术趋势]","2022","Smart Agriculture","4","2","","36","52","16","11","10.12133/j.smartag.SA202203011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146055703&doi=10.12133%2fj.smartag.SA202203011&partnerID=40&md5=2e8e4f54373db8aa277473c6840ca8d5","College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","Wang Z., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Song H., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Wang Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Hua Z., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Li R., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China; Xu X., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, China","The motion behavior of dairy cows contains much of health information. The application of information and intelligent technology will help farms grasp the health status of dairy cows in time and improve breeding efficiency. In this paper, the development trend of intelligent morning technology of cow's motion behavior was mainly analyzed. Firstly, on the basis of expounding the significance of monitoring the basic motion (lying, walking, standing), oestrus, breathing, rumination and limping of dairy cows, the necessity of behavior monitoring of dairy cows was introduced. Secondly, the current research status was summarized from contact monitoring methods and non-contact monitoring methods in chronological order. The principle and achievements of related research were introduced in detail and classified. It is found that the current contact monitoring methods mainly rely on acceleration sensors, pedometers and pressure sensors, while the non-contact monitoring methods mainly rely on video images, including traditional video image analysis and video image analysis based on deep learning. Then, the development status of cow behavior monitoring industry was analyzed, and the main businesses and mainstream products of representative livestock farm automation equipment suppliers were listed. Industry giants, such as Afimilk and DeLaval, as well as their products such as intelligent collar (AfiCollar), pedometer (AfiActll Tag) and automatic milking equipment (VMS™ V300) were introduced. After that, the problems and challenges of current contact and non-contact monitoring methods of dairy cow motion behavior were put forward. The current intelligent monitoring methods of dairy cows' motion behavior are mainly wearable devices, but they have some disadvantages, such as bring stress to dairy cows and are difficult to install and maintain. Although the non-contact monitoring methods based on video image analysis technology does not bring stress to dairy cows and is low cost, the relevant research is still in its infancy, and there is still a certain distance from commercial use. Finally, the future development directions of relevant key technologies were prospected, including miniaturization and integration of wearable monitoring equipment, improving the robustness of computer vision technology, multi-target monitoring with limited equipment and promoting technology industrialization. © 2022 Agricultural Information Institute, Chinese Academy of Agricultural Sciences. All rights reserved.","dairy cows; health condition; intelligent monitoring; motion behavior; smart animal husbandry","","","","H. Song; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, 712100, China; email: songhuaibo@nwafu.edu.cn","","Agricultural Information Institute, Chinese Academy of Agricultural Sciences","20968094","","","","Chinese","Smart. Agric.","Article","Final","","Scopus","2-s2.0-85146055703"
"Aravinda V.K.; Rakesh S.; Hussain H.; Yarlagadda R.; Reddy K.","Aravinda, V.K. (58626492700); Rakesh, S. (59390806600); Hussain, Huma (58160980800); Yarlagadda, Ritika (58625977900); Reddy, Kavya (58729336200)","58626492700; 59390806600; 58160980800; 58625977900; 58729336200","Animal Encroachment Detection in Farms: A Survey","2023","Cognitive Science and Technology","Part F1466","","","425","432","7","0","10.1007/978-981-99-2742-5_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172760063&doi=10.1007%2f978-981-99-2742-5_44&partnerID=40&md5=5cb1c652263919f7ee8b0f65e3dc6428","Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India","Aravinda V.K., Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India; Rakesh S., Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India; Hussain H., Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India; Yarlagadda R., Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India; Reddy K., Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India","Wildlife such as elephants, monkeys intruding farms that are meant for harvesting is extremely damaging and can result in injury, loss of life of humans and wildlife, damage to human property and crops. Farmers not only lose the harvest that they worked on for months, but it is also dangerous for other livestock and people living in the same vicinity. Farmers resort electrical fencing or barbed wires to avoid this which violate the Prevention of Cruelty to Animals Act. Manually guarding the farm is also not optimal as it takes a lot of effort and there is only limited land a person can cover. This increased the need for an application that will detect animal movement or any unusual activity and set out alarms that can avoid loss of money and effort. Using deep learning and computer vision will help process unstructured data such as images collected at different timestamps. Computer vision using Artificial Intelligence will compare colours and contours in images and indicate an encroachment, giving the owner a much more elaborate recognition of the trespasser. Bearing in mind how most of the farmers are relatively alien to a system of this sort, it requires least amount of intervention from the user as input retrieval and processing is automated. This application could also be incorporated in other areas like to detect wildfires in forests or fields that are often detected late. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2023.","Artificial intelligence; Computer vision; Contours; Trespasser","","","","H. Hussain; Department of IT, Chaitanya Bharathi Institute of Technology, Hyderabad, India; email: ugs19067_it.huma@cbit.ac.in","","Springer","21953988","","","","English","Cogn. Sci. Technol.","Book chapter","Final","","Scopus","2-s2.0-85172760063"
"Ruchay A.; Kober V.; Dorofeev K.; Kolpakov V.; Gladkov A.; Guo H.","Ruchay, Alexey (57192592568); Kober, Vitaly (7003270592); Dorofeev, Konstantin (57203992985); Kolpakov, Vladimir (58511094300); Gladkov, Alexey (57325710800); Guo, Hao (55331600800)","57192592568; 7003270592; 57203992985; 58511094300; 57325710800; 55331600800","Live Weight Prediction of Cattle Based on Deep Regression of RGB-D Images","2022","Agriculture (Switzerland)","12","11","1794","","","","25","10.3390/agriculture12111794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141702342&doi=10.3390%2fagriculture12111794&partnerID=40&md5=54ab6e70a29d49295c9d8aa8ecc50bae","Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation; Center of Scientific Research and Higher Education of Ensenada, Ensenada, 22860, Mexico; Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Orenburg, 460000, Russian Federation; College of Land Science and Technology, China Agricultural University, Beijing, 100083, China","Ruchay A., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation; Kober V., Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation, Center of Scientific Research and Higher Education of Ensenada, Ensenada, 22860, Mexico; Dorofeev K., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; Kolpakov V., Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation, Department of Biotechnology of Animal Raw Materials and Aquaculture, Orenburg State University, Orenburg, 460000, Russian Federation; Gladkov A., Department of Mathematics, Chelyabinsk State University, Chelyabinsk, 454001, Russian Federation; Guo H., College of Land Science and Technology, China Agricultural University, Beijing, 100083, China","Predicting the live weight of cattle helps us monitor the health of animals, conduct genetic selection, and determine the optimal timing of slaughter. On large farms, accurate and expensive industrial scales are used to measure live weight. However, a promising alternative is to estimate live weight using morphometric measurements of livestock and then apply regression equations relating such measurements to live weight. Manual measurements on animals using a tape measure are time-consuming and stressful for the animals. Therefore, computer vision technologies are now increasingly used for non-contact morphometric measurements. The paper proposes a new model for predicting live weight based on augmenting three-dimensional clouds in the form of flat projections and image regression with deep learning. It is shown that on real datasets, the accuracy of weight measurement using the proposed model reaches 91.6%. We also discuss the potential applicability of the proposed approach to animal husbandry. © 2022 by the authors.","cattle; deep learning; image regression; live body weight; prediction","","Russian Science Foundation, RSF, (21-76-20014); Russian Science Foundation, RSF","The research was carried out with the financial support of the Russian Science Foundation within the framework of the scientific project No. 21-76-20014.","A. Ruchay; Federal Research Centre of Biological Systems and Agro-Technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; email: ran@csu.ru","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141702342"
"Luo W.; Zhang Z.; Fu P.; Wei G.; Wang D.; Li X.; Shao Q.; He Y.; Wang H.; Zhao Z.; Liu K.; Liu Y.; Zhao Y.; Zou S.; Liu X.","Luo, Wei (57677752400); Zhang, Ze (57878492800); Fu, Ping (57652014600); Wei, Guosheng (57428443900); Wang, Dongliang (55713405600); Li, Xuqing (55938813300); Shao, Quanqin (7101974470); He, Yuejun (57799138000); Wang, Huijuan (51666126200); Zhao, Zihui (57427704100); Liu, Ke (57210930375); Liu, Yuyan (57428443800); Zhao, Yongxiang (57888749800); Zou, Suhua (57887622400); Liu, Xueli (57888305100)","57677752400; 57878492800; 57652014600; 57428443900; 55713405600; 55938813300; 7101974470; 57799138000; 51666126200; 57427704100; 57210930375; 57428443800; 57888749800; 57887622400; 57888305100","Intelligent Grazing UAV Based on Airborne Depth Reasoning","2022","Remote Sensing","14","17","4188","","","","14","10.3390/rs14174188","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137921487&doi=10.3390%2frs14174188&partnerID=40&md5=1b4e3eeeb75a6e4af2fed8f7b93081ee","North China Institute of Aerospace Engineering, Langfang, 065000, China; Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China; National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Key Laboratory of Advanced Motion Control, Fujian Provincial Education Department, Minjiang University, Fuzhou, 350108, China; University of Chinese Academy of Sciences, Beijing, 101407, China","Luo W., North China Institute of Aerospace Engineering, Langfang, 065000, China, Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Zhang Z., North China Institute of Aerospace Engineering, Langfang, 065000, China; Fu P., Key Laboratory of Advanced Motion Control, Fujian Provincial Education Department, Minjiang University, Fuzhou, 350108, China; Wei G., North China Institute of Aerospace Engineering, Langfang, 065000, China; Wang D., Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Li X., North China Institute of Aerospace Engineering, Langfang, 065000, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Shao Q., Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China, University of Chinese Academy of Sciences, Beijing, 101407, China; He Y., North China Institute of Aerospace Engineering, Langfang, 065000, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Wang H., North China Institute of Aerospace Engineering, Langfang, 065000, China; Zhao Z., North China Institute of Aerospace Engineering, Langfang, 065000, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Liu K., North China Institute of Aerospace Engineering, Langfang, 065000, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Liu Y., North China Institute of Aerospace Engineering, Langfang, 065000, China, Aerospace Remote Sensing Information Processing and Application Collaborative Innovation Center of Hebei Province, Langfang, 065000, China, National Joint Engineering Research Center of Space Remote Sensing Information Application Technology, Langfang, 065000, China; Zhao Y., North China Institute of Aerospace Engineering, Langfang, 065000, China; Zou S., North China Institute of Aerospace Engineering, Langfang, 065000, China; Liu X., North China Institute of Aerospace Engineering, Langfang, 065000, China","The existing precision grazing technology helps to improve the utilization rate of livestock to pasture, but it is still at the level of “collectivization” and cannot provide more accurate grazing management and control. (1) Background: In recent years, with the rapid development of agent-related technologies such as deep learning, visual navigation and tracking, more and more lightweight edge computing cell target detection algorithms have been proposed. (2) Methods: In this study, the improved YOLOv5 detector combined with the extended dataset realized the accurate identification and location of domestic cattle; with the help of the kernel correlation filter (KCF) automatic tracking framework, the long-term cyclic convolution network (LRCN) was used to analyze the texture characteristics of animal fur and effectively distinguish the individual cattle. (3) Results: The intelligent UAV equipped with an AGX Xavier high-performance computing unit ran the above algorithm through edge computing and effectively realized the individual identification and positioning of cattle during the actual flight. (4) Conclusion: The UAV platform based on airborne depth reasoning is expected to help the development of smart ecological animal husbandry and provide better precision services for herdsmen. © 2022 by the authors.","cattle monitoring; Inception V3; intelligent UAV; LSTM; precision grazing; YOLOv5","Agriculture; Aircraft detection; Animals; Edge computing; Long short-term memory; Textures; Cattle monitoring; Edge computing; Grazing management; Inception v3; Intelligent UAV; LSTM; Management and controls; Precision grazing; Utilization rates; YOLOv5; Unmanned aerial vehicles (UAV)","Doctoral Research Startup Fund Project, (BKY-2021-32, BKY-2021-35); Innovation Fund of Production, Study and Research in Chinese Universities, (2021ZYA08001); National Natural Science Foundation of China, NSFC, (42071289); National Natural Science Foundation of China, NSFC; National Key Research and Development Program of China, NKRDPC, (2019YFE0126600, 30-Y30F06-9003-20/22); National Key Research and Development Program of China, NKRDPC","This research was funded by the National Natural Science Foundation of China (no. 42071289); Innovation Fund of Production, Study and Research in Chinese Universities (2021ZYA08001); National Basic Research Program of China (grant number 2019YFE0126600); Major Special Project: The China High-Resolution Earth Observation System (30-Y30F06-9003-20/22); and Doctoral Research Startup Fund Project (BKY-2021-32; BKY-2021-35).","D. Wang; Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; email: wangdongliang@igsnrr.ac.cn","","MDPI","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137921487"
"Dommeti D.; Nallapati S.R.; Lokesh C.; Bhuvanesh S.P.; Vara Prasad Padyala V.; Srinivas P.V.V.S.","Dommeti, Dhiren (58102712400); Nallapati, Siva Ramakrishna (58102073700); Lokesh, Chalamalasetti (58313507500); Bhuvanesh, Singasani P (58313901800); Vara Prasad Padyala, Venkata (57195635954); Srinivas, P.V.V.S. (37067906600)","58102712400; 58102073700; 58313507500; 58313901800; 57195635954; 37067906600","Deep Learning Based Lumpy Skin Disease (LSD) Detection","2023","Proceedings - 2023 3rd International Conference on Smart Data Intelligence, ICSMDI 2023","","","","457","465","8","8","10.1109/ICSMDI57622.2023.00087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161942671&doi=10.1109%2fICSMDI57622.2023.00087&partnerID=40&md5=2554ca07f325eb076b69d87c39c9e223","Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India","Dommeti D., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India; Nallapati S.R., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India; Lokesh C., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India; Bhuvanesh S.P., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India; Vara Prasad Padyala V., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India; Srinivas P.V.V.S., Koneru Lakshmaiah Education Foundation, Department of Cse, Guntur, India","The emergence of the lumpy skin disease has become a major threat to the livestock industry in recent years, causing high economic losses and health risks to both animals and humans. This virus is difficult to detect due to its complexity, making the early detection and accurate diagnosis of this virus essential. This study will explore the utilization of convolutional neural networks (CNNs) to efficiently and accurately detect and identify the LSDV than traditional methods. Further, the advantages of using CNNs for this purpose has been discussed and some of the applications of this new technology has also been explored. Additionally, the future potential of using CNNs to perform virus detection is also discussed. However, Lumpy disease is classified differently based on its severity. To determine the extent to which the animal is impacted by lumpy skin disease, it is necessary to recognize various stages of the disease. This research study referred to the use of several CNN architectures and Regression algorithms to detect the Lumpy skin disease virus as early as possible. The architectures explored are and the EfficientNet-EfficientNetB7 architecture, MobileNetV2, EfficientNet-EfficientNetB3 architecture, VGG16, InceptionV3, ResNet50, VGG19, Xception and DenseNet201. The paper thoroughly describes all of the steps required to carry out the disease detection model, from data collection to process and outcome.  © 2023 IEEE.","CNN; Deep Learning; Detection; identification; Livestock; recognition; Regression; Skin Disease","Agriculture; Convolutional neural networks; Deep learning; Dermatology; Diagnosis; Health risks; Losses; Network architecture; Convolutional neural network; Deep learning; Detection; Disease detection; Economic loss; Identification; Livestock; Recognition; Regression; Skin disease; Viruses","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546487-1","","","English","Proc. - Int. Conf. Smart Data Intell., ICSMDI","Conference paper","Final","","Scopus","2-s2.0-85161942671"
"Dang C.; Choi T.; Lee S.; Lee S.; Alam M.; Park M.; Han S.; Lee J.; Hoang D.","Dang, Changgwon (55895072300); Choi, Taejeong (55270660400); Lee, Seungsoo (57192515518); Lee, Soohyun (57208764474); Alam, Mahboob (35510212600); Park, Mina (55322279700); Han, Seungkyu (57929025900); Lee, Jaegu (57195478511); Hoang, Duytang (57190287330)","55895072300; 55270660400; 57192515518; 57208764474; 35510212600; 55322279700; 57929025900; 57195478511; 57190287330","Machine Learning-Based Live Weight Estimation for Hanwoo Cow","2022","Sustainability (Switzerland)","14","19","12661","","","","17","10.3390/su141912661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139963220&doi=10.3390%2fsu141912661&partnerID=40&md5=9177294019092dfb58d931dcaf2d9543","National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; ZOOTOS Co., Ltd, R&D Center, Gyeonggi-do, Anyang, 14118, South Korea","Dang C., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Choi T., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Lee S., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Lee S., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Alam M., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Park M., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Han S., ZOOTOS Co., Ltd, R&D Center, Gyeonggi-do, Anyang, 14118, South Korea; Lee J., National Institute of Animal Science, RDA, Chungcheongnam-do, Cheonan, 31000, South Korea; Hoang D., ZOOTOS Co., Ltd, R&D Center, Gyeonggi-do, Anyang, 14118, South Korea","Live weight monitoring is an important step in Hanwoo (Korean cow) livestock farming. Direct and indirect methods are two available approaches for measuring live weight of cows in husbandry. Recently, thanks to the advances of sensor technology, data processing, and Machine Learning algorithms, the indirect weight measurement has been become more popular. This study was conducted to explore and evaluate the feasibility of machine learning algorithms in estimating the body live weight of Hanwoo cow using ten body measurements as input features. Various supervised Machine Learning algorithms, including Multilayer Perceptron, k-Nearest Neighbor, Light Gradient Boosting Machine, TabNet, and FT-Transformer, are employed to develop the models that estimate the body live weight using body measurement data. Data analysis is exploited to explore the correlation between the body size measurements (the features) and the weights (target values that need to be estimated) of cows. Data analysis results show that ten body measurements have a high correlation with the body live weight. High performance of all applied Machine Learning models was obtained. It can be concluded that estimating the body live weight of Hanwoo cow is feasible by utilizing Machine Learning algorithms. Among all of the tested algorithms, LightGBM regression demonstrates not only the best model in terms of performance, model complexity and development time. © 2022 by the authors.","deep learning; Hanwoo cow; live weight estimation; machine learning","body size; estimation method; livestock; machine learning; regression","Korea Smart Farm R&D Foundation; Ministry of Science, ICT and Future Planning, MSIP; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421050-03); Rural Development Administration, RDA; Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (421050-03).","J. Lee; National Institute of Animal Science, RDA, Cheonan, Chungcheongnam-do, 31000, South Korea; email: jindog2929@korea.kr; D. Hoang; ZOOTOS Co., Ltd, R&D Center, Anyang, Gyeonggi-do, 14118, South Korea; email: buffalo@zootos.com","","MDPI","20711050","","","","English","Sustainability","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139963220"
"Pacheco V.M.; Sousa R.V.; Sardinha E.J.S.; Rodrigues A.V.S.; Brown-Brandl T.M.; Martello L.S.","Pacheco, Verônica M. (57218882298); Sousa, Rafael V. (57197806074); Sardinha, Edson J.S. (57211190696); Rodrigues, Alex V.S. (57809741600); Brown-Brandl, Tami M. (57207606316); Martello, Luciane S. (6602090108)","57218882298; 57197806074; 57211190696; 57809741600; 57207606316; 6602090108","Deep learning-based model classifies thermal conditions in dairy cows using infrared thermography","2022","Biosystems Engineering","221","","","154","163","9","8","10.1016/j.biosystemseng.2022.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134429934&doi=10.1016%2fj.biosystemseng.2022.07.001&partnerID=40&md5=42e2a6b972cd7d2ab406a29acefc5227","Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Biological Systems Engineering Department, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States","Pacheco V.M., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Sousa R.V., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Sardinha E.J.S., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Rodrigues A.V.S., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil; Brown-Brandl T.M., Biological Systems Engineering Department, University of Nebraska-Lincoln, 1400 R Street, Lincoln, 68588, NE, United States; Martello L.S., Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Av. Duque de Caxias Norte, 225, SP, Pirassununga, 13635-900, Brazil","Infrared thermography is a technique that has been utilized to assess the thermal status of animals. This study proposes a convolutional neural network (CNN)-based method for the individual classification of the thermal condition of dairy cows using thermal images of specific body surface regions. The experiment was carried out with 26 lactating cows (Holstein) during summer and winter (40 days). Meteorological data were collected every 30 min and rectal temperature (trectal), respiratory rate (RR), and surface temperature (tsf) were measured three times a day (5 a.m., 1 p.m., and 7 p.m.). tsf was correlated with RR and trectal and selected as input data of the models. Thus, thermal images were labelled according to the RR and trectal categories and employed in the investigation of four CNN-based models constructed by supervised learning and cross-validation protocols. Performance was assessed by confusion matrix metrics (accuracy, precision, recall, and F1-score) comparing the predicted labels and true labels. The best results were obtained with the models that used forehead images. The model using images labelled according to three RR thermal levels has an accuracy of 76%, and the model labelled according to three trectal thermal levels has an accuracy of 71%. The method based on deep learning allowed us to generate a computational classifier that considers not only the temperature intensities from thermographic images but also their distribution profile to identify patterns referring to thermal conditions in dairy cows. © 2022 IAgrE","Convolutional Neural Network; Non-Invasive Measurement; Precision Livestock Farming; Thermal Comfort; Thermography","Agriculture; Convolution; Convolutional neural networks; Deep learning; Learning systems; Meteorology; Thermal comfort; Thermography (temperature measurement); Convolutional neural network; Dairy cow; Learning Based Models; Non- invasive measurements; Precision livestock farming; Respiratory rate; Thermal condition; Thermal images; Thermal level; Thermography; Thermography (imaging)","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES","This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior – Brasil (CAPES) – Finance Code 001.","L.S. Martello; Department of Biosystems Engineering, Faculty of Animal Science and Food Engineering (FZEA), University of São Paulo (USP), Pirassununga, Av. Duque de Caxias Norte, 225, SP, 13635-900, Brazil; email: martello@usp.br","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85134429934"
"Das S.; Sharma A.","Das, Saradindu (58564302900); Sharma, Abhilasha (57219254740)","58564302900; 57219254740","Apple Leaves Disease Detection Using Multilayer Convolutional Neural Network","2023","2023 3rd International Conference on Intelligent Technologies, CONIT 2023","","","","","","","0","10.1109/CONIT59222.2023.10205764","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169918047&doi=10.1109%2fCONIT59222.2023.10205764&partnerID=40&md5=f6e58822be94d10f38853c3aeb991dc0","Delhi Technological University, Department of Software Engineering, Delhi, India","Das S., Delhi Technological University, Department of Software Engineering, Delhi, India; Sharma A., Delhi Technological University, Department of Software Engineering, Delhi, India","Agriculture is the process of growing crops and raising livestock and cultivating other forms of food. It has been a fundamental activity for human societies throughout history, providing food and other resources necessary for survival. Hence plant diseases can have a significant impact on the economy, particularly in agricultural-dependent countries. Crop yield loss, trade restrictions, increased production costs and reduced agricultural productivity are some of the ways plant diseases can affect the economy. Farmers visually inspect their crops for symptoms of diseases, such as discoloration, spots, wilting, and deformities. Farmers can also use their sense of touch and smell to detect diseases, such as the sticky or slimy feel of plant leaves infected with fungal diseases and the foul smell of rotting or decaying plant material. However, traditional methods of plant disease detection do have limitations. Visual inspection and other traditional methods may not always detect diseases at an early stage, and there is a risk of misdiagnosis. Furthermore, conventional techniques may fail to identify illnesses that are not perceptible to the unaided observation. The other alternatives are the use of Artificial Intelligence(AI) which includes training computers to detect plant diseases using image recognition technology. These AI methods are increasingly being used for plant disease detection because they offer a fast, accurate, and cost-effective way to diagnose plant diseases, which can help prevent crop losses and increase yields. In this article, a Multilayer CNN model, inspired by InceptionNet, is put forward. This model is trained on the ""Plant Pathology 2021: FGVC8 dataset"". This proposed model is compared with three existing CNN models using RMSprop with Nesterov Momentum Optimizer. The suggested model outperforms other models that are already trained and achieves an accuracy of 92.56% in diagnosing the disease.  © 2023 IEEE.","Artificial Intelligence (AI); Convolutional Neural Network (CNN); Deep Learning (DL); Plant Disease","Convolution; Convolutional neural networks; Cost effectiveness; Crops; Deep learning; Diagnosis; Image recognition; Multilayer neural networks; Multilayers; Plants (botany); Artificial intelligence; Convolutional neural network; Crop yield; Deep learning; Disease detection; Human society; Leaf disease detections; Neural network model; Plant disease; Neural network models","","","S. Das; Delhi Technological University, Department of Software Engineering, Delhi, India; email: saradindu24@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","979-835033860-7","","","English","Int. Conf. Intell. Technol., CONIT","Conference paper","Final","","Scopus","2-s2.0-85169918047"
"Durai S.K.S.; Shamili M.D.","Durai, Senthil Kumar Swami (57222640663); Shamili, Mary Divya (59309895300)","57222640663; 59309895300","Smart farming using Machine Learning and Deep Learning techniques","2022","Decision Analytics Journal","3","","100041","","","","129","10.1016/j.dajour.2022.100041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135125164&doi=10.1016%2fj.dajour.2022.100041&partnerID=40&md5=c43f371a3a24ffed256d5609c2dacd20","School of Engineering, Presidency University, Bangalore, India; Department of CSE, School of Engineering, Presidency University, Bangalore, India","Durai S.K.S., School of Engineering, Presidency University, Bangalore, India; Shamili M.D., Department of CSE, School of Engineering, Presidency University, Bangalore, India","The practice of cultivating the soil, producing crops, and keeping livestock is referred to as farming. Agriculture is critical to a country's economic development. Nearly 58 percent of a country's primary source of livelihood is farming. Farmers till date had adopted conventional farming techniques. These techniques were not precise thus reduced the productivity and consumed a lot of time. Precise farming helps to increase the productivity by precisely determining the steps that needs to be practiced at its due season. Predicting the weather conditions, analyzing the soil, recommending the crops for cultivation, determine the amount of fertilizers, pesticides that need to be used are some elements of precision farming. Precise Farming uses advanced technologies such as IOT, Data Mining, Data Analytics, Machine Learning to collect the data, train the systems and predict the results. With the help of technologies Precise farming helps to reduce manual labor and increase productivity. Farmers have been facing various challenges in these recent times, this includes crop failure due to less rainfall, infertility of soil and so on. Due to the changes taking place in the environment the proposed work helps to identify how to manage crops and harvest in a smart way. It guides an individual for smart farming. The aim of this work is to help an individual cultivate crops efficiently and hence achieve high productivity at low cost. It also helps to predict the total cost needed for cultivation. This would help an individual to pre-plan the activities before cultivation resulting in an integrated solution in farming. © 2022 The Author(s)","Deep learning; Machine learning; Precise; Recommendation","","","","S.K.S. Durai; School of Engineering, Presidency University, Bangalore, India; email: harisen1234@yahoo.co.in","","Elsevier Inc.","27726622","","","","English","Decis. Anal. J.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135125164"
"Jinzhuo M.; Chenghua T.; Cong G.; Pei J.; Hongjian Z.","Jinzhuo, Mou (58179320700); Chenghua, Tian (57216504641); Cong, Gu (58179146800); Pei, Jia (57217991431); Hongjian, Zhao (57220008546)","58179320700; 57216504641; 58179146800; 57217991431; 57220008546","Research on Target Detection and Grabbing Positioning Method Based on Deep Learning","2023","2023 IEEE 3rd International Conference on Power, Electronics and Computer Applications, ICPECA 2023","","","","1133","1138","5","0","10.1109/ICPECA56706.2023.10075802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152224448&doi=10.1109%2fICPECA56706.2023.10075802&partnerID=40&md5=9d9745c4bda9b1aef0d09fd17b221d44","Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China","Jinzhuo M., Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China; Chenghua T., Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China; Cong G., Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China; Pei J., Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China; Hongjian Z., Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China","For livestock and poultry carcass segmentation, the labor intensity is high, the skill requirement is high, and the segmentation efficiency is low; The segmentation process of livestock and poultry meat takes a long time, which is easy to cause secondary pollution of meat and other problems. It is necessary to upgrade the existing livestock meat processing industry and improve the intelligence degree of the current livestock carcass processing technology. In this paper, a fast adaptive histogram equalization method is proposed to enhance the contrast of the pig leg image captured by 3D camera, and then the trained YOLOV4 model is used to detect and calibrate the specified bone position of the pig leg image. This method first needs to establish local coordinate systems for the manipulator and the grasping target respectively, then uses Eye to Hand hand calibration system to obtain the corresponding conversion matrix, and transmits the position information of the grasping target to the manipulator, so that the manipulator can move to the designated position correctly and complete the grasping operation successfully. Field experiments verify the feasibility of this method.  © 2023 IEEE.","Eye to Hand; Fast histogram equalization; local coordinate system; target detection; yolov4","Agriculture; Cameras; Deep learning; Equalizers; Graphic methods; Image enhancement; Mammals; Meats; Eye-to-hand; Fast histogram equalization; Histogram equalizations; Labour intensity; Local coordinate system; Positioning methods; Poultry carcass; Skill requirements; Targets detection; Yolov4; Manipulators","National Key Research and Development Program of China, NKRDPC, (2019YFB1311003); National Key Research and Development Program of China, NKRDPC","ACKNOWLEDGMENT Fund Project: National Key Research and Development Program ""Intelligent Robot"" special project ""rapid autonomous boning robot workstation for livestock and poultry carcasses"" (No.2019YFB1311003)","Z. Hongjian; Beijing Research Institute of Automation for Machinery Industry Co., Ltd, Robot Center, Beijing, China; email: zhaohj@163.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166547278-4","","","English","IEEE Int. Conf. Power, Electron. Comput. Appl., ICPECA","Conference paper","Final","","Scopus","2-s2.0-85152224448"
"Anilkumar C.; Sunitha N.C.; Harikrishna; Devate N.B.; Ramesh S.","Anilkumar, C. (57193726163); Sunitha, N.C. (57221262838); Harikrishna (57191445352); Devate, Narayana Bhat (57424412500); Ramesh, S. (57205203669)","57193726163; 57221262838; 57191445352; 57424412500; 57205203669","Advances in integrated genomic selection for rapid genetic gain in crop improvement: a review","2022","Planta","256","5","87","","","","22","10.1007/s00425-022-03996-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138464541&doi=10.1007%2fs00425-022-03996-y&partnerID=40&md5=89f15b10b4068b57d4ddf2d537a8deaf","ICAR-National Rice Research Institute, Cuttack, India; University of Agricultural Sciences, Bangalore, India; ICAR-Indian Agriculture Research Institute, New Delhi, India","Anilkumar C., ICAR-National Rice Research Institute, Cuttack, India; Sunitha N.C., University of Agricultural Sciences, Bangalore, India; Harikrishna, ICAR-Indian Agriculture Research Institute, New Delhi, India; Devate N.B., ICAR-Indian Agriculture Research Institute, New Delhi, India; Ramesh S., University of Agricultural Sciences, Bangalore, India","Main conclusion: Genomic selection and its importance in crop breeding. Integration of GS with new breeding tools and developing SOP for GS to achieve maximum genetic gain with low cost and time. Abstract: The success of conventional breeding approaches is not sufficient to meet the demand of a growing population for nutritious food and other plant-based products. Whereas, marker assisted selection (MAS) is not efficient in capturing all the favorable alleles responsible for economic traits in the process of crop improvement. Genomic selection (GS) developed in livestock breeding and then adapted to plant breeding promised to overcome the drawbacks of MAS and significantly improve complicated traits controlled by gene/QTL with small effects. Large-scale deployment of GS in important crops, as well as simulation studies in a variety of contexts, addressed G × E interaction effects and non-additive effects, as well as lowering breeding costs and time. The current study provides a complete overview of genomic selection, its process, and importance in modern plant breeding, along with insights into its application. GS has been implemented in the improvement of complex traits including tolerance to biotic and abiotic stresses. Furthermore, this review hypothesises that using GS in conjunction with other crop improvement platforms accelerates the breeding process to increase genetic gain. The objective of this review is to highlight the development of an appropriate GS model, the global open source network for GS, and trans-disciplinary approaches for effective accelerated crop improvement. The current study focused on the application of data science, including machine learning and deep learning tools, to enhance the accuracy of prediction models. Present study emphasizes on developing plant breeding strategies centered on GS combined with routine conventional breeding principles by developing GS-SOP to achieve enhanced genetic gain. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Accelerated crop improvement; Cross-validation; Genetic gain; Integrated GS; SOP for GS","Genome, Plant; Genomics; Phenotype; Plant Breeding; Selection, Genetic; genetic selection; genetics; genomics; phenotype; plant breeding; plant genome","Indian Council of Agricultural Research, ICAR","Authors thankful to ICAR for providing support for research in the area of genomic selection.","S. Ramesh; University of Agricultural Sciences, Bangalore, India; email: ramesh_uasb@rediffmail.com","","Springer Science and Business Media Deutschland GmbH","00320935","","PLANA","36149531","English","Planta","Review","Final","","Scopus","2-s2.0-85138464541"
"Evangelista I.R.S.; Catajay L.T.; Palconit M.G.B.; Bautista M.G.A.C.; Concepcion R.S., II; Sybingco E.; Bandala A.A.; Dadios E.P.","Evangelista, Ivan Roy S. (57207911065); Catajay, Lenmar T. (58070631800); Palconit, Maria Gemel B. (57202289323); Bautista, Mary Grace Ann C. (57189097152); Concepcion, Ronnie S. (57208041010); Sybingco, Edwin (55497208700); Bandala, Argel A. (55599317400); Dadios, Elmer P. (6602629924)","57207911065; 58070631800; 57202289323; 57189097152; 57208041010; 55497208700; 55599317400; 6602629924","Detection of Japanese Quails (Coturnix japonica) in Poultry Farms Using YOLOv5 and Detectron2 Faster R-CNN","2022","Journal of Advanced Computational Intelligence and Intelligent Informatics","26","6","","930","936","6","7","10.20965/jaciii.2022.p0930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146494898&doi=10.20965%2fjaciii.2022.p0930&partnerID=40&md5=4f8f03aee70bf4bfc1e58f68346723e1","Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Computer Engineering Department, Sultan Kudarat State University E.J.C. Montilla, Sultan Kudarat, Isulan, 9805, Philippines; Department of Manufacturing and Management Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines","Evangelista I.R.S., Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Catajay L.T., Computer Engineering Department, Sultan Kudarat State University E.J.C. Montilla, Sultan Kudarat, Isulan, 9805, Philippines; Palconit M.G.B., Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Bautista M.G.A.C., Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Concepcion R.S., II, Department of Manufacturing and Management Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Sybingco E., Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Bandala A.A., Department of Electronics and Computer Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines; Dadios E.P., Department of Manufacturing and Management Engineering, De La Salle University, 2401 Taft Avenue, Malate, Manila, 1004, Philippines","Poultry, like quails, is sensitive to stressful environments. Too much stress can adversely affect birds’ health, causing meat quality, egg production, and reproduction to degrade. Posture and behavioral activities can be indicators of poultry wellness and health condition. Animal welfare is one of the aims of precision livestock farming. Computer vision, with its real-time, non-invasive, and accurate monitoring capability, and its ability to obtain a myriad of information, is best for livestock monitoring. This paper introduces a quail detection mechanism based on computer vision and deep learning using YOLOv5 and Detectron2 (Faster R-CNN) models. An RGB camera installed 3 ft above the quail cages was used for video recording. The annotation was done in MATLAB video labeler using the temporal interpolator algorithm. 898 ground truth images were extracted from the annotated videos. Augmentation of images by change of orientation, noise addition, manipulating hue, saturation, and brightness was performed in Roboflow. Training, validation, and testing of the models were done in Google Colab. The YOLOv5 and Detectron2 reached average precision (AP) of 85.07 and 67.15, respectively. Both models performed satisfactorily in detecting quails in different backgrounds and lighting conditions. © Fuji Technology Press Ltd. Creative Commons CC BY-ND: This is an Open Access article distributed under the terms of the Creative Commons Attribution-NoDerivatives 4.0 InternationalLicense (https://creativecommons.org/licenses/by-nd/4.0/)","deep learning; Detectron2; precision livestock farming; smart poultry farming; YOLOv5","Agriculture; Computer vision; Deep learning; Deep learning; Detectron2; Egg production; Health condition; Japanese quail; Meat quality; Poultry farms; Precision livestock farming; Smart poultry farming; YOLOv5; Video recording","De La Salle University, DLSU; Department of Science and Technology, Philippines, DOST; Department of Science and Technology, Republic of the Philippines, DOST","This work is supported by the Intelligent System Laboratory of De La Salle University and the Department of Science and Technology – Engineering Research and Development for Technology (DOST-ERDT) of the Philippines.","I.R.S. Evangelista; Department of Electronics and Computer Engineering, De La Salle University, Manila, 2401 Taft Avenue, Malate, 1004, Philippines; email: ivan_roy_evangelista@dlsu.edu.ph","","Fuji Technology Press","13430130","","","","English","J. Adv. Comput. Intell. Intelligent Informatics","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146494898"
"Banerjee S.; Ckakraboity S.; Mondal A.C.","Banerjee, Saikat (57303680900); Ckakraboity, Skubha (58136272900); Mondal, Abhoy Chand (55160984000)","57303680900; 58136272900; 55160984000","Machine Learning Based Crop Prediction on Region Wise Weather Data","2023","International Journal on Recent and Innovation Trends in Computing and Communication","11","1","","145","153","8","12","10.17762/ijritcc.v11i1.6084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149736029&doi=10.17762%2fijritcc.v11i1.6084&partnerID=40&md5=d2cf67c734cab4ea071d6e9ea6effc5a","The Department of Computer Science, The University of Burdwan, West Bengal, India; The Department of Computer Science, The University of Burdwan, Golapbag, West Bengal, India","Banerjee S., The Department of Computer Science, The University of Burdwan, West Bengal, India; Ckakraboity S., The Department of Computer Science, The University of Burdwan, Golapbag, West Bengal, India; Mondal A.C., The Department of Computer Science, The University of Burdwan, West Bengal, India","Agriculture is a primordial occupation for human civilization, whereby farmers cultivate domesticated species of food. It refers to farming in general, which is an art and science that attempts to reform a component of the Earth's exterior through the cultivation of plants and other crops, as well as raising livestock for sustenance or other necessities for the soul and economic gain. As a result of the vital role that sustainable agriculture plays in the overall health of the nation, this sector of the economy has been the incubator for some of the most cutting-edge technological advances in recent history. Scientists and farmers have been working together to discover new methods that will allow them to increase crop production while simultaneously decreasing their water consumption and lessening then negative effects on the environment. Machine learning, deep learning, and a number of other methodologies are some examples of these approaches. A crop's expansion and maturation are both heavily influenced by the climate in which it is grown. The local climate, namely its wind speed, temperature, rainfall, and humidity, is the most exigent factor in determining the advancement or failure of crop production. If the weather is predicted prior to crop cultivation, it will be beneficial to the farmer. Machine learning is a new innovation that can solve people's real-life problems. It is a technique where a machine can act like a human and learn through experiences and the use of different types of data. Now a day. Agriculture is one of the fields of machine learning where we use different types of machine learning algorithms to predict crop production based on climate data which can benefited farmers to increase the production of the crop. In these studies, we are going to predict crop yield using LSTM based on predicted weather data. © Auricle Global Society of Education and Research. All rights reserved.","Long Short Term Memory (LSTM): Crop; Machine Learning; Multiple Linear Regression","","","","","","Auricle Global Society of Education and Research","23218169","","","","English","Int. J. Recent. Innov. Trend. Comput. Commun.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149736029"
"Xie Q.; Zhou H.; Bao J.; Li Q.","Xie, Qiuju (59157795100); Zhou, Hong (55649568604); Bao, Jun (55675564700); Li, Qingda (27168640800)","59157795100; 55649568604; 55675564700; 27168640800","Review on Machine Vision-based Weight Assessment for Livestock and Poultry; [基于机器视觉的畜禽体质量评估研究进展]","2022","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","53","10","","1","15","14","9","10.6041/j.issn.1000-1298.2022.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141103546&doi=10.6041%2fj.issn.1000-1298.2022.10.001&partnerID=40&md5=114fea2919093cca7d81b37b5319d7fa","College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China; Key Laboratory of Swine Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China; Engineering Research Center of Pig Intelligent Breeding and Farming in Northern Cold Region, Ministry of Education, Harbin, 150030, China","Xie Q., College of Electrical and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Swine Facilities Engineering, Ministry of Agriculture and Rural Affairs, Harbin, 150030, China; Zhou H., College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China; Bao J., College of Animal Science and Technology, Northeast Agricultural University, Harbin, 150030, China, Engineering Research Center of Pig Intelligent Breeding and Farming in Northern Cold Region, Ministry of Education, Harbin, 150030, China; Li Q., College of Engineering, Heilongjiang Bayi Agricultural University, Daqing, 163319, China","Body weight is an important indicator for reflecting the health and growth conditions, reproduction and production performance of livestock and poultry. Accurate and rapid assessment and monitoring of livestock and poultry body weight is a critical way to improve the level of breeding management and achieve precision livestock farming. The traditional weighing method is time-consuming and laborious, and easy to cause stress response on animals. Weight assessment based on machine vision technology, which can establish an intelligent assessment model between body weight and body shape characteristics by using visual detection technology, is a hotspot of intelligent technology research in livestock and poultry breeding at present. Firstly, the methods of weight assessment were categorically described. Then, the sensor types, methods and applications of animal and poultry body feature treatment were analyzed in detail. The comparative analysis of the research on body size, physical signs and weight assessment model based on machine learning method were focused on. The application effect and the latest research results of various machine learning algorithms in weight assessment were presented. The development potential of deep learning algorithm in the field of automatic weight assessment of livestock and poultry was discussed and analyzed. Finally, the problems and challenges of weight assessment researches on livestock and poultry and the development trend of the future work were pointed out, which can provide some references for the scholars and engineers in the field of the modern intelligent weight assessment for livestock and poultry. © 2022 Chinese Society of Agricultural Machinery. All rights reserved.","body size; deep learning; livestock and poultry; machine learning; machine vision; weight assessment","Agriculture; Animals; Anthropometry; Cell proliferation; Deep learning; Learning algorithms; Learning systems; Assessment models; Body sizes; Body weight; Deep learning; Livestock and poultry; Machine-learning; Machine-vision; On-machines; Vision based; Weight assessment; Computer vision","","","","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Review","Final","","Scopus","2-s2.0-85141103546"
"Parmiggiani A.; Liu D.; Norton T.","Parmiggiani, A. (58044597200); Liu, D. (57189588678); Norton, T. (35273348100)","58044597200; 57189588678; 35273348100","Is it possible to identify individual animal faces with state-of-the-art computer vision algorithms?","2022","Precision Livestock Farming 2022 - Papers Presented at the 10th European Conference on Precision Livestock Farming, ECPLF 2022","","","","610","616","6","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172313035&partnerID=40&md5=d6f56b3851e32d530bb70b50462a67ce","Division M3-BIORES: Measure, Model and Manage Bioresponses, Catholic University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium","Parmiggiani A., Division M3-BIORES: Measure, Model and Manage Bioresponses, Catholic University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Liu D., Division M3-BIORES: Measure, Model and Manage Bioresponses, Catholic University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium; Norton T., Division M3-BIORES: Measure, Model and Manage Bioresponses, Catholic University of Leuven, Kasteelpark Arenberg 30, Heverlee, 3001, Belgium","Individual animal identification allows producers to keep records on an animal’s health history, geographical origin, dietary background, and a host of other important management information. In the field of computer vision the identification and verification of human and faces are two major issues where Deep Learning models have proven to be successful. The question remains: how performant these methods when trained on animal images? This study focused on the development of a Deep Learning based algorithm for this task. Using photos of cattle faces a similar procedure developed for human face recognition was developed. The algorithm comprised of (1) face detection, which allowed the models to focus on the bounding box around the cattle’s head, (2) face alignment based on facial landmarks, to standardize the position of the head, and (3) a CNN model (called Arcface), which employs L2-regularization of the feature vector and a loss function designed to maximize of the angular margin between classes and thus separating individuals in the feature space. The resulting pipeline can be used for animal verification when a current picture of the animal and another picture from an alike animal are compared. When there is no prior knowledge on the identity of the animal, the same pipeline can be utilized to rank images based on similarity and therefore match with the closest result. To train the models involved in the cattle face recognition, a dataset of 9182 pictures of 3200 individual cattle was collected with the help of CattleTracs® software. This method can lead to a faster way to track down the outbreak of a disease or facilitate verification of intentional or unintentional miss-labelling of the animals. © ECPLF 2022. All rights reserved.","arcface loss; cattle identification; deeplearning; face recognition; precision livestock","Agriculture; Animals; Computer vision; Deep learning; Pipelines; Vector spaces; Animal identification; Arcface loss; Cattle identification; Computer vision algorithms; Deeplearning; Geographical origins; Learning models; Management information; Precision livestock; State of the art; Face recognition","","","","Berckmans D.; Oczak M.; Iwersen M.; Wagener K.","Organising Committee of the 10th European Conference on Precision Livestock Farming (ECPLF), University of Veterinary Medicine Vienna","","978-839653600-6","","","English","Precis. Livest. Farming - Pap. Present. Eur. Conf. Precis. Livest. Farming, ECPLF","Conference paper","Final","","Scopus","2-s2.0-85172313035"
"Kavindi Gunasinghe U.L.D.; Malaviarachchi H.W.; Konthasinghe V.T.; Diwantha K.S.; Sriyarathna D.; Kasthurirathna D.","Kavindi Gunasinghe, U.L.D. (57546268500); Malaviarachchi, Hiruni Wasana (57545490600); Konthasinghe, Viraj Thanuja (57547420000); Diwantha, Kolamunnage Senura (57546268600); Sriyarathna, Disni (57205440596); Kasthurirathna, Dharshana (54420261000)","57546268500; 57545490600; 57547420000; 57546268600; 57205440596; 54420261000","GreenHubLK: A Machine Learning Driven Solution for Crop Disease Detection and Post-Harvest Crisis","2022","Lecture Notes in Networks and Systems","439 LNNS","","","273","293","20","1","10.1007/978-3-030-98015-3_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126973506&doi=10.1007%2f978-3-030-98015-3_19&partnerID=40&md5=86f0a7982f6dc08b1ad7591e2d5142fb","Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","Kavindi Gunasinghe U.L.D., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Malaviarachchi H.W., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Konthasinghe V.T., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Diwantha K.S., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Sriyarathna D., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Kasthurirathna D., Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","Agriculture can be considered as the art and science of cultivating plants and livestock where it is the key development in the rise of sedentary human civilization whereby farming of domesticated species created food surpluses that enabled people to continue their life cycle. The area below the kernel ‘density’ for the agricultural sector is equivalent to 0.45, suggesting that 45% of the world population relies on agriculture for their living. Agriculture has the potential to alleviate poverty for 75% of the world’s poor, who live in rural regions and mostly work in agriculture. The main obstruction to a good cultivation solely depends on the health of the plant. Cultivators having lack of knowledge, their inability to go with the present technology and having no possible way to identify ways plants catch diseases can be identified as the main problems of a prolific cultivation. Many individuals have tried to discover answers over time, yet the majority of them are still trapped in the same position. Our research primarily focuses on diseases that plants may contract over time. In the post-harvesting phase, we focus on the food banking concept and linking farmers with sellers and small scale enterprises. We also focus on discovering the demanded market based on a current market price prediction forecast. Although our concept begins with vegetables, it could be expanded and applied on large-scale farms and estates. We anticipate that this research will provide a solution to the majority of the issues that farmers encounter allowing them to ease their exertion. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","AutoKeras; AutoML; CNN; Crop disease identification; Deep learning; Location based algorithms; Optimization; Random forest","","","","U.L.D. Kavindi Gunasinghe; Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; email: uldkavindigunasinghe@gmail.com","Arai K.","Springer Science and Business Media Deutschland GmbH","23673370","978-303098014-6","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85126973506"
"Barnetson J.; Phinn S.; Scarth P.","Barnetson, Jason (57198887500); Phinn, Stuart (7004077841); Scarth, Peter (6506680683)","57198887500; 7004077841; 6506680683","Climate-Resilient Grazing in the Pastures of Queensland: An Integrated Remotely Piloted Aircraft System and Satellite-Based Deep-Learning Method for Estimating Pasture Yield","2021","AgriEngineering","3","3","","681","702","21","2","10.3390/agriengineering3030044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132360546&doi=10.3390%2fagriengineering3030044&partnerID=40&md5=3eca6a92ad0b84d59a3f53ee8f20a775","Joint Remote Sensing Research Centre, University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia; Grazing Land Systems/Remote Sensing Centre, Queensland Department of Environment and Science, Eco-Sciences Precinct, Dutton Park, Brisbane, 4102, QLD, Australia","Barnetson J., Joint Remote Sensing Research Centre, University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia, Grazing Land Systems/Remote Sensing Centre, Queensland Department of Environment and Science, Eco-Sciences Precinct, Dutton Park, Brisbane, 4102, QLD, Australia; Phinn S., Joint Remote Sensing Research Centre, University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia; Scarth P., Joint Remote Sensing Research Centre, University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia","The aim of this research is to expand recent developments in the mapping of pasture yield with remotely piloted aircraft systems to that of satellite-borne imagery. To date, spatially explicit and accurate information of the pasture resource base is needed for improved climate-adapted livestock rangeland grazing. This study developed deep learning predictive models of pasture yield, as total standing dry matter in tonnes per hectare (TSDM (tha−1 )), from field measurements and both remotely piloted aircraft systems and satellite imagery. Repeated remotely piloted aircraft system structure measurements derived from structure from motion photogrammetry provided measures of pasture biomass from many overlapping high-resolution images. These measurements were taken throughout a growing season and were modelled with persistent photosynthetic pasture responses from various Planet Dove high spatial resolution satellite image-derived vegetation indices. Pasture height modelling as an input to the modelling of yield was assessed against terrestrial laser scanning and reported correlation coefficients (R2 ) from 0.3 to 0.8 for both a coastal grassland and inland woodland pasture. Accuracy of the predictive modelling from both the remotely piloted aircraft system and the Planet Dove satellite image estimates of pasture yield ranged from 0.8 to 1.8 TSDM (tha−1 ). These results indicated that the practical application of repeated remotely piloted aircraft system derived measures of pasture yield can, with some limitations, be scaled-up to satellite-borne imagery to provide more temporally and spatially explicit measures of the pasture resource base. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","artificial neural networks; deep-learning; photogrammetry; remotely piloted aircraft system; structure from motion","","University of Queensland Joint Remote Sensing Research Program; Queensland University of Technology, QUT; Department of Agriculture and Fisheries, Queensland Government, DAF; Department of Environment and Science, Queensland Government, DES","Funding text 1: The anonymous reviewers for their review and feedback of the manuscript. The Jess family and staff of the Esk pastoral property that kindly provided support and access to their property. The Queensland University of Technology for access to the Samford Ecological Research Facility, in particular staff Bek Christensen and Marcus Yates for their on-site support and management. Support from the Australian Government Research Training Program Scholarship. Support and funding from the University of Queensland Joint Remote Sensing Research Program. Support and funding from the QLD Department of Environment and Science and the Department of Agriculture and Fisheries—Drought and Climate Adaptation Program, staff at the Eco-sciences Precinct Remote Sensing Centre for assistance, support, training, and advice including: Ken Brook, Christina Jones, Jacqui Willcocks and assistance in collecting and management of field data from Rebecca Farrell, Al Healy, Mathew Pringle, Skye Byer, Patrick Halloway, Tom Franz, Deanna Vandenberg, Jane Bryden-Brown, Leo Hardtke and Grant Fraser.; Funding text 2: Acknowledgments: The anonymous reviewers for their review and feedback of the manuscript. The Jess family and staff of the Esk pastoral property that kindly provided support and access to their property. The Queensland University of Technology for access to the Samford Ecological Research Facility, in particular staff Bek Christensen and Marcus Yates for their on-site support and management. Support from the Australian Government Research Training Program Scholarship. Support and funding from the University of Queensland Joint Remote Sensing Research Program. Support and funding from the QLD Department of Environment and Science and the Department of Agriculture and Fisheries—Drought and Climate Adaptation Program, staff at the Eco-sciences Precinct Remote Sensing Centre for assistance, support, training, and advice including: Ken Brook, Christina Jones, Jacqui Willcocks and assistance in collecting and management of field data from Rebecca Farrell, Al Healy, Mathew Pringle, Skye Byer, Patrick Halloway, Tom Franz, Deanna Vandenberg, Jane Bryden-Brown, Leo Hardtke and Grant Fraser.","J. Barnetson; Joint Remote Sensing Research Centre, University of Queensland, Brisbane, St Lucia, 4072, Australia; email: jason.barnetson@des.qld.gov.au","","MDPI","26247402","","","","English","AgriEng.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132360546"
"Amato A.; Amato F.; Angrisani L.; Barolli L.; Bonavolontà F.; Neglia G.; Tamburis O.","Amato, Alessandra (57208166117); Amato, Flora (57208073345); Angrisani, Leopoldo (7006649427); Barolli, Leonard (6601911059); Bonavolontà, Francesco (55825402400); Neglia, Gianluca (6602529587); Tamburis, Oscar (13205546400)","57208166117; 57208073345; 7006649427; 6601911059; 55825402400; 6602529587; 13205546400","Artificial Intelligence-Based Early Prediction Techniques in Agri-Tech Domain","2022","Lecture Notes in Networks and Systems","312","","","42","48","6","13","10.1007/978-3-030-84910-8_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113735059&doi=10.1007%2f978-3-030-84910-8_5&partnerID=40&md5=4c5358c62d963f21259815351dae4e18","Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy; Department of Electrical Engineering and Information Technologies, University of Federico II, Naples, Italy; Department of Information and Communication Engineering, Fukuoka Institute of Technology, Fukuoka, Japan; Department of Veterinary Medicine and Animal Productions, University of Federico II, Naples, Italy","Amato A., Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy; Amato F., Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy, Department of Electrical Engineering and Information Technologies, University of Federico II, Naples, Italy; Angrisani L., Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy, Department of Electrical Engineering and Information Technologies, University of Federico II, Naples, Italy; Barolli L., Department of Information and Communication Engineering, Fukuoka Institute of Technology, Fukuoka, Japan; Bonavolontà F., Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy, Department of Electrical Engineering and Information Technologies, University of Federico II, Naples, Italy; Neglia G., Department of Veterinary Medicine and Animal Productions, University of Federico II, Naples, Italy; Tamburis O., Department of Veterinary Medicine and Animal Productions, University of Federico II, Naples, Italy","This work is aimed at presenting an application of Artificial Intelligence techniques in the Precise Livestock Farming domain. In particular, we focused on the possibility of identifying mastitis using non-invasive IR techniques, delegating to artificial intelligence techniques the task of early detecting ongoing mastitis situations. We conduct an experimental campaign finalized to verify that an infrared thermography measurement technique (which absorbs infrared radiation and generates images based on the amount of heat generated) can be profitably used in the early detection of mastitis in buffalo populations. For this experimental campaign, the surface temperature of the udder skin was measured using an infrared camera. We use a Deep Learning approach to perform an early classification of the infrared pictures, finalized to distinguish healthy breasts from those with ongoing mastitis symptoms. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Innovation; One Digital Health; Precision livestock farming; Smart farming","","","","F. Amato; Centro Servizi Metrologici e Tecnologici Avanzati (CeSMA), University of Federico II, Naples, Italy; email: flora.amato@unina.it","Barolli L.; Chen H.; Enokido T.","Springer Science and Business Media Deutschland GmbH","23673370","978-303084909-2","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85113735059"
"Xu B.; Wang W.; Guo L.; Chen G.; Wang Y.; Zhang W.; Li Y.","Xu, Beibei (57215186247); Wang, Wensheng (56937276700); Guo, Leifeng (56542160600); Chen, Guipeng (57115604400); Wang, Yaowu (57317046500); Zhang, Wenju (57317193200); Li, Yongfeng (57577901700)","57215186247; 56937276700; 56542160600; 57115604400; 57317046500; 57317193200; 57577901700","Evaluation of deep learning for automatic multi‐view face detection in cattle","2021","Agriculture (Switzerland)","11","11","1062","","","","38","10.3390/agriculture11111062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118245360&doi=10.3390%2fagriculture11111062&partnerID=40&md5=57a0529846f5cf202b3ca5d8c6ca2661","Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Information Centre, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China; Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, 100086, China; Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; Laboratory of Geo‐Information Science and Remote Sensing, Wageningen University, Wageningen, 6708 PB, Netherlands","Xu B., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Wang W., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China, Information Centre, Ministry of Agriculture and Rural Affairs, Beijing, 100125, China, Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, 100086, China; Guo L., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China, Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, 100086, China; Chen G., Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; Wang Y., Laboratory of Geo‐Information Science and Remote Sensing, Wageningen University, Wageningen, 6708 PB, Netherlands; Zhang W., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Li Y., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China","Individual identification plays an important part in disease prevention and control, traceability of meat products, and improvement of agricultural false insurance claims. Automatic and accurate detection of cattle face is prior to individual identification and facial expression recognition based on image analysis technology. This paper evaluated the possibility of the cutting-edge object detection algorithm, RetinaNet, performing multi‐view cattle face detection in housing farms with fluctuating illumination, overlapping, and occlusion. Seven different pretrained CNN models (ResNet 50, ResNet 101, ResNet 152, VGG 16, VGG 19, Densenet 121 and Densenet 169) were fine‐tuned by transfer learning and re‐trained on the dataset in the paper. Experimental results showed that RetinaNet incorporating the ResNet 50 was superior in accuracy and speed through performance evaluation, which yielded an average precision score of 99.8% and an average processing time of 0.0438 s per image. Compared with the typical competing algorithms, the proposed method was preferable for cattle face detection, especially in particularly challenging scenarios. This research work demonstrated the potential of artificial intelligence towards the incorporation of computer vision systems for individual identification and other animal welfare improvements. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cattle face detection; Deep learning; Precision livestock; RetinaNet","","Hebei Province Key Research and Development Plan, (20327202D, 20327401D); Inner Mongolia Autonomous Region Science and Technology Major Project, (2020ZD0004); National Natural Science Foundation of China, NSFC, (32060776); China Scholarship Council, CSC, (202003250122); Youth Science Foundation of Jiangxi Province, (20192ACBL21023)","Funding: This research was supported by China Scholarship Council (202003250122) and was funded by Inner Mongolia Autonomous Region Science and Technology Major Project (2020ZD0004), National Natural Science Foundation of China (32060776), Youth Science Founda‐ tion of Jiangxi Province (20192ACBL21023), and Hebei Province Key Research and Development Plan (20327202D, 20327401D).","W. Wang; Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; email: wangwensheng@caas.cn","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118245360"
"Hitelman A.; Edan Y.; Godo A.; Berenstein R.; Lepar J.; Halachmi I.","Hitelman, A. (57429550000); Edan, Y. (7004434501); Godo, A. (57397582700); Berenstein, R. (36458091500); Lepar, J. (57397767600); Halachmi, I. (6701325703)","57429550000; 7004434501; 57397582700; 36458091500; 57397767600; 6701325703","Short Communication: The effect of age on young sheep biometric identification","2022","Animal","16","2","100452","","","","4","10.1016/j.animal.2021.100452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123572500&doi=10.1016%2fj.animal.2021.100452&partnerID=40&md5=d6c40e681eda6da917539216e9fe78f0","Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel; Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B 653, Be'er Sheva, 8410501, Israel","Hitelman A., Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel, Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B 653, Be'er Sheva, 8410501, Israel; Edan Y., Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B 653, Be'er Sheva, 8410501, Israel; Godo A., Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel; Berenstein R., Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel; Lepar J., Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel; Halachmi I., Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel, Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B 653, Be'er Sheva, 8410501, Israel","Biometric identification provides an important tool for precision livestock farming. This study investigates the effect of weight gain and sheep maturation on recognition performance. Sheep facial identification was implemented using two convolutional neural network (CNN) called Faster R-CNN, and ResNet50V2, equipped with the state-of-art Additive Angular Margin (ArcFace) loss function. The identification model was tested on 47 young sheep at different stages, during a 3-month growth period, when they were between 2 and 5 months old, throughout which the sheep gained approximately 30 kilograms in weight. Results revealed that when the model was trained and tested on images of sheep aged 2 months, the average accuracy of the group was 95.4%, compared with 91.3% when trained on images of sheep aged 2 months but tested on images of sheep aged 5 months. © 2022 The Authors","Animal ageing; Biometric identification; Convolutional neural network; Deep learning; Lamb","Animals; Biometric Identification; Neural Networks, Computer; Sheep; animal; biometry; sheep","Israeli Chief Scientist of Agriculture fund, (101000471, 20-12-0029, 459451415); Israeli Chief Scientist of Agriculture fund ?; Horizon 2020 Framework Programme, H2020, (862050); Ben-Gurion University of the Negev, BGU","Funding text 1: This study was supported by the Israeli Chief Scientist of Agriculture fund “Kandel” PLF center of expertise (20-12-0029, 459451415), “TechCare” (GA No 862050) and “Sm@RT” (GA No 101000471). Partial support was provided by Ben-Gurion University of the Negev through the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering.; Funding text 2: Special thanks to Mr. N. Bergman for his advice and contribution on computer vision and to Mr. A. Rosov, who hosted us in the sheep pen of Volcani Center. We wish to thank all of the PLF laboratory members and staff who supported this research. This study was supported by the Israeli Chief Scientist of Agriculture fund ?Kandel? PLF center of expertise (20-12-0029, 459451415), ?TechCare? (GA No 862050) and ?Sm@RT? (GA No 101000471). Partial support was provided by Ben-Gurion University of the Negev through the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering.","I. Halachmi; Precision Livestock Farming (PLF) Lab., Agricultural Engineering Inst., Agricultural Research Organization (A.R.O.) – Volcani Institute, Rishon Lezion, 68 Hamaccabim Road, P.O.B 15159, 7505101, Israel; email: halachmi@volcani.agri.gov.il","","Elsevier B.V.","17517311","","","35093616","English","Animal","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85123572500"
"Wutke M.; Heinrich F.; Das P.P.; Lange A.; Gentz M.; Traulsen I.; Warns F.K.; Schmitt A.O.; Gültas M.","Wutke, Martin (57217124907); Heinrich, Felix (57208835958); Das, Pronaya Prosun (57217126023); Lange, Anita (57211910771); Gentz, Maria (57202136198); Traulsen, Imke (35410826800); Warns, Friederike K. (57222998642); Schmitt, Armin Otto (7201405093); Gültas, Mehmet (25653919300)","57217124907; 57208835958; 57217126023; 57211910771; 57202136198; 35410826800; 57222998642; 7201405093; 25653919300","Detecting animal contacts—A deep learning-based pig detection and tracking approach for the quantification of social contacts","2021","Sensors","21","22","7512","","","","36","10.3390/s21227512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118895942&doi=10.3390%2fs21227512&partnerID=40&md5=58d5ed32d48f5df0dc3fac4f2f2e665e","Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Livestock Systems, Department of Animal Sciences, Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Bioinformatics Group, Fraunhofer Institute for Toxicology and Experimental Medicine (Fraunhofer ITEM), Nikolai-Fuchs-Str. 1, Hannover, 30625, Germany; Agricultural Test and Education Centre House Düsse, Chamber of Agriculture North Rhine-Westphalia, Haus Düsse 2, Bad Sassendorf, 59505, Germany; Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Statistics and Data Science, Faculty of Agriculture, South Westphalia University of Applied Sciences, Soest, 59494, Germany","Wutke M., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany, Livestock Systems, Department of Animal Sciences, Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Heinrich F., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Das P.P., Bioinformatics Group, Fraunhofer Institute for Toxicology and Experimental Medicine (Fraunhofer ITEM), Nikolai-Fuchs-Str. 1, Hannover, 30625, Germany; Lange A., Livestock Systems, Department of Animal Sciences, Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Gentz M., Livestock Systems, Department of Animal Sciences, Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Traulsen I., Livestock Systems, Department of Animal Sciences, Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Warns F.K., Agricultural Test and Education Centre House Düsse, Chamber of Agriculture North Rhine-Westphalia, Haus Düsse 2, Bad Sassendorf, 59505, Germany; Schmitt A.O., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany, Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Gültas M., Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany, Statistics and Data Science, Faculty of Agriculture, South Westphalia University of Applied Sciences, Soest, 59494, Germany","The identification of social interactions is of fundamental importance for animal behavioral studies, addressing numerous problems like investigating the influence of social hierarchical struc-tures or the drivers of agonistic behavioral disorders. However, the majority of previous studies often rely on manual determination of the number and types of social encounters by direct observation which requires a large amount of personnel and economical efforts. To overcome this limitation and increase research efficiency and, thus, contribute to animal welfare in the long term, we propose in this study a framework for the automated identification of social contacts. In this framework, we apply a convolutional neural network (CNN) to detect the location and orientation of pigs within a video and track their movement trajectories over a period of time using a Kalman filter (KF) algorithm. Based on the tracking information, we automatically identify social contacts in the form of head–head and head–tail contacts. Moreover, by using the individual animal IDs, we construct a network of social contacts as the final output. We evaluated the performance of our framework based on two distinct test sets for pig detection and tracking. Consequently, we achieved a Sensitivity, Precision, and F1-score of 94.2%, 95.4%, and 95.1%, respectively, and a MOTA score of 94.4%. The findings of this study demonstrate the effectiveness of our keypoint-based tracking-by-detection strategy and can be applied to enhance animal monitoring systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Kalman filter; Pig detection; Pig tracking; Precision livestock farming","Algorithms; Animal Welfare; Animals; Deep Learning; Movement; Neural Networks, Computer; Swine; Agricultural robots; Agriculture; Behavioral research; Convolution; Convolutional neural networks; Deep learning; Economic and social effects; Mammals; Behavioural studies; Convolutional neural network; Detection and tracking; Detection approach; Pig detection; Pig tracking; Precision livestock farming; Social contacts; Social interactions; Tracking approaches; algorithm; animal; animal welfare; movement (physiology); pig; Kalman filters","Deutsche Forschungsgemeinschaft, DFG; Georg-August-Universität Göttingen, GAU; Landwirtschaftliche Rentenbank, (2817205413, 758914)","Funding text 1: The project (InnoPig) was supported by funds of the German Government’s Special Purpose Fund deposited at the Landwirtschaftliche Rentenbank (project no.: 2817205413; 758914).; Funding text 2: Acknowledgments: We thank A. Rajavel (Breeding Informatics Group, University of Göttingen) for proofreading and her valuable advice. We thank the Chamber of Agriculture of Schleswig-Holstein for their support in data acquisition. We acknowledge support by the German Research Foundation and the Open Access Publication Funds of the Göttingen University.","M. Wutke; Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Göttingen, Margarethe von Wrangell-Weg 7, 37075, Germany; email: martin.wutke@uni-goettingen.de; M. Gültas; Center for Integrated Breeding Research (CiBreed), Georg-August University, Göttingen, Albrecht-Thaer-Weg 3, 37075, Germany; email: gueltas.mehmet@fh-swf.de","","MDPI","14248220","","","34833588","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118895942"
"Han J.; Gondro C.; Reid K.; Steibel J.P.","Han, Junjie (57211723461); Gondro, Cedric (36058913100); Reid, Kenneth (57193713793); Steibel, Juan P. (11939709900)","57211723461; 36058913100; 57193713793; 11939709900","Heuristic hyperparameter optimization of deep learning models for genomic prediction","2021","G3: Genes, Genomes, Genetics","11","7","jkab032","","","","22","10.1093/g3journal/jkab032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111584544&doi=10.1093%2fg3journal%2fjkab032&partnerID=40&md5=edf5cdb7f5be39fb2242baf578bc748f","Department of Animal Science, Michigan State University, East Lansing, 48824, MI, United States; Department of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, 48824, MI, United States; Department of Fisheries and Wildlife, Michigan State University, East Lansing, 48824, MI, United States","Han J., Department of Animal Science, Michigan State University, East Lansing, 48824, MI, United States, Department of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, 48824, MI, United States; Gondro C., Department of Animal Science, Michigan State University, East Lansing, 48824, MI, United States; Reid K., Department of Animal Science, Michigan State University, East Lansing, 48824, MI, United States; Steibel J.P., Department of Animal Science, Michigan State University, East Lansing, 48824, MI, United States, Department of Fisheries and Wildlife, Michigan State University, East Lansing, 48824, MI, United States","There is a growing interest among quantitative geneticists and animal breeders in the use of deep learning (DL) for genomic prediction. However, the performance of DL is affected by hyperparameters that are typically manually set by users. These hyperparameters do not simply specify the architecture of the model; they are also critical for the efficacy of the optimization and model-fitting process. To date, most DL approaches used for genomic prediction have concentrated on identifying suitable hyperparameters by exploring discrete options from a subset of the hyperparameter space. Enlarging the hyperparameter optimization search space with continuous hyperparameters is a daunting combinatorial problem. To deal with this problem, we propose using differential evolution (DE) to perform an efficient search of arbitrarily complex hyperparameter spaces in DL models, and we apply this to the specific case of genomic prediction of livestock phenotypes. This approach was evaluated on two pig and cattle datasets with real genotypes and simulated phenotypes (N ¼ 7,539 animals and M ¼ 48,541 markers) and one real dataset (N ¼ 910 individuals and M ¼ 28,916 markers). Hyperparameters were evaluated using cross-validation. We compared the predictive performance of DL models using hyperparameters optimized by DE against DL models with “best practice” hyperparameters selected from published studies and baseline DL models with randomly specified hyperparameters. Optimized models using DE showed a clear improvement in predictive performance across all three datasets. DE optimized hyperparameters also resulted in DL models with less overfitting and less variation in predictive performance over repeated retraining compared to non-optimized DL models. © The Author(s) 2021. Published by Oxford University Press on behalf of Genetics Society of America.","Deep learning; Differential evolution; Evolutionary algorithm; Genomic prediction; Hyperparameter optimization","Animals; Cattle; Deep Learning; Genome; Genomics; Genotype; Heuristics; Swine; adult; animal experiment; article; bovine; cross validation; deep learning; evolutionary algorithm; female; genetic marker; genotype; human; livestock; male; nonhuman; phenotype; pig; prediction; simulation; validation process; animal; genome; genomics; heuristics","AFRI, (2019-67015-29323); US Pig Genome Coordinator; National Institute of Food and Agriculture, NIFA; National Pork Board, NPB, (11–042)","the National Institute of Food and Agriculture (AFRI Project No. 2019-67015-29323), and by funding from the National Pork Board Grant No. 11–042. Partial funding was also provided by the US Pig Genome Coordinator.","J.P. Steibel; 1205I Anthony Hall, East Lansing, 474 S Shaw Ln, 48824, United States; email: steibelj@msu.edu","","Genetics Society of America","21601836","","","33993261","English","G3 Genes Genome Genet.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85111584544"
"Hansen M.F.; Baxter E.M.; Rutherford K.M.D.; Futro A.; Smith M.L.; Smith L.N.","Hansen, Mark F. (57192115769); Baxter, Emma M. (23481382800); Rutherford, Kenny M. D. (7007171616); Futro, Agnieszka (56856836400); Smith, Melvyn L. (55495905800); Smith, Lyndon N. (9237709400)","57192115769; 23481382800; 7007171616; 56856836400; 55495905800; 9237709400","Towards facial expression recognition for on-farm welfare assessment in pigs","2021","Agriculture (Switzerland)","11","9","847","","","","13","10.3390/agriculture11090847","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114840747&doi=10.3390%2fagriculture11090847&partnerID=40&md5=f273303aed787bb3105697624694ed3a","Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom","Hansen M.F., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Baxter E.M., Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; Rutherford K.M.D., Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; Futro A., Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; Smith M.L., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Smith L.N., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom","Animal welfare is not only an ethically important consideration in good animal husbandry but can also have a significant effect on an animal’s productivity. The aim of this paper was to show that a reduction in animal welfare, in the form of increased stress, can be identified in pigs from frontal images of the animals. We trained a convolutional neural network (CNN) using a leave-one-out design and showed that it is able to discriminate between stressed and unstressed pigs with an accuracy of >90% in unseen animals. Grad-CAM was used to identify the animal regions used, and these supported those used in manual assessments such as the Pig Grimace Scale. This innovative work paves the way for further work examining both positive and negative welfare states with the aim of developing an automated system that can be used in precision livestock farming to improve animal welfare. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Animal welfare; Computer vision; Deep learning; Facial expression recognition; Pigs; Stress detection","","Biotechnology and Biological Sciences Research Council, BBSRC, (BB/S002138/1, BB/S002294/1)","Acknowledgments: This research was funded by the Biotechnology and Biological Sciences Research Council, UK (Grant Reference: BB/S002138/1 and BB/S002294/1). We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Maxwell Titan X GPU used for this research and are extremely grateful to farm and technical staff at SRUC’s Pig Research Centre.","M.F. Hansen; Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; email: mark.hansen@uwe.ac.uk","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85114840747"
"Shojaeipour A.; Falzon G.; Kwan P.; Hadavi N.; Cowley F.C.; Paul D.","Shojaeipour, Ali (36081316300); Falzon, Greg (13407283800); Kwan, Paul (7004369297); Hadavi, Nooshin (36666230300); Cowley, Frances C. (56315980900); Paul, David (36173145100)","36081316300; 13407283800; 7004369297; 36666230300; 56315980900; 36173145100","Automated muzzle detection and biometric identification via few-shot deep transfer learning of mixed breed cattle","2021","Agronomy","11","11","2365","","","","38","10.3390/agronomy11112365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120079086&doi=10.3390%2fagronomy11112365&partnerID=40&md5=b3652111188ef3e5f0e95ecf9e2a6d6d","School of Science and Technology, University of New England, Armidale, 2350, NSW, Australia; College of Science and Engineering, Flinders University, Adelaide, 5001, SA, Australia; School of IT and Engineering, Melbourne Institute of Technology, Melbourne, 3000, VIC, Australia; School of Environmental and Rural Science, University of New England, Armidale, 2350, NSW, Australia","Shojaeipour A., School of Science and Technology, University of New England, Armidale, 2350, NSW, Australia; Falzon G., School of Science and Technology, University of New England, Armidale, 2350, NSW, Australia, College of Science and Engineering, Flinders University, Adelaide, 5001, SA, Australia; Kwan P., School of IT and Engineering, Melbourne Institute of Technology, Melbourne, 3000, VIC, Australia; Hadavi N., School of Science and Technology, University of New England, Armidale, 2350, NSW, Australia; Cowley F.C., School of Environmental and Rural Science, University of New England, Armidale, 2350, NSW, Australia; Paul D., School of Science and Technology, University of New England, Armidale, 2350, NSW, Australia","Livestock welfare and management could be greatly enhanced by the replacement of branding or ear tagging with less invasive visual biometric identification methods. Biometric identification of cattle from muzzle patterns has previously indicated promising results. Significant barriers exist in the translation of these initial findings into a practical precision livestock monitoring system, which can be deployed at scale for large herds. The objective of this study was to investigate and address key limitations to the autonomous biometric identification of cattle. The contributions of this work are fourfold: (1) provision of a large publicly-available dataset of cattle face images (300 individual cattle) to facilitate further research in this field, (2) development of a two-stage YOLOv3-ResNet50 algorithm that first detects and extracts the cattle muzzle region in images and then applies deep transfer learning for biometric identification, (3) evaluation of model performance across a range of cattle breeds, and (4) utilizing few-shot learning (five images per individual) to greatly reduce both the data collection requirements and duration of model training. Results indicated excellent model performance. Muzzle detection accuracy was 99.13% (1024 × 1024 image resolution) and biometric identification achieved 99.11% testing accuracy. Overall, the two-stage YOLOv3-ResNet50 algorithm proposed has substantial potential to form the foundation of a highly accurate automated cattle biometric identification system, which is applicable in livestock farming systems. The obtained results indicate that utilizing livestock biometric monitoring in an advanced manner for resource management at multiple scales of production is possible for future agriculture decision support systems, including providing useful information to forecast acceptable stocking rates of pastures. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Biometric identification; Cattle; Deep learning; Few-shot learning; Livestock welfare; Muzzle detection; Precision livestock farming","","University of New England, UNE","Funding: The authors acknowledge the support provided through the International Post-graduate Award (IPRA) scholarship for Ali Shojaeipour.","A. Shojaeipour; School of Science and Technology, University of New England, Armidale, 2350, Australia; email: ashojae2@une.edu.au","","MDPI","20734395","","","","English","Agronomy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120079086"
"Noe S.M.; Zin T.T.; Tin P.; Kobayashi I.","Noe, Su Myat (57221527571); Zin, Thi Thi (6506258245); Tin, Pyke (24923729700); Kobayashi, Ikuo (24174822700)","57221527571; 6506258245; 24923729700; 24174822700","AUTOMATIC DETECTION AND TRACKING OF MOUNTING BEHAVIOR IN CATTLE USING A DEEP LEARNING-BASED INSTANCE SEGMENTATION MODEL","2022","International Journal of Innovative Computing, Information and Control","18","1","","211","220","9","22","10.24507/ijicic.18.01.211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122149443&doi=10.24507%2fijicic.18.01.211&partnerID=40&md5=a8042e0351a5b57e5b8afb28543d778a","Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan; Field Science Center, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan","Noe S.M., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan; Zin T.T., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan; Tin P., Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan; Kobayashi I., Field Science Center, Faculty of Agriculture University of Miyazaki, 1-1, Gakuenkibanadai-Nishi, Miyazaki, 889-2192, Japan","In precision livestock farming, estrus detection in cattle is particularly im-portant for cattle breeding management. With accurate estrus detection, artificial in-semination can be administered, which proportionally affects the productivity of livestock farms. Most estrus behaviors can be successfully detected by recognizing the mating postures of cattle. Therefore, in this paper, we propose an estrus detection approach that tracks and identifies cattle mating postures individually based on video inputs. To achieve precise identification and to obtain individual cattle information, segmenting each cattle from its background is a vital step. To solve pixel-level segmentation masks for the cattle in an outer ranch environment, an instance segmentation approach based on a Mask R-CNN deep learning framework is also proposed. In this paper, individual cattle segmentation for detecting the mounting behaviors is carried out first. This is followed by a lightweight tracking algorithm as a post-processing step which is our study innovation. The training data were collected by installing surveillance cameras at a livestock farm, and for the testing data, various datasets from different camera placements were used. The proposed approach achieved 95.5% detection accuracy in identifying the estrus be-haviors of cattle. © ICIC International 2022.","Cattle mounting behavior; Cattle tracking; Deep learning; Mask R-CNN","Agriculture; Cameras; Deep learning; Instance Segmentation; Security systems; Automatic Detection; Automatic tracking; Cattle breedings; Cattle mounting behavior; Cattle tracking; Deep learning; Detection and tracking; Matings; Precision livestock farming; Segmentation models; Mountings","Strategic Information and Communications R&D Promotion Program, (172310006); Japan Society for the Promotion of Science, KAKEN, (17K08066)","Acknowledgment. This work was supported in part by SCOPE: Strategic Information and Communications R&D Promotion Program (Grant No. 172310006) and JSPS KAK-ENHI (Grant No. 17K08066).","S.M. Noe; Graduate School of Engineering, Faculty of Agriculture University of Miyazaki, Miyazaki, 1-1, Gakuenkibanadai-Nishi, 889-2192, Japan; email: z319701@student.miyazaki.ac.jp","","ICIC International","13494198","","","","English","Int. J. Innov. Comput. Inf. Control","Article","Final","","Scopus","2-s2.0-85122149443"
"Su Q.; Tang J.; Zhai J.; Sun Y.; He D.","Su, Qingguo (57224578407); Tang, Jinglei (35176589500); Zhai, Jinhui (57224583254); Sun, Yurou (57205325161); He, Dongjian (19933691800)","57224578407; 35176589500; 57224583254; 57205325161; 19933691800","Automatic tracking of the dairy goat in the surveillance video","2021","Computers and Electronics in Agriculture","187","","106254","","","","11","10.1016/j.compag.2021.106254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107921161&doi=10.1016%2fj.compag.2021.106254&partnerID=40&md5=ec2c50d2fd5d3c1986efe137c991522f","College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China; The Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, Shaanxi, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, Shaanxi, China","Su Q., College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China; Tang J., College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China, The Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, Shaanxi, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, Shaanxi, China; Zhai J., College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China; Sun Y., College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China; He D., The Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture, Yangling, 712100, Shaanxi, China, Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Yangling, 712100, Shaanxi, China","Automatic tracking is an important basis for abnormal behavior management and disease prediction of livestock. In commercial farms, the use of surveillance video to track and monitor dairy goats is conducive to improving production efficiency and commercial welfare. In this paper, an algorithm-based on Siamese strategy is presented for the automated tracking of a single dairy goat in the surveillance video. The Dairy Goat Dataset (DG-dataset) containing 200 dairy goat motion videos with a total of 161,000 frames of images randomly collected from the farm was created, and the ImageNet VID, Youtube-BB, and GOT-10 k were used for training. First, the proposed tracker named SiamBNAN employs an effective and highly modular backbone network constructed by the Multi-Convolution Residual Blocks (MCRBs) and Down-sampling Multi-Convolution Residual Blocks (D-MCRBs). The MCRBs and D-MCRBs replace the original square convolution kernel with a kernel augmented by asymmetric convolution kernels and perform a set of group convolutions. Finally, the Regional Proposal Network (RPN) is used for foreground-background classification and proposal regression. The experimental results show that this algorithm outperforms SiamFC, SiamRPN, and SiamRPN + in terms of both Expected Average Overlap (EAO), Robustness (R), Success Rate (Succ), and Precision (Prec) on the DG-dataset. The SiamBNAN runs at 70 fps with low computing space requirements showing that it is effective and could be used for monitoring the behavior of dairy goats in the real farming scene. © 2021","Dairy goat; Deep learning; Livestock tracking; Siamese network","Varanidae; Convolution; Deep learning; Monitoring; Security systems; Abnormal behavior; Automatic tracking; Block sampling; Convolution kernel; Dairy goat; Deep learning; Down sampling; Livestock tracking; Siamese network; Surveillance video; Agriculture","Innovation Support Plan of Shaanxi Province; Fundamental Research Funds for the Central Universities, (2452020173); Key Research and Development Projects of Shaanxi Province, (NY098)","This work was supported in part by the Fundamental Research Funds for the Central Universities (No.2452020173), the Innovation Support Plan of Shaanxi Province (No.2021TD-31), the Key Research and Development Project of Shaanxi Province (Grant No.2021NY-138,2020NY098). The authors appreciate the funding organizations for their financial supports.","J. Tang; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: tangjinglei@nwsuaf.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85107921161"
"Padubidri C.; Kamilaris A.; Karatsiolis S.; Kamminga J.","Padubidri, Chirag (57222362784); Kamilaris, Andreas (36189564000); Karatsiolis, Savvas (55569345400); Kamminga, Jacob (57189244450)","57222362784; 36189564000; 55569345400; 57189244450","Counting sea lions and elephants from aerial photography using deep learning with density maps","2021","Animal Biotelemetry","9","1","27","","","","19","10.1186/s40317-021-00247-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112639293&doi=10.1186%2fs40317-021-00247-x&partnerID=40&md5=134b24b183feb2ecd99b871c42140ecf","Pervasive Systems, University of Twente, Enschede, Netherlands; CYENS Center of Excellence, Nicosia, Cyprus","Padubidri C., Pervasive Systems, University of Twente, Enschede, Netherlands, CYENS Center of Excellence, Nicosia, Cyprus; Kamilaris A., Pervasive Systems, University of Twente, Enschede, Netherlands, CYENS Center of Excellence, Nicosia, Cyprus; Karatsiolis S., CYENS Center of Excellence, Nicosia, Cyprus; Kamminga J., Pervasive Systems, University of Twente, Enschede, Netherlands","Background: The ability to automatically count animals is important to design appropriate environmental policies and to monitor their populations in relation to biodiversity and maintain balance among species. Out of all living mammals on Earth, 60% are livestock, 36% humans, and only 4% are animals that live in the wild. In a relatively short period, development of human civilization caused a loss of 83% of wildlife and 50% of plants. The rate of species extinction is accelerating. Traditional wildlife surveys provide rough population estimates. However, emerging technologies, such as aerial photography, allow to perform large-scale surveys in a short period of time with high accuracy. In this paper, we propose the use of computer vision, through deep learning (DL) architecture, together with aerial photography and density maps, to count the population of Steller sea lions and African elephants with high precision. Results: We have trained two deep learning models, a basic UNet without any feature extractor (Model-1) and another with the EfficientNet-B5 feature extractor (Model-2). We measured the model’s prediction accuracy, using Root Mean Square Error (RMSE) for the predicted and actual animal count. The results showed an RMSE of 1.88 and 0.60 to count Steller sea lions and African elephants, respectively, regardless of complex background, different illumination conditions, heavy overlapping and occlusion of the animals. Conclusions: Our proposed solution performed very well in the counting prediction problem, with relatively low training parameters and minimum annotation. The approach adopted, combining DL and density maps, provided better results than state-of-art deep learning models used for counting, indicating that the proposed method has the potential to be used more widely in large-scale wildlife surveying projects and initiatives. © 2021, The Author(s).","Aerial photography; Animal counting; Deep learning; Elephant; Steller sea-lions","","Directorate General for European Programmes, Coordination and Development; Horizon 2020 Framework Programme, H2020, (739578)","Andreas Kamilaris and Savvas Karatsiolis have received funding from the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 739578 complemented by the Government of the Republic of Cyprus through the Directorate General for European Programmes, Coordination and Development. ","C. Padubidri; CYENS Center of Excellence, Nicosia, Cyprus; email: c.padubidri@cyens.org.cy","","BioMed Central Ltd","20503385","","","","English","Anim. Biotelem.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85112639293"
"Zhang Y.; Thorburn P.J.","Zhang, Yifan (59071457500); Thorburn, Peter J. (7004313790)","59071457500; 7004313790","A dual-head attention model for time series data imputation","2021","Computers and Electronics in Agriculture","189","","106377","","","","18","10.1016/j.compag.2021.106377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112833465&doi=10.1016%2fj.compag.2021.106377&partnerID=40&md5=f56d6f3034e640db37dfc12328fa6b85","Agriculture & Food, CSIRO, Brisbane, 4067, QLD, Australia; Queensland Alliance for Agriculture and Food Innovation, The University of Queensland, Brisbane, 4072, QLD, Australia","Zhang Y., Agriculture & Food, CSIRO, Brisbane, 4067, QLD, Australia, Queensland Alliance for Agriculture and Food Innovation, The University of Queensland, Brisbane, 4072, QLD, Australia; Thorburn P.J., Agriculture & Food, CSIRO, Brisbane, 4067, QLD, Australia","Digital agriculture increasingly relies on the availability and accuracy of measurement data collected from various sensors. Of this data, water quality attracts great attention due to its intended use for crop irrigation, livestock, and other farming activities. Accurate and reliable water quality measurements enable farmers to understand the landscape comprehensively, optimising resource utilisation and reducing the negative impacts of agriculture on the environment. In practice, missing and incomplete data can create biased estimations and reduce the efficiency of many of the valuable applications provided by digital agriculture. The purpose of this paper is to propose a dual-head sequence-to-sequence imputation model (Dual-SSIM) designed to impute missing time series data in sensor networks, therefore reducing the negative consequences of missing and incomplete data. Unlike standard sequence-to-sequence architecture, the Dual-SSIM model features two encoders with gated recurrent units (GRUs) which are used to process temporal information before and after the missing gap separately. Furthermore, the attention mechanism is applied to two encoder outputs concurrently, in order to allow the model to focus on the high relative inputs when estimating missing data. The performance efficacy of Dual-SSIM has been investigated through the monitoring of water quality, sourced from an Australian water quality information system. Experimental results of this investigation indicate that Dual-SSIM outperforms associated alternatives based on the mean absolute error (MAE), root mean square error (RMSE), and dynamic time warping (DTW) scores in imputing two different water quality variables. Therefore, it can be concluded that Dual-SSIM provides an effective and promising approach for water quality data imputation. © 2021 The Author(s)","Data imputation; Deep learning; Sequence-to-sequence; Water quality","Grus grus; Agriculture; Deep learning; Mean square error; Sensor networks; Signal encoding; Time series; Accuracy of measurements; Attention model; Data imputation; Deep learning; Digital agriculture; Dual-head; Incomplete data; Missing data; Sequence-to-sequence; Time-series data; data interpretation; estimation method; landscape change; numerical model; sensor; time series analysis; water quality; Water quality","","","Y. Zhang; Agriculture & Food, CSIRO, Brisbane, 4067, Australia; email: yifan.zhang@uq.edu.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112833465"
"Shaikh F.K.; Memon M.A.; Mahoto N.A.; Zeadally S.; Nebhen J.","Shaikh, Faisal Karim (18435217400); Memon, Mohsin Ali (57034633400); Mahoto, Naeem Ahmed (36628922900); Zeadally, Sherali (7003472739); Nebhen, Jamel (42561810700)","18435217400; 57034633400; 36628922900; 7003472739; 42561810700","Artificial Intelligence Best Practices in Smart Agriculture","2022","IEEE Micro","42","1","","17","24","7","29","10.1109/MM.2021.3121279","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118245911&doi=10.1109%2fMM.2021.3121279&partnerID=40&md5=9853f1c7946dc6d4d67cf776ad8b4e2e","Mehran University of Engineering and Technology, Jamshoro, 76062, Pakistan; University of Kentucky, Lexington, 40506, KY, United States; Prince Sattam Bin Abdulaziz University, Al-Kharj, 16278, Saudi Arabia","Shaikh F.K., Mehran University of Engineering and Technology, Jamshoro, 76062, Pakistan; Memon M.A., Mehran University of Engineering and Technology, Jamshoro, 76062, Pakistan; Mahoto N.A., Mehran University of Engineering and Technology, Jamshoro, 76062, Pakistan; Zeadally S., University of Kentucky, Lexington, 40506, KY, United States; Nebhen J., Prince Sattam Bin Abdulaziz University, Al-Kharj, 16278, Saudi Arabia","Smart agriculture, with the aid of artificial intelligence (AI), is playing a pivotal role to ensure agriculture sustainability. AI techniques are employed in soil and irrigation management, weather forecasting, plant growth, disease prediction, and livestock management, which are considered to be significant domains of agriculture. We review recent AI techniques that have been deployed in these domains. We focus on the various AI algorithms used as well as their performance impact. This review not only highlights the effective use of AI at different layers of a smart agriculture architecture but also identifies future research directions in this field. We found that the deep learning algorithms that have been used in recent studies have performed far better than the conventional machine learning algorithms due to recent technological advances that can efficiently process vast amount of data and enable timely intelligent decisions similar to human decisions.  © 1981-2012 IEEE.","Irrigation; Livestock; Smart Agriculture; Soil Management; Weather forecasting","Deep learning; Irrigation; Learning algorithms; Soil moisture; Support vector machines; Artificial intelligence algorithms; Artificial intelligence techniques; Best practices; Cloud-computing; Irrigation management; Plant growth; Radiofrequencies; Smart agricultures; Soil management; Support vectors machine; Weather forecasting","","","","","IEEE Computer Society","02721732","","IEMID","","English","IEEE Micro","Article","Final","","Scopus","2-s2.0-85118245911"
"Oczak M.; Bayer F.; Vetter S.; Maschat K.; Baumgartner J.","Oczak, Maciej (55246342700); Bayer, Florian (57226749450); Vetter, Sebastian (56835289800); Maschat, Kristina (37761655400); Baumgartner, Johannes (7102679363)","55246342700; 57226749450; 56835289800; 37761655400; 7102679363","Comparison of the automated monitoring of the sow activity in farrowing pens using video and accelerometer data","2022","Computers and Electronics in Agriculture","192","","106517","","","","17","10.1016/j.compag.2021.106517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120462496&doi=10.1016%2fj.compag.2021.106517&partnerID=40&md5=2b1ee997898d6d9f43ccd41a7d348017","Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria; Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria; Austrian Competence Centre for Feed and Food Quality, Safety and Innovation, FFoQSI GmbH, Technopark 1C, Tulln, 3430, Austria","Oczak M., Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria, Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria; Bayer F., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria; Vetter S., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria; Maschat K., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria, Austrian Competence Centre for Feed and Food Quality, Safety and Innovation, FFoQSI GmbH, Technopark 1C, Tulln, 3430, Austria; Baumgartner J., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria","Patterns in pigs activity can be an indicator of health and welfare of the animals. This motivates researchers to develop Precision Livestock Farming (PLF) tools for automated monitoring of pig activity level. In this research we compared two important technologies that can be used for this purpose, ear tag accelerometer and computer vision. Additionally, we compared both technologies with gold standard based on human labelling. A state-of-the-art object detection algorithm RetinaNet was trained on 9969 images and validated on 4273 images to automatically detect head of a sow, body of a sow, left ear, right ear and a hay rack. It was possible to detect these objects with a performance of 0.26 mAP@0.5:0.95. Activity of 6 sows was derived from detected parts of animals’ bodies and compared with activity measurement based on ear tag accelerometer data. Dynamic relation between activity measurement based on both technologies was modelled with Transfer Function (TF) models. For all 6 animals activity of the body of a sow based on object detection was very similar to accelerometer based activity measurement (R2 > 0.7). Similarly R2 between activity of a head of a sow and accelerometer based activity was also very similar for most sows (R2 > 0.7). Results of fitting of TF models to animal activity data based on ear tag accelerometer and output of object detection on body of sows and head of sows suggests that both technologies, the accelerometer and computer vision provide very similar information on activity level of animals. The presented computer vision method is limited to monitoring one animal under camera view as detected body parts cannot be associated with multiple individuals. Moreover, we expect that the method requires re-training the RetinaNet object detection algorithm with additional images collected on additional farms to achieve satisfactory performance in different environments. Application of computer vision approach might be advantageous in some PLF applications as it is non-invasive and might be less laborious than method based on ear tag accelerometer data. © 2021 The Authors","Accelerometer; Activity; Automated monitoring; Computer vision; Deep learning; Sow","Indicator indicator; Agriculture; Automation; Computer vision; Deep learning; Mammals; Object detection; Object recognition; Signal detection; Accelerometer data; Activity; Activity levels; Activity measurements; Automated monitoring; Deep learning; Object detection algorithms; Performance; Precision livestock farming; Sow; accelerometer; algorithm; comparative study; computer vision; monitoring system; pig; videography; Accelerometers","","","M. Oczak; Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Veterinärplatz 1, 1210, Austria; email: Maciej.Oczak@vetmeduni.ac.at","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85120462496"
"Na M.H.; Cho W.H.; Kim S.K.; Na I.S.","Na, Myung Hwan (36874253400); Cho, Wan Hyun (56264852800); Kim, Sang Kyoon (56438872800); Na, In Seop (24825137400)","36874253400; 56264852800; 56438872800; 24825137400","Automatic Weight Prediction System for Korean Cattle Using Bayesian Ridge Algorithm on RGB-D Image","2022","Electronics (Switzerland)","11","10","1663","","","","20","10.3390/electronics11101663","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130986027&doi=10.3390%2felectronics11101663&partnerID=40&md5=9f34b59341c3037ee64f5b439fd498ed","Department of Statistics, Chonnam National University, Gwangju, 61186, South Korea; Department of Electronic Engineering, Mokpo National University, Muan, 58554, South Korea; National Program of Excellence in Software Centre, Gwangju, 61452, South Korea","Na M.H., Department of Statistics, Chonnam National University, Gwangju, 61186, South Korea; Cho W.H., Department of Statistics, Chonnam National University, Gwangju, 61186, South Korea; Kim S.K., Department of Electronic Engineering, Mokpo National University, Muan, 58554, South Korea; Na I.S., National Program of Excellence in Software Centre, Gwangju, 61452, South Korea","Weighting the Hanwoo (Korean cattle) is very important for Korean beef producers when selling the Hanwoo at the right time. Recently, research is being conducted on the automatic prediction of the weight of Hanwoo only through images with the achievement of research using deep learning and image recognition. In this paper, we propose a method for the automatic weight prediction of Hanwoo using the Bayesian ridge algorithm on RGB-D images. The proposed system consists of three parts: segmentation, extraction of features, and estimation of the weight of Korean cattle from a given RGB-D image. The first step is to segment the Hanwoo area from a given RGB-D image using depth information and color information, respectively, and then combine them to perform optimal segmentation. Additionally, we correct the posture using ellipse fitting on segmented body image. The second step is to extract features for weight prediction from the segmented Hanwoo image. We extracted three features: size, shape, and gradients. The third step is to find the optimal machine learning model by comparing eight types of well-known machine learning models. In this step, we compared each model with the aim of finding an efficient model that is lightweight and can be used in an embedded system in the real field. To evaluate the performance of the proposed weight prediction system, we collected 353 RGB-D images from livestock farms in Wonju, Gangwon-do in Korea. In the experimental results, random forest showed the best performance, and the Bayesian ridge model is the second best in MSE or the coefficient of determination. However, we suggest that the Bayesian ridge model is the most optimal model in the aspect of time complexity and space complexity. Finally, it is expected that the proposed system will be casually used to determine the shipping time of Hanwoo in wild farms for a portable commercial device. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","body and shape feature; cattle segmentation; livestock management; prediction of cattle weight; RGB-D sensor data","","Chosun University, CU","Funding: This study was supported by research fund from Chosun University, 2021. And Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Smart Farm R&D Foundation (KosFarm) through Smart Farm Innovation Technology Development Program, funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and ICT (MSIT), Rural Development Administration (RDA) (421017-04).","I.S. Na; National Program of Excellence in Software Centre, Gwangju, 61452, South Korea; email: ypencil@hanmail.net","","MDPI","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130986027"
"Rejas-Haddioui D.; Purcell W.; Neubauer T.","Rejas-Haddioui, D. (58620360500); Purcell, W. (57909292600); Neubauer, T. (19933872600)","58620360500; 57909292600; 19933872600","Deep neural network applications on pose estimation and action recognition for precision dairy farming","2022","Precision Livestock Farming 2022 - Papers Presented at the 10th European Conference on Precision Livestock Farming, ECPLF 2022","","","","885","890","5","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172290877&partnerID=40&md5=dccf3b95cf602a1d7b79dcfae64d346a","Information and Software Engineering Group, Institute of Information Systems Engineering, Faculty of Informatics, TU Wien, Favoritenstrasse 9-11/194, Vienna, 1040, Austria","Rejas-Haddioui D., Information and Software Engineering Group, Institute of Information Systems Engineering, Faculty of Informatics, TU Wien, Favoritenstrasse 9-11/194, Vienna, 1040, Austria; Purcell W., Information and Software Engineering Group, Institute of Information Systems Engineering, Faculty of Informatics, TU Wien, Favoritenstrasse 9-11/194, Vienna, 1040, Austria; Neubauer T., Information and Software Engineering Group, Institute of Information Systems Engineering, Faculty of Informatics, TU Wien, Favoritenstrasse 9-11/194, Vienna, 1040, Austria","Recent advances in Computer Vision (CV) have yielded great improvements in tasks such as Pose Estimation (PE) and Activity Recognition (AR) and their application to the field of Precision Livestock Farming (PLF) have the potential to enable truly non-intrusive animal monitoring. These systems can help detect health issues by providing detailed behaviour analysis and automatically detect problematic conditions like lameness. Past solutions rely on RFID chips and inertial measurement unit (IMU) to identify and classify cattle behaviour, while existing research of CV solutions for animal PE or AR often focus on scenes with single animals and in very clear conditions. Our work aims to study the capabilities and limitations of Computer Vision systems applied under industrial conditions. We train a Deep Neural Network (DNN) system that can predict the pose of each animal in the image, and to also predict the activity they are performing. Furthermore, we explore the deployment capabilities of these systems in industrial settings by studying the effect of neural network pruning in the inference accuracy and cost of the system in an effort to help future solutions be light-weight and with affordable hardware requirements. © ECPLF 2022. All rights reserved.","activity recognition; computer vision; deep learning; pose estimation","Animals; Deep neural networks; Farms; Action recognition; Activity recognition; Condition; Dairy farming; Deep learning; Health issues; Neural network application; Non-intrusive; Pose-estimation; Precision livestock farming; Computer vision","TU Wien University","This project is developed as part of Diego Anas Rejas Haddioui’s Master Thesis for the Master’s in Logic and Computation of the TU Wien University. It is developed thanks to the help and support of the Unit for Herd Health Management of the University Clinic for Ruminants of Vetmeduni Vienna, which provided the video surveillance data obtained at their research dairy farm at Kremesberg.","","Berckmans D.; Oczak M.; Iwersen M.; Wagener K.","Organising Committee of the 10th European Conference on Precision Livestock Farming (ECPLF), University of Veterinary Medicine Vienna","","978-839653600-6","","","English","Precis. Livest. Farming - Pap. Present. Eur. Conf. Precis. Livest. Farming, ECPLF","Conference paper","Final","","Scopus","2-s2.0-85172290877"
"","","","2022 ASABE Annual International Meeting","2022","2022 ASABE Annual International Meeting","","","","","","1733","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137602570&partnerID=40&md5=d2275dabf94ce01f2d46653238a8ae08","","","The proceedings contain 181 papers. The topics discussed include: cereal yield and yield components from cropped traffic lanes compared with non-trafficked crop beds in commercial controlled traffic farming systems; agronomic response of rainfed taro to improved soil and nutrient management practices in Samoa; an investigation into the effect of irrigation with sodium bicarbonate-rich water on soil physico-chemical properties of rugby fields under turfgrass; advanced application of terahertz time-domain spectral technique for in-situ monitoring of microwave vacuum dehydration; design and experiments of an integrated manual-automatic navigation hydraulic steering system for crawler combine harvesters and its control method; the status of precision agricultural technology adoption in Missourian farms; adapting precision technology on row crops vs. specialty crops; optimal sensors placement in controlled environment agriculture using a reinforcement learning approach; and deep learning-based autonomous cow body detection for smart livestock farming.","","","","","","","American Society of Agricultural and Biological Engineers","","","","","English","ASABE Annu. Int. Meet.","Conference review","Final","","Scopus","2-s2.0-85137602570"
"Siddthan R.; Shanthi P.M.","Siddthan, R. (58092673900); Shanthi, P.M. (57209949533)","58092673900; 57209949533","A Comprehensive Survey on CNN Models on Assessment of Nitrate Contamination in Groundwater","2022","6th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2022 - Proceedings","","","","1250","1254","4","2","10.1109/ICECA55336.2022.10009152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147508714&doi=10.1109%2fICECA55336.2022.10009152&partnerID=40&md5=f821164fb94a8de3d2231c208f417588","J J. College of Arts and Science, Bharthidasan University, Department of Computer Science, Pudukkotai, Tamil Nadu, India","Siddthan R., J J. College of Arts and Science, Bharthidasan University, Department of Computer Science, Pudukkotai, Tamil Nadu, India; Shanthi P.M., J J. College of Arts and Science, Bharthidasan University, Department of Computer Science, Pudukkotai, Tamil Nadu, India","In many places in the world, groundwater nitrate pollution is a major issue. Close to the livestock waste disposal site (LWDS), coprostanol and nitrate concentrations in the soil were altered by livestock manure. There was a considerable correlation between the nitrate contents in the groundwater and soil. There was evidence that nitrates were carried downstream in both soil and groundwater. It is, however, difficult to identify the main nitrate sources because of the diffuse and widespread spatial overlap of multiple non-point pollution sources. This research study presents a comprehensive survey and evaluation of various convolutional neural network (CNN) models for the assessment of groundwater nitrate contamination. The survey provides the accuracy of various models of CNN method that records the prediction accuracy of groundwater nitrate contamination. The model provides an accuracy evaluation with the proposed method on nitrate concentration and shows how well the proposed method archives better accuracy than other CNN models.  © 2022 IEEE.","Convolutional Neural Network; Deep Learning; Multi - Level Spatial Database; Residual Neural Network","Contamination; Convolution; Convolutional neural networks; Deep neural networks; Fertilizers; Groundwater pollution; Nitrates; Soil pollution; Soils; Waste disposal; Convolutional neural network; Deep learning; Multi - level spatial database; Multilevels; Neural network model; Neural-networks; Nitrate concentration; Nitrate contamination; Residual neural network; Spatial database; Groundwater","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548271-4","","","English","Int. Conf. Electron., Commun. Aerosp. Technol., ICECA - Proc.","Conference paper","Final","","Scopus","2-s2.0-85147508714"
"Weales D.; Moussa M.; Tarry C.","Weales, David (57226890115); Moussa, Medhat (35894523000); Tarry, Cole (15119751300)","57226890115; 35894523000; 15119751300","A robust machine vision system for body measurements of beef calves","2021","Smart Agricultural Technology","1","","100024","","","","11","10.1016/j.atech.2021.100024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138034283&doi=10.1016%2fj.atech.2021.100024&partnerID=40&md5=5dc55a660dfec73f753ab3ad605aacae","School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada","Weales D., School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Moussa M., School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada; Tarry C., School of Engineering, University of Guelph, Guelph, N1G 2W1, ON, Canada","Measuring body dimensions is a useful method of assessing the health and growth of young beef cattle. However, performing these measurements in a barn environment can present a number of unique challenges. The objective of this paper is to design an image capture system and image processing algorithm that can meet these challenges. The system uses two RGB-D cameras to collect images from the top-left and top-right of the calf. Images were collected in a barn environment along with ground truth body measurements. Colour image processing was used to remove the background by making use of a deep learning instance segmentation model for each camera. The segmented data from the two cameras was registered to create a 3D image of the calf, which was then used to measure a few key body dimensions. The experimental results showed a mean error of 0.2 cm for heart girth, -0.8 cm for withers height, -0.2 cm for midpiece height, and -2.1 cm for pin height. © 2021 The Author(s)","3D Vision; Beef cattle; Image processing; Livestock; RGB-D Camera","","University of Guelph, UG","We would like to acknowledge Vanessa Rotondo who provided the contact measurements data for the calves. This research is supported by funding from the Food from Thought Digital Agriculture Research Fund at the University of Guelph.","M. Moussa; School of Engineering, University of Guelph, Guelph, N1G 2W1, Canada; email: mmoussa@uoguelph.ca","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138034283"
"Moraitis M.; Vaiopoulos K.; Balafoutis A.T.","Moraitis, Michail (58700101100); Vaiopoulos, Konstantinos (57418005800); Balafoutis, Athanasios T. (55968864000)","58700101100; 57418005800; 55968864000","Design and Implementation of an Urban Farming Robot","2022","Micromachines","13","2","250","","","","24","10.3390/mi13020250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124977768&doi=10.3390%2fmi13020250&partnerID=40&md5=4669ebafb96ec9b46c3a299ab88e4675","Institute of Bio-Economy & Agro-Technology, Centre of Research & Technology Hellas, Dimarchou Georgiadou 118, Volos, 38333, Greece","Moraitis M., Institute of Bio-Economy & Agro-Technology, Centre of Research & Technology Hellas, Dimarchou Georgiadou 118, Volos, 38333, Greece; Vaiopoulos K., Institute of Bio-Economy & Agro-Technology, Centre of Research & Technology Hellas, Dimarchou Georgiadou 118, Volos, 38333, Greece; Balafoutis A.T., Institute of Bio-Economy & Agro-Technology, Centre of Research & Technology Hellas, Dimarchou Georgiadou 118, Volos, 38333, Greece","Urban agriculture can be shortly defined as the growing of plants and/or the livestock husbandry in and around cities. Although it has been a common occupation for the urban population all along, recently there is a growing interest in it both from public bodies and researchers, as well as from ordinary citizens who want to engage in self-cultivation. The modern citizen, though, will hardly find the free time to grow his own vegetables as it is a process that requires, in addition to knowledge and disposition, consistency. Given the above considerations, the purpose of this work was to develop an economic robotic system for the automatic monitoring and management of an urban garden. The robotic system was designed and built entirely from scratch. It had to have suitable dimensions so that it could be placed in a balcony or a terrace, and be able to scout vegetables from planting to harvest and primarily conduct precision irrigation based on the growth stage of each plant. Fertigation and weed control will also follow. For its development, a number of technologies were combined, such as Cartesian robots’ motion, machine vision, deep learning for the identification and detection of plants, irrigation dosage and scheduling based on plants’ growth stage, and cloud storage. The complete process of software and hardware development to a robust robotic platform is described in detail in the respective sections. The experimental procedure was performed for lettuce plants, with the robotic system providing precise movement of its actuator and applying precision irrigation based on the specific needs of the plants. © 2022 by the authors.","Plant detection; Precision irrigation; Robot; Urban agriculture; Vegetables","Cultivation; Deep learning; Lettuce; Machine design; Robotics; Robots; Vegetables; Weed control; Automatic management; Automatic monitoring; Design and implementations; Plant detections; Precision irrigation; Robot; Robotic systems; Self cultivations; Urban agricultures; Urban population; Irrigation","","","M. Moraitis; Institute of Bio-Economy & Agro-Technology, Centre of Research & Technology Hellas, Volos, Dimarchou Georgiadou 118, 38333, Greece; email: m.moraitis@certh.gr","","MDPI","2072666X","","","","English","Micromachines","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85124977768"
"Lei K.; Zong C.; Yang T.; Peng S.; Zhu P.; Wang H.; Teng G.; Du X.","Lei, Kaidong (57219096589); Zong, Chao (55372444000); Yang, Ting (59074011000); Peng, Shanshan (57421493100); Zhu, Pengfei (57421284200); Wang, Hao (57420955400); Teng, Guanghui (10140843100); Du, Xiaodong (57188848057)","57219096589; 55372444000; 59074011000; 57421493100; 57421284200; 57420955400; 10140843100; 57188848057","Detection and Analysis of Sow Targets Based on Image Vision","2022","Agriculture (Switzerland)","12","1","73","","","","12","10.3390/agriculture12010073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123241675&doi=10.3390%2fagriculture12010073&partnerID=40&md5=2bdcc10f8d9951f1b84f4c8a165965ab","College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Chongqing Academy of Animal Sciences, Chongqing, 402460, China; New Hope Liuhe Co., Ltd, Beijing, 100102, China","Lei K., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Zong C., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Yang T., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Peng S., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Zhu P., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Wang H., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China, Chongqing Academy of Animal Sciences, Chongqing, 402460, China; Teng G., College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; Du X., New Hope Liuhe Co., Ltd, Beijing, 100102, China","In large-scale sow production, real-time detection and recognition of sows is a key step towards the application of precision livestock farming techniques. In the pig house, the overlap of railings, floors, and sows usually challenge the accuracy of sow target detection. In this paper, a non-contact machine vision method was used for sow targets perception in complex scenarios, and the number position of sows in the pen could be detected. Two multi-target sow detection and recognition models based on the deep learning algorithms of Mask-RCNN and UNet-Attention were developed, and the model parameters were tuned. A field experiment was carried out. The data-set obtained from the experiment was used for algorithm training and validation. It was found that the Mask-RCNN model showed a higher recognition rate than that of the UNet-Attention model, with a final recognition rate of 96.8% and complete object detection outlines. In the process of image segmentation, the area distribution of sows in the pens was analyzed. The position of the sow’s head in the pen and the pixel area value of the sow segmentation were analyzed. The feeding, drinking, and lying behaviors of the sow have been identified on the basis of image recognition. The results showed that the average daily lying time, standing time, feeding and drinking time of sows were 12.67 h(MSE 1.08), 11.33 h(MSE 1.08), 3.25 h(MSE 0.27) and 0.391 h(MSE 0.10), respectively. The proposed method in this paper could solve the problem of target perception of sows in complex scenes and would be a powerful tool for the recognition of sows. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Animal welfare; Behavior; Computer vision; Image processing; Precision livestock; Sow","","Chongqing Technology Innovation and Application Development Project, (cstc2019jscx-gksbX0093); National Key Research and Development Program of China, NKRDPC, (2016YFD0700204, YF202103); National Key Research and Development Program of China, NKRDPC","Funding: This research was funded by the National Key Research and Development Program of China (grant number: 2016YFD0700204), Research on the Technology of Creating Comfortable Environment in Pig House (YF202103), and Chongqing Technology Innovation and Application Development Project (grant number: cstc2019jscx-gksbX0093).","C. Zong; College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; email: chaozong@cau.edu.cn; G. Teng; College of Water Conservancy & Civil Engineering, China Agricultural University, Beijing, 100083, China; email: futong@cau.edu.cn","","MDPI","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123241675"
"Yousefi D.B.M.; Rafie A.S.M.; Al-Haddad S.A.R.; Azrad S.","Yousefi, D. B. Mamehgol (57217017319); Rafie, A. S. Mohd (23985769700); Al-Haddad, S.A.R. (49963059800); Azrad, Syaril (35408682900)","57217017319; 23985769700; 49963059800; 35408682900","A Systematic Literature Review on the Use of Deep Learning in Precision Livestock Detection and Localization Using Unmanned Aerial Vehicles","2022","IEEE Access","10","","","80071","80091","20","22","10.1109/ACCESS.2022.3194507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135753997&doi=10.1109%2fACCESS.2022.3194507&partnerID=40&md5=aec32efaf0fef4088e2273faa33717cb","Universiti Putra Malaysia, Department of Aerospace Engineering, Selangor, Seri Kembangan, 43400, Malaysia; Universiti Putra Malaysia, Department of Computer and Communication Systems Engineering, Selangor, Seri Kembangan, 43400, Malaysia","Yousefi D.B.M., Universiti Putra Malaysia, Department of Aerospace Engineering, Selangor, Seri Kembangan, 43400, Malaysia; Rafie A.S.M., Universiti Putra Malaysia, Department of Aerospace Engineering, Selangor, Seri Kembangan, 43400, Malaysia; Al-Haddad S.A.R., Universiti Putra Malaysia, Department of Computer and Communication Systems Engineering, Selangor, Seri Kembangan, 43400, Malaysia; Azrad S., Universiti Putra Malaysia, Department of Aerospace Engineering, Selangor, Seri Kembangan, 43400, Malaysia","With the ever-increasing importance of dairy and meat production, precision livestock farming (PLF) using advanced information technologies is emerging to improve farming production systems. The latest automation, connectivity, and artificial intelligence developments open new horizons to monitor livestock in the pasture, controlled environments, and open environments. Due to the significance of livestock detection and tracking, this systematic review extracts and summarizes the existing deep learning (DL) techniques in PLF using unmanned aerial vehicles (UAV). In the context of livestock recognition studies, UAVs are receiving growing attention due to their flexible data acquisition and operation in different conditions. This review examines the implemented DL architectures and scrutinizes the broadly exploited evaluation metrics, attributes, and databases. The classification of most UAV livestock monitoring systems using DL techniques is in three categories: detection, classification, and localization. Correspondingly, this paper discusses the future benefits and drawbacks of these DL-based PLF approaches using UAV imagery. Additionally, this paper describes alternative methods used to mitigate issues in PLF. The aim of this work is to provide insights into the most relevant studies on the development of UAV-based PLF systems focused on deep neural network-based techniques.  © 2013 IEEE.","deep learning; detection; Livestock monitoring; localization; UAVs","Agriculture; Aircraft detection; Antennas; Data acquisition; Deep neural networks; Feature extraction; Object recognition; Aerial vehicle; Deep learning; Detection; Farming; Features extraction; Livestock monitoring; Localisation; Location awareness; Objects recognition; Precision livestock farming; Unmanned aerial vehicles (UAV)","Ministry of Higher Education, Malaysia, MOHE, (FRGS/1/2020/TK0/UPM/02/33); Ministry of Higher Education, Malaysia, MOHE","This work was supported by the Ministry of Education, Malaysia, through the Fundamental Research Grant Scheme (FRGS) under Grant FRGS/1/2020/TK0/UPM/02/33.","D.B.M. Yousefi; Universiti Putra Malaysia, Department of Aerospace Engineering, Seri Kembangan, Selangor, 43400, Malaysia; email: mamehgol.yousefi@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135753997"
"Hosseininoorbin S.; Layeghy S.; Kusy B.; Jurdak R.; Bishop-Hurley G.J.; Greenwood P.L.; Portmann M.","Hosseininoorbin, Seyedehfaezeh (57217245709); Layeghy, Siamak (55062253100); Kusy, Brano (6506475437); Jurdak, Raja (23097482900); Bishop-Hurley, Greg J. (6602321037); Greenwood, Paul L (8783048400); Portmann, Marius (16239564100)","57217245709; 55062253100; 6506475437; 23097482900; 6602321037; 8783048400; 16239564100","Deep learning-based cattle behaviour classification using joint time-frequency data representation","2021","Computers and Electronics in Agriculture","187","","106241","","","","35","10.1016/j.compag.2021.106241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107815245&doi=10.1016%2fj.compag.2021.106241&partnerID=40&md5=87bca50dcfa5206bfd5cfa2f50fe24a1","The University of Queensland School of ITEE, St Lucia, 4067, QLD, Australia; DATA61, Commonwealth Scientific and Industrial Research Organisation (CSIRO), Pullenvale, 4069, QLD, Australia; Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia; Queensland University of Technology, School of Computer Science, Brisbane, QLD, Australia; Department of Primary Industries, JSF Barker Building, Trevenna Road, University of New England, Armidale, 2351, NSW, Australia; Agriculture and Food, F.D. McMaster Laboratory Chiswick, CSIRO, Armidale, 2350, NSW, Australia","Hosseininoorbin S., The University of Queensland School of ITEE, St Lucia, 4067, QLD, Australia, DATA61, Commonwealth Scientific and Industrial Research Organisation (CSIRO), Pullenvale, 4069, QLD, Australia; Layeghy S., The University of Queensland School of ITEE, St Lucia, 4067, QLD, Australia; Kusy B., DATA61, Commonwealth Scientific and Industrial Research Organisation (CSIRO), Pullenvale, 4069, QLD, Australia; Jurdak R., Queensland University of Technology, School of Computer Science, Brisbane, QLD, Australia; Bishop-Hurley G.J., Agriculture and Food, CSIRO, St Lucia, 4067, QLD, Australia; Greenwood P.L., Department of Primary Industries, JSF Barker Building, Trevenna Road, University of New England, Armidale, 2351, NSW, Australia, Agriculture and Food, F.D. McMaster Laboratory Chiswick, CSIRO, Armidale, 2350, NSW, Australia; Portmann M., The University of Queensland School of ITEE, St Lucia, 4067, QLD, Australia","In this paper, a sequential deep neural network in conjunction with a joint time-frequency domain data representation is explored for the problem of cattle behaviour classification. The experimental evaluation is based on a real-world dataset with over 3 million samples, collected from sensors with a tri-axial accelerometer, magnetometer and gyroscope, attached to the collar tags of 10 beef steers. The experimental result demonstrate that the time–frequency domain data representation allows to efficiently trade-off a large reduction of model size and computational complexity for a very minor reduction in classification accuracy. This shows the potential of this classification approach to run on resource-constrained embedded and IoT devices. Most importantly, the proposed behaviour classification method achieves a high classification performance with an F1 Score of 94.9% for 3 behaviour classes, and 89.3% for 9 behaviour classes. This is in comparison to the current state-of-the-art with an F1 Score of 94.3% (for two classes) and 88.7% (for 8 classes). © 2021 Elsevier B.V.","DNN; Inertial Measurement Unit; Livestock; Sensors; TFD","Classification (of information); Deep neural networks; Economic and social effects; Frequency domain analysis; % reductions; Behaviour classification; Data representations; DNN; Frequency data; Frequency-domain data; Inertial measurements units; TFD; Time frequency; Time frequency domain; accuracy assessment; artificial neural network; behavioral response; cattle; classification; data set; experimental study; machine learning; Agriculture","NSW Department of Primary Industries, DPI; Commonwealth Scientific and Industrial Research Organisation, CSIRO","We would like to thank the following technical staff who were involved in the research at CSIRO FD McMaster Laboratory Chiswick: Alistair Donaldson (Technical Officer) and Reg Woodgate (Senior Technical Officer) with NSW Department of Primary Industries, and Jody McNally (Research Technician), Troy Kalinowski (Technical Officer) and Aaron Ingham (Research Scientist) with CSIRO Agriculture and Food. The research at the CSIRO FD McMaster Laboratory Chiswick was undertaken with Strategic Investment Funding from CSIRO and NSW Department of Primary Industries.","S. Hosseininoorbin; The University of Queensland School of ITEE, St Lucia, 4067, Australia; email: f.noorbin@uq.net.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107815245"
"Abhilasa S.; Srilakshmi A.; Geetha K.","Abhilasa, S. (57224451366); Srilakshmi, A. (57200387818); Geetha, K. (57196798859)","57224451366; 57200387818; 57196798859","Classification of agricultural leaf images using hybrid combination of activation functions","2021","Proceedings - 5th International Conference on Intelligent Computing and Control Systems, ICICCS 2021","","","9432221","785","791","6","5","10.1109/ICICCS51141.2021.9432221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107537252&doi=10.1109%2fICICCS51141.2021.9432221&partnerID=40&md5=3f6c764b704f7fc7e80e0f5b78c3a572","Sastra Deemed to Be University","Abhilasa S., Sastra Deemed to Be University; Srilakshmi A., Sastra Deemed to Be University; Geetha K., Sastra Deemed to Be University","Agriculture is the primary source of India for the raise of livestock and it's the backbone of the economy that gives surplus food materials for living and it conjointly acts as stuff for industries. India's twenty eighth percent of the national income depends on the agricultural sector. Agriculture contributes to the expansion rate of employment. The most important issue faced in agriculture is loss or damage of crops due to environmental conditions such as flood, lack of water during summer and various pests which affects the crops. Among these crop diseases and pests have an enormous contribution to agricultural huge loss. The diseases can be identified with advanced deep learning algorithms. Though many DL methods works well in identifying the disease still there are misclassifications during training. To avoid the misclassification in predicting the appropriate disease an ensemble activation approach is proposed. In this paper, deep learning based technique is enforced for the automatic detection of plant leaf diseases. The convolution Neural Network approach is applied during this paper. Activation functions play a vital role in convolution Neural Network (CNN). In this paper, a combination of hybrid activation function is proposed to improve the accuracy rate of the CNN model. The hybrid Activation function is tested and trained on different datasets. The hybrid activation function is compared with the standard activation function ReLU. The Experimental results by trial and error show that the hybrid activation function achieves higher accuracy than the ReLU activation function.  © 2021 IEEE.","Convolution neural network; Deep Learning; Hybrid activation function; Rectified Linear Unit","Agricultural robots; Control systems; Convolution; Crops; Deep learning; Image classification; Intelligent computing; Learning algorithms; Neural networks; Activation approaches; Activation functions; Agricultural sector; Automatic Detection; Convolution neural network; Environmental conditions; Hybrid activations; Misclassifications; Chemical activation","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166541272-8","","","English","Proc. - Int. Conf. Intell. Comput. Control Syst., ICICCS","Conference paper","Final","","Scopus","2-s2.0-85107537252"
"Singh A.; Jadoun Y.S.; Brar P.S.; Kour G.","Singh, Amandeep (59220745900); Jadoun, Y.S. (54792080500); Brar, Parkash Singh (6602583511); Kour, Gurpreet (57222507738)","59220745900; 54792080500; 6602583511; 57222507738","Smart Technologies in Livestock Farming","2022","Smart and Sustainable Food Technologies","","","","25","57","32","4","10.1007/978-981-19-1746-2_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162723809&doi=10.1007%2f978-981-19-1746-2_2&partnerID=40&md5=fb579eab6fbb0bf5c32c961c0612c66e","Guru Angad Dev Veterinary & Animal Sciences University, Punjab, Ludhiana, India","Singh A., Guru Angad Dev Veterinary & Animal Sciences University, Punjab, Ludhiana, India; Jadoun Y.S., Guru Angad Dev Veterinary & Animal Sciences University, Punjab, Ludhiana, India; Brar P.S., Guru Angad Dev Veterinary & Animal Sciences University, Punjab, Ludhiana, India; Kour G., Guru Angad Dev Veterinary & Animal Sciences University, Punjab, Ludhiana, India","Smart technologies and its application have shown great promise for the modernization of extension services in both developed and developing countries. Improving rural livelihoods through smart technologies is one of the key areas, which has potential to change the livestock economy. Enormous increase in the mobile and internet users has ushered in a revolution in ICT research and development. We wouldn’t be wrong if we dubbed this period the “ICT Era”. The government of India’s Digital India Mission, as well as telecom providers’ provision of affordable pricing to subscribers, have cleared the road for internet technology to reach everyone’s doorstep. Many public and private organizations involved in research related to the livestock sector have developed many such ICTs for the use of livestock farmers. Improved package of practices are being provided to the farmers by the use of mobile apps, expert systems, and web portals whereas the regular advisories are provided to them through tele-services, SMS and Remote Sensing based tools. The animals are being identified by the use of RFID tags which are helping livestock farmers as well as the resource-based companies for resource disposal. Furthermore, the farmers are connected to peers through social media and mobile telephony like Kisan Call Centre. The new buzzword, i.e. Artificial intelligence (AI) through its diverse applications has the potential to revolutionize the livestock industry, like; artificial neural networks, deep learning, machine learning, natural language processing, cloud computing, block chain technology, internet of things, precision farming, sensor based systems, robotics, and so forth. It is also predicted that AI will lead in the world’s “fourth industrial revolution”. Which will be a digital revolution. All of these technologies work together to create an “Information Web” for farmers, which is in charge of disseminating timely livestock development information. This chapter details the ICTs which are in use by the livestock farmers and the ones which are yet to come. © The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2022.","Artificial intelligence; Farmers; ICTs; Information; Internet; Livestock","","","","","","Springer Nature","","978-981191746-2; 978-981191745-5","","","English","Smart and Sustainable Food Technologies","Book chapter","Final","","Scopus","2-s2.0-85162723809"
"Ranjan P.; Garg R.; Rai J.K.","Ranjan, Pinku (55937691900); Garg, Rachit (57462484200); Rai, Jayant Kumar (58296573300)","55937691900; 57462484200; 58296573300","Artificial Intelligence Applications in Soil & Crop Management","2022","2022 IEEE Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation, IATMSI 2022","","","","","","","4","10.1109/IATMSI56455.2022.10119362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160785426&doi=10.1109%2fIATMSI56455.2022.10119362&partnerID=40&md5=5fec2c4184bdf8af1a0ca7f8289e144d","ABV-IIITM, Department of Electrical & Electronics, Gwalior, India; ABV-IIITM, Department of Information Technology, Gwalior, India","Ranjan P., ABV-IIITM, Department of Electrical & Electronics, Gwalior, India; Garg R., ABV-IIITM, Department of Information Technology, Gwalior, India; Rai J.K., ABV-IIITM, Department of Electrical & Electronics, Gwalior, India","Farming is the activity of preparing the soil for planting crops and raising livestock. A nation's economic development depends heavily on agriculture. Farming makes up nearly 60 percent of a nation's primary source of income. Farmers have so far used traditional farming methods. These methods took a lot of time and were not precise, which decreased productivity. By precisely identifying the steps that must be taken at the appropriate time of year, smart Farming helps to increase productivity. Precision farming includes a variety of techniques such as weather forecasting, soil analysis, crop recommendations, and calculating the necessary dosages of pesticides and fertilizers. Precise Farming gathers data, trains systems, and forecasts outcomes using cutting-edge technologies like the Internet of Things (IoT), Data Mining, Data Analytics, Machine Learning, and Deep Learning. Precise Farming boosts productivity by reducing manual labor with the aid of technology. Farmers have recently faced several difficulties, such as crop failure due to insufficient rainfall, soil fertility issues, and so forth. The proposed work assists in identifying the best practices for crop management and harvesting in light of the current environmental changes. It instructs someone on how to farm wisely. The purpose of this work is to assist a person in efficiently cultivating crops in order to maximize productivity. This would enable a person to plan their cultivation activities, resulting in an integrated farming solution. © 2022 IEEE.","Agriculture; Crop Recommender; Fertilizer Recommender; Machine Learning; Neural Network; Precision Farming; Smart Farming; Soil type detector","Agricultural technology; Cultivation; Data Analytics; Data mining; Deep learning; Farms; Fertilizers; Internet of things; Learning systems; Soils; Weather forecasting; Crop managements; Crop recommende; Fertilizer recommende; Machine-learning; Neural-networks; Precision-farming; Smart farming; Soil management; Soil type detector; Soil types; Crops","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166547719-2","","","English","IEEE Conf. Interdiscip. Approaches Technol. Manag. Soc. Innov., IATMSI","Conference paper","Final","","Scopus","2-s2.0-85160785426"
"Saar M.; Edan Y.; Godo A.; Lepar J.; Parmet Y.; Halachmi I.","Saar, M. (57397204900); Edan, Y. (7004434501); Godo, A. (57397582700); Lepar, J. (57397767600); Parmet, Y. (35262726300); Halachmi, I. (6701325703)","57397204900; 7004434501; 57397582700; 57397767600; 35262726300; 6701325703","A machine vision system to predict individual cow feed intake of different feeds in a cowshed","2022","Animal","16","1","100432","","","","27","10.1016/j.animal.2021.100432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122307007&doi=10.1016%2fj.animal.2021.100432&partnerID=40&md5=f4da9b61282a32a3d759fbbf9dd92e4c","Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B. 653, Be'er Sheva, 8410501, Israel; Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, P.O.B 15159, Rishon Lezion, 7505101, Israel","Saar M., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B. 653, Be'er Sheva, 8410501, Israel, Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, P.O.B 15159, Rishon Lezion, 7505101, Israel; Edan Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B. 653, Be'er Sheva, 8410501, Israel; Godo A., Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, P.O.B 15159, Rishon Lezion, 7505101, Israel; Lepar J., Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, P.O.B 15159, Rishon Lezion, 7505101, Israel; Parmet Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B. 653, Be'er Sheva, 8410501, Israel; Halachmi I., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, P.O.B. 653, Be'er Sheva, 8410501, Israel, Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, P.O.B 15159, Rishon Lezion, 7505101, Israel","Data on individual feed intake of dairy cows, an important variable for farm management, are currently unavailable in commercial dairies. A real-time machine vision system including models that are able to adapt to multiple types of feed was developed to predict individual feed intake of dairy cows. Using a Red-Green-Blue-Depth (RGBD) camera, images of feed piles of two different feed types (lactating cows' feed and heifers' feed) were acquired in a research dairy farm, for a range of feed weights under varied configurations and illuminations. Several models were developed to predict individual feed intake: two Transfer Learning (TL) models based on Convolutional Neural Networks (CNNs), one CNN model trained on both feed types, and one Multilayer Perceptron and Convolutional Neural Network model trained on both feed types, along with categorical data. We also implemented a statistical method to compare these four models using a Linear Mixed Model and a Generalised Linear Mixed Model, showing that all models are significantly different. The TL models performed best and were trained on both feeds with TL methods. These models achieved Mean Absolute Errors (MAEs) of 0.12 and 0.13 kg per meal with RMSE of 0.18 and 0.17 kg per meal for the two different feeds, when tested on varied data collected manually in a cowshed. Testing the model with actual cows’ meals data automatically collected by the system in the cowshed resulted in a MAE of 0.14 kg per meal and RMSE of 0.19 kg per meal. These results suggest the potential of measuring individual feed intake of dairy cows in a cowshed using RGBD cameras and Deep Learning models that can be applied and tuned to different types of feed. © 2021 The Authors","Deep learning; Individual feed intake; Precision livestock farming; Red-Green-Blue-Depth camera; Transfer learning","Animal Feed; Animals; Cattle; Diet; Eating; Farms; Female; Lactation; Milk; agricultural land; animal; animal food; bovine; diet; eating; female; lactation; milk; veterinary medicine","Harel Levit; Israeli Chief Scientist of Agriculture, (459-451415); Joseph Lepar; Israeli Dairy Board, IDB, (459-4490, 459-6715Y51)","Funding text 1: This study was supported by grants from the Israeli Chief Scientist of Agriculture, projects 459-451415 (‘‘Kendel”), the Israel Dairy Board, fund number 459-4490 and the TechCare grant agreement number 459-6715Y51. This research was partially supported by the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering at Ben-Gurion University of the Negev.; Funding text 2: Special thanks to Ran Bezen for his support throughout this study. We gratefully thank Assaf Godo, Joseph Lepar, Harel Levit and all other members of the Precision Livestock Farming (PLF) Laboratory and the staff of the Volcani Research Farm for their assistance in the study. The authors thank the editor Prof. Angelika Haeussermann and the anonymous reviewers of Animal for their crucial valuable comments that improved the paper readability and content. This study was supported by grants from the Israeli Chief Scientist of Agriculture, projects 459-451415 (??Kendel?), the Israel Dairy Board, fund number 459-4490 and the TechCare grant agreement number 459-6715Y51. This research was partially supported by the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering at Ben-Gurion University of the Negev.","I. Halachmi; Precision Livestock Farming (PLF) Lab., Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, Rishon Lezion, P.O.B 15159, 7505101, Israel; email: halachmi@volcani.agri.gov.il","","Elsevier B.V.","17517311","","","35007881","English","Animal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122307007"
"Latha R.S.; Sreekanth G.R.; Rajadevi R.; Nivetha S.K.; Kumar K.A.; Akash V.; Bhuvanesh S.; Anbarasu P.","Latha, R.S. (57211970396); Sreekanth, G.R. (57211025955); Rajadevi, R. (57201308395); Nivetha, S.K. (59534805100); Kumar, K.Ajith (57605928100); Akash, V. (57611438900); Bhuvanesh, S. (57614091800); Anbarasu, Pon (57614749200)","57211970396; 57211025955; 57201308395; 59534805100; 57605928100; 57611438900; 57614091800; 57614749200","Fruits and Vegetables Recognition using YOLO","2022","2022 International Conference on Computer Communication and Informatics, ICCCI 2022","","","","","","","17","10.1109/ICCCI54379.2022.9740820","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128736302&doi=10.1109%2fICCCI54379.2022.9740820&partnerID=40&md5=7e739e39440ac8842636fe3b98e06e29","Cse, Kongu Engineering College, Erode, India; It, Kongu Engineering College, Erode, India; CT-PG, Kongu Engineering College, Erode, India","Latha R.S., Cse, Kongu Engineering College, Erode, India; Sreekanth G.R., Cse, Kongu Engineering College, Erode, India; Rajadevi R., It, Kongu Engineering College, Erode, India; Nivetha S.K., Cse, Kongu Engineering College, Erode, India; Kumar K.A., Cse, Kongu Engineering College, Erode, India; Akash V., Cse, Kongu Engineering College, Erode, India; Bhuvanesh S., Cse, Kongu Engineering College, Erode, India; Anbarasu P., CT-PG, Kongu Engineering College, Erode, India","Real-time live detection of fruits and vegetables is the most important task to know the availability of the current stock of fruits and vegetables that the customers looking for in the vegetable market. For this problem, a new technique based on Deep learning and IoT is required. The proposed work has applied the YOLO model for identifying different types of vegetables and fruits available in the vegetable market and hence the customer can know the updates the livestock of fruits and vegetables in that shop. The images of commonly used fruits and vegetables in India are collected from Google and also from Kaggle. The collected images are labeled using the Roboflow framework. To implement the proposed model, YOLOv4 -tiny model is chosen which is a super-fast object detector with better accuracy and is also most suitable for embedded devices. Further, this tiny model performs fast prediction in real-time video processing scenarios. The result of the proposed system is evaluated using the metric mean Average Precision (mAP). A high-value of mAP indicates that the model performs better is in its detection. Our YOLOv4-tiny model produced an mAP value of 51% with an inference time of 18 milliseconds. © 2022 IEEE.","Convolutional Neural Network; Deep learning; Fruits and Vegetable detection; Object detection; Recognition; YOLO; YOLOv4 Tiny","Commerce; Convolutional neural networks; Deep learning; Fruits; Object recognition; Vegetables; Video signal processing; 'current; Convolutional neural network; Deep learning; Fruit and vegetable detection; Fruit and vegetables; Live detection; Real- time; Recognition; YOLO; YOLOv4 tiny; Object detection","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166548035-2","","","English","Int. Conf. Comput. Commun. Informatics, ICCCI","Conference paper","Final","","Scopus","2-s2.0-85128736302"
"Hossain M.E.; Kabir M.A.; Zheng L.; Swain D.L.; McGrath S.; Medway J.","Hossain, Md Ekramul (57204645584); Kabir, Muhammad Ashad (36630601000); Zheng, Lihong (17347560300); Swain, Dave L. (55171263800); McGrath, Shawn (55934587600); Medway, Jonathan (57901767900)","57204645584; 36630601000; 17347560300; 55171263800; 55934587600; 57901767900","A systematic review of machine learning techniques for cattle identification: Datasets, methods and future directions","2022","Artificial Intelligence in Agriculture","6","","","138","155","17","46","10.1016/j.aiia.2022.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138594457&doi=10.1016%2fj.aiia.2022.09.002&partnerID=40&md5=1a287a46ebe84666071ec8fcd602b9bd","School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia; Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia; TerraCipher Pty. Ltd, Alton Downs, 4702, QLD, Australia; Fred Morley Centre, School of Animal and Veterinary Sciences, Wagga Wagga, 2678, NSW, Australia; Food Agility CRC Ltd, Sydney, 2000, NSW, Australia","Hossain M.E., School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia; Kabir M.A., School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia, Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia; Zheng L., School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, 2795, NSW, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia; Swain D.L., Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia, TerraCipher Pty. Ltd, Alton Downs, 4702, QLD, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia; McGrath S., Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia, Fred Morley Centre, School of Animal and Veterinary Sciences, Wagga Wagga, 2678, NSW, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia; Medway J., Gulbali Institute for Agriculture, Water and Environment, Charles Sturt University, Wagga Wagga, 2678, NSW, Australia, Food Agility CRC Ltd, Sydney, 2000, NSW, Australia","Increased biosecurity and food safety requirements may increase demand for efficient traceability and identification systems of livestock in the supply chain. The advanced technologies of machine learning and computer vision have been applied in precision livestock management, including critical disease detection, vaccination, production management, tracking, and health monitoring. This paper offers a systematic literature review (SLR) of vision-based cattle identification. More specifically, this SLR is to identify and analyse the research related to cattle identification using Machine Learning (ML) and Deep Learning (DL). This study retrieved 731 studies from four online scholarly databases. Fifty-five articles were subsequently selected and investigated in depth. For the two main applications of cattle detection and cattle identification, all the ML based papers only solve cattle identification problems. However, both detection and identification problems were studied in the DL based papers. Based on our survey report, the most used ML models for cattle identification were support vector machine (SVM), k-nearest neighbour (KNN), and artificial neural network (ANN). Convolutional neural network (CNN), residual network (ResNet), Inception, You Only Look Once (YOLO), and Faster R-CNN were popular DL models in the selected papers. Among these papers, the most distinguishing features were the muzzle prints and coat patterns of cattle. Local binary pattern (LBP), speeded up robust features (SURF), scale-invariant feature transform (SIFT), and Inception or CNN were identified as the most used feature extraction methods. This paper details important factors to consider when choosing a technique or method. We also identified major challenges in cattle identification. There are few publicly available datasets, and the quality of those datasets are affected by the wild environment and movement while collecting data. The processing time is a critical factor for a real-time cattle identification system. Finally, a recommendation is given that more publicly available benchmark datasets will improve research progress in the future. © 2022 The Authors","Cattle detection; Cattle farming; Cattle identification; Deep learning; Machine learning","Agriculture; Convolutional neural networks; Data handling; Deep learning; Learning systems; Local binary pattern; Nearest neighbor search; Supply chains; Cattle detection; Cattle farming; Cattle identification; Convolutional neural network; Deep learning; Identification problem; Machine learning techniques; Machine-learning; Systematic literature review; Systematic Review; Support vector machines","","","M.A. Kabir; School of Computing, Mathematics and Engineering, Charles Sturt University, Bathurst, Panorama Ave, 2795, Australia; email: akabir@csu.edu.au","","KeAi Communications Co.","25897217","","","","English","Artif. Intell. Agric.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85138594457"
"Hoffmann M.; Stanly J.; Sundararaj S.; Rilling S.; Kotsopoulos T.; Firfiris V.; Moshou D.","Hoffmann, M. (58727190700); Stanly, J. (58618581700); Sundararaj, S. (58620396000); Rilling, S. (36810440400); Kotsopoulos, T. (13806694500); Firfiris, V. (55308266000); Moshou, D. (6602862688)","58727190700; 58618581700; 58620396000; 36810440400; 13806694500; 55308266000; 6602862688","ATLAS livestock monitoring architecture and services","2022","Precision Livestock Farming 2022 - Papers Presented at the 10th European Conference on Precision Livestock Farming, ECPLF 2022","","","","1022","1029","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172370883&partnerID=40&md5=ba58576ff14bc2ba610f1f8c4f6aeff9","Fraunhofer Institute for intelligent Analysis and Information Systems, Schloss Birlinghoven, Sankt Augustin, Germany; Institute for Bio-Economy and Agri-Technology (IBO), Centre for Research and Technology-Hellas (CERTH), 6th km Charilaou-Thermi Rd, Thessaloniki, GR,57001, Greece; Department of Hydraulics, Soil Science and Agricultural Engineering, School of Agriculture, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Greece","Hoffmann M., Fraunhofer Institute for intelligent Analysis and Information Systems, Schloss Birlinghoven, Sankt Augustin, Germany; Stanly J., Fraunhofer Institute for intelligent Analysis and Information Systems, Schloss Birlinghoven, Sankt Augustin, Germany; Sundararaj S., Fraunhofer Institute for intelligent Analysis and Information Systems, Schloss Birlinghoven, Sankt Augustin, Germany; Rilling S., Fraunhofer Institute for intelligent Analysis and Information Systems, Schloss Birlinghoven, Sankt Augustin, Germany; Kotsopoulos T., Institute for Bio-Economy and Agri-Technology (IBO), Centre for Research and Technology-Hellas (CERTH), 6th km Charilaou-Thermi Rd, Thessaloniki, GR,57001, Greece, Department of Hydraulics, Soil Science and Agricultural Engineering, School of Agriculture, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Greece; Firfiris V., Institute for Bio-Economy and Agri-Technology (IBO), Centre for Research and Technology-Hellas (CERTH), 6th km Charilaou-Thermi Rd, Thessaloniki, GR,57001, Greece, Department of Hydraulics, Soil Science and Agricultural Engineering, School of Agriculture, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Greece; Moshou D., Institute for Bio-Economy and Agri-Technology (IBO), Centre for Research and Technology-Hellas (CERTH), 6th km Charilaou-Thermi Rd, Thessaloniki, GR,57001, Greece, Department of Hydraulics, Soil Science and Agricultural Engineering, School of Agriculture, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Greece","The continuous monitoring of livestock behaviour is an important task in precision livestock farming. An early recognition of critical events in a herd can lead to an improved animal welfare and thus for example in a reduced need of antibiotics. Furthermore, it has been found that even feeding intake and efficiency can be affected by animal behaviour. Due to the size of modern livestock farming operations, the usage of automated systems becomes necessary. We present a scalable automated monitoring system composed of low-cost IP video surveillance cameras and affordable edge-computing hardware on-site, complemented by standardized web services that allow for long term video storage and analysis. For the automated analysis of eating- and resting time as well as activity levels in videos, we perform state-of-the-art object detection and tracking using a YOLOv3 DeepSORT Convolutional Neural Network. Data management and the integration of the analysis system into the farmers’ work-flows are achieved using a distributed service interoperability network that allows the exchange of data between different services related to livestock management. The interoperability network is completely federated and based on established web technologies. Interoperability is achieved through standardized APIs and data formats. This enables farmers for example to seamlessly integrate behaviour analysis results into feeding management systems, or to share the results with external consultancy services or other stakeholders in the production chain. Furthermore, the standardized data exchange removes the extra workload that is caused by entering the same data into different systems. © ECPLF 2022. All rights reserved.","behaviour analysis; data integration; Deep Learning","Animals; Automation; Data integration; Deep learning; Digital storage; Electronic data interchange; Farms; Information management; Object detection; Security systems; Web services; Animal behaviour; Animal welfare; Behavior analysis; Continuous monitoring; Critical events; Deep learning; Livestock farming; Monitoring architecture; Monitoring services; Precision livestock farming; Interoperability","Horizon 2020 Framework Programme, H2020, (857125)","This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no. 857125","","Berckmans D.; Oczak M.; Iwersen M.; Wagener K.","Organising Committee of the 10th European Conference on Precision Livestock Farming (ECPLF), University of Veterinary Medicine Vienna","","978-839653600-6","","","English","Precis. Livest. Farming - Pap. Present. Eur. Conf. Precis. Livest. Farming, ECPLF","Conference paper","Final","","Scopus","2-s2.0-85172370883"
"Castagnolo G.; Mancuso D.; Palazzo S.; Spampinato C.; Porto S.M.C.","Castagnolo, G. (57425543300); Mancuso, D. (57564725400); Palazzo, S. (57223706450); Spampinato, C. (23391134800); Porto, S.M.C. (35369226600)","57425543300; 57564725400; 57223706450; 23391134800; 35369226600","Cow behavioural activities classification by convolutional neural networks","2022","Precision Livestock Farming 2022 - Papers Presented at the 10th European Conference on Precision Livestock Farming, ECPLF 2022","","","","48","55","7","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153959816&partnerID=40&md5=25d39cf4b1fb71b5d2e1ca3c2cea4709","Department of Electrical, Electronic and Computer Engineering, University of Catania, Viale Andrea Doria, 6, Catania, 95125, Italy; Department of Agriculture, Food and Environment, University of Catania, Via Santa Sofia, 100, Catania, 95123, Italy","Castagnolo G., Department of Electrical, Electronic and Computer Engineering, University of Catania, Viale Andrea Doria, 6, Catania, 95125, Italy; Mancuso D., Department of Agriculture, Food and Environment, University of Catania, Via Santa Sofia, 100, Catania, 95123, Italy; Palazzo S., Department of Electrical, Electronic and Computer Engineering, University of Catania, Viale Andrea Doria, 6, Catania, 95125, Italy; Spampinato C., Department of Electrical, Electronic and Computer Engineering, University of Catania, Viale Andrea Doria, 6, Catania, 95125, Italy; Porto S.M.C., Department of Agriculture, Food and Environment, University of Catania, Via Santa Sofia, 100, Catania, 95123, Italy","As has been shown in several studies, behavioural activities of animals provide important parameters for the evaluation of their health and welfare. In recent years the use of wearable sensors to record animal activity has become an important practice especially in extensive farms, where there is an infrequent farmer-to-animal contact. Accelerometers allow the measurements of movements of a body in space and are currently very popular in the zootechnical field for monitoring livestock, as they can be worn without being invasive for animals. The objective of this work was to address the task of classifying cow behavioural activities using a Convolutional Neural Network (CNN) to discriminate five classes: feeding in standing position, feeding while walking, walking, lying and rumination in lying position. To carry out this study, accelerometer data were acquired at 4 Hz by customized devices attached to cow collars, containing triaxial accelerometers. The acquired samples were previously labelled by using video-labelling, and then grouped in windows and pre-processed. The developed model is a CNN with 1D convolutions, which receives as input a 3-channel batch of windows, where channels are the three axes. Firstly, the model processes the data in parallel branches, which analyse different combination of channels. Features maps obtained from each branch are concatenated and provided as input to another cascade of convolutional layers. The model finally returns the prediction of the behavioural class. Our approach classified the five behavioural classes with an average F1 score of 81.51%. When merging the feeding in standing position and feeding while walking classes, F1 score reached 90.01%. © ECPLF 2022. All rights reserved.","automated monitoring systems; convolutional neural networks; cow welfare; deep learning; MEMS; sensor-based systems","Accelerometers; Agriculture; Animals; Convolution; Convolutional neural networks; Deep learning; Wearable sensors; Accelerometer data; Activity classifications; Animal activities; Automated monitoring systems; Convolutional neural network; Cow welfare; Deep learning; Measurements of; Sensor based systems; Triaxial accelerometer; Feeding","PRIN, (E64I18002270001); Università di Catania, UNICT","The research study was funded by PRIN 2017 project “Smart dairy farming: innovative solutions to improve herd productivity” (ID: E64I18002270001) coordinated by Prof. Si-mona M.C. Porto for the University of Catania. The authors are grateful to Massimo Pollino and Paolo Vasta of Trecastagni s.r.l., developers of the device and the WebAPP.","","Berckmans D.; Oczak M.; Iwersen M.; Wagener K.","Organising Committee of the 10th European Conference on Precision Livestock Farming (ECPLF), University of Veterinary Medicine Vienna","","978-839653600-6","","","English","Precis. Livest. Farming - Pap. Present. Eur. Conf. Precis. Livest. Farming, ECPLF","Conference paper","Final","","Scopus","2-s2.0-85153959816"
"Qiao Y.; Kong H.; Clark C.; Lomax S.; Su D.; Eiffert S.; Sukkarieh S.","Qiao, Yongliang (56486770900); Kong, He (57203456679); Clark, Cameron (7403546385); Lomax, Sabrina (56151402400); Su, Daobilige (56594211700); Eiffert, Stuart (57216910992); Sukkarieh, Salah (6602844626)","56486770900; 57203456679; 7403546385; 56151402400; 56594211700; 57216910992; 6602844626","Intelligent perception-based cattle lameness detection and behaviour recognition: A review","2021","Animals","11","11","3033","","","","31","10.3390/ani11113033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117519871&doi=10.3390%2fani11113033&partnerID=40&md5=1d81e06feeb1a86d909321f856f6744f","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia; College of Engineering, China Agricultural University, Beijing, 100083, China","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Kong H., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Clark C., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia; Lomax S., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia; Su D., College of Engineering, China Agricultural University, Beijing, 100083, China; Eiffert S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia","The growing world population has increased the demand for animal-sourced protein. However, animal farming productivity is faced with challenges from traditional farming practices, socioeconomic status, and climate change. In recent years, smart sensors, big data, and deep learning have been applied to animal welfare measurement and livestock farming applications, including behaviour recognition and health monitoring. In order to facilitate research in this area, this review summarises and analyses some main techniques used in smart livestock farming, focusing on those related to cattle lameness detection and behaviour recognition. In this study, more than 100 relevant papers on cattle lameness detection and behaviour recognition have been evaluated and discussed. Based on a review and a comparison of recent technologies and methods, we anticipate that intelligent perception for cattle behaviour and welfare monitoring will develop towards standardisation, a larger scale, and intelligence, combined with Internet of things (IoT) and deep learning technologies. In addition, the key challenges and opportunities of future research are also highlighted and discussed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cattle behaviour; Cattle welfare; Intelligent perception; Lameness detection; Precision livestock farming","agricultural worker; bovine; deep learning; intelligence; internet of things; livestock; nonhuman; perception; review; standardization; welfare","Meat & Livestock Australia Donor Company, (P.PSH.0819)","Funding: This research was funded by the Meat & Livestock Australia Donor Company (grant number P.PSH.0819).","Y. Qiao; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, Australia; email: yongliang.qiao@sydney.edu.au","","MDPI","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117519871"
"Mücher C.A.; Los S.; Franke G.J.; Kamphuis C.","Mücher, C.A. (7801367383); Los, S. (57554983400); Franke, G.J. (57554050500); Kamphuis, C. (9237616800)","7801367383; 57554983400; 57554050500; 9237616800","Detection, identification and posture recognition of cattle with satellites, aerial photography and UAVs using deep learning techniques","2022","International Journal of Remote Sensing","43","7","","2377","2392","15","16","10.1080/01431161.2022.2051634","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127248649&doi=10.1080%2f01431161.2022.2051634&partnerID=40&md5=4e251d21ac75ce79143c43bcd12a2055","Wageningen Environmental Research, Wageningen University and Research, Wageningen, Netherlands; Wageningen Livestock Research, Wageningen University and Research, Wageningen, Netherlands","Mücher C.A., Wageningen Environmental Research, Wageningen University and Research, Wageningen, Netherlands; Los S., Wageningen Environmental Research, Wageningen University and Research, Wageningen, Netherlands; Franke G.J., Wageningen Environmental Research, Wageningen University and Research, Wageningen, Netherlands; Kamphuis C., Wageningen Livestock Research, Wageningen University and Research, Wageningen, Netherlands","To obtain specific information about cattle in extensive production systems, the usual labor intensive work done by the farmer to find and visit cattle herds in large pastures can be replaced by using UAVs. UAVs are capable of assessing traits in cows, like distinguishing individuals and postures. Although these traits and the detection of cattle, do not represent resilience and efficiency directly, these may contain information associated to resilience. We performed a feasibility study of remotely sensed imagery (using datasets from satellites, manned aircrafts, and UAVs), and deep learning techniques to detect, count, identify and characterize posture of individual cows in grassland production systems. With these techniques, we focused on : (1) automatic detection of cattle locations and animal counting; (2) cow postures like standing, grazing or lying; and (3) individual cow identification. Data were collected during three field trials in the Netherlands and Poland. Artificial Intelligence was used to classify the objects (cattle) in the drone imagery. Classification accuracies of >95% were obtained for detecting cows. Accuracies of ~91% were obtained for identifying individual cows, and accuracies of ~88% were obtained for cow postures. These results make camera-mounted drones a promising new technology for monitoring extensive beef production systems. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","","Netherlands; Poland [Central Europe]; Aerial photography; Aircraft detection; Antennas; Deep learning; Drones; Learning algorithms; Cattle herd; Detection/identification; Feasibility studies; Labour-intensive; Learning techniques; Manned aircraft; Posture recognition; Production system; Remotely sensed imagery; Specific information; aerial photography; cattle; detection method; identification method; livestock farming; machine learning; pasture; posture; remotely operated vehicle; satellite data; Agriculture","European Union’s Horizon research and innovation programme; Horizon 2020 Framework Programme, H2020, (727213)","This work is part of the GenTORE project that has received funding from the European Union’s Horizon research and innovation programme, under grant agreement No. 727213. We would also like to thank CARUS farm and Juchowo farm for their services and hospitality, and Henk Kramer for his contribution in processing some of the UAV data. ","C.A. Mücher; Wageningen Environmental Research (WENR), Wageningen University and Research, Wageningen Campus, Wageningen, Building 101, Droevendaalsesteeg 3, P.O Box 47, 6700 AA, Netherlands; email: sander.mucher@wur.nl","","Taylor and Francis Ltd.","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127248649"
"","","","2022 IEEE Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2022 - Proceedings","2022","2022 IEEE Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2022 - Proceedings","","","","","","377","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144681757&partnerID=40&md5=6b5a8aece117f37aec34ca427c0d795b","","","The proceedings contain 71 papers. The topics discussed include: performance evaluation of a benchtop wood pellet length analyzer based on visual imaging; assessment of energy content of industrial woodchip based on prediction models of moisture content using portable NIR spectrophotometer; performance of a retrofitting solution for the reduction of particulate matter in small sized pellet stoves; wood pellet bulk density determination by machine vision deep learning technique; energy characterization of wood briquettes and possible use in automated domestic heating systems; monitoring and analysis of the gaseous emissions collected in a livestock farm; development and evaluation of an in vitro procedure to assess enteric methane production; treatment of dairy cattle slurry for biogas production and nitrogen recovery; biochar covering to mitigate the ammonia emissions from the manure storage tank: effect of the pyrolysis temperature; and comparison of cosmic-ray neutron sensing and gamma-ray spectrometry for non-invasive soil moisture estimation over a large cropped field.","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546998-2","","","English","IEEE Workshop Metrol. Agric. For., MetroAgriFor - Proc.","Conference review","Final","","Scopus","2-s2.0-85144681757"
"Jácome-Galarza L.-R.","Jácome-Galarza, Luis-Roberto (57222902414)","57222902414","Multimodal Deep Learning for Crop Yield Prediction","2022","Communications in Computer and Information Science","1647 CCIS","","","106","117","11","2","10.1007/978-3-031-18347-8_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142669826&doi=10.1007%2f978-3-031-18347-8_9&partnerID=40&md5=3c03b2f6c5cf2c3bae4d45a743a76fbe","Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Gustavo Galindo Km. 30.5 Vía Perimetral, Guayaquil, Ecuador","Jácome-Galarza L.-R., Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Gustavo Galindo Km. 30.5 Vía Perimetral, Guayaquil, Ecuador","Precision agriculture is a vital practice for improving the production of crops. The present work is aimed to develop a deep learning multimodal model that can predict the crop yield in Ecuadorian corn farms. The model takes multispectral images and field sensor data (humidity, temperature, or soil status) to obtain the yield of a crop. The use of multimodal data is aimed to extract hidden patterns in the status of crops and in this way obtain better results than the use of vegetation indices or other state-of-the-art methods. For the experiments, we utilized multi-spectral satellite images obtained from the google earth engine platform and monthly precipitation and temperature data of the 24 Ecuadorian provinces collected from the Ecuadorian Ministry of agriculture and livestock; likewise, we obtained the area of corn plantation in each province and their corn production for the years 2016 to 2020. Results indicate that the use of multimodal deep learning models (pre-trained CNN for images and LSTM for time series sensor data) gives better prediction accuracy than monomodal prediction models. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Convolutional neural networks; Multimodal deep learning; Precision agriculture; Recurrent neural networks; Remote sensing","Convolutional neural networks; Forecasting; Learning systems; Long short-term memory; Precision agriculture; Remote sensing; Convolutional neural network; Crop yield; Multi-modal; Multimodal deep learning; Multimodal models; Multispectral images; Precision Agriculture; Remote-sensing; Sensors data; Yield prediction; Crops","","","L.-R. Jácome-Galarza; Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Guayaquil, Gustavo Galindo Km. 30.5 Vía Perimetral, Ecuador; email: lrjacome@espol.edu.ec","Abad K.; Berrezueta S.; Berrezueta S.","Springer Science and Business Media Deutschland GmbH","18650929","978-303118346-1","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85142669826"
"Li X.; Hou J.; Huang C.","Li, Xianghua (57226632939); Hou, Jinliang (48861439400); Huang, Chunlin (55207460700)","57226632939; 48861439400; 55207460700","High-resolution gridded livestock projection for western China based on machine learning","2021","Remote Sensing","13","24","5038","","","","17","10.3390/rs13245038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121394538&doi=10.3390%2frs13245038&partnerID=40&md5=d0b791750ba694bc29c2733c1a999262","Key Laboratory of Remote Sensing of Gansu Province, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China; Heihe Remote Sensing Experimental Research Station, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Zhangye, 734000, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100049, China","Li X., Key Laboratory of Remote Sensing of Gansu Province, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China, Heihe Remote Sensing Experimental Research Station, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Zhangye, 734000, China, College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100049, China; Hou J., Key Laboratory of Remote Sensing of Gansu Province, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China, Heihe Remote Sensing Experimental Research Station, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Zhangye, 734000, China; Huang C., Key Laboratory of Remote Sensing of Gansu Province, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China, Heihe Remote Sensing Experimental Research Station, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Zhangye, 734000, China","Accurate high-resolution gridded livestock distribution data are of great significance for the rational utilization of grassland resources, environmental impact assessment, and the sustain-able development of animal husbandry. Traditional livestock distribution data are collected at the administrative unit level, which does not provide a sufficiently detailed geographical description of livestock distribution. In this study, we proposed a scheme by integrating high-resolution gridded geographic data and livestock statistics through machine learning regression models to spatially disaggregate the livestock statistics data into 1 km × 1 km spatial resolution. Three machine learning models, including support vector machine (SVM), random forest (RF), and deep neural network (DNN), were constructed to represent the complex nonlinear relationship between various environmental factors (e.g., land use practice, topography, climate, and socioeconomic factors) and livestock density. By applying the proposed method, we generated a set of 1 km × 1 km spatial distribution maps of cattle and sheep for western China from 2000 to 2015 at five-year intervals. Our projected cattle and sheep distribution maps reveal the spatial heterogeneity structures and change trend of livestock distribution at the grid level from 2000 to 2015. Compared with the traditional census livestock density, the gridded livestock distribution based on DNN has the highest accuracy, with the determinant coefficient (R2) of 0.75, root mean square error (RMSE) of 9.82 heads/km2 for cattle, and the R2 of 0.73, RMSE of 31.38 heads/km2 for sheep. The accuracy of the RF is slightly lower than the DNN but higher than the SVM. The projection accuracy of the three machine learning models is superior to those of the published Gridded Livestock of the World (GLW) datasets. Consequently, deep learning has the potential to be an effective tool for high-resolution gridded livestock projection by combining geographic and census data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Livestock; Machine learning; Spatialization; Western China","Climate models; Decision trees; Economics; Environmental impact; Environmental impact assessments; Land use; Regression analysis; Risk assessment; Support vector machines; Surveys; Topography; Geographic data; Grassland resource; High resolution; Livestock density; Machine learning models; Random forests; Root mean square errors; Spatialization; Support vectors machine; Western China; Agriculture","Basic Research Innovative Groups of Gansu province, (21JR7RA068); National Natural Science Foundation of China, NSFC, (42130113); Chinese Academy of Sciences, CAS, (XDA19040504)","Funding: This work is supported by the National Science Foundation of China under grants (42130113), the Strategic Priority Research Program of the Chinese Academy of Sciences under grants XDA19040504, and the Basic Research Innovative Groups of Gansu province, China (Grant No. 21JR7RA068).","J. Hou; Key Laboratory of Remote Sensing of Gansu Province, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, 730000, China; email: jlhours@lzb.ac.cn","","MDPI","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121394538"
"Asfahan H.M.; Sajjad U.; Sultan M.; Hussain I.; Hamid K.; Ali M.; Wang C.-C.; Shamshiri R.R.; Khan M.U.","Asfahan, Hafiz M. (57222898764); Sajjad, Uzair (57202950663); Sultan, Muhammad (56532340300); Hussain, Imtiyaz (57221769068); Hamid, Khalid (57221767325); Ali, Mubasher (57221004199); Wang, Chi-Chuan (8934793900); Shamshiri, Redmond R. (55308184200); Khan, Muhammad Usman (58263369700)","57222898764; 57202950663; 56532340300; 57221769068; 57221767325; 57221004199; 8934793900; 55308184200; 58263369700","Artificial intelligence for the prediction of the thermal performance of evaporative cooling systems","2021","Energies","14","13","3946","","","","34","10.3390/en14133946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109908410&doi=10.3390%2fen14133946&partnerID=40&md5=f781924fac11235377a8c674d15ef0ad","Department of Agricultural Engineering, Bahauddin Zakariya University, Bosan Road, Multan, 60800, Pakistan; Department of Mechanical Engineering, National Yang Ming Chiao Tung University, 1001 University Road, Hsinchu, 300, Taiwan; Department of Power Mechanical Engineering, National Tsing Hua University, No. 101, Section 2, Guangfu Road, East District, Hsinchu, 300, Taiwan; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam‐Bornim, 14469, Germany; Department of Energy Systems Engineering, Faculty of Agricultural Engineering and Technology, University of Agriculture, Faisalabad, Punjab, 38040, Pakistan","Asfahan H.M., Department of Agricultural Engineering, Bahauddin Zakariya University, Bosan Road, Multan, 60800, Pakistan; Sajjad U., Department of Mechanical Engineering, National Yang Ming Chiao Tung University, 1001 University Road, Hsinchu, 300, Taiwan; Sultan M., Department of Agricultural Engineering, Bahauddin Zakariya University, Bosan Road, Multan, 60800, Pakistan; Hussain I., Department of Power Mechanical Engineering, National Tsing Hua University, No. 101, Section 2, Guangfu Road, East District, Hsinchu, 300, Taiwan; Hamid K., Department of Mechanical Engineering, National Yang Ming Chiao Tung University, 1001 University Road, Hsinchu, 300, Taiwan; Ali M., Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Wang C.-C., Department of Mechanical Engineering, National Yang Ming Chiao Tung University, 1001 University Road, Hsinchu, 300, Taiwan; Shamshiri R.R., Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam‐Bornim, 14469, Germany; Khan M.U., Department of Energy Systems Engineering, Faculty of Agricultural Engineering and Technology, University of Agriculture, Faisalabad, Punjab, 38040, Pakistan","The present study reports the development of a deep learning artificial intelligence (AI) model for predicting the thermal performance of evaporative cooling systems, which are widely used for thermal comfort in different applications. The existing, conventional methods for the analysis of evaporation‐assisted cooling systems rely on experimental, mathematical, and empirical approaches in order to determine their thermal performance, which limits their applications in diverse and ambient spatiotemporal conditions. The objective of this research was to predict the thermal performance of three evaporation‐assisted air‐conditioning systems—direct, indirect, and Maisotsenko evaporative cooling systems—by using an AI approach. For this purpose, a deep learning algorithm was developed and lumped hyperparameters were initially chosen. A correlation analysis was performed prior to the development of the AI model in order to identify the input features that could be the most influential for the prediction efficiency. The deep learning algorithm was then optimized to increase the learning rate and predictive accuracy with respect to experimental data by tuning the hyperparameters, such as by manipulating the activation functions, the number of hidden layers, and the neurons in each layer by incorporating optimizers, including Adam and RMsprop. The results confirmed the applicability of the method with an overall value of R2 = 0.987 between the input data and ground‐truth data, showing that the most competent model could predict the designated output features (Tdboutdbout, wout, and Eairout). The suggested method is straightforward and was found to be practical in the evaluation of the thermal performance of deployed air conditioning systems under different conditions. The results supported the hypothesis that the proposed deep learning AI algorithm has the potential to explore the feasibility of the three evaporative cooling systems in dynamic ambient conditions for various agricultural and livestock applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Direct evaporative cooling; Evaporative cooling; Indirect evaporative cooling; Maisotsenko evaporative cooling","Agricultural robots; Agriculture; Air conditioning; Cooling; Deep learning; Evaporation; Forecasting; Learning algorithms; Learning systems; Predictive analytics; Thermoelectric equipment; Activation functions; Ambient conditions; Conditioning systems; Conventional methods; Correlation analysis; Empirical approach; Predictive accuracy; Thermal Performance; Evaporative cooling systems","Leibniz-Gemeinschaft; , (105140)","Acknowledgments: The authors acknowledge the financial support from the Open Access Publica\u2010 tion Fund of the Leibniz Association, Germany, as well as the partial research funding and editorial support from Adaptive AgroTech Consultancy International.","U. Sajjad; Department of Mechanical Engineering, National Yang Ming Chiao Tung University, Hsinchu, 1001 University Road, 300, Taiwan; email: energyengneer01@gmail.com; M. Sultan; Department of Agricultural Engineering, Bahauddin Zakariya University, Multan, Bosan Road, 60800, Pakistan; email: muhammadsultan@bzu.edu.pk; R.R. Shamshiri; Department of Engineering for Crop Production, Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam‐Bornim, 14469, Germany; email: rshamshiri@atb‐potsdam.de","","MDPI AG","19961073","","","","English","Energies","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85109908410"
"Xie B.; Jiao W.; Wen C.; Hou S.; Zhang F.; Liu K.; Li J.","Xie, Bin (57205889983); Jiao, Weipeng (57292401400); Wen, Changkai (57208058726); Hou, Songtao (57221164738); Zhang, Fan (57208263334); Liu, Kaidong (57221163892); Li, Junlin (57291710000)","57205889983; 57292401400; 57208058726; 57221164738; 57208263334; 57221163892; 57291710000","Feature detection method for hind leg segmentation of sheep carcass based on multi-scale dual attention U-Net","2021","Computers and Electronics in Agriculture","191","","106482","","","","16","10.1016/j.compag.2021.106482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116864935&doi=10.1016%2fj.compag.2021.106482&partnerID=40&md5=bbc603a4a142bfce1a4b414e353b7ee5","College of Engineering, China Agricultural University, Beijing, China; Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China","Xie B., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Jiao W., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Wen C., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Hou S., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Zhang F., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Liu K., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China; Li J., College of Engineering, China Agricultural University, Beijing, China, Beijing Key Laboratory of Optimized Design for Modern Agricultural Equipment, Beijing, China","Due to the variable size of the sheep carcass and the complex characteristics of the surface tissue of the hind legs, the recognition accuracy of the segmented target muscle area is low. This paper proposes a method for detecting the segmentation features of sheep carcass hind legs and carries out a segmentation test to validate it. The approach takes the multi-scale dual attention U-Net (MDAU-Net) semantic segmentation network as its core. It effectively combines different layer features, spatial attention modules, and channel attention modules. We design a multi-scale dual attention (MDA) module to enhance multi-scale contextual semantic and local detail features, and embeds it into the U-Net hopping layer connection to obtain the specific semantic features and local details features of the coding stage. The experimental results show that the Pre and MIoU of the MDAU-Net semantic segmentation network on the self-built sheep carcass hind leg region data set are 93.76% and 86.94% respectively. Both are better than the control group, which proves the segmentation accuracy of this method on the sheep carcass hind leg dataset. The actual segmentation is implemented based on the result of feature recognition. The average offset distance of the tool target undercutting point was 4.02 mm, and the average segmentation residual rate was 6.28%, which basically met the requirements of the primary segmentation of the sheep carcass hind legs. This study can provide more technical references for the autonomous segmentation technology of livestock meat. © 2021 Elsevier B.V.","Deep learning; Intelligent segmentation; MDAU-Net; Semantic segmentation; Sheep carcass hind legs","Agriculture; Deep learning; Feature extraction; Semantics; Deep learning; Detection methods; Features detections; Hind legs; Intelligent segmentation; Multi-scale dual attention U-net; Multi-scales; Semantic segmentation; Sheep carcass hind leg; Variable sizes; body size; detection method; experimental study; livestock; segmentation; semantic standardization; sheep; Semantic Segmentation","National Key Research and Development Program of China, NKRDPC, (2018YFD0700804); National Key Research and Development Program of China, NKRDPC","All of the authors would like to acknowledge the National Key Research and Development Plan of China (2018YFD0700804).","C. Wen; College of Engineering, China Agricultural University, Beijing, China; email: 18813003909@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85116864935"
"Su W.-T.; Jiang L.-Y.; Tang-Hsuan O.; Lin Y.-C.; Hung M.-H.; Chen C.-C.","Su, Wei-Tsung (35087768100); Jiang, Lin-Yi (57721716500); Tang-Hsuan, O. (57223092075); Lin, Yu-Chuan (57015720600); Hung, Min-Hsiung (7202454332); Chen, Chao-Chun (7501954690)","35087768100; 57721716500; 57223092075; 57015720600; 7202454332; 7501954690","AIoT-Cloud-Integrated Smart Livestock Surveillance via Assembling Deep Networks with Considering Robustness and Semantics Availability","2021","IEEE Robotics and Automation Letters","6","4","9460764","6140","6147","7","5","10.1109/LRA.2021.3090453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112282553&doi=10.1109%2fLRA.2021.3090453&partnerID=40&md5=42ed1d99f572ed563942c2a54a7d8a10","Department of Computer Science and Information Engineering, Aletheia University, Taipei City, 251, Taiwan; Institute of Manufacturing Information and Systems, Department of Computer Science and Information Engineering, NationalChengKung University (NCKU), Tainan City, 701, Taiwan; Office of the Secretary-General of the IMRC, National Cheng Kung University (NCKU), Tainan City, 701, Taiwan; Department of Computer Science and Information Engineering, Chinese Culture University, Taipei City, 111, Taiwan","Su W.-T., Department of Computer Science and Information Engineering, Aletheia University, Taipei City, 251, Taiwan; Jiang L.-Y., Department of Computer Science and Information Engineering, Aletheia University, Taipei City, 251, Taiwan; Tang-Hsuan O., Institute of Manufacturing Information and Systems, Department of Computer Science and Information Engineering, NationalChengKung University (NCKU), Tainan City, 701, Taiwan; Lin Y.-C., Institute of Manufacturing Information and Systems, Department of Computer Science and Information Engineering, NationalChengKung University (NCKU), Tainan City, 701, Taiwan, Office of the Secretary-General of the IMRC, National Cheng Kung University (NCKU), Tainan City, 701, Taiwan; Hung M.-H., Department of Computer Science and Information Engineering, Chinese Culture University, Taipei City, 111, Taiwan; Chen C.-C., Institute of Manufacturing Information and Systems, Department of Computer Science and Information Engineering, NationalChengKung University (NCKU), Tainan City, 701, Taiwan","In this letter, we propose a novel smart livestock surveillance system through cooperation of AIoT (artificial intelligence of things) devices and the cloud computing platform, aiming at providing semantic information via assembling deep networks with AIoT devices of limited resource. The key of the proposed system includes two designs: Deep-net assembling as a semantic surveillance service and the expandable-convolutional-block neural network (ECB-Net). The first is a development architecture of the divide-and-conquer philosophy for establishing semantic surveillance systems, and this work provides a concrete instance for promoting deep-net assembling to livestock industries. The second is an AIoT device-friendly neural network for filtering insignificant camera images to achieve high robustness of smart surveillance systems. The technical details from the architecture design to optimal ECB-Net model creation are presented in related sections. Finally, we develop the prototype of the smart livestock surveillance system and deploy it by swine rooms for conducting real-world integrated tests. Testing results reveal the superior performance of our proposed smart livestock surveillance scheme.  © 2021 IEEE.","Ag-Tech; AIoT; Deep Learning; farming automation; intelligent agriculture; parameter optimization","Agricultural robots; Agriculture; Concrete industry; Convolutional neural networks; Monitoring; Network architecture; Semantic Web; Semantics; Architecture designs; Cloud computing platforms; Divide and conquer; Semantic information; Smart surveillance systems; Surveillance services; Surveillance systems; Technical details; Security systems","Ministry of Education in Taiwan; Ministry of Science and Technology, Taiwan, MOST, (108-2221-E-034-015-MY2, 109-2218-E-006-007, 109-2221-E-006-199); National Cheng Kung University Hospital, NCKU; Intelligent Manufacturing Research Center, iMRC","Manuscript received February 27, 2021; accepted June 7, 2021. Date of publication June 18, 2021; date of current version July 6, 2021. This letter was recommended for publication by Associate Editor S. Wang and Editor J. Yi upon evaluation of the reviewers’ comments. Authors thank Jason Chao with Playsure Technology Co. for giving insightful comments to meet industrial needs. This work was supported by Ministry of Science and Technology (MOST) of Taiwan under Grants MOST 109-2221-E-006-199, 109-2218-E-006-007, and 108-2221-E-034-015-MY2, in part by the “Intelligent Manufacturing Research Center” (iMRC) in NCKU from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education in Taiwan. (Corresponding authors: Wei-Tsung Su; Chao-Chun Chen.) Wei-Tsung Su is with the Department of Computer Science and Information Engineering, Aletheia University, Taipei City 251, Taiwan (e-mail: suwt@au.edu.tw).","W.-T. Su; Department of Computer Science and Information Engineering, Aletheia University, Taipei City, 251, Taiwan; email: suwt@au.edu.tw; C.-C. Chen; Institute of Manufacturing Information and Systems, Department of Computer Science and Information Engineering, NationalChengKung University (NCKU), Tainan City, 701, Taiwan; email: chaochun@mail.ncku.edu.tw","","Institute of Electrical and Electronics Engineers Inc.","23773766","","","","English","IEEE Robot. Autom.","Article","Final","","Scopus","2-s2.0-85112282553"
"Noe S.M.; Zin T.T.; Tin P.; Kobayashi I.","Noe, Su Myat (57221527571); Zin, Thi Thi (6506258245); Tin, Pyke (24923729700); Kobayashi, Ikuo (24174822700)","57221527571; 6506258245; 24923729700; 24174822700","A Deep Learning-based solution to Cattle Region Extraction for Lameness Detection","2022","LifeTech 2022 - 2022 IEEE 4th Global Conference on Life Sciences and Technologies","","","","572","573","1","8","10.1109/LifeTech53646.2022.9754780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129163576&doi=10.1109%2fLifeTech53646.2022.9754780&partnerID=40&md5=a7abbfad6d4ef938ae13435a36c2e978","University of Miyazaki, Graduate School of Engineering, Miyazaki, Japan; University of Miyazaki, Field Science Center, Faculty of Agriculture, Miyazaki, Japan","Noe S.M., University of Miyazaki, Graduate School of Engineering, Miyazaki, Japan; Zin T.T., University of Miyazaki, Graduate School of Engineering, Miyazaki, Japan; Tin P., University of Miyazaki, Graduate School of Engineering, Miyazaki, Japan; Kobayashi I., University of Miyazaki, Field Science Center, Faculty of Agriculture, Miyazaki, Japan","In precision livestock farming, lameness detection in cattle is particularly important for breeding management. The accurate detection of lameness is crucial for delivering effective and economical treatment and for preventing future diseases. The noticeable sign of lameness is that their speed of walking, arching their backs and drop their heads during walking. Here, we emphasis on lameness of dairy cattle by implementing the intelligent visual perception system on the laneways after milking process. Employing a deep learning technique of Mask-RCNN for cattle region detection and identification. The novelty of this work noticeably implies that deep learning instance segmentation could be effectively employed as a cattle region extraction from complex background prior to using identification and tracking.  © 2022 IEEE.","cattle identification; cattle tracking; instance segmentation; lameness behavior in cattle; Mask R-CNN","Agriculture; Convolutional neural networks; Deep learning; Extraction; Cattle identification; Cattle tracking; Dairy cattles; Lameness behavior in cattle; Lameness detection; Laneway; Perception systems; Precision livestock farming; Region extraction; Visual perception","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166541904-8","","","English","LifeTech - IEEE Glob. Conf. Life Sci. Technol.","Conference paper","Final","","Scopus","2-s2.0-85129163576"
"Saradha S.; Asha J.; Sreemathy J.","Saradha, S. (58519739400); Asha, J. (56938822300); Sreemathy, J. (57195427029)","58519739400; 56938822300; 57195427029","A Deep Learning-based Framework for Sheep Identification System based on Facial Bio-Metrics Analysis","2022","6th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud), I-SMAC 2022 - Proceedings","","","","560","564","4","4","10.1109/I-SMAC55078.2022.9987431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420087&doi=10.1109%2fI-SMAC55078.2022.9987431&partnerID=40&md5=ccd564afa83606b87ad4b7f4430f537c","Sri Eshwar College of Engineering, Department of Cse, Coimbatore, India","Saradha S., Sri Eshwar College of Engineering, Department of Cse, Coimbatore, India; Asha J., Sri Eshwar College of Engineering, Department of Cse, Coimbatore, India; Sreemathy J., Sri Eshwar College of Engineering, Department of Cse, Coimbatore, India","Through the use of livestock, information sharing is becoming increasingly popular around the world. This study aims to see biometric face analysis be used on sheep recognition to improve sheep monitoring in the centralized database. Anchor-free region convolutional neural networks were used to detect sheep identities (AF-RCNN). Face recognition's effectiveness as a biometric-based identification for sheep was studied utilizing reviews of face images using the deep earing approach. The method is standalone on a set of standardized facial photos from 50 sheep, using an augmentation strategy to expand the number of sheep images. The proposed method outperforms earlier methods for sheep recognition with high accuracy.  © 2022 IEEE.","anchor-free region convolutional neural network and; augmentation method; biometric face analysis; sheep management","Agriculture; Convolution; Convolutional neural networks; Deep learning; Face recognition; Anchor-free; Anchor-free region convolutional neural network and; Augmentation methods; Bio-metric; Biometric face analyse; Convolutional neural network; Face analysis; Free region; Metric analysis; Sheep management; Biometrics","","","S. Saradha; Sri Eshwar College of Engineering, Department of Cse, Coimbatore, India; email: saradha.s@sece.ac.in","","Institute of Electrical and Electronics Engineers Inc.","","978-166546941-8","","","English","Int. Conf. I-SMAC (IoT Soc., Mob., Anal. Cloud), I-SMAC - Proc.","Conference paper","Final","","Scopus","2-s2.0-85146420087"
"Qiao Y.; Kong H.; Clark C.; Lomax S.; Su D.; Eiffert S.; Sukkarieh S.","Qiao, Yongliang (56486770900); Kong, He (57203456679); Clark, Cameron (7403546385); Lomax, Sabrina (56151402400); Su, Daobilige (56594211700); Eiffert, Stuart (57216910992); Sukkarieh, Salah (6602844626)","56486770900; 57203456679; 7403546385; 56151402400; 56594211700; 57216910992; 6602844626","Intelligent perception for cattle monitoring: A review for cattle identification, body condition score evaluation, and weight estimation","2021","Computers and Electronics in Agriculture","185","","106143","","","","116","10.1016/j.compag.2021.106143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104677946&doi=10.1016%2fj.compag.2021.106143&partnerID=40&md5=27b7b3a15e867127f9659b1a64c7c72f","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, 2006, NSW, Australia; College of Engineering, China Agricultural University, Beijing, 100083, China","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Kong H., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Clark C., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, 2006, NSW, Australia; Lomax S., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, 2006, NSW, Australia; Su D., College of Engineering, China Agricultural University, Beijing, 100083, China; Eiffert S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia","There has been an increasing demand for animal protein due to several factors such as global population growth, rising incomes, etc. However, farming productivity is stagnating due to a mix of traditional practice, climate change, socio-economic, and environmental phenomena. Precision livestock farming, with intelligent perception tools at its core, and vast amounts of data being acquired from different sensors or platforms, has the ability to analyse individual animal for improved management, and the potential to dramatically enhance farm productivity. In order to facilitate research and promote the development of related areas, this review summarises and analyses the main existing techniques used in precision cattle farming, focusing on those related to identification, body condition score evaluation, and live weight estimation. More than 100 relevant papers have been discussed in a cohesive manner. From this review and extensive discussions of recent trends, we anticipate that intelligent perception for precision cattle farming will develop through non-contact, high precision, automated technologies, combined with emerging 3D model reconstruction and deep learning technologies. Existing challenges and future research opportunities will also be highlighted and discussed. © 2021 Elsevier B.V.","Cattle welfare; Computer vision; Deep learning; Intelligent perception; Precision livestock farming","3D modeling; Animals; Climate change; Computer vision; Deep learning; Engineering education; Population statistics; Productivity; Three dimensional computer graphics; Animal proteins; Body condition score; Cattle monitoring; Cattle welfare; Deep learning; Global population; Intelligent perception; Population growth; Precision livestock farming; Weights estimation; abundance estimation; body condition; cattle; estimation method; identification method; perception; precision; sensor; Agriculture","Meat & Livestock Australia Donor Company","The authors acknowledge the support of the Meat & Livestock Australia Donor Company through the project: Objective, robust, real-time animal welfare measures for the Australian red meat industry. The authors also express their gratitude to Khalid Rafique for his unwavering support and help in project management matters. ","Y. Qiao; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, Australia; email: yongliang.qiao@sydney.edu.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85104677946"
"Billah M.; Wang X.; Yu J.; Jiang Y.","Billah, Masum (58589436500); Wang, Xihong (57200762255); Yu, Jiantao (57195423771); Jiang, Yu (55733627100)","58589436500; 57200762255; 57195423771; 55733627100","Real-time goat face recognition using convolutional neural network","2022","Computers and Electronics in Agriculture","194","","106730","","","","59","10.1016/j.compag.2022.106730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124271945&doi=10.1016%2fj.compag.2022.106730&partnerID=40&md5=8ac662ff931a3df2d31998f01bb2af15","College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, Shaanxi, China; College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China","Billah M., College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, Shaanxi, China; Wang X., College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, Shaanxi, China; Yu J., College of Information Engineering, Northwest A&F University, Yangling, 712100, Shaanxi, China; Jiang Y., College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, Shaanxi, China","Automatic identification of individual animals is an important step towards achieving accurate breeding histories, significant contributions to breeding and genetic management programmers. Currently, a different type of tags, tattoos, paint brands and microchips are used to uniquely identify livestock animals. However, the manual identification system is time-consuming, expensive and unreliable. In this paper, we present a deep learning approach that aims to fully automated pipeline for face detection and recognition of goats. Due to the high similarity and the lack of adequate dataset this problem is more complex than human face recognition. We composed two different publicly available datasets for detection and recognition. State-of-the-art convolutional neural networks (CNN) model are trained on this dataset. To evaluate the robustness of our approach, we compared it with different face recognition methods. The results show better performance with an accuracy of 96.4%. Furthermore, this paper reports 93%, 83%, 92% and 85% detection accuracy (average precision) for face, eye, nose and ear, respectively. The findings of this research could be helpful to improve animal health and welfare, individual monitoring, activity monitoring and phenotypic data collection. All the dataset and the related outcome are publicly available (https://doi.org/10.17632/4skwhnrscr.2). © 2022 Elsevier B.V.","Convolutional neural network; Goat face and landmarks detection; Goat identity recognition; Goat image dataset","Agriculture; Animals; Automation; Convolution; Deep learning; Face recognition; Convolutional neural network; Faces detection; Genetic management; Goat face and landmark detection; Goat identity recognition; Goat image dataset; Identity recognition; Image datasets; Landmark detection; Real- time; artificial neural network; data set; genetic analysis; identification method; real time; Convolutional neural networks","National Natural Science Foundation of China, NSFC, (31822052); Northwest A and F University, NWAFU","Thanks for the useful advice from Prof. Jifeng Ning and Associate Prof. Meili Wang in the College of Information Engineering, Northwest A&F University. We would also like to thanks Jiahong Li, Jiawei Liu, Jing Geng, Yifang Zhang for helping us with data collection. Our gratitude extends to the National Natural Science Foundation of China (31822052) for supporting this project.","Y. Jiang; College of Animal Science and Technology, Northwest A&F University, Yangling, 712100, China; email: yu.jiang@nwafu.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85124271945"
"Dhanya V.G.; Subeesh A.; Kushwaha N.L.; Vishwakarma D.K.; Nagesh Kumar T.; Ritika G.; Singh A.N.","Dhanya, V.G. (57932268900); Subeesh, A. (57207879906); Kushwaha, N.L. (57219726089); Vishwakarma, Dinesh Kumar (57351531900); Nagesh Kumar, T. (57932129100); Ritika, G. (57931706300); Singh, A.N. (57205406370)","57932268900; 57207879906; 57219726089; 57351531900; 57932129100; 57931706300; 57205406370","Deep learning based computer vision approaches for smart agricultural applications","2022","Artificial Intelligence in Agriculture","6","","","211","229","18","136","10.1016/j.aiia.2022.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140081203&doi=10.1016%2fj.aiia.2022.09.007&partnerID=40&md5=26a6327df3ff3945e6b2bbfd0afe3cae","ICAR- Indian Institute of Seed Science, Uttar Pradesh, Mau, 275101, India; ICAR- Central Institute of Agricultural Engineering, Madhya Pradesh, Bhopal, 462038, India; ICAR- Indian Agricultural Research Institute, New Delhi, 110012, India; Govind Ballabh Pant University of Agriculture and Technology, Uttarakhand, Pantnagar, 263145, India; ICAR - National Institute on Natural Fibre Engineering and Technology, Kolkata, 700040, India","Dhanya V.G., ICAR- Indian Institute of Seed Science, Uttar Pradesh, Mau, 275101, India; Subeesh A., ICAR- Central Institute of Agricultural Engineering, Madhya Pradesh, Bhopal, 462038, India; Kushwaha N.L., ICAR- Indian Agricultural Research Institute, New Delhi, 110012, India; Vishwakarma D.K., Govind Ballabh Pant University of Agriculture and Technology, Uttarakhand, Pantnagar, 263145, India; Nagesh Kumar T., ICAR - National Institute on Natural Fibre Engineering and Technology, Kolkata, 700040, India; Ritika G., ICAR- Indian Agricultural Research Institute, New Delhi, 110012, India; Singh A.N., ICAR- Indian Institute of Seed Science, Uttar Pradesh, Mau, 275101, India","The agriculture industry is undergoing a rapid digital transformation and is growing powerful by the pillars of cutting-edge approaches like artificial intelligence and allied technologies. At the core of artificial intelligence, deep learning-based computer vision enables various agriculture activities to be performed automatically with utmost precision enabling smart agriculture into reality. Computer vision techniques, in conjunction with high-quality image acquisition using remote cameras, enable non-contact and efficient technology-driven solutions in agriculture. This review contributes to providing state-of-the-art computer vision technologies based on deep learning that can assist farmers in operations starting from land preparation to harvesting. Recent works in the area of computer vision were analyzed in this paper and categorized into (a) seed quality analysis, (b) soil analysis, (c) irrigation water management, (d) plant health analysis, (e) weed management (f) livestock management and (g) yield estimation. The paper also discusses recent trends in computer vision such as generative adversarial networks (GAN), vision transformers (ViT) and other popular deep learning architectures. Additionally, this study pinpoints the challenges in implementing the solutions in the farmer's field in real-time. The overall finding indicates that convolutional neural networks are the corner stone of modern computer vision approaches and their various architectures provide high-quality solutions across various agriculture activities in terms of precision and accuracy. However, the success of the computer vision approach lies in building the model on a quality dataset and providing real-time solutions. © 2022 The Authors","Agriculture automation; Computer vision; Deep learning; Machine learning; Smart agriculture; Vision transformers","Automation; Computer vision; Convolutional neural networks; Deep learning; Generative adversarial networks; Irrigation; Learning systems; Network architecture; Water management; Agriculture automation; Agriculture industries; Computer vision techniques; Cutting edges; Deep learning; Digital transformation; High quality images; Machine-learning; Smart agricultures; Vision transformer; Quality control","","","V.G. Dhanya; ICAR- Indian Institute of Seed Science, Mau, Uttar Pradesh, 275101, India; email: dhanya.vg@icar.gov.in; A. Subeesh; ICAR - Central Institute of Agricultural Engineering (CIAE), Bhopal, Madhya Pradesh, 462038, India; email: subeesh.a@icar.gov.in","","KeAi Communications Co.","25897217","","","","English","Artif. Intell. Agric.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140081203"
"Duan E.; Wang L.; Wang H.; Hao H.; Li R.","Duan, Enze (57197857825); Wang, Liangju (55312342500); Wang, Hongying (55688840900); Hao, Hongyun (57223212527); Li, Rangling (57216289180)","57197857825; 55312342500; 55688840900; 57223212527; 57216289180","Feed weight estimation model for health monitoring of meat rabbits based on deep learning","2022","International Journal of Agricultural and Biological Engineering","15","1","","233","240","7","3","10.25165/j.ijabe.20221501.6797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125776735&doi=10.25165%2fj.ijabe.20221501.6797&partnerID=40&md5=42fd0b3bfb107ffe6f755b004f28b27b","College of Engineering, China Agricultural University, Beijing, 100083, China","Duan E., College of Engineering, China Agricultural University, Beijing, 100083, China; Wang L., College of Engineering, China Agricultural University, Beijing, 100083, China; Wang H., College of Engineering, China Agricultural University, Beijing, 100083, China; Hao H., College of Engineering, China Agricultural University, Beijing, 100083, China; Li R., College of Engineering, China Agricultural University, Beijing, 100083, China","With the development of precision livestock farming, non-contact health monitoring technology is particularly important in the breeding process. To help improve the management of the rabbit breeding industry, a remaining feed weight (RFW) estimation model was developed based on the image segmentation method. The model proposed in this study consisted of a feed instance segmentation neural network and feed weight estimation network. Feed instance segmentation neural network was based on the improved Mask Region-based Convolution Neural Network (Mask RCNN), the state-of-art image segmentation method, and the PointRend algorithm was used to replace the original network head. Through an adaptive subdivision strategy, the boundary points were segmented with fine details. Features were extracted from the segmentation results and used as the input of the feed weight estimation network based on the Back Propagation (BP) algorithm. The model was applied in rabbit breeding to explore the relationship between RFW and the mortality probability of meat rabbits. The model evaluation results showed that the Average Precision (AP) value of the feed instance segmentation neural network was 0.987, the Mean Pixel Accuracy (MPA) value was 0.985. The correlation coefficient of the feed weight estimation network was 0.97, the Mean Squared Error (MSE) was 208.3, and the Mean Absolute Error (MAE) was 10.51 g. The practical application results showed that the feed intake of the unhealthy meat rabbits would decrease significantly. When the RFW was more than 50% of feed quantity, the mortality probability of the rabbit was more than 85%; when the RFW was more than 65% of feed quantity, all the rabbits finally died in a short time. Therefore, there is a significant correlation between RFW and the mortality probability of rabbits, by which this proposed model can help farms to monitor the health of meat rabbits by predicting RFW. © 2022, Chinese Society of Agricultural Engineering. All rights reserved.","Convolutional neural network; Deep learning; Health monitoring; Meat rabbit; Remaining feed; Weight estimation","","MARA, (CARS-43-D-2); Agriculture Research System of China","This work was financially supported by the China Agriculture Research System of MOF and MARA (Grant CARS-43-D-2).","H. Wang; College of Engineering, China Agricultural University, Beijing, 100083, China; email: hongyingw@cau.edu.cn","","Chinese Society of Agricultural Engineering","19346344","","","","English","Int. J. Agric. Biol. Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85125776735"
"Borges Oliveira D.A.; Ribeiro Pereira L.G.; Bresolin T.; Pontes Ferreira R.E.; Reboucas Dorea J.R.","Borges Oliveira, Dario Augusto (27567900100); Ribeiro Pereira, Luiz Gustavo (57211239891); Bresolin, Tiago (55618681200); Pontes Ferreira, Rafael Ehrich (57327777200); Reboucas Dorea, Joao Ricardo (57220899675)","27567900100; 57211239891; 55618681200; 57327777200; 57220899675","A review of deep learning algorithms for computer vision systems in livestock","2021","Livestock Science","253","","104700","","","","87","10.1016/j.livsci.2021.104700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118744270&doi=10.1016%2fj.livsci.2021.104700&partnerID=40&md5=df3b93be9bbafeea5db4e10159b5efc8","Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States; Embrapa Dairy Cattle, Av. Eugênio do Nascimento, 610 - Aeroporto, Juiz de Fora - MG, 36038-330, Brazil","Borges Oliveira D.A., Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States; Ribeiro Pereira L.G., Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States, Embrapa Dairy Cattle, Av. Eugênio do Nascimento, 610 - Aeroporto, Juiz de Fora - MG, 36038-330, Brazil; Bresolin T., Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States; Pontes Ferreira R.E., Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States; Reboucas Dorea J.R., Department of Animal and Dairy Sciences, 1675 Observatory Drive, 266 Animal Sciences Building, Madison, 53706-1205, WI, United States","In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions. © 2021 Elsevier B.V.","Artificial intelligence; Cattle; Machine learning; Precision farming; Swine","body weight; bovine; breeding; classification algorithm; computer vision; convolutional neural network; cow; deep learning; drinking behavior; feeding behavior; food intake; learning algorithm; livestock; nonhuman; pig; Review","National Institute of Food and Agriculture, NIFA, (2020-67015-30831, WIS03085); National Institute of Food and Agriculture, NIFA","The authors would like to acknowledge the Dairy Innovation Hub for providing the funding for D. A. B. Oliveira, and also thank the financial support from the USDA National Institute of Food and Agriculture (Washington, DC; grant 2020-67015-30831) and Hatch project (WIS03085). ","J.R. Reboucas Dorea; Department of Animal and Dairy Sciences, Madison, 1675 Observatory Drive, 266 Animal Sciences Building, 53706-1205, United States; email: joao.dorea@wisc.edu","","Elsevier B.V.","18711413","","","","English","Livest. Sci.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85118744270"
"Wang L.; Diakogiannis F.; Mills S.; Bajema N.; Atkinson I.; Bishop-Hurley G.J.; Charmley E.","Wang, Liang (57267647600); Diakogiannis, Foivos (55978861500); Mills, Scott (57202448352); Bajema, Nigel (8599774600); Atkinson, Ian (25941109900); Bishop-Hurley, Greg J. (6602321037); Charmley, Ed (55962835500)","57267647600; 55978861500; 57202448352; 8599774600; 25941109900; 6602321037; 55962835500","A noise robust automatic radiolocation animal tracking system","2021","Animal Biotelemetry","9","1","26","","","","2","10.1186/s40317-021-00248-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111699333&doi=10.1186%2fs40317-021-00248-w&partnerID=40&md5=df564c5d8560548d2e6d6c1e9ca138a0","Agriculture and Food, CSIRO, Townsville, 4814, Australia; Agriculture and Food, CSIRO, Brisbane, 4067, Australia; ICRAR, The University of Western Australia, Perth, 6009, Australia; Data61, CSIRO, Perth, 6014, Australia; eResearch Centre, James Cook University, Townsville, 4814, Australia","Wang L., Agriculture and Food, CSIRO, Townsville, 4814, Australia, Agriculture and Food, CSIRO, Brisbane, 4067, Australia; Diakogiannis F., ICRAR, The University of Western Australia, Perth, 6009, Australia, Data61, CSIRO, Perth, 6014, Australia; Mills S., eResearch Centre, James Cook University, Townsville, 4814, Australia; Bajema N., eResearch Centre, James Cook University, Townsville, 4814, Australia; Atkinson I., eResearch Centre, James Cook University, Townsville, 4814, Australia; Bishop-Hurley G.J., Agriculture and Food, CSIRO, Brisbane, 4067, Australia; Charmley E., Agriculture and Food, CSIRO, Townsville, 4814, Australia","Agriculture is becoming increasingly reliant upon accurate data from sensor arrays, with localization an emerging application in the livestock industry. Ground-based time difference of arrival (TDoA) radio location methods have the advantage of being lightweight and exhibit higher energy efficiency than methods reliant upon Global Navigation Satellite Systems (GNSS). Such methods can employ small primary battery cells, rather than rechargeable cells, and still deliver a multi-year deployment. In this paper, we present a novel deep learning algorithm adapted from a one-dimensional U-Net implementing a convolutional neural network (CNN) model, originally developed for the task of semantic segmentation. The presented model (ResUnet-1d) both converts TDoA sequences directly to positions and reduces positional errors introduced by sources such as multipathing. We have evaluated the model using simulated animal movements in the form of TDoA position sequences in combination with real-world distributions of TDoA error. These animal tracks were simulated at various step intervals to mimic potential TDoA transmission intervals. We compare ResUnet-1d to a Kalman filter to evaluate the performance of our algorithm to a more traditional noise reduction approach. On average, for simulated tracks having added noise with a standard deviation of 50 m, the described approach was able to reduce localization error by between 66.3% and 73.6%. The Kalman filter only achieved a reduction of between 8.0% and 22.5%. For a scenario with larger added noise having a standard deviation of 100 m, the described approach was able to reduce average localization error by between 76.2% and 81.9%. The Kalman filter only achieved a reduction of between 31.0% and 39.1%. Results indicate that this novel 1D CNN U-Net like encoder/decoder for TDoA location error correction outperforms the Kalman filter. It is able to reduce average localization errors to between 16 and 34 m across all simulated experimental treatments while the uncorrected average TDoA error ranged from 55 to 188 m. © 2021, The Author(s).","Convolutional neural network; Encoder/decoder; Machine learning; Radiolocation","","Scientific Computing team of CSIRO","The authors acknowledge the support of the Scientific Computing team of CSIRO. We would also like to acknowledge Gordon Foyster and Richard Keaney for their useful discussions and comments on this manuscript. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.","L. Wang; Agriculture and Food, CSIRO, Brisbane, 4067, Australia; email: l.wang@csiro.au","","BioMed Central Ltd","20503385","","","","English","Anim. Biotelem.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85111699333"
"Rajesh; Dhankhar A.; Solanki K.","Rajesh (58027920700); Dhankhar, Amita (57191096430); Solanki, Kamna (57188767624)","58027920700; 57191096430; 57188767624","Simulating Optimized Stock Price Prediction Using Deep Learning Mechanism","2022","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2022","","","","","","","0","10.1109/ICRITO56286.2022.9964748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144598991&doi=10.1109%2fICRITO56286.2022.9964748&partnerID=40&md5=bf33b85502c3725ddef7b1caf87ae850","University Insititute of Engineering & Technology, Mdu, Rohtak, India","Rajesh, University Insititute of Engineering & Technology, Mdu, Rohtak, India; Dhankhar A., University Insititute of Engineering & Technology, Mdu, Rohtak, India; Solanki K., University Insititute of Engineering & Technology, Mdu, Rohtak, India","Investors seeking passive income are driving the stock market's rapid expansion. In order to improve stock market predictions, this paper proposes a novel artificial recurrent neural network technique. An increasing number of traders and investors are focusing on the stocks. Investors and investing businesses have long placed a premium on identifying patterns in the stock market's volatility. The study of predicting stock prices is attracting a lot of attention from investors. Many people who desire to invest money in the stock market are interested in predicting its future performance. Livestock market data is collected, analyzed, and visualized, allowing for both online and offline study. The goal of the study is to increase stock price forecast accuracy. The use of optimization techniques like PSO and MVO has been taken into consideration to increase accuracy. There have been multiple studies that looked at using deep learning to forecast stock prices, but those studies came up short in terms of accuracy. Therefore, the suggested study focuses on improving accuracy by combining an optimization mechanism with a deep learning technique. PSO and MVO approaches to optimization were taken into consideration. The accuracy parameters for the optimized dataset and the non-optimized dataset are compared in simulation work. © 2022 IEEE.","Accuracy; Deep Learning; Inflation; Prediction; Reinforcement Learning; Stock Market","Agriculture; Commerce; Costs; Financial markets; Investments; Learning systems; Particle swarm optimization (PSO); Recurrent neural networks; Reinforcement learning; Accuracy; Deep learning; Inflation; Learning mechanism; Optimisations; Optimized datasets; Rapid expansion; Reinforcement learnings; Stock price; Stock price prediction; Forecasting","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166547433-7","","","English","Int. Conf. Reliability, Infocom Technol. Optim. (Trends Future Dir.), ICRITO","Conference paper","Final","","Scopus","2-s2.0-85144598991"
"Feng L.Q.; Wei L.J.; Hong M.W.; Hua G.R.; Gen Y.L.; Yu D.L.; Yang Y.Q.","Feng, Li Qi (57205964175); Wei, Li Jia (57224778087); Hong, Ma Wei (57224770096); Hua, Gao Rong (57224782465); Gen, Yu Li (57224776926); Yu, Ding Lu (57224767101); Yang, Yu Qin (57224781658)","57205964175; 57224778087; 57224770096; 57224782465; 57224776926; 57224767101; 57224781658","Research progress of intelligent sensing technology for diagnosis of livestock and poultry diseases","2021","Scientia Agricultura Sinica","54","11","","2445","2463","18","9","10.3864/j.issn.0578-1752.2021.11.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108270366&doi=10.3864%2fj.issn.0578-1752.2021.11.016&partnerID=40&md5=dec88a82b7541107aaddf9146db59435","Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China","Feng L.Q., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Wei L.J., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Hong M.W., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Hua G.R., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Gen Y.L., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Yu D.L., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Yang Y.Q., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China","Animal husbandry is an important part of agriculture. At present, animal husbandry is developing towards large-scale and intensive development, which also increases the difficulty of diagnosis of livestock and poultry diseases. In recent years, in order to improve the level of animal welfare in livestock and poultry breeding, and to reduce the economic losses and public health safety risks caused by animal diseases and health abnormalities in livestock breeding, a number of automated methods for the diagnosis and treatment of livestock and poultry diseases through digital and intelligent means have emerged, such as machine vision analysis, animal audio analysis, infrared temperature perception, deep learning classification, etc. These methods could effectively improve the diagnosis efficiency of diseased or abnormal livestock and poultry, shorten the diagnosis cycle, and reduce the labor force of manual inspection in animal husbandry. The automatic diagnosis and treatment method of livestock and poultry diseases is different from the conventional diagnosis methods based on pathological knowledge, which mainly uses various sensors to automatically obtain various characteristics information of livestock and poultry during the breeding process, such as images, sounds, body temperature, heart rate, and excrement. The collected information is comprehensively analyzed and processed through mathematical models, such as Mel cepstrum coefficient, Logistics regression analysis and intelligent algorithms such as support vector machines and deep learning, and then the animal s health status is evaluated and predicted. The current research progress of animal disease intelligent diagnosis technology and some basic method principles was summarized from several aspects, such as livestock and poultry morphological diagnosis technology, behavior diagnosis technology, sound diagnosis technology, body temperature diagnosis technology, and other physiological parameter diagnosis technology. Those methods were based on the digital characteristics of animal appearance and body size, behavior and movement, call and sound, body temperature, excrement, respiration and heart rate, the characteristics collected by the sensor, which were analyzed and classified in real time through mathematical models, and the analysis was basically achieved. The current research results on automatic diagnosis and treatment of livestock and poultry diseases were abundant, but most of the related diagnosis methods were carried out in an ideal environment. However, the interference factors in the actual production and breeding environment were very large, and the most of the current diagnostic methods could not eliminate the interference well and accurately extract the required characteristic information. Besides, the current digital livestock disease diagnosis methods were mostly based on the analysis and diagnosis of one kind of livestock feature information, which affected the diagnosis accuracy of the diagnosis system and the diagnosis results were not convincing. At the same time, the most of the current digital diagnosis methods for poultry and livestock diseases still had some problems such as poor diagnosis generalization ability and poor anti-interference ability, which restricted their promotion and application. The focus of future research on automatic diagnosis of livestock and poultry diseases is to improve the accuracy of its sensing algorithms and the applicability and robustness of mathematical models, and to develop an intelligent diagnosis and treatment expert system for livestock and poultry diseases based on multiple feature coupling and data fusion, realize real-Time, efficient, intelligent and accurate livestock and poultry health diagnosis. © 2021 Editorial Department of Scientia Agricultura Sinica. All rights reserved.","behavioral diagnosis; disease intelligent diagnosis for livestock and poultry; physiological diagnosis; sensor monitoring for livestock and poultry","animal husbandry; disease control; livestock farming; physiological response; poultry; regression analysis","","","M.W. Hong; Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; email: mawh@nercita.org.cn","","Editorial Department of Scientia Agricultura Sinica","05781752","","","","Chinese","Sci. Agricultura Sin.","Article","Final","","Scopus","2-s2.0-85108270366"
"Ma R.; Park J.-S.; Kim S.H.; Kim S.-C.","Ma, Ruihan (57580081500); Park, Jin-Seong (57377244000); Kim, Sung Hoon (57579252300); Kim, Sang-Cheol (55561975800)","57580081500; 57377244000; 57579252300; 55561975800","Application of a Deep Learning-Based Instance Segmentation Model for Behavior Classification of Pigs","2022","Journal of Institute of Control, Robotics and Systems","28","4","","326","333","7","0","10.5302/J.ICROS.2022.22.8001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128389399&doi=10.5302%2fJ.ICROS.2022.22.8001&partnerID=40&md5=25c568c23b398c4bd40f3f8185745363","Intellignet Robots Research Center, Jeonbuk National University, South Korea; Division of Electronics and Information Engineering, Jeonbuk National University, South Korea","Ma R., Intellignet Robots Research Center, Jeonbuk National University, South Korea; Park J.-S., Division of Electronics and Information Engineering, Jeonbuk National University, South Korea; Kim S.H., Division of Electronics and Information Engineering, Jeonbuk National University, South Korea; Kim S.-C., Intellignet Robots Research Center, Jeonbuk National University, South Korea","In pig-breeding livestock farms, increasing yearly sow productivity and reducing piglet mortality are very important for farmers because they are directly related to profitability. To meet these requirements of the livestock industry, smart livestock technology is being developed and propagated. Recently, many studies have been conducted to obtain information on the health and physiological status of livestock by applying image processing-and artificial intelligence-based technology. In particular, because changes in pig behavior patterns can provide considerable information about their health and physiological status, this study proposes a method to classify pig behavior patterns from images of a pig kennel by applying a deep learning-based instance segmentation model. For this purpose, we first constructed a Pig Motion dataset by building a robot operation system (ROS)-based data collection system that could synchronize and collect voice and climate information along with pig motion images in time. This dataset was organized by classifying four types of pig behavior, namely, lying, standing, sitting, and eating, into motion classes. By learning this dataset using the instance segmentation model, an object is extracted from the pig’s motion characteristic information to create a model that divides and classifies it. We propose a behavioral pattern classification model that can be used to estimate the health and physiological state of pigs by classifying their behavioral patterns from streaming videos and accumulated data and statisticalizing them.. © ICROS 2022.","Deep-learning; Instance segmentation; Motion classification; Pig behavior; Smart livestock","Classification (of information); Deep learning; Health; Mammals; Motion analysis; Physiological models; Behavioral patterns; Behaviour classification; Behaviour patterns; Deep-learning; Health status; Motion classification; Physiological status; Pig behavior; Segmentation models; Smart livestock; Agriculture","","","S.-C. Kim; Intellignet Robots Research Center, Jeonbuk National University, South Korea; email: sckim7777@jbnu.ac.kr","","Institute of Control, Robotics and Systems","19765622","","","","English","J. Inst. Control Rob. Syst.","Article","Final","","Scopus","2-s2.0-85128389399"
"Pallavi C.; Usha S.","Pallavi, C. (58133930100); Usha, S. (36549121400)","58133930100; 36549121400","IoT Based Site Specific Nutrient Management System for Soil Health Monitoring","2022","Proceedings - 2022 International Conference on Smart and Sustainable Technologies in Energy and Power Sectors, SSTEPS 2022","","","","166","170","4","3","10.1109/SSTEPS57475.2022.00050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161434927&doi=10.1109%2fSSTEPS57475.2022.00050&partnerID=40&md5=01a3aa9651cb0360e5e0c925a21fe29e","BNMIT, Dept of CSE, Bangalore, India; RRCE, Dept of CSE, Bangalore, India","Pallavi C., BNMIT, Dept of CSE, Bangalore, India; Usha S., RRCE, Dept of CSE, Bangalore, India","Agriculture is the art and science of cultivating the soil, growing crops, and raising livestock, which is the major backbone of India. Automation is known as the revolution in technology and has an enormous positive impact across domains regions. Automation's foremost objective is to increase production. Exponential growth in the population of the country demands the need to have exponential growth in the food production. Question arises what governs the increased food productionƒ One of the answers is adequate availability of nutrient contents in the soil is the governing factor that affects the food production which needs to be monitored regularly. The Exponential growth in the food production can only be achieved through Agriculture Automation. Agriculture automation can be achieved by adopting multiple technologies that are available like wireless communication, AI and ML, deep learning, big data analytics, and IoT. etc. Maintaining adequate availability of nutrient contents, that are site crop specific in the soil, using Soil Health Monitoring [SHM] system becomes the major challenge that needs to be addressed using the Automation technologies. This article discusses various SSNM [Site Specific Nutrient Management] systems available, possibility of integration with Automation technologies that in turn will aid in SHM by proposing sitecrop specific fertilizer timely.  © 2022 IEEE.","Agriculture; Automation; IoT; sensors; SHM; Soil; SSNM","Crops; Cultivation; Data Analytics; Deep learning; Internet of things; Nutrients; Population statistics; Soils; Automation technology; Exponential growth; Food production; Health monitoring; IoT; Management systems; Nutrient contents; Site-specific nutrient managements; Soil health; Soil health monitoring; Automation","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166546414-7","","","English","Proc. - Int. Conf. Smart Sustain. Technol. Energy Power Sect., SSTEPS","Conference paper","Final","","Scopus","2-s2.0-85161434927"
"Bonicelli L.; Trachtman A.R.; Rosamilia A.; Liuzzo G.; Hattab J.; Alcaraz E.M.; Del Negro E.; Vincenzi S.; Dondona A.C.; Calderara S.; Marruchella G.","Bonicelli, Lorenzo (57238864800); Trachtman, Abigail Rose (57209712272); Rosamilia, Alfonso (55614373600); Liuzzo, Gaetano (7003774021); Hattab, Jasmine (57218339671); Alcaraz, Elena Mira (57338370000); Del Negro, Ercole (57208837362); Vincenzi, Stefano (57216824271); Dondona, Andrea Capobianco (57219585063); Calderara, Simone (23099524400); Marruchella, Giuseppe (8638634100)","57238864800; 57209712272; 55614373600; 7003774021; 57218339671; 57338370000; 57208837362; 57216824271; 57219585063; 23099524400; 8638634100","Training convolutional neural networks to score pneumonia in slaughtered pigs","2021","Animals","11","11","3290","","","","14","10.3390/ani11113290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119058965&doi=10.3390%2fani11113290&partnerID=40&md5=5eb5132b6f7607c3ffc1f0ed33b039ba","AImageLab, University of Modena and Reggio Emilia, Via Vivarelli 10/1, Modena, 41125, Italy; Faculty of Veterinary Medicine, University of Teramo, Loc. Piano d’Accio, Teramo, 64100, Italy; Department of Veterinary Public Health, Azienda Unità Sanitaria Locale di Modena, via S. Giovanni del Cantone 23, Modena, 41121, Italy; Farm4Trades.r.l., Via IV Novembre, Atessa, 66041, Italy","Bonicelli L., AImageLab, University of Modena and Reggio Emilia, Via Vivarelli 10/1, Modena, 41125, Italy; Trachtman A.R., Faculty of Veterinary Medicine, University of Teramo, Loc. Piano d’Accio, Teramo, 64100, Italy; Rosamilia A., Department of Veterinary Public Health, Azienda Unità Sanitaria Locale di Modena, via S. Giovanni del Cantone 23, Modena, 41121, Italy; Liuzzo G., Department of Veterinary Public Health, Azienda Unità Sanitaria Locale di Modena, via S. Giovanni del Cantone 23, Modena, 41121, Italy; Hattab J., Faculty of Veterinary Medicine, University of Teramo, Loc. Piano d’Accio, Teramo, 64100, Italy; Alcaraz E.M., Faculty of Veterinary Medicine, University of Teramo, Loc. Piano d’Accio, Teramo, 64100, Italy; Del Negro E., AImageLab, University of Modena and Reggio Emilia, Via Vivarelli 10/1, Modena, 41125, Italy, Farm4Trades.r.l., Via IV Novembre, Atessa, 66041, Italy; Vincenzi S., Farm4Trades.r.l., Via IV Novembre, Atessa, 66041, Italy; Dondona A.C., Farm4Trades.r.l., Via IV Novembre, Atessa, 66041, Italy; Calderara S., AImageLab, University of Modena and Reggio Emilia, Via Vivarelli 10/1, Modena, 41125, Italy; Marruchella G., Faculty of Veterinary Medicine, University of Teramo, Loc. Piano d’Accio, Teramo, 64100, Italy","The slaughterhouse can act as a valid checkpoint to estimate the prevalence and the economic impact of diseases in farm animals. At present, scoring lesions is a challenging and time‐consuming activity, which is carried out by veterinarians serving the slaughter chain. Over recent years, artificial intelligence(AI) has gained traction in many fields of research, including livestock production. In particular, AI‐based methods appear able to solve highly repetitive tasks and to consistently analyze large amounts of data, such as those collected by veterinarians during postmortem inspection in high‐throughput slaughterhouses. The present study aims to develop an AI‐based method capable of recognizing and quantifying enzootic pneumonia‐like lesions on digital images captured from slaughtered pigs under routine abattoir conditions. Overall, the data indicate that the AI‐based method proposed herein could properly identify and score enzootic pneumonia‐like lesions without interfering with the slaughter chain routine. According to European legislation, the application of such a method avoids the handling of carcasses and organs, decreasing the risk of microbial contamination, and could provide further alternatives in the field of food hygiene. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Convolutional neural networks; Deep learning; Pig; Pneumonia; Scoring methods; Slaughterhouse","animal experiment; Article; artificial intelligence; autopsy; convolutional neural network; deep learning; enzootic pneumonia; image segmentation; livestock; microbial contamination; nonhuman; pig; pneumonia; prevalence; slaughterhouse; training","Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR","Funding: The present study has been carried out in the framework of the Project “Demetra” (Dipartimenti di Eccellenza 2018–2022, CUP_C46C18000530001), funded by the Italian Ministry for Education, University and Research.","G. Marruchella; Faculty of Veterinary Medicine, University of Teramo, Teramo, Loc. Piano d’Accio, 64100, Italy; email: gmarruchella@unite.it","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85119058965"
"Psiroukis V.; Malounas I.; Mylonas N.; Grivakis K.-E.; Fountas S.; Hadjigeorgiou I.","Psiroukis, Vasilis (57202829261); Malounas, Ioannis (57216763450); Mylonas, Nikolaos (57215033015); Grivakis, Konstantinos-Elenos (58125463600); Fountas, Spyros (12753870500); Hadjigeorgiou, Ioannis (6506418610)","57202829261; 57216763450; 57215033015; 58125463600; 12753870500; 6506418610","Monitoring of free-range rabbits using aerial thermal imaging","2021","Smart Agricultural Technology","1","","100002","","","","10","10.1016/j.atech.2021.100002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135958460&doi=10.1016%2fj.atech.2021.100002&partnerID=40&md5=065c86126bb348b07bbf23ae6093b1fc","Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece","Psiroukis V., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Malounas I., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Mylonas N., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Grivakis K.-E., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Fountas S., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece; Hadjigeorgiou I., Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Iera Odos 75, Athens, 11855, Greece","Unmanned Aerial Vehicles (UAV) imagery is a mature technology, which has found use in a number of applications in agriculture and environmental sciences. However, its application for monitoring and classification of livestock and wild animals has not yet been developed. This study presents a robust methodology to count wild and free-range rabbits and monitor their population. The aims of this study were to 1) test the capacity of the methodology in counting small nocturnal animals such as rabbits in the field, 2) assess the rabbit's density at different sites and different periods of the year and 3) record the temporal pattern of rabbits’ activity during the night hours, with the overall aim to provide a reliable and accurate tool in management studies. For this purpose, a UAV equipped with a thermal camera was used to perform night flights on the island of Lemnos, scanning selected sites and collecting aerial nadir thermal imagery data of the ground. The derived thermal images were analysed using deep learning techniques towards counting the individual animals in each image and the results were compared with manual counting conducted by a researcher. The results revealed that the deep learning approach for automated counting and rabbit recognition overall achieved comparable results to physical counting, with the final model yielding an F1-score of 0.87. However, there were differences between seasons in the methods’ accuracy. This method could be a helpful tool in assessing populations of small nocturnal animals and other free-range livestock animals. © 2021 The Authors","Animal_detection; Animal_population; Deep_learning; Free-range_rabbits; Thermal_images; UAV","","","","V. Psiroukis; Department of Natural Resources Management & Agricultural Engineering, Agricultural University of Athens, Athens, Iera Odos 75, 11855, Greece; email: vpsiroukis@aua.gr","","Elsevier B.V.","27723755","","","","English","Smart Agric. Technol.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135958460"
"Evangelista I.R.S.; Concepcion R.; Palconit M.G.B.; Bandala A.A.; Dadios E.P.","Evangelista, Ivan Roy S. (57207911065); Concepcion, Ronnie (57208041010); Palconit, Maria Gemel B. (57202289323); Bandala, Argel A. (55599317400); Dadios, Elmer P. (6602629924)","57207911065; 57208041010; 57202289323; 55599317400; 6602629924","YOLOv7 and DeepSORT for Intelligent Quail Behavioral Activities Monitoring","2022","2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management, HNICEM 2022","","","","","","","2","10.1109/HNICEM57413.2022.10109608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159398912&doi=10.1109%2fHNICEM57413.2022.10109608&partnerID=40&md5=3507b9d9d35b43963d400adefa28ad7a","De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines","Evangelista I.R.S., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Concepcion R., De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines; Palconit M.G.B., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Bandala A.A., De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; Dadios E.P., De la Salle University, Department of Manufacturing and Management Engineering, Manila, Philippines","The use of modern technology, such as Artificial Intelligence (AI), for livestock farming applications continue to grow. Computer Vision (CV) enables remote, real-time, and noninvasive observation and monitoring of animals by employing detection and tracking algorithms. The integration of Deep Learning (DL) enhances the ability of computers to analyze, interpret and infer the information received. In this study, an Intelligent quail activity monitoring system (I-QAMS) is introduced. It provides detection and monitoring of quail feeding pattern and locomotion activities reared in a cage. In addition, it estimates the activeness based on the level of agility of the poultry animal. These parameters can be used as indicators to assess animal health. YOLOv7, a DL-based single stage detector, is employed for detection and classification of behavioral activities. The DeepSORT algorithm, an algorithm utilizing Kalman filter, Hungarian Algorithm, and Convolutional Neural Network (CCN), is employed for tracking. To evaluate the bird's movement, the displacement of the centroid is determined using Euclidean distance. A 500-frame video sequence, approximately 20 seconds in length, is utilized for the agility assessment. The YOLOv7 achieved a mAP of 85.28 in training, and an accuracy of 90.49 on test scenes. The results show the potential of CV- and DL-based detectors and trackers such as YOLO and DeepSORT for poultry behavior monitoring and estimation of the animal's welfare. © 2022 IEEE.","computer vision; deep learning; deepSORT; object tracking; precision livestock farming; quail farming; YOLOv7","Animals; Convolutional neural networks; Deep learning; Farms; Tracking (position); Activity monitoring; Deep learning; Deepsort; Livestock farming; Modern technologies; Object Tracking; Precision livestock farming; Quail farming; Real- time; YOLOv7; Computer vision","Intelligent System Laboratory; Department of Science and Technology, Philippines, DOST; Department of Science and Technology, Republic of the Philippines, DOST","The authors are grateful for the support received from the Intelligent System Laboratory and the Department of Science and Technology - Engineering Research and Developmnt for Technology (DOST-ERDT) of the Philippines.","I.R.S. Evangelista; De la Salle University, Department of Electronics and Computer Engineering, Manila, Philippines; email: ivan_roy_evangelista@dlsu.edu.ph","","Institute of Electrical and Electronics Engineers Inc.","","978-166546493-2","","","English","IEEE Int. Conf. Humanoid, Nanotechnol., Inf. Technol., Commun. Control, Environ., Manag., HNICEM","Conference paper","Final","","Scopus","2-s2.0-85159398912"
"Qiao Y.; Guo Y.; He D.; Chai L.","Qiao, Yongliang (56486770900); Guo, Yangyang (57200132879); He, Dongjian (19933691800); Chai, Lilong (57222280822)","56486770900; 57200132879; 19933691800; 57222280822","Deep Learning-based Autonomous cow body detection for smart livestock farming","2022","2022 ASABE Annual International Meeting","","","","","","","1","10.13031/aim.202200120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137550046&doi=10.13031%2faim.202200120&partnerID=40&md5=b44ade7e1fef1ec5471fe0fdcdacb6e5","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Department of Poultry Science, College of Agricultural and Environmental Sciences, University of Georgia, Athens, 30602, GA, United States","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Guo Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; He D., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; Chai L., Department of Poultry Science, College of Agricultural and Environmental Sciences, University of Georgia, Athens, 30602, GA, United States","Autonomous detection cow and its key body parts is of significance to precision livestock farming. The advancement in sensor technology, deep learning and field robotics have paved the way for farm management. In this study, a deep learning network named YOLOv4-CSP was used, to achieve the detection of key parts of dairy cows in complex scenes. In order to verify the effectiveness of the algorithm, a challenging cow dataset consisting of adult and calf with complex environments (e.g., day and night) was constructed for experimental testing. The proposed YOLOv4-CSP based approach achieved a precision of 98.00%, a recall of 99.00%, an F1 score of 99.00%, and an mAP@0.5 of 96.86%. Experimental results demonstrated that the proposed YOLOv4-CSP approach could capture key biometric-related features for cow visual representation, improving the performance of cow detection. Overall, the proposed deep learning-based cow detection approach is favorable for long-term autonomous cow monitoring and management in smart livestock farming. © 2022 ASABE. All Rights Reserved.","Autonomous detection; Deep learning; Precision livestock farming; YOLOv4","Agriculture; Complex networks; Learning systems; Statistical tests; Autonomous detection; Body parts; Deep learning; Farm management; Field robotics; Learning network; Livestock farming; Precision livestock farming; Sensor technologies; YOLOv4; Deep learning","National Natural Science Foundation of China, NSFC, (61473235); National Key Research and Development Program of China, NKRDPC, (2017YFD0701603)","The authors particular thanks to other members of the team for their involvement and efforts in the whole experiment organization and information collection. The authors also acknowledge the support by the project: National Natural Science Foundation of China (grant number 61473235) and the National Key Technology R&D Program of China (grant number 2017YFD0701603).","Y. Guo; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; email: guoyangyang_gyy@163.com","","American Society of Agricultural and Biological Engineers","","","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85137550046"
"Neethirajan S.","Neethirajan, Suresh (57217318680)","57217318680","Automated Tracking Systems for the Assessment of Farmed Poultry","2022","Animals","12","3","232","","","","25","10.3390/ani12030232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122871022&doi=10.3390%2fani12030232&partnerID=40&md5=22aa22818d5d95ccb6422fe015ac1cf2","Farmworx, Adaptation Physiology Group, Department of Animal Sciences, Wageningen University & Research, Wageningen, 6700 AH, Netherlands","Neethirajan S., Farmworx, Adaptation Physiology Group, Department of Animal Sciences, Wageningen University & Research, Wageningen, 6700 AH, Netherlands","The world’s growing population is highly dependent on animal agriculture. Animal products provide nutrient-packed meals that help to sustain individuals of all ages in communities across the globe. As the human demand for animal proteins grows, the agricultural industry must continue to advance its efficiency and quality of production. One of the most commonly farmed livestock is poultry and their significance is felt on a global scale. Current poultry farming practices result in the premature death and rejection of billions of chickens on an annual basis before they are processed for meat. This loss of life is concerning regarding animal welfare, agricultural efficiency, and economic impacts. The best way to prevent these losses is through the individualistic and/or group level assessment of animals on a continuous basis. On large-scale farms, such attention to detail was generally considered to be inaccurate and inefficient, but with the integration of artificial intelligence (AI)-assisted technology individualised, and per-herd assessments of livestock became possible and accurate. Various studies have shown that cameras linked with specialised systems of AI can properly analyse flocks for health concerns, thus improving the survival rate and product quality of farmed poultry. Building on recent advancements, this review explores the aspects of AI in the detection, counting, and tracking of poultry in commercial and research-based applications. © 2022 by the author. Licensee MDPI, Basel, Switzerland.","Deep learning; Poultry behaviour; Poultry production systems; Precision livestock farming; Target tracking","animal welfare; artificial intelligence; bird disease; broiler; chicken; computer vision; data mining; deep learning; fracture; image analysis; livestock; machine learning; nonhuman; optic flow; poultry; product quality; Review; stocking density; survival rate","","","S. Neethirajan; Farmworx, Adaptation Physiology Group, Department of Animal Sciences, Wageningen University & Research, Wageningen, 6700 AH, Netherlands; email: suresh.neethirajan@wur.nl","","MDPI","20762615","","","","English","Animals","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122871022"
"Toro A.P.S.G.D.; Werner J.P.S.; Dos Reis A.A.; Esquerdo J.C.D.M.; Antunes J.F.G.; Coutinho A.C.; Lamparelli R.A.C.; Magalhães P.S.G.; Figueiredo G.K.D.A.","Toro, A.P.S.G.D. (57741497200); Werner, J.P.S. (57212420665); Dos Reis, A.A. (55320336300); Esquerdo, J.C.D.M. (15020462500); Antunes, J.F.G. (15020172200); Coutinho, A.C. (55513012600); Lamparelli, R.A.C. (6602713722); Magalhães, P.S.G. (7003731076); Figueiredo, G.K.D.A. (57185295400)","57741497200; 57212420665; 55320336300; 15020462500; 15020172200; 55513012600; 6602713722; 7003731076; 57185295400","EVALUATION OF EARLY SEASON MAPPING OF INTEGRATED CROP LIVESTOCK SYSTEMS USING SENTINEL-2 DATA","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","1335","1340","5","0","10.5194/isprs-archives-XLIII-B3-2022-1335-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131953005&doi=10.5194%2fisprs-archives-XLIII-B3-2022-1335-2022&partnerID=40&md5=eaaef739b4dc8d2b3c00d8f0ba19afbd","School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil; Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation - Embrapa, SP, Campinas, 13083-886, Brazil; Interdisciplinary Center of Energy Planning - Nipe, University of Campinas SP, Brazil","Toro A.P.S.G.D., School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil; Werner J.P.S., School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil; Dos Reis A.A., School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil, Interdisciplinary Center of Energy Planning - Nipe, University of Campinas SP, Brazil; Esquerdo J.C.D.M., School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil, Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation - Embrapa, SP, Campinas, 13083-886, Brazil; Antunes J.F.G., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation - Embrapa, SP, Campinas, 13083-886, Brazil; Coutinho A.C., Embrapa Digital Agriculture, Brazilian Agricultural Research Corporation - Embrapa, SP, Campinas, 13083-886, Brazil; Lamparelli R.A.C., Interdisciplinary Center of Energy Planning - Nipe, University of Campinas SP, Brazil; Magalhães P.S.G., Interdisciplinary Center of Energy Planning - Nipe, University of Campinas SP, Brazil; Figueiredo G.K.D.A., School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, SP, Campinas, 13083-875, Brazil","Various approaches were developed considering the need to increase agricultural productivity in cultivated areas without more deforestation, such as the Integrated Crop livestock systems (ICLS). The ICLS could be composed of annual crops followed by pastureland with the presence of cattle. Due to the high temporal dynamic of rotation between crops over the season, monitoring these areas is a big challenge. Also, agricultural organizations worldwide highlight the need for early-season maps for this kind of work. In this context, this study evaluated the potential of open data (Sentinel-2) data to map ICLS areas. The performance of two classifiers was evaluated: one of Machine Learning (random forest) and the other of Deep Learning (LSTM). Three different time windows of data were tested (Entire season, 180 days, and 120 days). Using the RF classifier, it was possible to achieve satisfactory results (Overall accuracy higher than 80%) for the early season (180 days). However, further studies are needed to explain better the lower(when compared to Random Forest) accuracy achieved by LSTM net (0.79 % for 180 days) and compare the results achieved here with results for a study area with different rates of cloud cover.  © Authors 2022","crop identification; deep learning; LSTM; random forest; Regenerative agriculture","Decision trees; Deforestation; Long short-term memory; Open Data; Productivity; Random forests; Agricultural productivity; Crop identification; Deep learning; Livestock systems; LSTM; Open datum; Random forests; Regenerative agriculture; Season maps; Temporal dynamics; Crops","","","A.P.S.G.D. Toro; School of Agricultural Engineering - Feagri, University of Campinas - Unicamp, Campinas, SP, 13083-875, Brazil; email: a265622@dac.unicamp.br","Jiang J.; Shaker A.; Zhang Z.","International Society for Photogrammetry and Remote Sensing","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131953005"
"Mahmud M.S.; Zahid A.; Das A.K.; Muzammil M.; Khan M.U.","Mahmud, Md Sultan (57225945756); Zahid, Azlan (57210903697); Das, Anup Kumar (57225978446); Muzammil, Muhammad (57194527271); Khan, Muhammad Usman (58263369700)","57225945756; 57210903697; 57225978446; 57194527271; 58263369700","A systematic literature review on deep learning applications for precision cattle farming","2021","Computers and Electronics in Agriculture","187","","106313","","","","79","10.1016/j.compag.2021.106313","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109849018&doi=10.1016%2fj.compag.2021.106313&partnerID=40&md5=c40eab5482d16f4dad3e7928380c4516","Department of Agricultural and Biological Engineering, The Pennsylvania State University, University Park, 16802, PA, United States; Department of Agricultural and Biosystems Engineering, North Dakota State University, Fargo, 58102, ND, United States; Institute for Landscape Ecology and Resources Management, Justus Liebig University, Giessen, 35392, Germany; Department of Energy Systems Engineering, University of Agriculture, Faisalabad, 38000, Pakistan","Mahmud M.S., Department of Agricultural and Biological Engineering, The Pennsylvania State University, University Park, 16802, PA, United States; Zahid A., Department of Agricultural and Biological Engineering, The Pennsylvania State University, University Park, 16802, PA, United States; Das A.K., Department of Agricultural and Biosystems Engineering, North Dakota State University, Fargo, 58102, ND, United States; Muzammil M., Institute for Landscape Ecology and Resources Management, Justus Liebig University, Giessen, 35392, Germany; Khan M.U., Department of Energy Systems Engineering, University of Agriculture, Faisalabad, 38000, Pakistan","In animal agriculture, deep learning-based approaches have been widely implemented as a decision support tool for precision farming. Several deep learning models have been applied to solve diverse problems related to cattle health and identification. However, an overview of the state-of-the-art of deep learning in precision cattle farming is needed, for which we performed a systematic literature review (SLR). This study aims to provide an overview of the recent progress in deep learning applications for precision cattle farming, in particular health and identification. In the initial search, we retrieved 678 studies from different electronic databases. Only 56 studies qualify the selection criteria, which were then analyzed to extract the data to answer the research questions. The two major applications of deep learning for cattle farming were identified: identification and health monitoring. About 58% of the selected studies are dedicated to cattle identification and the rest for health monitoring. We identified 20 deep learning models that were used to solve different problems, and Convolutional Neural Networks (CNNs) is the most adopted model than others, including Long Short-Term Memory (LSTM), Mask-Region Based Convolutional Neural Networks (Mask-RCNN), and Faster-RCNN. We identified 19 training networks and of which ResNet is by far the most used. From our selection, 12 model evaluation parameters were determined, of which seven were used more than five times. The challenges most encountered with image quality, data processing speed, dataset size, redundant information, and motion of the cattle during data acquisition. In closing, we consider that this SLR study will pave the way for future research towards developing automatic systems for cattle farming. © 2021 Elsevier B.V.","Artificial intelligence; Cattle behavior; Cattle detection; Cattle health; Cattle identification; Digital agriculture; Precision livestock farming","Agriculture; Convolution; Data acquisition; Data handling; Decision support systems; Long short-term memory; Cattle behavior; Cattle detection; Cattle health; Cattle identification; Convolutional neural network; Digital agriculture; Health monitoring; Learning models; Precision livestock farming; Systematic literature review; data processing; data quality; database; decision support system; identification method; literature review; livestock farming; machine learning; precision; Health","","","A. Zahid; Department of Agricultural and Biological Engineering, The Pennsylvania State University, University Park, 16802, United States; email: axz264@psu.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","","Scopus","2-s2.0-85109849018"
"Easwaran A.; Arvindan P.; Dhanyasree E.; Surya R.; Selvakumar S.","Easwaran, Arjun (57359347300); Arvindan, P. (57359347400); Dhanyasree, E. (57359470000); Surya, R. (58830401500); Selvakumar, S. (57204729780)","57359347300; 57359347400; 57359470000; 58830401500; 57204729780","Internet of things enabled smart animal farm prototype","2021","Journal of Physics: Conference Series","2070","1","012115","","","","1","10.1088/1742-6596/2070/1/012115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120415901&doi=10.1088%2f1742-6596%2f2070%2f1%2f012115&partnerID=40&md5=278a8b51ea92a9be6c53f6416706ec6a","Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","Easwaran A., Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Arvindan P., Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Dhanyasree E., Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Surya R., Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Selvakumar S., Department of Electrical and Electronics Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","Livestock plays very important economic, social and cultural roles in the well being of rural communities across the world. Quality environmental conditions, automation and monitoring are the key necessities of running a good and profitable livestock farm. Air quality, temperature of the surroundings and humidity play a major role while deciding the fan speeds of the exhaust System used in all aspects of livestock farming. Another important part of livestock production is increasing incubation speeds of eggs by performing artificial incubation. It is a requirement to maintain the temperature at a constant value in this system. This paper describes two mutually exclusive Fuzzy Logic algorithm-based systems to automate the exhaust system and an artificial egg incubator. The other important part of a livestock farm is production of milk and milk products. It is required to monitor the health of cows by overseeing their activities at any point of time. This can be done by determining and monitoring the activities performed by the cow. This paper describes a simple Deep Learning Model to classify the activities of a cow broadly as standing, walking or grazing. The Exhaust and the Incubator system are controlled and monitored using Internet of Things (IOT) System using a native web application developed using the Flask framework. © Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence.","","Air quality; Deep learning; Fuzzy logic; Internet of things; Constant values; Environmental conditions; Environmental Monitoring; Fan speed; Fuzzy logic algorithms; Livestock farming; Livestock production; Milk products; Rural community; Well being; Agriculture","","","","Vivekanandhan P.; Kumaravelan R.; Senthilkumar S.","IOP Publishing Ltd","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120415901"
"Hansen M.F.; Oparaeke A.; Gallagher R.; Karimi A.; Tariq F.; Smith M.L.","Hansen, Mark F. (57192115769); Oparaeke, Alphonsus (13407271400); Gallagher, Ryan (57475108400); Karimi, Amir (57475108500); Tariq, Fahim (57475506700); Smith, Melvyn L. (55495905800)","57192115769; 13407271400; 57475108400; 57475108500; 57475506700; 55495905800","Towards Machine Vision for Insect Welfare Monitoring and Behavioural Insights","2022","Frontiers in Veterinary Science","9","","835529","","","","10","10.3389/fvets.2022.835529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125656943&doi=10.3389%2ffvets.2022.835529&partnerID=40&md5=bc21ece13b9965ec04daa0f74024090e","The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom; Department of Crop Science, University of Calabar, Calabar, Nigeria; SciFlair Ltd, Bristol, United Kingdom","Hansen M.F., The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom; Oparaeke A., Department of Crop Science, University of Calabar, Calabar, Nigeria; Gallagher R., The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom; Karimi A., The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom; Tariq F., SciFlair Ltd, Bristol, United Kingdom; Smith M.L., The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom","Machine vision has demonstrated its usefulness in the livestock industry in terms of improving welfare in such areas as lameness detection and body condition scoring in dairy cattle. In this article, we present some promising results of applying state of the art object detection and classification techniques to insects, specifically Black Soldier Fly (BSF) and the domestic cricket, with the view of enabling automated processing for insect farming. We also present the low-cost “Insecto” Internet of Things (IoT) device, which provides environmental condition monitoring for temperature, humidity, CO2, air pressure, and volatile organic compound levels together with high resolution image capture. We show that we are able to accurately count and measure size of BSF larvae and also classify the sex of domestic crickets by detecting the presence of the ovipositor. These early results point to future work for enabling automation in the selection of desirable phenotypes for subsequent generations and for providing early alerts should environmental conditions deviate from desired values. Copyright © 2022 Hansen, Oparaeke, Gallagher, Karimi, Tariq and Smith.","black soldier fly; deep learning; domestic crickets; insect farming; machine vision; sex classification","agricultural worker; article; automation; deep learning; Ensifera; human; maggot; nonhuman; phenotype; soldier (insect); vision; welfare","Biotechnology and Biological Sciences Research Council, UK Research and Innovation; UKRI-BBSRC, (GCRF-SA-2020-UWE)","This work was partly funded by the GCRF Agri-tech Catalyst Seeding Award managed by Biotechnology and Biological Sciences Research Council, UK Research and Innovation, (UKRI-BBSRC). Award Ref: GCRF-SA-2020-UWE.","M.F. Hansen; The Centre for Machine Vision, Bristol Robotics Laboratory, UWE Bristol, Bristol, United Kingdom; email: mark.hansen@uwe.ac.uk","","Frontiers Media S.A.","22971769","","","","English","Front. Vet. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85125656943"
"Tung T.C.; Khairuddin U.; Shapiai M.I.; Nor N.M.; Hiew M.W.H.; Suhaimie N.A.M.","Tung, Tan Chen (57904441800); Khairuddin, Uswah (37013632100); Shapiai, Mohd Ibrahim (36662724200); Nor, Norhariani Md (55169607600); Hiew, Mark Wen Han (55973534300); Suhaimie, Nurul Aisyah Mohd (57904838200)","57904441800; 37013632100; 36662724200; 55169607600; 55973534300; 57904838200","Livestock Posture Recognition Using Deep Learning","2022","4th International Conference on Smart Sensors and Application: Digitalization for Societal Well-Being, ICSSA 2022","","","","81","85","4","0","10.1109/ICSSA54161.2022.9870946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138704789&doi=10.1109%2fICSSA54161.2022.9870946&partnerID=40&md5=430b2cfc7f9b9dc103fbe95a51cb5775","Malaysia-Japan International Institute of Technology Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Universiti Putra Malaysia, Faculty of Veterinary Medicine, Selangor, Malaysia; Universiti Sultan Zainal Abidin, Faculty of Bioresources and Food Industry, Terengganu, Malaysia","Tung T.C., Malaysia-Japan International Institute of Technology Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Khairuddin U., Malaysia-Japan International Institute of Technology Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Shapiai M.I., Malaysia-Japan International Institute of Technology Universiti Teknologi Malaysia, Kuala Lumpur, Malaysia; Nor N.M., Universiti Putra Malaysia, Faculty of Veterinary Medicine, Selangor, Malaysia; Hiew M.W.H., Universiti Putra Malaysia, Faculty of Veterinary Medicine, Selangor, Malaysia; Suhaimie N.A.M., Universiti Sultan Zainal Abidin, Faculty of Bioresources and Food Industry, Terengganu, Malaysia","Calf posture recognition could be one of the required steps for a complete automated calf monitoring system, as sometimes the calf is required to be in a standing posture before being able to proceed to the next stage. To distinguish calf postures such as between standing or lying, machines require complicated frameworks, especially one that involves deep learning models. Previously, most of the works utilized video inputs rather than image inputs, which would make the model unnecessarily complicated compared to a conventional Convolutional Neural Network (CNN) model, which accepts image inputs. In this paper, to overcome all the problems mentioned earlier, two deep learning models with the exact same ResNet-50 based architecture have been built and trained on two different image datasets, respectively sourced from separate cameras placed at different angles to be compared and analyzed. The performance for both CNN models were 99.7% and 99.99% in accuracy, respectively, significantly better than the 92.61% accuracy of a similar work, and is adequate for a real-Time calf monitoring system.  © 2022 IEEE.","calf posture; deep learning; machine vision; transfer learning","Agriculture; Computer vision; Convolutional neural networks; Deep learning; Learning systems; Monitoring; Transfer learning; Calf posture; Convolutional neural network; Deep learning; Image inputs; Learning models; Machine-vision; Monitoring system; Neural network model; Posture recognition; Transfer learning; Neural network models","MRUN-MYRGS, (324927, R.K130000.7343.4B558); Malaysian Research University Networks; Universiti Teknologi Malaysia, UTM","ACKNOWLEDGMENT The authors would like to acknowledge the Malaysian Research University Networks (MRUN) for the funding of this project under Malaysian Young Researchers grant scheme (MRUN-MYRGS) Project Id: 324927, Vote number: R.K130000.7343.4B558 (Universiti Teknologi Malaysia) Title: Precision surveillance system to support dairy young stock rearing decisions.","","","Institute of Electrical and Electronics Engineers Inc.","","978-166549981-1","","","English","Int. Conf. Smart Sensors Appl.: Digit. Soc. Well-Being, ICSSA","Conference paper","Final","","Scopus","2-s2.0-85138704789"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.; Sani S.; Ab Wahab M.N.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900); Sani, Salisu (57522765300); Ab Wahab, Mohd Nadhir (36471236100)","57209469141; 57190968285; 35570816900; 57522765300; 36471236100","Behavior Recognition of Group-ranched Cattle from Video Sequences using Deep Learning","2022","Indian Journal of Animal Research","56","4","","505","512","7","4","10.18805/IJAR.B-1369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131322545&doi=10.18805%2fIJAR.B-1369&partnerID=40&md5=a306a5730fdf64bae72ce6f0511cede4","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria; Department of Information Technology, Federal University, Jigawa State, Dutse, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Sani S., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Information Technology, Federal University, Jigawa State, Dutse, Nigeria; Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Background: One important indicator for the wellbeing status of livestock is their daily behavior. More often than not, daily behavior recognition involves detecting the heads or body gestures of the livestock using conventional methods or tools. To prevail over such limitations, an effective approach using deep learning is proposed in this study for cattle behavior recognition. Methods: The approach for detecting the behavior of individual cows was designed in terms of their eating, drinking, active, and inactive behaviors captured from video sequences and based on the investigation of the attributes and practicality of the state-of-the-art deep learning methods. Result: Among the four models employed, Mask R-CNN achieved average recognition accuracies of 93.34%, 88.03%, 93.51% and 93.38% for eating, drinking, active and inactive behaviors. This implied that Mask R-CNN achieved higher cow detection accuracy and speed than the remaining models with 20 fps, making the proposed approach competes favorably well with other approaches and suitable for behavior recognition of group-ranched cattle in real-time. © 2022 Agricultural Research Communication Centre. All rights reserved.","Behavior recognition; Deep learning; Group-ranched cattle; Mask R-CNN","accuracy; Article; augmentation index; behavior; behavior recognition; cow; deep learning; differential threshold; drinking; drinking behavior; eating; eating habit; learning; mathematical model; nonhuman; training; velocity; videorecording","Universiti Sains Malaysia, (304/ PKOMP/6315262); Universiti Sains Malaysia","The authors would like to acknowledge the financial support from Universiti Sains Malaysia (USM) under short term university grant (304/ PKOMP/6315262).","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sufril@usm.my; M.N. Ab Wahab; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: mohdnadhir@usm.my","","Agricultural Research Communication Centre","03676722","","","","English","Ind. J. Ani. Res","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131322545"
"Park H.; Park D.; Kim S.","Park, Hyeon (57192084164); Park, Daeheon (7403245696); Kim, Sehan (54393433400)","57192084164; 7403245696; 54393433400","Anomaly detection of operating equipment in livestock farms using deep learning techniques","2021","Electronics (Switzerland)","10","16","1958","","","","9","10.3390/electronics10161958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112415309&doi=10.3390%2felectronics10161958&partnerID=40&md5=5d78852259167a6c3fdb3d9b55b8700a","SDF Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea","Park H., SDF Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Park D., SDF Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Kim S., SDF Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea","In order to establish a smart farm, many kinds of equipment are built and operated inside and outside of a pig house. Thus, the environment for livestock (limited to pigs in this paper) in the barn is properly maintained for its growth conditions. However, due to poor environments such as closed pig houses, lack of stable power supply, inexperienced livestock management, and power outages, the failure of these environment equipment is high. Thus, there are difficulties in detecting its malfunctions during equipment operation. In this paper, based on deep learning, we provide a mechanism to quickly detect anomalies of multiple equipment (environmental sensors and controllers, etc.) in each pig house at the same time. In particular, environmental factors (temperature, humidity, CO2, ventilation, radiator temperature, external temperature, etc.) to be used for learning were extracted through the analysis of data accumulated for the generation of predictive models of each equipment. In addition, the optimal recurrent neural network (RNN) environment was derived by analyzing the characteristics of the learning RNN. In this way, the accuracy of the prediction model can be improved. In this paper, the real-time input data (only in the case of temperature) was intentionally induced above the threshold, and 93% of the abnormalities were detected to determine whether the equipment was abnormal. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Anomaly detection; Environmental monitoring; OneM2M; RNN; Smart farming","","ICT based Intelligent Smart Welfare Housing System; Institute for Information and Communications Technology Promotion, IITP; Ministry of Science and ICT, South Korea, MSIT, (2018-0-00387); Ministry of Science and ICT, South Korea, MSIT","This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2018-0-00387, Development of ICT based Intelligent Smart Welfare Housing System for the Prevention and Control of Livestock Disease).","H. Park; SDF Convergence Research Department, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; email: hpark@etri.re.kr","","MDPI AG","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112415309"
"Bonneau M.; Godard X.; Bambou J.-C.","Bonneau, Mathieu (55759851900); Godard, Xavier (22134033100); Bambou, Jean-Christophe (6508078356)","55759851900; 22134033100; 6508078356","Assessing Goats' Fecal Avoidance Using Image Analysis-Based Monitoring","2022","Frontiers in Animal Science","3","","835516","","","","2","10.3389/fanim.2022.835516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152897797&doi=10.3389%2ffanim.2022.835516&partnerID=40&md5=17566aaaf337eeade7a839668b479bab","Agroecology, Genetic and Tropical Livestock Farming System (ASSET), INRAE, Petit-Bourg, France; UE PTEA Tropical Platform For Animal Experimentation, INRAE, Petit-Bourg, France","Bonneau M., Agroecology, Genetic and Tropical Livestock Farming System (ASSET), INRAE, Petit-Bourg, France; Godard X., UE PTEA Tropical Platform For Animal Experimentation, INRAE, Petit-Bourg, France; Bambou J.-C., Agroecology, Genetic and Tropical Livestock Farming System (ASSET), INRAE, Petit-Bourg, France","The recent advances in sensor technologies and data analysis could improve our capacity to acquire long-term and individual dataset on animal behavior. In livestock management, this is particularly interesting when behavioral data could be linked to production performances, physiological or genetical information, with the objective of improving animal health and welfare management. In this study, we proposed a framework, based on computer vision and deep learning, to automatically estimate animal location within pasture and discuss the relationship with the risk of gastrointestinal nematode (GIN) infection. We illustrated our framework for the monitoring of goats allowed to graze an experimental plot, where feces containing GIN infective larvae were previously dropped in delimited areas. Four animals were monitored, during two grazing weeks on the same pasture (week 1 from April 12 to 19, 2021 and week 2, from June 28 to July 5, 2021). Using the monitoring framework, different components of animal behavior were analyzed, and the relationship with the risk of GIN infection was explored. First, in average, 87.95% of the goats were detected, the detected individuals were identified with an average sensitivity of 94.9%, and an average precision of 94.8%. Second, the monitoring of the ability of the animal to avoid infected feces on pasture showed an important temporal and individual variability. Interestingly, the avoidance behavior of 3 animals increased during the second grazing week (Wilcoxon rank sum, p-value < 0.05), and the level of increase was correlated with the level of infection during week 1 (Pearson's correlation coefficient = 0.9). The relationship between the time spent on GIN-infested areas and the level of infection was also studied, but no clear relationship was found. In conclusion, due to the low number of studied animals, biological results should be interpreted with caution; nevertheless, the framework provided here is a new relevant tool to explore the relationship between ruminant behavior and GIN parasitism in experimental studies. Copyright © 2022 Bonneau, Godard and Bambou.","animal behavior; Creole goats; fecal avoidance; gastro-intestinal nematodes; image analysis","","European Union Fund; Conseil Régional de Guadeloupe; European Regional Development Fund, ERDF; Institut National de Recherche pour l'Agriculture, l'Alimentation et l'Environnement, INRAE","The cameras were funded by the project suiRAvi, supported by the animal genetics division of INRAE. The study was supported by Région Guadeloupe and the European Union Fund (FEDER) in the framework of the AgroEcoDiv project. ","M. Bonneau; Agroecology, Genetic and Tropical Livestock Farming System (ASSET), INRAE, Petit-Bourg, France; email: mathieu.bonneau@inrae.fr","","Frontiers Media S.A.","26736225","","","","English","Front. Anim. Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85152897797"
"Chen C.; Zhu W.; Norton T.","Chen, Chen (56471069100); Zhu, Weixing (35333396600); Norton, Tomas (35273348100)","56471069100; 35333396600; 35273348100","Behaviour recognition of pigs and cattle: Journey from computer vision to deep learning","2021","Computers and Electronics in Agriculture","187","","106255","","","","130","10.1016/j.compag.2021.106255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107920833&doi=10.1016%2fj.compag.2021.106255&partnerID=40&md5=7e432e00b18c11d1de24c79000621663","School of Electrical and Information Engineering, Jiangsu University, Zhenjiang, 212013, Jiangsu, China; Division of Measure, Model & Manage Bioresponses (M3-Biores), KU Leuven, Kasteelpark Arenberg 30, Leuven, 3001, Belgium","Chen C., School of Electrical and Information Engineering, Jiangsu University, Zhenjiang, 212013, Jiangsu, China, Division of Measure, Model & Manage Bioresponses (M3-Biores), KU Leuven, Kasteelpark Arenberg 30, Leuven, 3001, Belgium; Zhu W., School of Electrical and Information Engineering, Jiangsu University, Zhenjiang, 212013, Jiangsu, China; Norton T., Division of Measure, Model & Manage Bioresponses (M3-Biores), KU Leuven, Kasteelpark Arenberg 30, Leuven, 3001, Belgium","The increasing demand for sustainable livestock products also demands new considerations in animal breeding. Breeding programs are now seeking to integrate animal behavioural phenotypes, as these relate to the productivity, health and welfare of the animals and thereby can influence yield and economic benefits in the industry. Traditional manual observation of pig behaviour is time-consuming, laborious, subjective, and difficult to achieve in continuous and large-scale operations. It is not surprising that computer vision technology with the advantages of being objective, non-invasive and continuous has been widely researched for its use in the recognition of livestock behaviours over recent years. Nevertheless, in studies of livestock behaviour recognition, computer vision technology faces some challenges, e.g., complex scenes, variable illumination, occlusion, touching and overlapping between livestock, which has limited the fast translation of technology to industry. On the other hand, deep learning technology has proven to solve these difficulties to a certain extent and is being adopted to recognise livestock behaviours. This paper mainly evaluates the recent developments in computer vision methods for recognition of these behaviours in pigs and cattle. The focus on these species is made possible by the number of studies exist quantifying behaviours that are of importance for their health, welfare and productivity such as aggression, drinking, feeding, lameness, mounting, posture, tail-biting and nursing. This review paper especially analyses the development of image segmentation, identification and behaviour recognition using tradition computer vision and more recent deep learning methods, and evaluates the evolution of key research in the field. We elaborate the research trend of livestock behaviour recognition from four aspects, i.e., development of robust livestock identification algorithms, recognition of livestock behaviours for different growth stages, further quantification of the results of behaviour recognition, and building evaluation system of growth status, health and welfare. © 2021 Elsevier B.V.","Computer vision; Deep learning; Identification; Livestock behaviour recognition; Research trend","Behavioral research; Computer vision; Deep learning; Health; Identification (control systems); Image segmentation; Mammals; Productivity; Behaviour recognition; Breeding programmes; Computer vision technology; Continuous scale; Deep learning; Economic benefits; Identification; Livestock behavior recognition; Pig behavior; Research trends; algorithm; breeding; cattle; computer simulation; demand analysis; machine learning; phenotype; pig; recognition; research work; sustainability; Agriculture","Pig Improvement Company; UK Research and Innovation, UKRI, (104828); National Natural Science Foundation of China, NSFC, (31872399)","This work was a part of the project funded by the \u201CNational Natural Science Foundation of China\u201D, China (grant number: 31872399). Tomas Norton would like to acknowledge the support from Pig Improvement Company (PIC) for his contribution to this work.","C. Chen; School of Electrical and Information Engineering, Jiangsu University, Zhenjiang, 212013, China; email: ainicc1987@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107920833"
"Gan H.; Xu C.; Hou W.; Guo J.; Liu K.; Xue Y.","Gan, Haiming (56531479600); Xu, Chengguo (57222241786); Hou, Wenhao (57556734700); Guo, Jingfeng (57557116900); Liu, Kai (55823366100); Xue, Yueju (12241464400)","56531479600; 57222241786; 57556734700; 57557116900; 55823366100; 12241464400","Spatiotemporal graph convolutional network for automated detection and analysis of social behaviours among pre-weaning piglets","2022","Biosystems Engineering","217","","","102","114","12","19","10.1016/j.biosystemseng.2022.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127337192&doi=10.1016%2fj.biosystemseng.2022.03.005&partnerID=40&md5=0834744dfbc6965a5458f3938c7bedbf","College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong; Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China; Guangdong Engineering Research Center for Datamation of Modern Pig Production, Guangdong, Guangzhou, 510642, China","Gan H., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China, Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong, Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China; Xu C., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Hou W., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Guo J., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China; Liu K., Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong; Xue Y., College of Electronic Engineering, South China Agricultural University, Guangdong, Guangzhou, 510642, China, Guangdong Laboratory for Lingnan Modern Agriculture, Guangdong, Guangzhou, 510642, China, Guangdong Engineering Research Center for Datamation of Modern Pig Production, Guangdong, Guangzhou, 510642, China","In the pig industry, social behaviors of preweaning piglets are critical indicators of their livability, growth, health, and welfare status, for which there is an urgent need for using precision livestock farming tools. In this study, a novel method based on graph convolutional networks (GCNs) was developed to characterize preweaning piglet social behaviors such as snout–snout as well as snout-body social nosing and snout–snout as well as snout-body aggressive/playing behavior. Using an integrated CNN-based network, the proposed method first detected and tracked individual piglets. After that, a self-adaptive spatial affinity kernel function was used to detect suspected social behaviors and spatiotemporal graphs with high-quality node features (coordinates, node-estimation confidence, distance from each node to the centroid, and node motion) were built for pairwise piglets for further analysis. The spatiotemporal graph sequences were fed into a self-adaptive GCN combined with an attention mechanism to classify the suspected social behaviours. Our method performed well in detecting piglet social behaviours with a recall of 0.9405, a precision of 0.9669, and an F1 score of 0.9535. In an 8-h video episode, the time budges of snout–snout as well as snout-body social nosing and snout–snout as well as snout-body aggressive/playing behaviour were 33.82%, 38.34%, 13.74%, and 14.10%, respectively. The findings show that detecting body part-associated social behaviours in piglets using a GCN is feasible, yielding practical computer vision technologies for improved piglet behaviour monitoring and management. © 2022 IAgrE","attention mechanism; computer vision; deep learning; precision livestock farming; spatio-temporal feature","Agriculture; Convolution; Convolutional neural networks; Deep learning; Feature extraction; Mammals; Quality control; Social behavior; Attention mechanisms; Automated analysis; Automated detection; Convolutional networks; Deep learning; Pig industry; Precision livestock farming; Social behaviour; Spatio-temporal graphs; Spatiotemporal feature; Computer vision","International Training Program for Outstanding Young Scientists in Universities in Guangdong Province, (2020YQGP_BS010); City University of Hong Kong; Science and Technology Planning Project of Guangdong Province; International Science and Technology Cooperation Programme, ISTCP, (2020ZDZX1041, 2021A0505030058)","This work was supported by the “Science and Technology Planning Project of Guangdong Province of China” (International Science and Technology Cooperation Projects , grant numbers: 2021A0505030058 ), the “Special and grand field project for Guangdong Province Higher Education” (grant numbers: 2020ZDZX1041 ), the “International Training Program for Outstanding Young Scientists in Universities in Guangdong Province” (grant numbers: 2020YQGP_BS010 ), and the new research initiative at the City University of Hong Kong .","Y. Xue; College of Electronic Engineering, South China Agricultural University, Guangzhou, Guangdong, 510642, China; email: xueyj@scau.edu.cn; K. Liu; Department of Infectious Diseases and Public Health, Jockey Club College of Veterinary Medicine and Life Sciences, City University of Hong Kong, Hong Kong SAR, Hong Kong; email: kailiu@cityu.edu.hk","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85127337192"
"Oczak M.; Bayer F.; Vetter S.G.; Maschat K.; Baumgartner J.","Oczak, Maciej (55246342700); Bayer, Florian (57226749450); Vetter, Sebastian G. (56835289800); Maschat, Kristina (37761655400); Baumgartner, Johannes (7102679363)","55246342700; 57226749450; 56835289800; 37761655400; 7102679363","Where is the sow’s nose: RetinaNet object detector as a basis for monitoring the use of rack with nest-building material","2022","Frontiers in Animal Science","3","","913407","","","","3","10.3389/fanim.2022.913407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149184599&doi=10.3389%2ffanim.2022.913407&partnerID=40&md5=35ecadfc27071b9adb7bb35bc4d818be","Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Veterinary Public Health and Epidemiology, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Austrian Competence Centre for Feed and Food Quality, Safety and Innovation, FFoQSI GmbH, Tulln, Austria","Oczak M., Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria, Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Bayer F., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Vetter S.G., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria, Veterinary Public Health and Epidemiology, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; Maschat K., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria, Austrian Competence Centre for Feed and Food Quality, Safety and Innovation, FFoQSI GmbH, Tulln, Austria; Baumgartner J., Institute of Animal Welfare Science, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria","Access to nest-building material in the preparturient period is beneficial for sows’ welfare. However, on slatted floors, long-stem forage can drop into the slurry and block the drainage system. As a compromise considering the needs of sows for access to adequate nest-building material, farrowing pens with slatted floors are equipped with dispensers (racks) accessible by sows. In this study, we developed a computer vision method to monitor the use of the racks with nest-building material. In total, 12 sows were included in the experiment from 5 days before farrowing to the end of farrowing. Hay rack use behaviors were labeled for all the sows, i.e., pulling hay, nose close to the rack, exploratory behavior, and bar biting. The object detection algorithm RetinaNet was used to extract centroids of parts of the sow’s body and the hay rack. Several feature variables were estimated from the centroids of detected parts of the sow’s body, and random forest was used for the classification of hay rack use behaviors. The model for the detection of pulling hay behavior had the best performance: 83.5% sensitivity, 98.7% specificity, and 98.6% accuracy. The distance between the sows’ nose and the hay rack was the most important feature variable, which indicated the importance of nose location for the recognition of behaviors in which pigs interact with other objects. The developed models could be applied for automated monitoring of the use of nest-building material in preparturient sows. Such monitoring might be especially important in sows housed on slatted floors. Copyright © 2022 Oczak, Bayer, Vetter, Maschat and Baumgartner.","automated monitoring; computer vision; deep learning; hay rack use; nest building; precision livestock farming; sow","","BGBl, (114/2012); Bundesministerium für Wissenschaft, Forschung und Wirtschaft, BMWFW, (BMWFV-68.205/0082-WF/II/3b/2014); Bundesministerium für Wissenschaft, Forschung und Wirtschaft, BMWFW","Project PIGwatch was authorized by the ethical committee of the Austrian Federal Ministry of Science, Research and Economy and by the ethical committee of Vetmeduni Vienna (GZ: BMWFV-68.205/0082-WF/II/3b/2014) according to the Austrian Tierversuchsgesetz 2012, BGBl. I Nr. 114/2012. Written informed consent was obtained from the owners for the participation of their animals in this study. ","M. Oczak; Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Vienna, Austria; email: Maciej.Oczak@vetmeduni.ac.at","","Frontiers Media S.A.","26736225","","","","English","Front. Anim. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149184599"
"Witte J.-H.; Gerberding J.; Lensches C.; Traulsen I.","Witte, Jan-Hendrik (57667657200); Gerberding, Johann (57667319900); Lensches, Clara (57216583979); Traulsen, Imke (35410826800)","57667657200; 57667319900; 57216583979; 35410826800","Using Deep Learning for automated birth detection during farrowing","2022","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-328","","","141","153","12","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139866386&partnerID=40&md5=da6a3304b1e4406eccea8de1a82abf66","University of Oldenburg, Germany; University of Göttingen, Germany","Witte J.-H., University of Oldenburg, Germany; Gerberding J., University of Oldenburg, Germany; Lensches C., University of Göttingen, Germany; Traulsen I., University of Göttingen, Germany","Pig livestock farming has been undergoing major structural change for years. The number of animals per farm is constantly increasing, while competition is becoming more intense due to volatile slaughter prices. Sustainable, welfare-oriented livestock farming becomes increasingly difficult under these conditions. Studies have shown that animal-specific birth monitoring of sows can significantly reduce piglet losses. However, continuous monitoring by human staff is inconceivable, which is why systems need to be created that assist farmers in these tasks. For this reason, this paper aims to introduce the first step towards an automated birth monitoring system. The goal is to use deep learning methods from the field of computer vision to enable the detection of individual piglet births based on image data. This information can be used to develop systems that detect the beginning of a birth process, measure the duration of piglet births, and determine the time intervals between piglet births. © 2022 Gesellschaft fur Informatik (GI). All rights reserved.","birth monitoring; computer vision; deep learning; precision livestock farming","Agriculture; Deep learning; Learning systems; Mammals; Monitoring; Birth monitoring; Birth process; Condition; Continuous monitoring; Deep learning; Image data; Learning methods; Livestock farming; Monitoring system; Precision livestock farming; Computer vision","","","","Wohlgemuth V.; Naumann S.; Arndt H.-K.; Behrens G.; Hob M.","Gesellschaft fur Informatik (GI)","16175468","978-388579722-7","","","English","Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)","Conference paper","Final","","Scopus","2-s2.0-85139866386"
"Suparwito H.; Thomas D.T.; Wong K.W.; Xie H.; Rai S.","Suparwito, Hari (57212196155); Thomas, Dean T. (16242777700); Wong, Kok Wai (7404759276); Xie, Hong (35231294900); Rai, Shri (8250275300)","57212196155; 16242777700; 7404759276; 35231294900; 8250275300","The use of animal sensor data for predicting sheep metabolisable energy intake using machine learning","2021","Information Processing in Agriculture","8","4","","494","504","10","9","10.1016/j.inpa.2020.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099828320&doi=10.1016%2fj.inpa.2020.12.004&partnerID=40&md5=efbd0cccd9ddef06318a963a54c1dcec","Murdoch University, Murdoch, 6150, WA, Australia; CSIRO Agriculture and Food, Private Bag 5, Wembley, 6913, WA, Australia","Suparwito H., Murdoch University, Murdoch, 6150, WA, Australia; Thomas D.T., CSIRO Agriculture and Food, Private Bag 5, Wembley, 6913, WA, Australia; Wong K.W., Murdoch University, Murdoch, 6150, WA, Australia; Xie H., Murdoch University, Murdoch, 6150, WA, Australia; Rai S., Murdoch University, Murdoch, 6150, WA, Australia","The use of sensors for monitoring livestock has opened up new possibilities for the management of livestock in extensive grazing systems. The work presented in this paper aimed to develop a model for predicting the metabolisable energy intake (MEI) of sheep by using temperature, pitch angle, roll angle, distance, speed, and grazing time data obtained directly from wearable sensors on the sheep. A Deep Belief Network (DBN) algorithm was used to predict MEI, which to our knowledge, has not been attempted previously. The results demonstrated that the DBN method could predict the MEI for sheep using sensor data alone. The mean square error (MSE) values of 4.46 and 20.65 have been achieved using the DBN model for training and testing datasets, respectively. We also evaluated the influential sensor data variables, i.e., distance and pitch angle, for predicting the MEI. Our study demonstrates that the application of machine learning techniques directly to on-animal sensor data presents a substantial opportunity to interpret biological interactions in grazing systems directly from sensor data. We expect that further development and refinement of this technology will catalyse a step-change in extensive livestock management, as wearable sensors become widely used by livestock producers. © 2021 China Agricultural University","Energy intake; Livestock behaviour; Machine learning; Predictions; Sensor data","Agriculture; Animals; Deep learning; Forecasting; Learning systems; Mean square error; Biological interactions; Deep belief network (DBN); Energy intake; Extensive grazing systems; Grazing systems; Machine learning techniques; Metabolisable; Training and testing; algorithm; grazing management; livestock; machine learning; metabolism; monitoring system; prediction; sensor; Wearable sensors","Commonwealth Scientific and Industrial Research Organisation, CSIRO","This research was supported by CSIRO Australia. We are grateful for their cooperation and permission to use their data. We are also grateful for the generous support of farmers Simon and Tony York for hosting the grazing experiment for this study. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.","H. Suparwito; Murdoch University, Murdoch, 6150, Australia; email: shirsj@jesuits.net","","China Agricultural University","22143173","","","","English","Inf. Process. Agric.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099828320"
"Huang E.; Mao A.; Gan H.; Camila Ceballos M.; Parsons T.D.; Xue Y.; Liu K.","Huang, Endai (57226523976); Mao, Axiu (57238219000); Gan, Haiming (56531479600); Camila Ceballos, Maria (55958638400); Parsons, Thomas D. (57210708339); Xue, Yueju (12241464400); Liu, Kai (55823366100)","57226523976; 57238219000; 56531479600; 55958638400; 57210708339; 12241464400; 55823366100","Center clustering network improves piglet counting under occlusion","2021","Computers and Electronics in Agriculture","189","","106417","","","","24","10.1016/j.compag.2021.106417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114128077&doi=10.1016%2fj.compag.2021.106417&partnerID=40&md5=cca8aef0ddc06bf07deb59f402e13404","Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Colleges of Electronic Engineering and Artificial Intelligence, South China Agricultural University, Guangzhou, Guangdong, China; Department of Production Animal Health, Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States; Animal Health Research Center, Chengdu Research Institute, City University of Hong Kong, Chengdu, Sichuan, China","Huang E., Department of Computer Science, City University of Hong Kong, Hong Kong; Mao A., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; Gan H., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, Colleges of Electronic Engineering and Artificial Intelligence, South China Agricultural University, Guangzhou, Guangdong, China; Camila Ceballos M., Department of Production Animal Health, Faculty of Veterinary Medicine, University of Calgary, Calgary, AB, Canada; Parsons T.D., Swine Teaching and Research Center, School of Veterinary Medicine, University of Pennsylvania, Kennett Square, PA, United States; Xue Y., Colleges of Electronic Engineering and Artificial Intelligence, South China Agricultural University, Guangzhou, Guangdong, China; Liu K., Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong, Animal Health Research Center, Chengdu Research Institute, City University of Hong Kong, Chengdu, Sichuan, China","Counting nursing piglets is an essential task on commercial sow farms and provides a core parameter for evaluating sow reproductive performance. As a current management practice, piglets in farrowing pens are usually manually counted by caretakers several times during and after parturition and at weaning, which is time-consuming, labor-intensive, and often subject to careless errors. In recent years, automated counting tools based on computer vision have drawn increasing attention from the pig industry. However, piglets frequently can be occluded by farrowing stalls or sows, either fully or partially, which results in substantial counting errors by existing automated methods. To address problems caused by the partial occlusion, a two-stage center clustering network (CClusnet) was developed to improve automated piglet counting performance. We constructed a dataset consisting of 2,600 images from three farrowing pens to test and validate the CClusnet. The images were under heavy occlusions with one sow (DNA Genetics Line 241) and 7–13 piglets per image, and 97.7% of images having one or more piglets partially occluded. In the first stage, the CClusnet predicted a semantic segmentation map and a center offset vector map for each image. In the second stage, scattered center points were produced by combining the two maps, and the mean-shift algorithm was applied to determine the piglet count. The results showed that CClusnet achieved 0.43 mean absolute error per image for piglet counting, had a better performance than previous network architectures, and outperformed existing counting methods. Furthermore, our technique is a nonocclusion-specific method and can be applied in other similar settings with different types of occlusions, and with potential to achieve high accuracy in animal position detection and monitoring. © 2021 Elsevier B.V.","Computer vision; Deep learning; Object counting; Pig industry; Precision livestock farming","Automation; Computer vision; Deep learning; Errors; Image segmentation; Mammals; Network architecture; Semantics; Statistical tests; 'current; Clustering networks; Deep learning; Labour-intensive; Management practises; Object counting; Performance; Pig industry; Precision livestock farming; Reproductive performance; algorithm; cluster analysis; computer vision; detection method; numerical method; pig; segmentation; Agriculture","International Science and Technology Cooperation Project of Guangdong Province, (2021A0505030058); Pennsylvania Pork Producers Council; School of Veterinary Medicine, University of Pennsylvania; Special and Grand Field Project for Higher Education of Guangdong Province, (2020ZDZX1041); City University of Hong Kong, (9610450); National Pork Board, NPB","Funding text 1: Thanks to the staff at the swine teaching and research center, School of Veterinary Medicine, University of Pennsylvania for animal care. Funding was provided in part by the National Pork Board and the Pennsylvania Pork Producers Council in the U.S., the International Science and Technology Cooperation Project of Guangdong Province (grant number: 2021A0505030058), the Special and Grand Field Project for Higher Education of Guangdong Province (grant number: 2020ZDZX1041), and the new research initiatives at City University of Hong Kong (Project number: 9610450).; Funding text 2: Thanks to the staff at the swine teaching and research center, School of Veterinary Medicine, University of Pennsylvania for animal care. Funding was provided in part by the National Pork Board and the Pennsylvania Pork Producers Council in the U.S. the International Science and Technology Cooperation Project of Guangdong Province (grant number: 2021A0505030058), the Special and Grand Field Project for Higher Education of Guangdong Province (grant number: 2020ZDZX1041), and the new research initiatives at City University of Hong Kong (Project number: 9610450).","K. Liu; Department of Infectious Diseases and Public Health, City University of Hong Kong, Hong Kong; email: kailiu@cityu.edu.hk","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85114128077"
"Xu J.; Zhou S.; Xia F.; Xu A.; Ye J.","Xu, Jinyang (57449193300); Zhou, Suyin (57201635408); Xia, Fang (9037250100); Xu, Aijun (8564267200); Ye, Junhua (57449082800)","57449193300; 57201635408; 9037250100; 8564267200; 57449082800","Research on the lying pattern of grouped pigs using unsupervised clustering and deep learning","2022","Livestock Science","260","","104946","","","","12","10.1016/j.livsci.2022.104946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129541489&doi=10.1016%2fj.livsci.2022.104946&partnerID=40&md5=e26508dc2f1b2ba6a8d7bdcea3523f6d","School of Mathematics and Computer Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; Institute of Digital Country, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; School of Environmental and Resource Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China","Xu J., School of Mathematics and Computer Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; Zhou S., School of Mathematics and Computer Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China, Institute of Digital Country, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; Xia F., Institute of Digital Country, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; Xu A., School of Mathematics and Computer Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China, Institute of Digital Country, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; Ye J., School of Environmental and Resource Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China","Changes in lying pattern and lying center position can reflect information about the production efficiency, health, and welfare of pigs. This study investigates the lying pattern and lying center position of grouped pigs by using unsupervised clustering, deep learning, and image processing technology under commercial farming conditions. Images of the lying pattern of grouped pigs under commercial conditions were collected over 110 days, and then ellipse fitting was used to calculate the center of the target pigs. Unsupervised clustering was used to calculate the cluster center of pigs (lying pattern central location). Variance and standard deviation of the distance from the cluster center to each pig were calculated, Finally, the lying patterns of pigs were classified as “Crowded,” “Close,” “Normal,” “Dispersed,” and “Far.” These patterns were based on the standard deviation. In this research, a Convolutional Neural Network-Support Vector Machine (CNN-SVM) classification model was used to classify five lying patterns of grouped pigs; classification accuracy was up to 97.1%. We studied the change in the lying pattern center of grouped pigs over time, and found that the lying pattern center of grouped pigs showed positional variations over time. We collected experimental site data during the period from July 15, 2021 to October 31, 2021 and analyzed the lying patterns of the grouped pigs. Our results showed that pigs mainly had three lying patterns: “Crowded,” “Close,” and “Normal.” Their proportions were 35%, 33.75%, and 18%, respectively. In addition, the study also found that there were significant changes among the five lying patterns in a day. Standard deviation of grouped pigs showed positive correlation with pigpen temperature, and lying pattern variation of grouped pigs was affected by pigpen temperature. Based on these conclusions, suitable environmental conditions could be created for grouped pigs. In the future, more experiments and research will be extended to other types of livestock, as this technology is helpful for large-scale automatic welfare breeding of livestock. © 2022 Elsevier B.V.","CNN-SVM; Deep learning; Lying pattern; Pig; Unsupervised clustering","agricultural worker; animal experiment; animal model; article; breeding; controlled study; convolutional neural network; deep learning; image processing; livestock; nonhuman; pig; support vector machine; welfare","Key R&D Program of Zhejiang, China, (2022C02050); Key Technology Collaborative Promotion program of Zhejiang Province, (2021XTTGXM01); Zhejiang Qingzhu Pasturing Co. Ltd.","Funding text 1: We sincerely thank the Zhejiang Qingzhu Pasturing Co. Ltd. pig farm, located in Hangzhou, Zhejiang, China, for providing the site and equipment. Technical guidance was provided by the Taihuyuan Breeding Monitoring Station, Linan, Hangzhou, Zhejiang, China. In this experiment, the Tensorflow framework was used to train the experimental model. This study is sponsored by the Key R&D Program of Zhejiang, China (Grant 2022C02050), and the Key Technology Collaborative Promotion program of Zhejiang Province, China (Grant 2021XTTGXM01).; Funding text 2: We sincerely thank the Zhejiang Qingzhu Pasturing Co., Ltd. pig farm, located in Hangzhou, Zhejiang, China, for providing the site and equipment. Technical guidance was provided by the Taihuyuan Breeding Monitoring Station, Linan, Hangzhou, Zhejiang, China. In this experiment, the Tensorflow framework was used to train the experimental model. This study is sponsored by the Key R&D Program of Zhejiang, China (Grant 2022C02050 ), and the Key Technology Collaborative Promotion program of Zhejiang Province, China (Grant 2021XTTGXM01 ).","A. Xu; School of Mathematics and Computer Science, Zhejiang Agriculture and Forestry University, Hangzhou, 311300, China; email: xuaj1976@163.com","","Elsevier B.V.","18711413","","","","English","Livest. Sci.","Article","Final","","Scopus","2-s2.0-85129541489"
"","","","7th EAI International Conference on Science and Technologies for Smart Cities, SmartCity360° 2021","2022","Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","442 LNICST","","","","","716","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133270990&partnerID=40&md5=2cc726dd06037d25721f2a2299c54e98","","","The proceedings contain 45 papers presendted at a virtual meeting. The special focus in this conference is on Science and Technologies for Smart Cities. The topics include: Edge AI System Using a Thermal Camera for Industrial Anomaly Detection; power Consumption Analysis for the Development of Energy Efficient Bluetooth 5 Based Real-Time Industrial IoT Systems; towards Orchestration of Cloud-Edge Architectures with Kubernetes; simulated LoRa Sensor Network as Support for Route Planning in Solid Waste Collection; exploring the Effects of Precision Livestock Farming Notification Mechanisms on Canadian Dairy Farmers; concept for Safe Interaction of Driverless Industrial Trucks and Humans in Shared Areas; feature Fusion in Deep-Learning Semantic Image Segmentation: A Survey; MR-Based UAV Route Planning for the Coverage Task; inter-satellite Optical Analog Network Coding Using Modulated Retro Reflectors; low-cost Real-time IoT-Based Air Quality Monitoring and Forecasting; Channel Allocation Mechanism in C-RAN for Smart Transportation; Low Cost ICS Network Scanning for Vulnerability Prevention; the Development of the Sati Interactive System: A Computer-Based Interactive System that Creates a Sense of Deep Engagement in the User; benefits and Obstacles of Smart Governance in Cities; new Environmental Indicators for Sustainable Cities of Varying Size Scale: The Use Case of France; scalable and Sustainable Community Networks for Inclusive Smart Cities; security in V2X Communications: A Comparative Analysis of Simulation/Emulation Tools; implementation and Comparison of Four Algorithms on Transportation Problem; critical Review of Citizens’ Participation in Achieving Smart Sustainable Cities: The Case of Saudi Arabia; 3-Dimensional Reconstruction of a Highly Specular or Transparent Cylinder from a Single Image; An Overview of the Status of DNS and HTTP Security Services in Higher Education Institutions in Portugal; Prevention of IoT-Enabled Crime Using Home Routers (PITCHR); preface.","","","","","","Paiva S.; Li X.; Lopes S.I.; Gupta N.; Rawat D.B.; Patel A.; Karimi H.R.","Springer Science and Business Media Deutschland GmbH","18678211","978-303106370-1","","","English","Lect. Notes Inst. Comput. Sci. Soc. Informatics Telecommun. Eng.","Conference review","Final","","Scopus","2-s2.0-85133270990"
"Li G.; Xiong Y.; Du Q.; Shi Z.; Gates R.S.","Li, Guoming (57200960751); Xiong, Yijie (56421066000); Du, Qian (7202060063); Shi, Zhengxiang (57207027510); Gates, Richard S. (7102178031)","57200960751; 56421066000; 7202060063; 57207027510; 7102178031","Classifying ingestive behavior of dairy cows via automatic sound recognition","2021","Sensors","21","15","5231","","","","19","10.3390/s21155231","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111453683&doi=10.3390%2fs21155231&partnerID=40&md5=9263a8ef6ce8f13a47ce8629285af19b","Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, 50011, IA, United States; Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Department of Biological Systems Engineering, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, 39762, MS, United States; Department of Agricultural Structure and Bioenvironmental Engineering, College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China; Egg Industry Center, Departments of Agricultural and Biosystems Engineering, and Animal Science, Iowa State University, Ames, 50011, IA, United States","Li G., Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, 50011, IA, United States; Xiong Y., Department of Animal Science, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States, Department of Biological Systems Engineering, University of Nebraska-Lincoln, Lincoln, 68588, NE, United States; Du Q., Department of Electrical and Computer Engineering, Mississippi State University, Starkville, 39762, MS, United States; Shi Z., Department of Agricultural Structure and Bioenvironmental Engineering, College of Water Resources and Civil Engineering, China Agricultural University, Beijing, 100083, China; Gates R.S., Egg Industry Center, Departments of Agricultural and Biosystems Engineering, and Animal Science, Iowa State University, Ames, 50011, IA, United States","Determining ingestive behaviors of dairy cows is critical to evaluate their productivity and health status. The objectives of this research were to (1) develop the relationship between forage species/heights and sound characteristics of three different ingestive behaviors (bites, chews, and chew-bites); (2) comparatively evaluate three deep learning models and optimization strategies for classifying the three behaviors; and (3) examine the ability of deep learning modeling for classifying the three ingestive behaviors under various forage characteristics. The results show that the amplitude and duration of the bite, chew, and chew-bite sounds were mostly larger for tall forages (tall fescue and alfalfa) compared to their counterparts. The long short-term memory network using a filtered dataset with balanced duration and imbalanced audio files offered better performance than its counterparts. The best classification performance was over 0.93, and the best and poorest performance difference was 0.4–0.5 under different forage species and heights. In conclusion, the deep learning technique could classify the dairy cow ingestive behaviors but was unable to differentiate between them under some forage characteristics using acoustic signals. Thus, while the developed tool is useful to support precision dairy cow management, it requires further improvement. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Audio; Dairy cow; Deep learning; Forage management; Jaw movement; Mastication; Precision livestock management","Animal Feed; Animals; Cattle; Diet; Feeding Behavior; Female; Lactation; Mastication; Medicago sativa; Learning systems; Acoustic signals; Classification performance; Forage species; Learning models; Learning techniques; Optimization strategy; Short term memory; Sound recognition; alfalfa; animal; animal food; bovine; diet; feeding behavior; female; lactation; mastication; Deep learning","University of Nebraska-Lincoln, UNL; Iowa State University, ISU; College of Agriculture and Life Sciences, University of Arizona, CALS, UA","Funding: This research was funded by College of Agriculture and Life Sciences, Iowa State University, and the Institution of Agriculture and Natural Resources, the University of Nebraska-Lincoln.","G. Li; Department of Agricultural and Biosystems Engineering, Iowa State University, Ames, 50011, United States; email: gmli@iastate.edu","","MDPI AG","14248220","","","34372468","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85111453683"
"Alfred R.; Leikson C.; Boniface B.; Tanakinjal G.H.; Kamu A.; Kogid M.; Sondoh S.L.; Mohd Nawi N.; Arumugam N.; Andrias R.M.","Alfred, Rayner (24722539300); Leikson, Christylyn (57980527800); Boniface, Bonaventure (55365922300); Tanakinjal, Geoffrey Harvey (36807375700); Kamu, Assis (37065739400); Kogid, Mori (36193945300); Sondoh, Stephen L. (56682884700); Mohd Nawi, Nolila (57209294954); Arumugam, Nalini (57200832938); Andrias, Ryan Macdonell (36241454500)","24722539300; 57980527800; 55365922300; 36807375700; 37065739400; 36193945300; 56682884700; 57209294954; 57200832938; 36241454500","Modelling and Forecasting Fresh Agro-Food Commodity Consumption Per Capita in Malaysia Using Machine Learning","2022","Mobile Information Systems","2022","","6106557","","","","4","10.1155/2022/6106557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142534964&doi=10.1155%2f2022%2f6106557&partnerID=40&md5=3d052af7378094c935633dd481e41a86","Knowledge Technology Research Unit, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Computing & Informatics, Multimedia University, Melaka, Malaysia; Pusat Penataran Ilmu dan Bahasa, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Labuan Faculty of International Finance, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Science and Natural Resources, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Business, Economics and Accountancy, Universiti Malaysia Sabah, Kota Kinabalu ums.edu.my, Malaysia; Faculty of Agriculture, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Universiti Sultan Zainal Abidin, Kuala Terengganu, Trengganu, Malaysia","Alfred R., Knowledge Technology Research Unit, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Leikson C., Faculty of Computing & Informatics, Multimedia University, Melaka, Malaysia; Boniface B., Pusat Penataran Ilmu dan Bahasa, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Tanakinjal G.H., Labuan Faculty of International Finance, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Kamu A., Faculty of Science and Natural Resources, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Kogid M., Faculty of Business, Economics and Accountancy, Universiti Malaysia Sabah, Kota Kinabalu ums.edu.my, Malaysia; Sondoh S.L., Faculty of Business, Economics and Accountancy, Universiti Malaysia Sabah, Kota Kinabalu ums.edu.my, Malaysia; Mohd Nawi N., Faculty of Agriculture, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Arumugam N., Universiti Sultan Zainal Abidin, Kuala Terengganu, Trengganu, Malaysia; Andrias R.M., Knowledge Technology Research Unit, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia","This study focuses on identifying and analyzing spending trend profiles and developing the per capita consumption models to forecast the fresh agro-food per capita consumption in Malaysia. Previous published works have looked at statistical and machine learning methods to forecast the demand of agro-food such as ARIMA and SVM methods. However, ordinary least squares (OLS) and neural network (NN) methods have shown better results in modelling time series data. For that reason, the primary objective of this study is to model and forecast the consumption per capita (PCC) of several selected fresh agro-food commodities in Malaysia using the OLS and NN methods. The secondary objectives of the paper include investigating the performance of OLS against NNs with three different topologies, discussing the correlation between Malaysia GDP per capita and the agro-food commodity PCC, and finally assessing whether the PCC data are increasing over time or decreasing over time and whether the trend in either direction is statistically significant by using the Mann-Kendall statistical test. Based on the results of the agro-food consumption per capita (PCC) forecasting, several critical agro-food commodities are also identified in this work. The material of the study consists of the per capita consumption of thirty-three (33) agro-food items that can be categorized into rice, livestock, vegetables, fisheries, and fruits, total gross domestic product (GDP) per capita, and the total population of Malaysia between 2010 and 2017. Based on the results obtained, the neural network (NNT1) model was found to produce the lowest total MSE of 17.95, for all 33 fresh agro-food investigated in this study. Several agro-food commodities have been identified as having significant positive (e.g., rice, spinach, cabbage, celery cabbage, eggplant, cucumber, poultry, lamb, squid, tuna, star fruit, jackfruit, durian, sweet corn, and coconut) or negative (e.g., pork, mackerel, papaya, guava, mangosteen, pineapple, banana, rambutan, and watermelon) trends using the Mann-Kendall trend test. This study also demonstrated that the production of critical agro-food commodities (e.g., rice, chili, cabbage, celery cabbage, poultry (chicken/duck), beef, lamb, crab, mango, and coconut) should be improved to ensure self-sufficiency ratios (SSRs) of more than 100% to accommodate the increased projected consumption in Malaysia by the year 2025. This paper concludes that neural network methods produce better prediction, and future works include forecasting agro-food demand based on other independent variables such as weather conditions, disease outbreak, and stock market trends. There is a need to explore further the capability of ensemble models or hybrid models based on deep learning methods using multi-source data, as these have been shown to improve the performance of the base model. With these ensemble models combined with multi-source data, a more comprehensive analysis of the PCC can be obtained. © 2022 Rayner Alfred et al.","","Economics; Financial markets; Food supply; Forecasting; Least squares approximations; Machine learning; Meats; Plants (botany); Shellfish; Time series; Agro foods; Food commodity; Gross domestic products; Malaysia; Neural network method; Ordinary least squares; Per capita; Percapita consumption; Performance; Square networks; Fruits","","","R. Alfred; Knowledge Technology Research Unit, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; email: ralfred@ums.edu.my","","Hindawi Limited","1574017X","","","","English","Mob. Inf. Sys.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142534964"
"","","","21st EPIA Conference on Artificial Intelligence, EPIA 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13566 LNAI","","","","","805","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138701445&partnerID=40&md5=78bac7599afc005b8fc682ffa1753e9b","","","The proceedings contain 64 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Enriching Legal Knowledge Through Intelligent Information Retrieval Techniques: A Review; region of Interest Identification in the Cervical Digital Histology Images; Audio Feature Ranking for Sound-Based COVID-19 Patient Detection; using a Siamese Network to Accurately Detect Ischemic Stroke in Computed Tomography Scans; determining Internal Medicine Length of Stay by Means of Predictive Analytics; Improving the Prediction of Age of Onset of TTR-FAP Patients Using Graph-Embedding Features; cloud-Based Privacy-Preserving Medical Imaging System Using Machine Learning Tools; an Active Learning-Based Medical Diagnosis System; comparative Evaluation of Classification Indexes and Outlier Detection of Microcytic Anaemias in a Portuguese Sample; hierarchically Structured Scheduling and Execution of Tasks in a Multi-agent Environment; A General Preprocessing Pipeline for Deep Learning on Radiology Images: A COVID-19 Case Study; automatic Configuration of Genetic Algorithm for the Optimization of Electricity Market Participation Using Sequential Model Algorithm Configuration; modeling Stand-Alone Photovoltaic Systems with Matlab/Simulink; a Learning Approach to Improve the Selection of Forecasting Algorithms in an Office Building in Different Contexts; Comparison of Different Deployment Approaches of FPGA-Based Hardware Accelerator for 3D Object Detection Models; generating the Users Geographic Map Using Mobile Phone Data; driver Equitability and Customer Optimality in Intelligent Vehicle Applications; Assessing Communication Strategies in C-ITS Using n-Person Prisoner’s Dilemma; on Demand Waste Collection for Smart Cities: A Case Study; LoRaWAN Module for the Measurement of Environmental Parameters and Control of Irrigation Systems for Agricultural and Livestock Facilities; content-Based Lawsuits Document Image Retrieval; diabetic-Friendly Multi-agent Recommendation System for Restaurants Based on Social Media Sentiment Analysis and Multi-criteria Decision Making; On Developing Ethical AI.","","","","","","Marreiros G.; Martins B.; Paiva A.; Sardinha A.; Ribeiro B.","Springer Science and Business Media Deutschland GmbH","03029743","978-303116473-6","","","English","Lect. Notes Comput. Sci.","Conference review","Final","","Scopus","2-s2.0-85138701445"
"Alanezi M.A.; Shahriar M.S.; Hasan M.B.; Ahmed S.; Sha'aban Y.A.; Bouchekara H.R.E.H.","Alanezi, Mohammed A. (54583201800); Shahriar, Mohammad Shoaib (56073493100); Hasan, Md. Bakhtiar (57247978600); Ahmed, Sabbir (57223210153); Sha'aban, Yusuf A. (57220063346); Bouchekara, Houssem R. E. H. (24066258400)","54583201800; 56073493100; 57247978600; 57223210153; 57220063346; 24066258400","Livestock Management With Unmanned Aerial Vehicles: A Review","2022","IEEE Access","10","","","45001","45028","27","45","10.1109/ACCESS.2022.3168295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129147295&doi=10.1109%2fACCESS.2022.3168295&partnerID=40&md5=fe5fb1da8af86c6d270d163513757f4a","University Of Hafr Al Batin, Department Of Computer Science And Engineering, Hafr Al Batin, 31991, Saudi Arabia; University Of Hafr Al Batin, Department Of Electrical Engineering, Hafr Al Batin, 31991, Saudi Arabia; Islamic University Of Technology, Department Of Computer Science And Engineering, Gazipur, 1704, Bangladesh","Alanezi M.A., University Of Hafr Al Batin, Department Of Computer Science And Engineering, Hafr Al Batin, 31991, Saudi Arabia; Shahriar M.S., University Of Hafr Al Batin, Department Of Electrical Engineering, Hafr Al Batin, 31991, Saudi Arabia; Hasan M.B., Islamic University Of Technology, Department Of Computer Science And Engineering, Gazipur, 1704, Bangladesh; Ahmed S., Islamic University Of Technology, Department Of Computer Science And Engineering, Gazipur, 1704, Bangladesh; Sha'aban Y.A., University Of Hafr Al Batin, Department Of Electrical Engineering, Hafr Al Batin, 31991, Saudi Arabia; Bouchekara H.R.E.H., University Of Hafr Al Batin, Department Of Electrical Engineering, Hafr Al Batin, 31991, Saudi Arabia","The ease of use and advancements in drone technology is resulting in the widespread application of Unmanned Aerial Vehicles (UAVs) to diverse fields, making it a booming technology. Among UAVs' several applications, livestock agriculture is one of the most promising, where UAVs facilitate various operations for efficient animal management. But the field is characterized by multiple environmental, technical, economic, and strategic challenges. However, the use of advanced technological techniques like Artificial Intelligence (AI), Internet of Things (IoT), Machine Learning (ML), Deep Learning (DL), advanced sensors, etc., along with the assurance of animal welfare while operating the UAVs, can lead to widespread adoption of drone technology amongst livestock farmers. This paper discusses livestock management research where UAVs monitor farm animals via detection, counting, tracking animals, etc. In this article, an attempt has been made to elucidate different aspects and broader issues around livestock management while highlighting the associated challenges, opportunities, and prospects. This work is the first review paper on the subject matter with all the necessary information and analysis, to the best of our knowledge. Therefore, the article promises to provide interested researchers with detailed information about the field, guiding future research.  © 2013 IEEE.","Cattle; Livestock agriculture; Livestock management; Unmanned aerial vehicle","Agricultural technology; Aircraft detection; Animals; Antennas; Deep learning; Engineering education; Internet of things; Aerial vehicle; Cattle; Diverse fields; Ease-of-use; Economic challenges; Environmental challenges; Livestock agriculture; Livestock management; Technical challenges; Unmanned aerial vehicle; Drones","","","H.R.E.H. Bouchekara; University Of Hafr Al Batin, Department Of Electrical Engineering, Hafr Al Batin, 31991, Saudi Arabia; email: bouchekara.houssem@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129147295"
"Wang K.; Wu P.; Cui H.; Xuan C.; Su H.","Wang, Kui (57219200941); Wu, Pei (7403119561); Cui, Hongmei (56039161100); Xuan, Chuanzhong (36618394100); Su, He (56032339000)","57219200941; 7403119561; 56039161100; 36618394100; 56032339000","Identification and classification for sheep foraging behavior based on acoustic signal and deep learning","2021","Computers and Electronics in Agriculture","187","","106275","","","","32","10.1016/j.compag.2021.106275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108347458&doi=10.1016%2fj.compag.2021.106275&partnerID=40&md5=22c65f3f708535c149ec40f7b59a1d1d","College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China","Wang K., College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China; Wu P., College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China; Cui H., College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China; Xuan C., College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China; Su H., College of Mechanical and Electrical Engineering of Inner, Mongolia Agricultural University, Inner Mongolia Engineering Research Center for Intelligent Facilities on Prataculture and Aquaculture, Hohhot, 010018, China","It is very significant to monitor livestock foraging behavior accurately and in real-time to further improve pasture management and livestock welfare. Although various algorithms have been developed to identify and classify animals' foraging behavior, it still has room to be improved in generality and function. In this study, a representative acoustic dataset generated by typical bodyweight sheep when grazing on various grasses and subsequent ruminating was created. Then an algorithm for the identification and classification of foraging behavior was proposed based on feature extraction technique and deep learning. Specifically, all prominent fragments in the acoustic signal were identified as events by the identification algorithm, and the events were classified as noise, chew, bite, chew-bite, or ruminating behavior through the classification model. The effect of each parameter on the identification algorithm was analyzed, and an optimal set of parameters was derived. As a result, the accuracy of 96.13% was achieved by the identification algorithm with the optimal parameters. Meanwhile, the performances of three common deep network models, including deep neural network (DNN), convolutional neural network (CNN), and recurrent neural network (RNN), were compared. The results showed that the RNN, CNN, and DNN model's accuracy were 93.17%, 92.53%, and 79.43%, respectively. The RNN model had a stronger classification capacity than the CNN model since it involved the inter-dependent information of adjacent events. The CNN model achieved a superior classification performance than the DNN model because the log-scaled Mel-spectrogram representation of the event waveform was more effective than the waveform itself. The algorithm proposed in this study could be well applied to identify and classify all foraging behaviors of typical weight sheep foraging freely on various grasses in the future. © 2021 Elsevier B.V.","Acoustic monitoring; Deep learning; Foraging behavior; Free-ranging sheep; Signal processing","Poaceae; Varanidae; Acoustic waves; Agriculture; Classification (of information); Recurrent neural networks; Signal processing; Acoustic monitoring; Acoustic signals; Convolutional neural network; Deep learning; Foraging behaviours; Free-ranging sheep; Identification algorithms; Neural network modelling; Neural-networks; Signal-processing; algorithm; artificial neural network; detection method; foraging behavior; identification method; livestock farming; Deep neural networks","National Natural Science Foundation of China, NSFC","We thankfully acknowledge the funding support through the projects (Grant No. 31860666) provided by the National Natural Science Foundation of China for carrying out this research work. We would like to thank Feilong Liu and Ding Han for their help during the data collection phase. Thanks to Yunqi Meng and Xiantao Fan for their help with data labeling. Thanks to Jing Xue for revising the paper.","","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85108347458"
"Mudau F.; Zyl T.L.V.; Molotsi A.H.; Waldmann P.; Dzama K.; Marufu M.C.","Mudau, Fhulufhelo (57878977800); Zyl, Terence L Van (26532116600); Molotsi, Annelin H (57194016322); Waldmann, Patrik (6603822225); Dzama, Kennedy (8708119000); Marufu, Munyaradzi C (30267881500)","57878977800; 26532116600; 57194016322; 6603822225; 8708119000; 30267881500","Application of Convolutional Neural Networks to the Quantification of Tick Burdens on Cattle Using Infrared Thermographic Imaging","2022","2022 IST-Africa Conference, IST-Africa 2022","","","","","","","1","10.23919/IST-Africa56635.2022.9845623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137480759&doi=10.23919%2fIST-Africa56635.2022.9845623&partnerID=40&md5=5f30c9acb3b5d3a8717574e6fce33dbc","Stellenbosch University, P. Bag X1, Matieland, 7602, South Africa; University of Johannesburg, Johannesburg, South Africa; FI-90014 University of Oulu, Research Unit of Mathematical Sciences, P.O.Box 8000, Oulu, 90570, Finland; University of Pretoria, Department of Veterinary Tropical Diseases, Private Bag X4, Pretoria, Onderstepoort, 0110, South Africa","Mudau F., Stellenbosch University, P. Bag X1, Matieland, 7602, South Africa; Zyl T.L.V., University of Johannesburg, Johannesburg, South Africa; Molotsi A.H., Stellenbosch University, P. Bag X1, Matieland, 7602, South Africa; Waldmann P., FI-90014 University of Oulu, Research Unit of Mathematical Sciences, P.O.Box 8000, Oulu, 90570, Finland; Dzama K., Stellenbosch University, P. Bag X1, Matieland, 7602, South Africa; Marufu M.C., University of Pretoria, Department of Veterinary Tropical Diseases, Private Bag X4, Pretoria, Onderstepoort, 0110, South Africa","Ticks and tick-borne diseases (TTBDs) are one of the biggest economic threats to livestock production systems in the world endangering approximately 80% of the global cattle population, especially in the sub- and tropical regions. It remains a challenge to effectively control ticks with acaricides due to the ability of ticks to develop resistance against acaricides. Algorithms for a cheap, rapid, and accurate method of quantifying tick burdens on cattle using infrared thermographic imaging technology could mitigate the danger of TTBDs in cattle. Tick counts were conducted once a month under natural challenge over a six-month period on 19 Bonsmara and 36 Nguni cattle located at ARC Roodeplaat and Loskop farms throughout both warmer climates and cooler climates. Thermographic images of both engorged & unfed females and males ticks were taken from cattle from February 2021 until July 2021. The deep learning models with architectures: 'ConvNet' and 'MobileNet' were trained on a dataset of 1124 'thermograms' to detect ticks on cattle. ConvNet model achieved a training and validation accuracy of ∼ 90 and 60%, respectively. Whereas MobileNet scored a training and validation accuracy of ∼ 95 and 75%, respectively. Finally, deep learning was successfully used to detect ticks on cattle using pretrained convolutional neural networks (CNNS). © 2022 IST-Africa Institute and Authors.","Algorithms; convolutional neural networks; infrared thermographic imaging","Agriculture; Convolution; Deep learning; Acaricide; Convnet; Convolutional neural network; Imaging technology; Infrared thermographic imaging; Livestock production; Production system; Sub-regions; Tropical regions; Warm climates; Convolutional neural networks","","","","Cunningham M.; Cunningham P.","Institute of Electrical and Electronics Engineers Inc.","","978-190582469-4","","","English","IST-Africa Conf., IST-Africa","Conference paper","Final","","Scopus","2-s2.0-85137480759"
"Wen C.; Zhang X.; Wu J.; Yang C.; Li Z.; Shi L.; Yu H.","Wen, Changji (54792355900); Zhang, Xiaoran (57383685200); Wu, Jianshuang (57258091800); Yang, Ce (8297096200); Li, Zhuoshi (57206872801); Shi, Lei (58384157900); Yu, Helong (55510987100)","54792355900; 57383685200; 57258091800; 8297096200; 57206872801; 58384157900; 55510987100","Pig facial expression recognition using multi-attention cascaded LSTM model; [基于多注意力机制级联LSTM模型的猪脸表情识别]","2021","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","37","12","","181","190","9","9","10.11975/j.issn.1002-6819.2021.12.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114870900&doi=10.11975%2fj.issn.1002-6819.2021.12.021&partnerID=40&md5=f93ac60a671b99df8d946b483cf42fcd","College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China; College of Food, Agricultural and Natural Resource Sciences, University of Minnesota, Paul, 55108, United States; Institute for the Smart Agriculture, Jilin Agricultural University, Changchun, 130118, China; Key Laboratory of Bionic Engineering (Ministry of Education), Jilin University, Changchun, 130022, China","Wen C., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China, Institute for the Smart Agriculture, Jilin Agricultural University, Changchun, 130118, China; Zhang X., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China; Wu J., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China; Yang C., College of Food, Agricultural and Natural Resource Sciences, University of Minnesota, Paul, 55108, United States; Li Z., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China, Key Laboratory of Bionic Engineering (Ministry of Education), Jilin University, Changchun, 130022, China; Shi L., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China, Institute for the Smart Agriculture, Jilin Agricultural University, Changchun, 130118, China; Yu H., College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China, Institute for the Smart Agriculture, Jilin Agricultural University, Changchun, 130118, China","Facial expression recognition has widely been used in various life scenarios, such as medicine, criminology, education, and deep learning. Deep learning also makes this technology highly efficient and accurate at present. Much effort has been made to consider the migration of relatively mature facial recognition to animal expressions. The reason was that animals can also express their emotions through facial expressions, according to zoologists. Once the complex emotions expressed by animals can be understood, the incidence of injuries and illnesses can be early monitored in the freedom of animal expressions, thereby maintaining a happy mood for a long time, without hunger, thirst, and worries in a fully guaranteed life. As such, facial expressions can be expected to evaluate animal welfare, due mainly to a comprehensive reflection of physiology, psychology, and behavior of livestock. However, it is difficult to recognize the subtle changes in different areas of facial expressions, particularly for the simple tissue structure of facial muscles in domestic animals. In this study, a Multi-Attention cascaded Long Short Term Memory (MA-LSTM) model was proposed for the recognition of pig facial expression. The specific procedure was as follows: firstly, a simplified multi-task convolution neural network (SMTCNN) was used to detect and then locate the pig face in the frame image, where the influence of the non-pig face region on the recognition performance was removed. Secondly, a multi-attention mechanism was introduced to characterize various feature channels with different visual information and peak response regions. The facial salient regions caused by the changes of facial expression were captured via clustering the regions with similar peak responses. Then the facial salient regions were used to focus on subtle changes in the pig face. Finally, the convolution and attention features were fused and subsequently input into LSTM to classify the data. Data enhancement was performed on the original dataset, thereby obtaining a self-annotated expression dataset of domestic pigs. The expanded datasets were then utilized in the experiments. The experimental results showed that the recognition accuracy of the module with closing the multi-attention mechanism increased by 6.3 percentage points on average, while the misclassification rate was also reduced significantly, compared with the MA-LSTM model. Additionally, the average recognition accuracy of the MA-LSTM model increased by about 32.6, 18.0, 5.9, and 4.4 percentage points, respectively, compared with commonly-used facial video expression recognition. Four types of expressions were classified in visualization, such as anger, happiness, fear, and neutral. Specifically, there was a more obvious variation in the facial area of domestic pigs that was caused by anger and happiness, where the recognition accuracy was higher than others. Nevertheless, the misclassification rate was also higher, due mainly to the fact that the changes of two areas were relatively similar. In any way, the proposed MA-LSTM model was also verified by all the test data in pig face recognition. © 2021, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Animal welfare; Facial expression recognition; Long short-term memory network; Models; Multi-attention mechanism; Multi-task cascaded convolutional network; Pig","Agriculture; Behavioral research; Classification (of information); Convolution; Deep learning; Face recognition; Mammals; Attention mechanisms; Convolution neural network; Facial expression recognition; Facial Expressions; Facial recognition; Misclassification rates; Recognition accuracy; Visual information; Long short-term memory","","","H. Yu; College of Information and Technology, Jilin Agricultural University, Changchun, 130118, China; email: 264496469@qq.com","","Chinese Society of Agricultural Engineering","10026819","","NGOXE","","Chinese","Nongye Gongcheng Xuebao","Article","Final","","Scopus","2-s2.0-85114870900"
"Qiao Y.; Xue T.; Kong H.; Clark C.; Lomax S.; Rafique K.; Sukkarieh S.","Qiao, Yongliang (56486770900); Xue, Tengfei (57367852100); Kong, He (57203456679); Clark, Cameron (7403546385); Lomax, Sabrina (56151402400); Rafique, Khalid (57310787500); Sukkarieh, Salah (6602844626)","56486770900; 57367852100; 57203456679; 7403546385; 56151402400; 57310787500; 6602844626","One-Shot Learning with Pseudo-Labeling for Cattle Video Segmentation in Smart Livestock Farming","2022","Animals","12","5","558","","","","7","10.3390/ani12050558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131311730&doi=10.3390%2fani12050558&partnerID=40&md5=ad27a6cecd7fc4518497237009b6508f","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Xue T., School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Kong H., Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Clark C., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia; Lomax S., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, 2006, NSW, Australia; Rafique K., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, NSW, Australia","Computer vision-based technologies play a key role in precision livestock farming, and video-based analysis approaches have been advocated as useful tools for automatic animal monitoring, behavior analysis, and efficient welfare measurement management. Accurately and efficiently segmenting animals' contours from their backgrounds is a prerequisite for vision-based technologies. Deep learning-based segmentation methods have shown good performance through training models on a large amount of pixel-labeled images. However, it is challenging and time-consuming to label animal images due to their irregular contours and changing postures. In order to reduce the reliance on the number of labeled images, one-shot learning with a pseudo-labeling approach is proposed using only one labeled image frame to segment animals in videos. The proposed approach is mainly comprised of an Xception-based Fully Convolutional Neural Network (Xception-FCN) module and a pseudo-labeling (PL) module. Xception-FCN utilizes depth-wise separable convolutions to learn different-level visual features and localize dense prediction based on the one single labeled frame. Then, PL leverages the segmentation results of the Xception-FCN model to fine-tune the model, leading to performance boosts in cattle video segmentation. Systematic experiments were conducted on a challenging feedlot cattle video dataset acquired by the authors, and the proposed approach achieved a mean intersection-over-union score of 88.7% and a contour accuracy of 80.8%, outperforming state-of-the-art methods (OSVOS and OSMN). Our proposed one-shot learning approach could serve as an enabling component for livestock farming-related segmentation and detection applications. Simple Summary: Deep learning-based segmentation methods rely on large-scale pixel-labeled datasets to achieve good performance. However, it is resource-costly to label animal images due to their irregular contours and changing postures. To keep a balance between segmentation accuracy and speed using limited label data, we propose a one-shot learning-based approach with pseudo-labeling to segment animals in videos, relying on only one labeled frame. Experiments were conducted on a challenging feedlot cattle video dataset acquired by the authors, and the results show that the proposed method outperformed state-of-the-art methods such as one-shot video object segmentation (OSVOS) and one-shot modulation network (OSMN). Our proposed one-shot learning with pseudolabeling reduces the reliance on labeled data and could serve as an enabling component for smart farming-related applications. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; One-shot learning; Precision livestock farming; Pseudo-labeling; Video segmentation","agricultural worker; article; body position; bovine; convolutional neural network; deep learning; human; learning; livestock; nonhuman; prediction; velocity; videorecording","Meat and Livestock Australia Donor Company, (P.PSH.0819)","Funding: This research was funded by the Meat and Livestock Australia Donor Company (grant number P.PSH.0819).","Y. Qiao; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, Sydney, 2006, Australia; email: y.qiao@acfr.usyd.edu.au","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131311730"
"Prabu K.; Sudhakar P.","Prabu, K. (57220026785); Sudhakar, P. (57200618531)","57220026785; 57200618531","Design and Implementation of an Automated Control System for Anomaly Detection Using an Enhanced Intrusion Detection System","2022","Proceedings of the 3rd International Conference on Smart Technologies in Computing, Electrical and Electronics, ICSTCEE 2022","","","","","","","2","10.1109/ICSTCEE56972.2022.10100003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158160013&doi=10.1109%2fICSTCEE56972.2022.10100003&partnerID=40&md5=dc7da61b4e9266eaea2d010f59bd50be","School of Computing Science and Engineering, Galgotias University, Greater Noida, India","Prabu K., School of Computing Science and Engineering, Galgotias University, Greater Noida, India; Sudhakar P., School of Computing Science and Engineering, Galgotias University, Greater Noida, India","In forest zone and the field of agriculture, the conflict between human beings and wild animals is a vital problem to be addressed since it results in the extinction of various resources and the lives of human being is also in danger. The farmers are the major victims in such issues where they lose their crops to be cultivated, livestock, other related properties and resources. Henceforth, such regions and zones should be monitored round the clock to avoid the intrusion of any wild animal that could probably create a negative impact in the field and human lives. The Smart intrusion detection system can be implemented in the field with sensors and controllers that can detect the presence and movements of anonymous objects. This smart system will be implemented with a mechanism to detect and classify the objects with deep learning algorithm on further running of the system, using Content-Based Image Retrieval (CBIR) the captured image is investigated and classified based on the extracted features and alarm only if the intruder is recognized to be an unauthenticated object. Instant notifications will be triggered to the farmers through their mobile applications and the intruder will be reinforced to move out of the field with the alarming sound, which could annoy the object based on its type. © 2022 IEEE.","Animal detection; Deep learning; Intrusion detection; IoT; Sensors","Agriculture; Animals; Anomaly detection; Computer crime; Deep learning; Internet of things; Learning algorithms; Object detection; Search engines; Signal detection; Animal detection; Anomaly detection; Automated control systems; Deep learning; Design and implementations; Human being; Intrusion Detection Systems; Intrusion-Detection; IoT; Wild animals; Intrusion detection","","","","Divakar B.P.; Hulipalled V.R.; Kodabagi M.M.; Devanathan M.; Parthasarathy G.","Institute of Electrical and Electronics Engineers Inc.","","978-166545664-7","","","English","Proc. Int. Conf. Smart Technol. Comput., Electr. Electron., ICSTCEE","Conference paper","Final","","Scopus","2-s2.0-85158160013"
"Sarwar F.; Griffin A.; Rehman S.U.; Pasang T.","Sarwar, Farah (57224466004); Griffin, Anthony (55708552200); Rehman, Saeed Ur (57193652261); Pasang, Timotius (56962784100)","57224466004; 55708552200; 57193652261; 56962784100","Detecting sheep in UAV images","2021","Computers and Electronics in Agriculture","187","","106219","","","","36","10.1016/j.compag.2021.106219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107703965&doi=10.1016%2fj.compag.2021.106219&partnerID=40&md5=0f7aab1056c9681faa50d851862bfca1","Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; College of Science and Engineering, Flinders University, South Australia, Australia; Department of Manufacturing and Mechanical Engineering and Technology, Oregon Institute of Technology, Oregon, United States","Sarwar F., Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; Griffin A., Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; Rehman S.U., College of Science and Engineering, Flinders University, South Australia, Australia; Pasang T., Department of Manufacturing and Mechanical Engineering and Technology, Oregon Institute of Technology, Oregon, United States","In the last decade, researchers have focused more on deep convolutional neural networks (CNNs) than other machine learning algorithms for object detection, localization, classification and segmentation. Such CNNs have achieved remarkable results in these fields and use the bounding boxes as the ground truth data. In this research article, we have used a fully connected network (FCN) for livestock detection in aerial images captured by an unmanned aerial vehicle (UAV), that used centroids as ground truth data. For performance evaluation and comparison, we have proposed a single-layered and a seven-layered CNN network in this article. These proposed networks are trained using state-of-the-art method, Region-based CNN. In addition, AlexNet, GoogLeNet, VGG16, VGG19 and ResNet50 were also fine-tuned for livestock detection. The results of the FCN and one of our proposed networks are then merged to improve the recall of the complete system from 90% to 98%. © 2021","Deep learning; Livestock; Object detection; UAV","Agriculture; Aircraft detection; Antennas; Deep neural networks; Learning algorithms; Network layers; Object recognition; Unmanned aerial vehicles (UAV); Aerial vehicle; Convolutional neural network; Deep learning; Fully connected networks; Ground truth data; Machine learning algorithms; Object localization; Objects detection; Objects segmentation; Vehicle images; algorithm; artificial neural network; detection method; image analysis; machine learning; sheep; unmanned vehicle; Object detection","","","A. Griffin; Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; email: anthony.griffin@aut.ac.nz","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85107703965"
"Said Mohamed E.; Belal A.A.; Kotb Abd-Elmabod S.; El-Shirbeny M.A.; Gad A.; Zahran M.B.","Said Mohamed, Elsayed (55418870300); Belal, A.A. (36026682300); Kotb Abd-Elmabod, Sameh (36781947600); El-Shirbeny, Mohammed A (53663373000); Gad, A. (7005090935); Zahran, Mohamed B (55658056059)","55418870300; 36026682300; 36781947600; 53663373000; 7005090935; 55658056059","Smart farming for improving agricultural management","2021","Egyptian Journal of Remote Sensing and Space Science","24","3","","971","981","10","258","10.1016/j.ejrs.2021.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114414365&doi=10.1016%2fj.ejrs.2021.08.007&partnerID=40&md5=01bb6f97e92df4227088ff3761454dd4","National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; Soils & Water Use Department, Agricultural and Biological Research Division, National Research Centre, Cairo, 12622, Egypt","Said Mohamed E., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; Belal A.A., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; Kotb Abd-Elmabod S., Soils & Water Use Department, Agricultural and Biological Research Division, National Research Centre, Cairo, 12622, Egypt; El-Shirbeny M.A., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; Gad A., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; Zahran M.B., National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt","The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to high-speed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector. © 2021","5G; Decision support systems; IoT; Smart Agriculture; Smart sensing","5G mobile communication systems; Agricultural robots; Aircraft detection; Antennas; Data transfer; Decision support systems; Deep learning; Developing countries; Digital storage; Facings; Irrigation; Population statistics; Real time systems; Unmanned aerial vehicles (UAV); Water levels; Weed control; Agricultural management; Decision management; High-speed data transfer; Internet of thing (IOT); Irrigation efficiency; Mathematical relation; Soil characteristics; Wireless communications; agricultural management; data processing; decision support system; developing world; farming system; Internet; irrigation system; sensor; Internet of things","","","E. Said Mohamed; National Authority for Remote Sensing and Space Sciences (NARSS), Cairo, 11843, Egypt; email: salama55@mail.ru","","Elsevier B.V.","11109823","","","","English","Egypt. J. Remote Sens. Space Sci.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114414365"
"Bhujel A.; Arulmozhi E.; Moon B.-E.; Kim H.-T.","Bhujel, Anil (57214077989); Arulmozhi, Elanchezhian (57214070594); Moon, Byeong-Eun (56070940700); Kim, Hyeon-Tae (8662923200)","57214077989; 57214070594; 56070940700; 8662923200","Deep-learning-based automatic monitoring of pigs’ physico-temporal activities at different greenhouse gas concentrations","2021","Animals","11","11","3089","","","","22","10.3390/ani11113089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118138527&doi=10.3390%2fani11113089&partnerID=40&md5=f1a9264b19198d2390e89d5d23361647","Department of Biosystems Engineering, Institute of Smart Farm, Gyeongsang National University, Jinju, 52828, South Korea; Ministry of Communication and Information Technology, Singha Durbar, Kathmandu, 44600, Nepal; Smart Farm Research Center, Gyeongsang National University, Jinju, 52828, South Korea","Bhujel A., Department of Biosystems Engineering, Institute of Smart Farm, Gyeongsang National University, Jinju, 52828, South Korea, Ministry of Communication and Information Technology, Singha Durbar, Kathmandu, 44600, Nepal; Arulmozhi E., Department of Biosystems Engineering, Institute of Smart Farm, Gyeongsang National University, Jinju, 52828, South Korea; Moon B.-E., Smart Farm Research Center, Gyeongsang National University, Jinju, 52828, South Korea; Kim H.-T., Department of Biosystems Engineering, Institute of Smart Farm, Gyeongsang National University, Jinju, 52828, South Korea","Pig behavior is an integral part of health and welfare management, as pigs usually reflect their inner emotions through behavior change. The livestock environment plays a key role in pigs’ health and wellbeing. A poor farm environment increases the toxic GHGs, which might deteriorate pigs’ health and welfare. In this study a computer-vision-based automatic monitoring and tracking model was proposed to detect pigs’ short-term physical activities in the compromised environment. The ventilators of the livestock barn were closed for an hour, three times in a day (07:00–08:00, 13:00–14:00, and 20:00–21:00) to create a compromised environment, which increases the GHGs level significantly. The corresponding pig activities were observed before, during, and after an hour of the treatment. Two widely used object detection models (YOLOv4 and Faster R-CNN) were trained and compared their performances in terms of pig localization and posture detection. The YOLOv4, which outperformed the Faster R-CNN model, was coupled with a Deep-SORT tracking algorithm to detect and track the pig activities. The results revealed that the pigs became more inactive with the increase in GHG concentration, reducing their standing and walking activities. Moreover, the pigs shortened their sternal-lying posture, increasing the lateral lying posture duration at higher GHG concentration. The high detection accuracy (mAP: 98.67%) and tracking accuracy (MOTA: 93.86% and MOTP: 82.41%) signify the models’ efficacy in the monitoring and tracking of pigs’ physical activities non-invasively. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Animal welfare; Deep-SORT; Faster R-CNN; Greenhouse gas; Object tracking; Pig posture detection; YOLOv4","carbon dioxide; carbon monoxide; methane; nitric oxide; nitrous oxide; algorithm; animal behavior; animal experiment; animal model; animal welfare; Article; behavior change; body position; computer model; computer vision; controlled study; deep learning; detection algorithm; emotion; environmental enrichment; environmental monitoring; female; gas chromatography; greenhouse gas; image processing; information processing; livestock; locomotion; machine learning; male; nerve cell network; nonhuman; physical activity; standing; validation process; walking distance; walking speed","Ministry of Agriculture, Food and Rural Affairs, MAFRA, (717001-7); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","Funding: This work was supported by the Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries (IPET) through the Agriculture, Food and Rural Affairs Convergence Technologies Program for Educating Creative Global Leader, funded by the Ministry of Agriculture, Food and Rural Affairs (MAFRA) (717001-7).","H.-T. Kim; Department of Biosystems Engineering, Institute of Smart Farm, Gyeongsang National University, Jinju, 52828, South Korea; email: bioani@gnu.ac.kr","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118138527"
"Wu Y.; Li Y.; Zhao Y.; Yang P.; Li Z.; Guo H.","Wu, Yufeng (56093420000); Li, Yiming (57225006490); Zhao, Yuanyang (57375123600); Yang, Pu (57710371300); Li, Zhenbo (55613118900); Guo, Hao (55331600800)","56093420000; 57225006490; 57375123600; 57710371300; 55613118900; 55331600800","Review of Research on Body Condition Score for Dairy Cows Based on Computer Vision; [基于计算机视觉的奶牛体况评分研究综述]","2021","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","52","","","268","275","7","9","10.6041/j.issn.1000-1298.2021.S0.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121300016&doi=10.6041%2fj.issn.1000-1298.2021.S0.033&partnerID=40&md5=a8fe4d53147eacd69869a0c147cd1c2a","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Wu Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Li Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Zhao Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Yang P., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Li Z., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Guo H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","At present, body condition score for dairy cows mainly adopts manual methods, but the reliability of the scoring results is poor due to manual subjectivity, and the assessment process is time-consuming and laborious, which relies heavily on the experience of experts. The development of body condition score for dairy cows has mainly gone through manual scoring stage, traditional machine learning stage and deep learning stage, the latter two can be subdivided into 2D field and 3D field research. Body condition score method for dairy cows based on machine learning mainly suffers from the problem of relying on manual markers and simply improving the method of dimensionality reduction and feature extraction, which can only be improved in specific situations, with limited improvement in results. With the rise of deep learning, researchers have begun to explore methods that do not require manually labeled features. The use of deep learning and 3D technology has further improved the accuracy of automatic body condition scoring, but in actual production, to meet the nutritional management needs of cows at different growth stages, the difference between the body condition score and the ideal score should always be maintained within ±0.25, and the accuracy of existing automatic scoring systems still has a certain gap with the ideal standard of actual farm management. The current research hotspots and theories of body condition score methods were summarized for dairy cows using computer vision by analyzing the literature and potential research directions were proposed. With the development of artificial intelligence, a large number of deep learning algorithms emerged that can be used for target detection and classification. These methods were also applicable to target detection and classification in the field of animal husbandry. In fact, artificial intelligence and deep learning techniques were increasingly being used in the livestock sector as well. Deep learning methods were needed for dairy cattle condition scoring, and as the development of agricultural information technology became more mature, research on automated body condition score methods for dairy cows based on deep learning would also become more advanced. © 2021, Chinese Society of Agricultural Machinery. All right reserved.","Body condition score; Computer vision; Dairy cow; Deep learning; Machine learning","Agriculture; Deep learning; Learning algorithms; 2D fields; Assessment process; Body condition score; Dairy cow; Deep learning; Dimensionality reduction; Field research; Manual methods; On-body; Target detection and classifications; Computer vision","","","Z. Li; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; email: lizb@cau.edu.cn","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Review","Final","","Scopus","2-s2.0-85121300016"
"Qiao Y.; Guo Y.; Yu K.; He D.","Qiao, Yongliang (56486770900); Guo, Yangyang (57200132879); Yu, Keping (56316023300); He, Dongjian (19933691800)","56486770900; 57200132879; 56316023300; 19933691800","C3D-ConvLSTM based cow behaviour classification using video data for precision livestock farming","2022","Computers and Electronics in Agriculture","193","","106650","","","","47","10.1016/j.compag.2021.106650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122303759&doi=10.1016%2fj.compag.2021.106650&partnerID=40&md5=aa6ff11cdb46c202b6ff865a54e7dcfa","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW, 2006, Australia; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; Global Information and Telecommunication Institute, Waseda University, Tokyo, 169-0072, Japan","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW, 2006, Australia; Guo Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China; Yu K., Global Information and Telecommunication Institute, Waseda University, Tokyo, 169-0072, Japan; He D., College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, 712100, China","Cow behaviour provides valuable information about animal welfare, activities and livestock production. Therefore, monitoring of behaviour is gaining importance in the improvement of animal health, fertility and production yield. However, recognizing or classifying different behaviours with high accuracy is challenging, because of the high similarity of movements among these behaviours. In this study, we propose a deep learning framework to monitor and classify dairy behaviours, which is intelligently combined with C3D (Convolutional 3D) network and ConvLSTM (Convolutional Long Short-Term Memory) to classify the five common behaviours included feeding, exploring, grooming, walking and standing. For this, 3D CNN features were firstly extracted from video frames using C3D network; then ConvLSTM was applied to further extract spatio-temporal features, and the final obtained features were fed to a softmax layer for behaviour classification. The proposed approach using 30-frame video length achieved 90.32% and 86.67% classification accuracy on calf and cow datasets respectively, which outperformed the state-of-the-art methods including Inception-V3, SimpleRNN, LSTM, BiLSTM and C3D. Additionally, the influence of video length on behaviour classification was also investigated. It was found that increasing video sequence length to 30-frames enhanced classification performance. Extensive experiments show that combining C3D and ConvLSTM together can improve video-based behaviour classification accuracy noticeably using spatial–temporal features, which enables automated behaviour classification for precision livestock farming. © 2022 Elsevier B.V.","3D CNN; Behaviour classification; Deep learning; LSTM; Precision livestock farming","Agriculture; Animals; Classification (of information); Convolutional neural networks; Long short-term memory; Video recording; 3d CNN; 3D networks; Animal welfare; Behaviour classification; Classification accuracy; Cow behavior; Deep learning; LSTM; Precision livestock farming; Video data; animal welfare; crop yield; data set; experimental study; livestock farming; videography; Convolution","National Natural Science Foundation of China, NSFC, (61473235); National Key Research and Development Program of China, NKRDPC, (2017YFD0701603)","The authors express their gratitude to Kaixuan Zhao for their help with data collection. Also particular thanks to other members of the team for their involvement and efforts in the whole experiment organization and information collection. The authors also acknowledge the support by the project: National Natural Science Foundation of China (Grant No. 61473235) and the National Key Technology R&D Program of China (Grant No. 2017YFD0701603).","Y. Guo; College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, 712100, China; email: gyy_113529@nwsuaf.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85122303759"
"Shanthakumari R.; Nalini C.; Vinothkumar S.; Govindaraj B.; Dharani S.; Chindhana S.","Shanthakumari, R. (57208394448); Nalini, C. (57202704763); Vinothkumar, S. (57211206987); Govindaraj, B. (57656388800); Dharani, S. (57222117692); Chindhana, S. (57656713900)","57208394448; 57202704763; 57211206987; 57656388800; 57222117692; 57656713900","Image Detection and Recognition of different species of animals using Deep Learning","2022","2022 International Mobile and Embedded Technology Conference, MECON 2022","","","","236","241","5","7","10.1109/MECON53876.2022.9752203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129203370&doi=10.1109%2fMECON53876.2022.9752203&partnerID=40&md5=0f70d59583c20c7761263d2d64a3861c","Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India","Shanthakumari R., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India; Nalini C., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India; Vinothkumar S., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India; Govindaraj B., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India; Dharani S., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India; Chindhana S., Kongu Engineering College Perundurai, Department of Information Technology, Tamil Nadu, Erode, India","Deep Learning has grown in popularity as a method for solving computer vision difficulties. Deep learning models are becoming increasingly efficient in solving complex real-world issues, as evidenced by numerous recent articles. However, training the networks slowly takes time and money for certain real-world situations that are difficult to handle with deep learning. It is vital to find ways to collect data rapidly so that it can use it in real-time applications. With a growing awareness of the need for habitat conservation, researchers are examining animal density, presence, and absence to determine the visibility of endangered species. Animal detection tools are required to estimate and monitor the quantity of animals in a specific area. This can be considered a subset of environmental monitoring. Camera-based technologies, as well as acoustic and seismic measures, can all be used to detect animals. Because cameras are a helpful tool for monitoring wildlife behaviour and assessing ecosystems, they have a long history of use. Animals, on the other hand, can move quickly and conceal themselves to avoid being spotted. The YOLOV5 model could be used to detect several types of animals in the forest. From photos, real-time camera feeds, and recorded videos, the YOLOV5 model can recognize horses, sheep, cows, elephants, bears, zebras, and giraffes etc. It also has the ability to detect birds. Objects can be detected in real time by YOLOV5 using a camera feed attached to the CPU via USB connection. The advantages of the YOLOv5 network's is lightweight so it reduces the deployment cost of the identification model. Furthermore, animal detection may be required for a variety of purposes, including livestock management, wildlife management, pest control, and security.  © 2022 IEEE.","Camera; Convolutional Neural Networks (CNN); Deep learning; Object Detection; You Look Only Once Version 5(YOLOV5)","Agriculture; Conservation; Convolutional neural networks; Deep learning; Mammals; Object detection; Object recognition; Convolutional neural network; Deep learning; Image detection; Learning models; Objects detection; Real- time; Real-world; Vision difficulties; You look only once version 5; Cameras","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166542020-4","","","English","Int. Mob. Embed. Technol. Conf., MECON","Conference paper","Final","","Scopus","2-s2.0-85129203370"
"Li S.; Fu L.; Sun Y.; Mu Y.; Chen L.; Li J.; Gong H.","Li, Shijun (57388569500); Fu, Lili (57358513300); Sun, Yu (57867909400); Mu, Ye (57204363173); Chen, Lin (57358657700); Li, Ji (57223106563); Gong, He (57206748744)","57388569500; 57358513300; 57867909400; 57204363173; 57358657700; 57223106563; 57206748744","Individual dairy cow identification based on lightweight convolutional neural network","2021","PLoS ONE","16","11 November","e0260510","","","","20","10.1371/journal.pone.0260510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120381721&doi=10.1371%2fjournal.pone.0260510&partnerID=40&md5=f8a5bd0efd3fd781e4e0d1cc48698b7a","College of Electronic and Information Engineering, Wuzhou University, Wuzhou, China; College of Information Technology, Jilin Agricultural University, Changchun, China; Jilin Province Agricultural Internet of Things Technology, Collaborative Innovation Center, Changchun, China; Jilin Province Intelligent Environmental Engineering Research Center, Changchun, China; Jilin Province Information Technology and Intelligent Agricultural Engineering Research Center, Changchun, China","Li S., College of Electronic and Information Engineering, Wuzhou University, Wuzhou, China; Fu L., College of Information Technology, Jilin Agricultural University, Changchun, China; Sun Y., College of Information Technology, Jilin Agricultural University, Changchun, China, Jilin Province Agricultural Internet of Things Technology, Collaborative Innovation Center, Changchun, China, Jilin Province Intelligent Environmental Engineering Research Center, Changchun, China, Jilin Province Information Technology and Intelligent Agricultural Engineering Research Center, Changchun, China; Mu Y., College of Information Technology, Jilin Agricultural University, Changchun, China, Jilin Province Agricultural Internet of Things Technology, Collaborative Innovation Center, Changchun, China, Jilin Province Intelligent Environmental Engineering Research Center, Changchun, China, Jilin Province Information Technology and Intelligent Agricultural Engineering Research Center, Changchun, China; Chen L., College of Information Technology, Jilin Agricultural University, Changchun, China; Li J., College of Information Technology, Jilin Agricultural University, Changchun, China; Gong H., College of Information Technology, Jilin Agricultural University, Changchun, China, Jilin Province Agricultural Internet of Things Technology, Collaborative Innovation Center, Changchun, China, Jilin Province Intelligent Environmental Engineering Research Center, Changchun, China, Jilin Province Information Technology and Intelligent Agricultural Engineering Research Center, Changchun, China","In actual farms, individual livestock identification technology relies on large models with slow recognition speeds, which seriously restricts its practical application. In this study, we use deep learning to recognize the features of individual cows. Alexnet is used as a skeleton network for a lightweight convolutional neural network that can recognise individual cows in images with complex backgrounds. The model is improved for multiple multiscale convolutions of Alexnet using the short-circuit connected BasicBlock to fit the desired values and avoid gradient disappearance or explosion. An improved inception module and attention mechanism are added to extract features at multiple scales to enhance the detection of feature points. In experiments, side-view images of 13 cows were collected. The proposed method achieved 97.95% accuracy in cow identification with a single training time of only 6 s, which is one-sixth that of the original Alexnet. To verify the validity of the model, the dataset and experimental parameters were kept constant and compared with the results of Vgg16, Resnet50, Mobilnet V2 and GoogLenet. The proposed model ensured high accuracy while having the smallest parameter size of 6.51 MB, which is 1.3 times less than that of the Mobilnet V2 network, which is famous for its light weight. This method overcomes the defects of traditional methods, which require artificial extraction of features, are often not robust enough, have slow recognition speeds, and require large numbers of parameters in the recognition model. The proposed method works with images with complex backgrounds, making it suitable for actual farming environments. It also provides a reference for the identification of individual cows in images with complex backgrounds. Copyright: © 2021 Li et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","","Algorithms; Animals; Cattle; Dairying; Female; Image Processing, Computer-Assisted; Neural Networks, Computer; agricultural worker; animal experiment; article; attention; convolutional neural network; dairy cattle; deep learning; explosion; extraction; nonhuman; residual neural network; skeleton; validity; velocity; algorithm; anatomy and histology; animal; bovine; dairying; female; image processing; physiology; procedures","Jilin Province Development and Reform Commission, (2019C021); Jilin Province Development and Reform Commission; Ministry of Science and Technology of the People's Republic of China, MOST, (2018YFF0213606-03); Ministry of Science and Technology of the People's Republic of China, MOST; Department of Science and Technology of Jilin Province, (20210202128NC); Department of Science and Technology of Jilin Province","This research is supported by the Science and Technology Department of Jilin Province [20210202128NC]; The People’s Republic of China Ministry of Science and Technology [2018YFF0213606-03]; Jilin Province Development and Reform Commission[2019C021]. They play the role of providing paper layout fee in the paper.","H. Gong; College of Information Technology, Jilin Agricultural University, Changchun, China; email: gonghe@jlau.edu.cn","","Public Library of Science","19326203","","POLNC","34843562","English","PLoS ONE","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120381721"
"Angarita-Zapata J.S.; Alonso-Vicario A.; Masegosa A.D.; Legarda J.","Angarita-Zapata, Juan S. (57194111013); Alonso-Vicario, Ainhoa (57200854385); Masegosa, Antonio D. (24344824700); Legarda, Jon (24175877500)","57194111013; 57200854385; 24344824700; 24175877500","A taxonomy of food supply chain problems from a computational intelligence perspective","2021","Sensors","21","20","6910","","","","18","10.3390/s21206910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117710418&doi=10.3390%2fs21206910&partnerID=40&md5=e3bd935f09705110f3c43b0f0ee05974","Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain; Ikerbasque, Basque Foundation for Science, Bilbao, 48009, Spain","Angarita-Zapata J.S., Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain; Alonso-Vicario A., Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain; Masegosa A.D., Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain, Ikerbasque, Basque Foundation for Science, Bilbao, 48009, Spain; Legarda J., Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain","In the last few years, the Internet of Things, and other enabling technologies, have been progressively used for digitizing Food Supply Chains (FSC). These and other digitalization-enabling technologies are generating a massive amount of data with enormous potential to manage supply chains more efficiently and sustainably. Nevertheless, the intricate patterns and complexity embedded in large volumes of data present a challenge for systematic human expert analysis. In such a datadriven context, Computational Intelligence (CI) has achieved significant momentum to analyze, mine, and extract the underlying data information, or solve complex optimization problems, striking a balance between productive efficiency and sustainability of food supply systems. Although some recent studies have sorted the CI literature in this field, they are mainly oriented towards a single family of CI methods (a group of methods that share common characteristics) and review their application in specific FSC stages. As such, there is a gap in identifying and classifying FSC problems from a broader perspective, encompassing the various families of CI methods that can be applied in different stages (from production to retailing) and identifying the problems that arise in these stages from a CI perspective. This paper presents a new and comprehensive taxonomy of FSC problems (associated with agriculture, fish farming, and livestock) from a CI approach; that is, it defines FSC problems (from production to retail) and categorizes them based on how they can be modeled from a CI point of view. Furthermore, we review the CI approaches that are more commonly used in each stage of the FSC and in their corresponding categories of problems. We also introduce a set of guidelines to help FSC researchers and practitioners to decide on suitable families of methods when addressing any particular problems they might encounter. Finally, based on the proposed taxonomy, we identify and discuss challenges and research opportunities that the community should explore to enhance the contributions that CI can bring to the digitization of the FSC. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Agriculture; Computational intelligence; Deep learning; Fish farming; Food supply chain; Fuzzy systems; Livestock; Machine learning; Meta-heuristics; Neural networks; Probabilistic methods","Agriculture; Animals; Artificial Intelligence; Food; Food Supply; Humans; Technology; Complex networks; Deep learning; Fish; Food supply; Fuzzy systems; Heuristic methods; Supply chains; Taxonomies; Computational intelligence methods; Deep learning; Enabling technologies; Fish farming; Food supply chain; Human expert; Large volumes; Metaheuristic; Neural-networks; Probabilistic methods; agriculture; animal; artificial intelligence; catering service; food; human; technology; Agriculture","Horizon 2020 Framework Programme, H2020, (101000617, 861540); Horizon 2020 Framework Programme, H2020; Ministerio de Ciencia e Innovación, MICINN, (PID2019-109393RA-I00); Ministerio de Ciencia e Innovación, MICINN","Funding: This work has been funded by the European Union’s Horizon 2020 Research and Innovation Programme under Grants 101000617 and 861540. This work has also been funded by the Prize UD-Grupo Santander 2019 and the Spanish Ministry of Science and Innovation through research project PID2019-109393RA-I00.","J.S. Angarita-Zapata; Deusto Institute of Technology (DeustoTech), Faculty of Engineering, University of Deusto, Bilbao, 48007, Spain; email: js.angarita@deusto.es","","MDPI","14248220","","","34696123","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117710418"
"Dohmen R.; Catal C.; Liu Q.","Dohmen, Roel (57221671832); Catal, Cagatay (22633325800); Liu, Qingzhi (56127832600)","57221671832; 22633325800; 56127832600","Computer vision-based weight estimation of livestock: a systematic literature review","2022","New Zealand Journal of Agricultural Research","65","2-3","","227","247","20","38","10.1080/00288233.2021.1876107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099748955&doi=10.1080%2f00288233.2021.1876107&partnerID=40&md5=7b5c7792754da0f579ee63d574dd7740","Information Technology Group, Wageningen University Research, Wageningen, Netherlands; Department of Computer Engineering, Bahcesehir University, Istanbul, Turkey; Department of Computer Science and Engineering, Qatar University, Doha, Qatar","Dohmen R., Information Technology Group, Wageningen University Research, Wageningen, Netherlands; Catal C., Department of Computer Engineering, Bahcesehir University, Istanbul, Turkey, Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Liu Q., Information Technology Group, Wageningen University Research, Wageningen, Netherlands","Body weight measurement of animals is often labor-intensive for farmers and stressful for animals. To this end, several methods have been researched and implemented to automate this process. In this study, we performed a Systematic Literature Review to identify and synthesise the published studies on the body weight estimation approaches for livestock (i.e. cattle and pigs). Information about features of models, underlying methods, performance evaluation parameters, challenges, and solutions using computer vision-based weight estimation, and characteristics of the future vision-based weight estimation models were presented based on the identified scientific papers. We found 151 papers, of which 26 papers were selected as primary studies that we analyzed in detail. We identified that: (1) seven features, namely top view body area, withers height, hip height, body length, hip-width, body volume, and chest girth are widely used in approaches; (2) 3D Time of Flight camera is the most preferred one; (3) the linear regression is the most used algorithm; (4) the application of Deep Learning algorithms is still very limited; and (5) coefficient of determination is the most used evaluation parameter for weight estimation. In addition to these observations, 13 challenges, 22 solutions, and guidelines for future research direction were presented. © 2021 The Royal Society of New Zealand.","Animal body weight estimation; computer vision; livestock; machine learning; systematic literature review (SLR)","body size; cattle; computer vision; literature review; machine learning; pig; weight","","","C. Catal; Department of Computer Engineering, Bahcesehir University, Istanbul, Turkey; email: cagatay.catal@eng.bau.edu.tr","","Taylor and Francis Ltd.","00288233","","NEZFA","","English","New Zealand J. Agric. Res.","Review","Final","","Scopus","2-s2.0-85099748955"
"Li K.; Teng G.","Li, Keqiang (57355461000); Teng, Guifa (23096254400)","57355461000; 23096254400","Study on Body Size Measurement Method of Goat and Cattle under Different Background Based on Deep Learning","2022","Electronics (Switzerland)","11","7","993","","","","12","10.3390/electronics11070993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126938506&doi=10.3390%2felectronics11070993&partnerID=40&md5=a6243704102f802ed2cfd6de126f89ab","School of Mechanical and Electrical Engineering, Hebei Agricultural University, Baoding, 071001, China; School of Mathematics and Information Technology, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China; Hebei Key Laboratory of Specialty Animal Germplasm Resources Exploration and Innovation, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China; Hebei Agricultural Data Intelligent Perception and Application Technology Innovation Center, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China; School of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China; Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China","Li K., School of Mechanical and Electrical Engineering, Hebei Agricultural University, Baoding, 071001, China, School of Mathematics and Information Technology, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China, Hebei Key Laboratory of Specialty Animal Germplasm Resources Exploration and Innovation, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China, Hebei Agricultural Data Intelligent Perception and Application Technology Innovation Center, Hebei Normal University of Science and Technology, Qinhuangdao, 066004, China; Teng G., School of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China, Hebei Key Laboratory of Agricultural Big Data, Baoding, 071001, China","The feasibility of using depth sensors to measure the body size of livestock has been extensively tested. Most existing methods are only capable of measuring the body size of specific livestock in a specific background. In this study, we proposed a unique method of livestock body size measurement using deep learning. By training the data of cattle and goat with same feature points, different animal sizes can be measured under different backgrounds. First, a novel penalty function and an autoregressive model were introduced to reconstruct the depth image with super-resolution, and the effect of distance and illumination on the depth image was reduced. Second, under the U-Net neural network, the characteristics exhibited by the attention module and the DropBlock were adopted to improve the robustness of the background and trunk segmentation. Lastly, this study initially exploited the idea of human joint point location to accurately locate the livestock body feature points, and the livestock was accurately measured. According to the results, the average accuracy of this method was 93.59%. The correct key points for detecting the points of withers, shoulder points, shallowest part of the chest, highest point of the hip bones and ischia tuberosity had the percentages of 96.7%, 89.3%, 95.6%, 90.5% and 94.5%, respectively. In addition, the mean relative errors of withers height, hip height, body length and chest depth were only 1.86%, 2.07%, 2.42% and 2.72%, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Body segmentation; Body size measurement; Feature point location","","Graduate Innovation Foundation of Hebei, (CXZZBS2021045); National Natural Science Foundation of China, NSFC, (U20A20180)","Funding: This work was supported by the National Natural Science Foundation of China (Grant No. U20A20180), the Graduate Innovation Foundation of Hebei (Grant No. CXZZBS2021045).","G. Teng; School of Information Science and Technology, Hebei Agricultural University, Baoding, 071001, China; email: tguifa@126.com","","MDPI","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126938506"
"Neethirajan S.","Neethirajan, Suresh (57217318680)","57217318680","ChickTrack – A quantitative tracking tool for measuring chicken activity","2022","Measurement: Journal of the International Measurement Confederation","191","","110819","","","","71","10.1016/j.measurement.2022.110819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124237105&doi=10.1016%2fj.measurement.2022.110819&partnerID=40&md5=dd7289ecdaa5b7dc5546b9ae26e152f3","Farmworx, Adaptation Physiology Group, Animal Sciences Department, Wageningen University, Netherlands","Neethirajan S., Farmworx, Adaptation Physiology Group, Animal Sciences Department, Wageningen University, Netherlands","The automatic detection, counting and tracking of individual and flocked chickens in the poultry industry is of paramount to enhance farming productivity and animal welfare. Due to methodological difficulties, such as the complex background of images, varying lighting conditions, and occlusions from e.g., feeding stations, water nipple stations and barriers in the chicken rearing production floor, it is a challenging task to automatically recognize and track birds using computer software. Here, a deep learning model based on You Only Look Once (Yolov5) is proposed for detecting domesticated chickens from videos with varying complex backgrounds. A multiscale feature is being adapted to the Yolov5 network for mapping modules in the counting and tracking of the trajectories of the chickens. The Yolov5 network was trained and tested on our dataset which resulted in an enhanced tracking precision accuracy. Using Kalman Filter, the proposed model was able to track multiple chickens simultaneously with the focus to associate individual chickens across the frames of the video for real time and online applications. By being able to detect the chickens amid diverse background interference and counting them precisely along with tracking the movement and measuring their travelled path and direction, the proposed model provides excellent performance for on-farm applications. Artificial intelligence enabled automatic measurements of chicken behavior on-farm using cameras offers continuous monitoring of the chicken's ability to perch, walk, interact with other birds and the farm environment, as well as the assessment of dustbathing, thigmotaxis, and foraging frequency, which are important indicators for their ability to express natural behaviors. This study highlights the potential of automated monitoring of poultry through the usage of ChickTrack model as a digital tool in enabling science-based animal husbandry practices and thereby promote positive welfare for chickens in animal farming. © 2022 The Author(s)","Chicken automated measurements; Deep learning; Digital agriculture; Multi-target detection and tracking; Precision livestock farming; Tracking; Yolo","Birds; Complex networks; Deep learning; Digital devices; E-learning; Automated measurement; Automatic Detection; Chicken automated measurement; Complex background; Deep learning; Digital agriculture; Multi-target detection and tracking; Precision livestock farming; Tracking; Yolo; Agriculture","","","","","Elsevier B.V.","02632241","","MSRMD","","English","Meas J Int Meas Confed","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124237105"
"Jung D.-H.; Kim N.Y.; Moon S.H.; Kim H.S.; Lee T.S.; Yang J.-S.; Lee J.Y.; Han X.; Park S.H.","Jung, Dae-Hyun (55823060200); Kim, Na Yeon (56315305100); Moon, Sang Ho (8333204200); Kim, Hyoung Seok (57193389559); Lee, Taek Sung (7501441271); Yang, Jung-Seok (9333318900); Lee, Ju Young (57206731846); Han, Xiongzhe (55542768900); Park, Soo Hyun (57205063566)","55823060200; 56315305100; 8333204200; 57193389559; 7501441271; 9333318900; 57206731846; 55542768900; 57205063566","Classification of Vocalization Recordings of Laying Hens and Cattle Using Convolutional Neural Network Models","2021","Journal of Biosystems Engineering","46","3","","217","224","7","12","10.1007/s42853-021-00101-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113664866&doi=10.1007%2fs42853-021-00101-1&partnerID=40&md5=4b2896e279ec36af7af70be572cd0277","Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea; Asia Pacific Ruminant Institute, Majang-myeon, Icheon, 17385, South Korea; Department of Biosystems Engineering, College of Agriculture and Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea","Jung D.-H., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Kim N.Y., Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea, Asia Pacific Ruminant Institute, Majang-myeon, Icheon, 17385, South Korea; Moon S.H., Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea; Kim H.S., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Lee T.S., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Yang J.-S., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Lee J.Y., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Han X., Department of Biosystems Engineering, College of Agriculture and Life Sciences, Kangwon National University, Chuncheon, 24341, South Korea; Park S.H., Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea","Purpose: Vocalizations of livestock convey information about the health and behavior of the animals, and vocal analysis could be a useful method to monitor livestock. We propose a deep learning classification of vocal recordings of laying hens and cattle with the aim of automatically classifying laying hen and cattle sounds in South Korea using a deep learning model. Methods: Audio and video recordings of laying hens and cattle were acquired. We classified laying hens’ sounds into eight classes and cattle sounds into nine classes. Classified audio files were used for the development of convolutional neural network (CNN) models. Two types of CNN structures, one based on 2D ConVnet and the other based on a 1D model with long short-term memory, were developed and tested for modeling to classify the vocalizations of laying hens and cattle. Results: The classification model based on 2D ConVnet performed better with a satisfactory classification accuracy of 75.78% for laying hens and 91.02% for cattle. Conclusion: Based on the results for the developed CNN models, it is expected that real-time voice monitoring could be applicable for providing animal physiological information to growers. © 2021, The Korean Society for Agricultural Machinery.","Cattle; Convolutional neural network (CNN); Laying hens; Mel-frequency cepstrum; Vocalization classification","Agriculture; Animals; Convolution; Deep learning; Physiological models; Video recording; Audio files; Classification accuracy; Classification models; Laying hens; Learning models; Physiological informations; Real-time voice; South Korea; Convolutional neural networks","National Institute of Agricultural Sciences; Rural Development Administration, RDA","This study was carried out with the support of the Research Program for Agricultural Science & Technology Development (Project No. PJ01389103), National Institute of Agricultural Sciences, Rural Development Administration, Republic of Korea. ","S.H. Park; Smart Farm Research Center, Gangneung Institute of Natural Products, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; email: ecoloves@kist.re.kr","","Springer Science and Business Media Deutschland GmbH","17381266","","","","English","J. Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85113664866"
"Xu B.; Wang W.; Guo L.; Chen G.; Li Y.; Cao Z.; Wu S.","Xu, Beibei (57215186247); Wang, Wensheng (56937276700); Guo, Leifeng (56542160600); Chen, Guipeng (57115604400); Li, Yongfeng (57577901700); Cao, Zhen (57222378464); Wu, Saisai (57222512030)","57215186247; 56937276700; 56542160600; 57115604400; 57577901700; 57222378464; 57222512030","CattleFaceNet: A cattle face identification approach based on RetinaFace and ArcFace loss","2022","Computers and Electronics in Agriculture","193","","106675","","","","67","10.1016/j.compag.2021.106675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122520791&doi=10.1016%2fj.compag.2021.106675&partnerID=40&md5=3aada050829d428068724112140d8f67","Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; Information Technology Group, Wageningen University and Research, PB Wageningen, 6708, Netherlands","Xu B., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Wang W., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Guo L., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Chen G., Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; Li Y., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Cao Z., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China, Information Technology Group, Wageningen University and Research, PB Wageningen, 6708, Netherlands; Wu S., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China","Cattle identification is crucial to be registered for breeding association, food quality tracing, disease prevention and control and fake insurance claims. Traditional non-biometrics methods for cattle identification is not really satisfactory in providing reliability due to theft, fraud, and duplication. In this study, a computer vision technique was proposed to facilitate precision animal management and improve livestock welfare. This paper presents a novel face identification framework by integrating light-weight RetinaFace-mobilenet with Additive Angular Margin Loss (ArcFace), namely CattleFaceNet. RetinaFace-mobilenet is designed for face detection and location, and ArcFace is adopted to strengthen the within-class compactness and also between-class discrepancy during training. Experiments on real-word scenarios dataset prove that RetinaFace-mobilenet achieves superior detection performance and significantly accelerates the computation time against RetinaNet. Three loss functions utilized in human face recognition combined with RetinaFace-mobilenet are compared and results indict that the proposed CattleFaceNet outperforms others with identification accuracy of 91.3% and processing time of 24 frames per second (FPS). This research work demonstrates the potential candidate of CattleFaceNet for livestock identification in real time in practical production scenarios. © 2022 The Authors","ArcFace loss; Deep learning; Face recognition; Precision livestock; RetinaFace","Agriculture; Crime; Deep learning; Disease control; Insurance; Arcface loss; Deep learning; Disease prevention and controls; Face identification; Food quality; Identification approach; Insurance claims; Precision livestock; Quality tracing; Retinaface; animal community; cattle; food quality; recognition; reliability analysis; research work; Face recognition","China Scholarship Council, China, (202003250122); Hebei Province Key Research and Development Plan, China, (19220119D, 20327202D); Inner Mongolia Autonomous Region Science and Technology Major Project, (2020ZD0004); National Natural Science Foundation of China, NSFC, (32060776); Youth Science Foundation of Jiangxi Province, (20192ACBL21023); Guangxi Academy of Agricultural Sciences, GXAAS, (20181CBS006)","This research was supported by China Scholarship Council, China ( 202003250122 ) and was funded by Inner Mongolia Autonomous Region Science and Technology Major Project, China ( 2020ZD0004 ), National Natural Science Foundation of China, China ( 32060776 ), Hebei Province Key Research and Development Plan, China ( 20327202D , 19220119D ), Youth Science Foundation of Jiangxi Province, China ( 20192ACBL21023 ) and Doctoral program of innovation fund of Jiangxi Academy of Agricultural Sciences, China ( 20181CBS006 ). We are also grateful to two private housing farms in China for their kindly support with data collection. ","B. Xu; Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; email: xuxiaobei224@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122520791"
"Kim N.-K.; Park M.-S.; Jeong M.-J.; Hwang D.-H.; Yoon H.-J.","Kim, Na-Kyeong (57223370169); Park, Mi-So (57223361004); Jeong, Min-Ji (57216335390); Hwang, Do-Hyun (55350108000); Yoon, Hong-Joo (14032408300)","57223370169; 57223361004; 57216335390; 55350108000; 14032408300","A study on field compost detection by using unmanned aerial vehicle image and semantic segmentation technique based deep learning","2021","Korean Journal of Remote Sensing","37","3","","367","378","11","7","10.7780/kjrs.2020.37.3.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111167054&doi=10.7780%2fkjrs.2020.37.3.1&partnerID=40&md5=0c6c823207ba7a27baab99a09884c819","Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University","Kim N.-K., Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University; Park M.-S., Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University; Jeong M.-J., Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University; Hwang D.-H., Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University; Yoon H.-J., Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University","Field compost is a representative non-point pollution source for livestock. If the field compost flows into the water system due to rainfall, nutrients such as phosphorus and nitrogen contained in the field compost can adversely affect the water quality of the river. In this paper, we propose a method for detecting field compost using unmanned aerial vehicle images and deep learning-based semantic segmentation. Based on 39 ortho images acquired in the study area, about 30,000 data were obtained through data augmentation. Then, the accuracy was evaluated by applying the semantic segmentation algorithm developed based on U-net and the filtering technique of Open CV. As a result of the accuracy evaluation, the pixel accuracy was 99.97%, the precision was 83.80%, the recall rate was 60.95%, and the F1-Score was 70.57%. The low recall compared to precision is due to the underestimation of compost pixels when there is a small proportion of compost pixels at the edges of the image. After, It seems that accuracy can be improved by combining additional data sets with additional bands other than the RGB band. © 2021 Korean Journal of Remote Sensing. All rights reserved.","Compost; Deep Learning; Semantic Segmentation; UAV","","","","H.-J. Yoon; Division of Earth Environmental System Science, Major of Spatial Information Engineering, Pukyong National University; email: yoonhj@pknu.ac.kr","","Korean Society of Remote Sensing","12256161","","","","Korean","Kor. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85111167054"
"Wang D.-L.; Liao X.-H.; Zhang Y.-J.; Cong N.; Ye H.-P.; Shao Q.-Q.; Xin X.-P.","Wang, Dong-Liang (55713405600); Liao, Xiao-Han (57193401553); Zhang, Yang-Jian (58614810700); Cong, Nan (55327325100); Ye, Hu-Ping (57189520569); Shao, Quan-Qin (7101974470); Xin, Xiao-Ping (16317691300)","55713405600; 57193401553; 58614810700; 55327325100; 57189520569; 7101974470; 16317691300","Real-time detection and weight estimation of grassland livestock based on unmanned aircraft system video streams; [基于无人机视频流的草原放牧家畜在线检测和体重估算]","2021","Chinese Journal of Ecology","40","12","","4099","4108","9","3","10.13292/j.1000-4890.202111.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125344077&doi=10.13292%2fj.1000-4890.202111.008&partnerID=40&md5=4d23c791af9396549b614b117bffcd6c","Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Science, Beijing, 100101, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Key Laboratory of Ecosystem Network Observation and Modeling, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Lhasa Plateau Ecosystem Research Station, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; National Hulunber Grassland Ecosystem Observation and Research Station, Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, 100081, China","Wang D.-L., Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Science, Beijing, 100101, China, State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Liao X.-H., State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Zhang Y.-J., Key Laboratory of Ecosystem Network Observation and Modeling, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Cong N., Lhasa Plateau Ecosystem Research Station, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Ye H.-P., State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; Shao Q.-Q., Key Laboratory of Land Surface Pattern and Simulation, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Science, Beijing, 100101, China; Xin X.-P., National Hulunber Grassland Ecosystem Observation and Research Station, Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, 100081, China","Accurate and real-time livestock data are crucial to developing modern animal husbandry, ensuring effective supply of animal products, and promoting ecosystem balance and sustainable development of grasslands. The acquirement of livestock data mainly relies on field surveys and grassroots' reports. These data are laborious and non-real-time. In this study, a realtime monitoring system is developed based on browser/ server architecture ( http: / / 218. 202. 104.82:5806 / vid). A deep-learning-based livestock detection model and a weight estimation model are developed. The system could detect and count livestock, and estimate their weight using unmanned aircraft system ( UAS) live video streams. The livestock detection model is trained using 13803 UAS image tiles and video picture frames. The true positive rate, false positive rate, and loss positive rate of the model for cattle detection are 90. 51%, 11. 64%, and 9.49%, respectively. The true positive rate, false positive rate, and loss positive rate of the model for sheep detection are 91. 47%, 7. 04%, and 8. 53%, respectively. The weight estimation model is built based on the head-body length and weight data collected in Inner Mongolia Autonomous Region and Qinghai Province, with accuracy of 90.28% and 90.00% for cattle and sheep weight estimation, respectively. The system utilizes UASs and deep learning technologies for livestock monitoring, having an expected application prospect in the fields of grassland supervision (including grazing prohibition and rest grazing), and assisting herdsmen in remotely monitoring their livestock. © 2021 Chinese Journal of Anesthesiology. All rights reserved.","deep learning; livestock detection; UAS live video streams; weight estimation","China; Nei Monggol; Qinghai; animal husbandry; grassland; grazing; livestock; machine learning; unmanned vehicle; weight","","","N. Cong; Lhasa Plateau Ecosystem Research Station, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, 100101, China; email: congnan@igsnrr.ac.cn","","Chinese Society of Ecology","10004890","","SZAZE","","Chinese","Chinese J. Ecol.","Article","Final","","Scopus","2-s2.0-85125344077"
"Yan H.; Hu Z.; Cui Q.","Yan, Hongwen (57213947547); Hu, Zhiwei (57204826103); Cui, Qingliang (16204934800)","57213947547; 57204826103; 16204934800","STUDY ON FEATURE EXTRACTION OF PIG FACE BASED ON PRINCIPAL COMPONENT ANALYSI; [基于主成分分析的猪脸特征提取研究]","2022","INMATEH - Agricultural Engineering","68","3","","333","340","7","4","10.35633/inmateh-68-33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146642522&doi=10.35633%2finmateh-68-33&partnerID=40&md5=6d5f8881e931a26f6cd11bcbe0f4a0ac","College of Information Science and Engineering, Shanxi Agricultural University, Taigu, China","Yan H., College of Information Science and Engineering, Shanxi Agricultural University, Taigu, China; Hu Z., College of Information Science and Engineering, Shanxi Agricultural University, Taigu, China; Cui Q., College of Information Science and Engineering, Shanxi Agricultural University, Taigu, China","Individual identification and behavioural analysis of pigs is a key link in the intelligent management of a piggery, for which the computer vision technology based on application and improvement of deep learning model has become the mainstream. However, the operation of the model has high requirements to hardware, also the model is of weak interpretability, which makes it difficult to adapt to both the mobile terminals and the embedded applications. In this study, it is first put forward that the key facial features of pigs can be extracted by Principal Component Analysis method first before the eigenface method is adopted for verification tests to reach an average accuracy rate of 74.4%; the key features, for which the most identifiable ones are in turn, respectively, face contour, nose, ears and other parts of the pigs, can be visualized, and this is different from the identification features adopted in manual identification. This method not only reduces the computational complexity but is also of strong interpretability, so it is suitable for both the mobile terminals and the embedded applications. In some way, this study provides a systematic and stable guidance for livestock and poultry production. © 2022,INMATEH - Agricultural Engineering.All Rights Reserved.","Eigenface; Feature extraction; Identification; Intelligent management of pig breeding; Pca","Agriculture; Computer terminals; Deep learning; Mammals; Mobile telecommunication systems; Eigenfaces; Embedded application; Features extraction; Identification; Intelligent management; Intelligent management of pig breeding; Interpretability; Mobile terminal; Pca; Principal Components; Principal component analysis","Doctor Scientific Research Foundation of Shanxi Agricultural University, (2020BQ14); Key Laboratory of Biomechanics; Shanxi Province Basic Research Program Project, (202103021223141, 202103021224149, 20210302124523); National Key Research and Development Program of China, NKRDPC, (2016YFD0701801)","This research, titled ‘Study on Feature Extraction of Pig Face Based on Principal Component Analysis’, was funded by the National Key Research and Development Plan of China (2016YFD0701801), the Shanxi Province Basic Research Program Project (Free Exploration) (Grant No.20210302124523, 202103021224149, 202103021223141), the Doctor Scientific Research Foundation of Shanxi Agricultural University (2020BQ14). The authors are grateful and honoured to have obtained support from the Key Laboratory of Biomechanics.","H. Yan; College of Information Science and Engineering, Shanxi Agricultural University, Taigu, China; email: yhwhxh@126.com","","INMA Bucharest","20684215","","","","English","INMATEH Agric. Eng.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146642522"
"Weng Z.; Fan L.; Zhang Y.; Zheng Z.; Gong C.; Wei Z.","Weng, Zhi (53867547700); Fan, Longzhen (57772951300); Zhang, Yong (55949844000); Zheng, Zhiqiang (57714705600); Gong, Caili (57205025286); Wei, Zhongyue (57772779000)","53867547700; 57772951300; 55949844000; 57714705600; 57205025286; 57772779000","Facial Recognition of Dairy Cattle Based on Improved Convolutional Neural Network∗","2022","IEICE Transactions on Information and Systems","E105D","6","","1234","1238","4","8","10.1587/transinf.2022EDP7008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133204127&doi=10.1587%2ftransinf.2022EDP7008&partnerID=40&md5=788d364bfae090552ed5848e5e858656","College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China; College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; School of Mathematical Sciences, Inner Mongolia University, Hohhot, 010021, China","Weng Z., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China, College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; Fan L., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; Zhang Y., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China; Zheng Z., College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; Gong C., College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China, College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; Wei Z., School of Mathematical Sciences, Inner Mongolia University, Hohhot, 010021, China","As the basis of fine breeding management and animal husbandry insurance, individual recognition of dairy cattle is an important issue in the animal husbandry management field. Due to the limitations of the traditional method of cow identification, such as being easy to drop and falsify, it can no longer meet the needs of modern intelligent pasture management. In recent years, with the rise of computer vision technology, deep learning has developed rapidly in the field of face recognition. The recognition accuracy has surpassed the level of human face recognition and has been widely used in the production environment. However, research on the facial recognition of large livestock, such as dairy cattle, needs to be developed and improved. According to the idea of a residual network, an improved convolutional neural network (Res 5 2Net) method for individual dairy cow recognition is proposed based on dairy cow facial images in this letter. The recognition accuracy on our self-built cow face database (3012 training sets, 1536 test sets) can reach 94.53%. The experimental results show that the efficiency of identification of dairy cows is effectively improved. © 2022 The Institute of Electronics, Information and Communication Engineers","convolutional neural network; deep learning; facial recognition of dairy cattle; network architecture","Agriculture; Animals; Convolution; Convolutional neural networks; Deep learning; Face recognition; Image enhancement; Animal husbandry; Convolutional neural network; Dairy cattles; Dairy cow; Deep learning; Facial recognition; Facial recognition of dairy cattle; Individual recognition; Pasture management; Recognition accuracy; Network architecture","National Training Program of Innovation and Entrepreneurship for Undergraduates, (201910126041); National Natural Science Foundation of China, NSFC, (31660678, 61966026)","Manuscript received January 15, 2022. Manuscript publicized March 2, 2022. †The authors are with the College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot 010018, China. ††The authors are with the College of Electronic Information Engineering, Inner Mongolia University, Hohhot 010021, China. †††The author is with the School of Mathematical Sciences, Inner Mongolia University, Hohhot 010021, China. ∗This work was supported in part by the National Training Program of Innovation and Entrepreneurship for Undergraduates under Grant 201910126041, in part by the National Natural Science Foundation of China under Grant 31660678, and Grant 61966026. a) E-mail: yongz@imau.edu.cn (Corresponding author) b) E-mail: zqzheng@imu.edu.cn (Corresponding author) DOI: 10.1587/transinf.2022EDP7008","Y. Zhang; College of Mechanical and Electrical Engineering, Inner Mongolia Agricultural University, Hohhot, 010018, China; email: yongz@imau.edu.cn; Z. Zheng; College of Electronic Information Engineering, Inner Mongolia University, Hohhot, 010021, China; email: zqzheng@imu.edu.cn","","Institute of Electronics Information Communication Engineers","09168532","","ITISE","","English","IEICE Trans Inf Syst","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85133204127"
"Bocaj E.; Uzunidis D.; Kasnesis P.; Patrikakis C.Z.","Bocaj, Enkeleda (57200191571); Uzunidis, Dimitris (57203915109); Kasnesis, Panagiotis (57044812500); Patrikakis, Charalampos Z. (8244299800)","57200191571; 57203915109; 57044812500; 8244299800","On the Benefits of Deep Convolutional Neural Networks on Animal Activity Recognition","2020","Proceedings of 2020 International Conference on Smart Systems and Technologies, SST 2020","","","9263702","83","88","5","15","10.1109/SST49455.2020.9263702","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098633790&doi=10.1109%2fSST49455.2020.9263702&partnerID=40&md5=8c1f11877c1b9c39361ac2b545357fa0","University of West Attica, Department of Electrical and Electronics Engineering, Athens, Greece","Bocaj E., University of West Attica, Department of Electrical and Electronics Engineering, Athens, Greece; Uzunidis D., University of West Attica, Department of Electrical and Electronics Engineering, Athens, Greece; Kasnesis P., University of West Attica, Department of Electrical and Electronics Engineering, Athens, Greece; Patrikakis C.Z., University of West Attica, Department of Electrical and Electronics Engineering, Athens, Greece","Monitoring the behavior of animals (e.g., eating habits) can lead to conclusions regarding animal's welfare. To achieve this, remote monitoring of animal activity with the aid of inertial sensors and use of machine learning algorithms over the collected data can be used. However, these algorithms rely on handcrafted features extracted by statistical or heuristic functions over raw motion data. To this purpose, we employ deep Convolutional Neural Networks (ConvNets) for activity recognition of livestock animals, such as goats and horses. We investigate the potential gains of ConvNets compared with other machine learning algorithms of the literature, which are about 12.5% greater accuracy and more than 7% higher F1-score. Moreover, we designate the advantages of late sensor fusion (2D convolution) and also show that an increase on the number of filters on each layer does not necessarily lead to a greater classification accuracy. To this end, we benchmark various ConvNet architectures and demonstrate the role of hyperparameter tuning to optimize the overall accuracy. To the best of our knowledge, ConvNets are employed for animal activity recognition here for the first time.  © 2020 IEEE.","animal activity recognition; classification algorithms; convolutional neural networks; deep learning","Agriculture; Animals; Convolution; Deep neural networks; Heuristic algorithms; Learning algorithms; Pattern recognition; Activity recognition; Animal activities; Animal activity recognition; Animal welfare; Classification algorithm; Convolutional neural network; Deep learning; Eating habits; Machine learning algorithms; Remote monitoring; Convolutional neural networks","European Union and Greek national funds; Support of groups of Small and Medium-sized Enterprises","This research has been co-financed by the European Union and Greek national funds through the Operational Program Epirus 2014-2020, Topic: ""Support of groups of Small and Medium-sized Enterprises (SME) for Research &Technology Development activities in the fields of agri-food, health and biotechnology"".","","Zagar D.; Martinovic G.; Drlje S.R.; Galic I.","Institute of Electrical and Electronics Engineers Inc.","","978-172819759-3","","","English","Proc. Int. Conf. Smart Syst. Technol., SST","Conference paper","Final","","Scopus","2-s2.0-85098633790"
"Kuan C.-Y.; Tsai Y.-C.; Hsu J.-T.; Ding S.-T.; Lin T.-T.","Kuan, Cheng-Yu (57215366865); Tsai, Yu-Chi (56315856400); Hsu, Jih-Tay (36108145000); Ding, Shih-Torng (7401542836); Lin, Ta-Te (7404860130)","57215366865; 56315856400; 36108145000; 7401542836; 7404860130","An imaging system based on deep learning for monitoring the feeding behavior of dairy cows","2019","2019 ASABE Annual International Meeting","","","","","","","14","10.13031/aim.201901469","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084012169&doi=10.13031%2faim.201901469&partnerID=40&md5=3fc7bd61f9c13dffb17bb684fefacb2d","Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan; Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan","Kuan C.-Y., Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan; Tsai Y.-C., Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan; Hsu J.-T., Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan; Ding S.-T., Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan; Lin T.-T., Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan","Observation of animal behavior in dairy farms is necessary for precision livestock farming as well as to prevent ailments that may affect the herd. Heat stress in dairy farms has been reported to be a serious problem that leads to the drastic decline in milk production and fertility of dairy cows. In recent studies, wearable devices were used to monitor the behavior of individual dairy cows. However, such kinds of contact devices may affect the behavior of the dairy cows for several reasons. Therefore, designing a non-contact system that can monitor the behavior of dairy cows is recommended. In this work, an embedded imaging system that can monitor the feeding behavior of dairy cows was developed. The system includes cameras that were fixed in front of the feeding area to acquire cow face images. Cow face detection and recognition were performed on the acquired images using a deep convolution neural network (CNN) to record feeding behavior and identify individual cow faces. The cow face detector has an F1-score of 0.971 based on validation with a static image testing dataset. Meanwhile, the cow face recognition model was also validated with an average F1-score of 0.85 on 19 different cows. Finally, the predictions of feeding time were compared with the manual observation with the R2 0.7802 without the cows having lower recall from the recognition results. In the future, combining the feeding time and temperature and humidity index can be a health indicator of individual cows. © 2019 ASABE Annual International Meeting. All rights reserved.","Cow face detection; Embedded system; Heat stress; Precision livestock farming; Recognition","Dairies; Deep learning; Embedded systems; Farms; Feeding; Image acquisition; Imaging systems; Monitoring; Statistical tests; Thermal stress; Convolution neural network; Face detection and recognition; Health indicators; Heat stress; Precision livestock farming; Recognition; Recognition models; Temperature and humidities; Face recognition","Council of Agriculture, COA; National Taiwan University of Science and Technology, NTUST, (107A3028)","The authors would like to thank the members of the Department of Animal Science and Technology of National Taiwan University for providing the experimental site. This work was supported by a grant (Grant No. 107A3028) from Council of Agriculture, Taiwan, ROC.","","","American Society of Agricultural and Biological Engineers","","","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85084012169"
"Kim Y.J.; Park D.-H.; Park H.; Kim S.-H.","Kim, You Jin (56066671900); Park, Dae-Heon (7403245696); Park, Hyeon (57192084164); Kim, Se-Han (54393433400)","56066671900; 7403245696; 57192084164; 54393433400","Pig Datasets of Livestock for Deep Learning to detect Posture using Surveillance Camera","2020","International Conference on ICT Convergence","2020-October","","9289401","1196","1198","2","10","10.1109/ICTC49870.2020.9289401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098941514&doi=10.1109%2fICTC49870.2020.9289401&partnerID=40&md5=39adc7b7d821e437a313c29858d6c194","Electronics and Telecommunications Research Institute, Sdf Convergence Research Laboratory, Daejeon, South Korea","Kim Y.J., Electronics and Telecommunications Research Institute, Sdf Convergence Research Laboratory, Daejeon, South Korea; Park D.-H., Electronics and Telecommunications Research Institute, Sdf Convergence Research Laboratory, Daejeon, South Korea; Park H., Electronics and Telecommunications Research Institute, Sdf Convergence Research Laboratory, Daejeon, South Korea; Kim S.-H., Electronics and Telecommunications Research Institute, Sdf Convergence Research Laboratory, Daejeon, South Korea","This paper proposes the pig Datasets for deep learning to detect a posture using a surveillance camera. The proposal aims to develop the best quality Datasets that can be input to the object detection pipeline for building a deep learning model. The proposed Datasets have two types with 7 and 9 categories(each called Class 7 and Class 9), and each evaluated. In Class 9, one of the categories labels to the ground- truth bounding box as a fake data assuming that the hidden head of a pig. The hidden head of pig shows to decrease performance. When training YOLOv2 and SSD against proposed Datasets, the performance an average precision (AP) of each training result evaluates the trained model with three types of optimization algorithms: Adam, SGDM, and RMSProp. The detection accuracy for proposed Datasets Class 7 reaches 97%. YOLOv2 using the proposed Datasets of Class 7 shows seven times better performance than SSD. © 2020 IEEE.","Datasets; livestock; object detection; pig; posture detection; SSD; YOLO","Agriculture; Cameras; Object detection; Security systems; Bounding box; Detection accuracy; Ground truth; Learning models; Optimization algorithms; Surveillance cameras; Deep learning","ICT based Intelligent Smart Welfare Housing System; Institute for Information and Communications Technology Promotion, IITP; Ministry of Science and ICT, South Korea, MSIT, (2018-0-00387)","ACKNOWLEDGMENT This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No. 2018-0-00387, Development of ICT based Intelligent Smart Welfare Housing System for the Prevention and Control of Livestock Disease)","","","IEEE Computer Society","21621233","978-172816758-9","","","English","Int. Conf. ICT Convergence","Conference paper","Final","","Scopus","2-s2.0-85098941514"
"Brünger J.; Gentz M.; Traulsen I.; Koch R.","Brünger, Johannes (57202813802); Gentz, Maria (57202136198); Traulsen, Imke (35410826800); Koch, Reinhard (7403189785)","57202813802; 57202136198; 35410826800; 7403189785","Panoptic segmentation of individual pigs for posture recognition","2020","Sensors (Switzerland)","20","13","3710","1","21","20","26","10.3390/s20133710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087735257&doi=10.3390%2fs20133710&partnerID=40&md5=6b1b3a7838d6264716ae5aad2cfd9e53","Department of Computer Science, Kiel University, Kiel, 24118, Germany; Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Göttingen, 37075, Germany","Brünger J., Department of Computer Science, Kiel University, Kiel, 24118, Germany; Gentz M., Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Göttingen, 37075, Germany; Traulsen I., Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Göttingen, 37075, Germany; Koch R., Department of Computer Science, Kiel University, Kiel, 24118, Germany","Behavioural research of pigs can be greatly simplified if automatic recognition systems are used. Systems based on computer vision in particular have the advantage that they allow an evaluation without affecting the normal behaviour of the animals. In recent years, methods based on deep learning have been introduced and have shown excellent results. Object and keypoint detector have frequently been used to detect individual animals. Despite promising results, bounding boxes and sparse keypoints do not trace the contours of the animals, resulting in a lot of information being lost. Therefore, this paper follows the relatively new approach of panoptic segmentation and aims at the pixel accurate segmentation of individual pigs. A framework consisting of a neural network for semantic segmentation as well as different network heads and postprocessing methods will be discussed. The method was tested on a data set of 1000 hand-labeled images created specifically for this experiment and achieves detection rates of around 95% (F1 score) despite disturbances such as occlusions and dirty lenses. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Animal detection; Computer vision; Deep learning; Image processing; Pose estimation; Precision livestock","Animals; Image Processing, Computer-Assisted; Neural Networks, Computer; Posture; Swine; Deep learning; Mammals; Object detection; Semantics; Automatic recognition system; Detection rates; Key-point detectors; Labeled images; New approaches; Postprocessing methods; Posture recognition; Semantic segmentation; animal; body position; image processing; pig; Behavioral research","Deutsche Forschungsgemeinschaft, DFG; Landwirtschaftliche Rentenbank, (2817205413, 758914)","Funding: The data acquisition was partly funded by the Federal Office for Agriculture and Food of Germany and the Landwirtschaftliche Rentenbank (project no.: 2817205413; 758914). We acknowledge financial support by DFG within the funding programme Open Access Publizieren.","J. Brünger; Department of Computer Science, Kiel University, Kiel, 24118, Germany; email: jobr@informatik.uni-kiel.de","","MDPI AG","14248220","","","32630794","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85087735257"
"","","","24th International Conference on Business Information Systems, BIS 2021","2021","Business Information Systems","1","","","","","380","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129543453&partnerID=40&md5=13996cb27ed0c9378c1caddd3780a9af","","","The proceedings contain 33 papers. The special focus in this conference is on Business Information Systems. The topics include: Post-Brexit power of European Union from the world trade network analysis; towards a Guideline Affording Overarching Knowledge Building in Data Analysis Projects; optimization-based Business Process Model Matching; database-less Extraction of Event Logs from Redo Logs; towards a Concept for Building a Big Data Architecture with Microservices; execution of Multi-Perspective Declarative Process Models using Complex Event Processing; exploring Potential Impacts of Self-Sovereign Identity on Smart Service Systems An Analysis of Electric Vehicle Charging Services; domain-specific Event Abstraction; ontological Modeling of the State Economic Development Policy for Cultural Industries; Semantic Representation of Domain Knowledge for Professional VR Training; mapping of ImageNet and Wikidata for Knowledge Graphs Enabled Computer Vision; contextual Personality-aware Recommender System Versus Big Data Recommender System; generating a Condensed Representation for Positive and Negative Association Rules A Condensed Representation for Association Rules; supporting an Expert-centric Process of New Product Introduction with Statistical Machine Learning; Evaluating the New AI and Data Driven Insurance Business Models for Incumbents and Disruptors: Is there Convergence?; evaluation of Deep Learning Instance Segmentation models for Pig Precision Livestock Farming; text-Aware Predictive Monitoring of Business Processes; predicting E-commerce Item Sales with Web Environment Temporal Background; social Media Crisis Communication Model for Building Public Resilience: A Preliminary Study; stream Processing Tools for Analyzing Objects in Motion Sending High-Volume Location Data; enterprise-Wide Metadata Management An Industry Case on the Current State and Challenges.","","","","","","Abramowicz W.; Lewanska E.; Auer S.","Technische Informationsbibliothek (TIB)","27479986","","","","English","Bus. Inf. Sys.","Conference review","Final","","Scopus","2-s2.0-85129543453"
"Geffen O.; Yitzhaky Y.; Barchilon N.; Druyan S.; Halachmi I.","Geffen, O. (57211395532); Yitzhaky, Y. (6701497110); Barchilon, N. (57210945786); Druyan, S. (6602250529); Halachmi, I. (6701325703)","57211395532; 6701497110; 57210945786; 6602250529; 6701325703","A machine vision system to detect and count laying hens in battery cages","2020","Animal","14","12","","2628","2634","6","42","10.1017/S1751731120001676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088592512&doi=10.1017%2fS1751731120001676&partnerID=40&md5=07fda9f062c4223a60fd7d730c6cda20","Precision Livestock Farming (PLF) Lab, Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel; Electro Optical Engineering Department, School of Electrical and Computer Engineering, Ben-Gurion University of the Negev, 1 Ben Gurion Avenue, P.O.B 653, Be'er Sheva, 8410501, Israel; Animal Science Institute, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 7505101, Rishon Lezion, 7505101, Israel","Geffen O., Precision Livestock Farming (PLF) Lab, Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel, Electro Optical Engineering Department, School of Electrical and Computer Engineering, Ben-Gurion University of the Negev, 1 Ben Gurion Avenue, P.O.B 653, Be'er Sheva, 8410501, Israel, Animal Science Institute, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 7505101, Rishon Lezion, 7505101, Israel; Yitzhaky Y., Electro Optical Engineering Department, School of Electrical and Computer Engineering, Ben-Gurion University of the Negev, 1 Ben Gurion Avenue, P.O.B 653, Be'er Sheva, 8410501, Israel; Barchilon N., Precision Livestock Farming (PLF) Lab, Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel, Animal Science Institute, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 7505101, Rishon Lezion, 7505101, Israel; Druyan S., Animal Science Institute, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 7505101, Rishon Lezion, 7505101, Israel; Halachmi I., Precision Livestock Farming (PLF) Lab, Institute of Agricultural Engineering, Agricultural Research Organization (A.R.O.) – The Volcani Center, 68 Hamaccabim Road, P.O.B 15159, Rishon Lezion, 7505101, Israel","Manually counting hens in battery cages on large commercial poultry farms is a challenging task: time-consuming and often inaccurate. Therefore, the aim of this study was to develop a machine vision system that automatically counts the number of hens in battery cages. Automatically counting hens can help a regulatory agency or inspecting officer to estimate the number of living birds in a cage and, thus animal density, to ensure that they conform to government regulations or quality certification requirements. The test hen house was 87 m long, containing 37 battery cages stacked in 6-story high rows on both sides of the structure. Each cage housed 18 to 30 hens, for a total of approximately 11 000 laying hens. A feeder moves along the cages. A camera was installed on an arm connected to the feeder, which was specifically developed for this purpose. A wide-angle lens was used in order to frame an entire cage in the field of view. Detection and tracking algorithms were designed to detect hens in cages; the recorded videos were first processed using a convolutional neural network (CNN) object detection algorithm called Faster R-CNN, with an input of multi-angular view shifted images. After the initial detection, the hens’ relative location along the feeder was tracked and saved using a tracking algorithm. Information was added with every additional frame, as the camera arm moved along the cages. The algorithm count was compared with that made by a human observer (the ‘gold standard’). A validation dataset of about 2000 images achieved 89.6% accuracy at cage level, with a mean absolute error of 2.5 hens per cage. These results indicate that the model developed in this study is practicable for obtaining fairly good estimates of the number of laying hens in battery cages. © 2020 The Animal Consortium","deep learning; Faster R-CNN; object tracking; poultry; precision livestock farming","Animals; Chickens; Housing, Animal; Oviposition; animal; animal housing; chicken; egg laying","Israeli Chief Scientist of Agriculture fund ?, (20-12-0029); Israeli Chief Scientist of Agriculture fund ‘Kandel; Poultry Board of Israel, (IL-801/18)","Funding text 1: This study was supported by the Israeli Chief Scientist of Agriculture fund ‘Kandel’ 20-12-0029. We wish to thank all of the PLF lab members who supported this research and to the Egg and Poultry Board of Israel. Special thanks to Mr O. Salomon, who hosted us on his property. ; Funding text 2: This study was supported by the Israeli Chief Scientist of Agriculture fund ?Kandel? 20-12-0029. We wish to thank all of the PLF lab members who supported this research and to the Egg and Poultry Board of Israel. Special thanks to Mr O. Salomon, who hosted us on his property. O. Geffen 0000-0003-3552-8131, None. All the procedures in this study were carried out in accordance with the accepted ethical and welfare standards of the Israel Ethics Committee (approval number IL-801/18). The commercial hen house that was used in this study is 87 m long. Two rows of 37 battery cages, stacked in six stories, line the sides of the structure. Each cage is 2.4 m long, 0.54 m tall and 0.74 m depth, housing 18 to 30 Lohman hens per cage. None of the data were deposited in an official repository. Access rights are confidential.","","","Elsevier B.V.","17517311","","","32662766","English","Animal","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85088592512"
"Hirata T.; Zin T.T.; Kobayashi I.; Hama H.","Hirata, Tetsuya (57202515490); Zin, Thi Thi (6506258245); Kobayashi, Ikuo (24174822700); Hama, Hiromitsu (7101642717)","57202515490; 6506258245; 24174822700; 7101642717","A study on estrus detection of cattle combining video image and sensor information","2019","Advances in Intelligent Systems and Computing","744","","","267","273","6","8","10.1007/978-981-13-0869-7_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048627032&doi=10.1007%2f978-981-13-0869-7_30&partnerID=40&md5=4c798136679a36dc2a69b44c4e29a484","Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, Japan; Osaka City University, Osaka, Japan","Hirata T., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Zin T.T., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Kobayashi I., Field Science Center, Faculty of Agriculture, University of Miyazaki, Miyazaki, Japan; Hama H., Osaka City University, Osaka, Japan","In Japan, the detection rate of estrus behavior of cattle has declined from 70% to 55% in about 20 years. Causes include the burden of the monitoring system due to the aging of livestock farmers and oversight of detection of estrus behavior by multiple rearing. Because the time period during which estrus behavior appears conspicuously is nearly the same at day and night, it is necessary to monitor on a 24-h system. In the method proposed in this paper, region extraction of black cattle is performed by combining frame difference and MHI (Motion History Image), then feature detection of count formula is performed using the characteristic and features of the riding behaviors. In addition, as a consideration of the model experiment, a method of detecting the riding behavior by combining the vanishing point of the camera and the height from the foot of the cattle was proposed. The effectiveness of both methods were confirmed through experimental results. © 2019, Springer Nature Singapore Pte Ltd.","Estrus detection; Frame difference; Vanishing line","Agriculture; Data handling; Deep learning; Information analysis; Monitoring; Feature detection; Frame differences; Model experiments; Monitoring system; Motion history images; Region extraction; Sensor informations; Vanishing line; Big data","Strategic Information and Communications R&D Promotion Program, (172310006); Japan Society for the Promotion of Science, JSPS, (17K08066)","Acknowledgment. This work was supported in part by SCOPE: Strategic Information and Communications R&D Promotion Program (Grant No. 172310006) and JSPS KAKENHI Grant Number 17K08066.","T. Hirata; Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; email: hl13037@student.miyazaki-u.ac.jp","Zin T.T.; Lin J.C.W.","Springer Verlag","21945357","978-981130868-0","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85048627032"
"Srinivasan K.; Singh D.; Lonkar V.; Vutla P.; Alla D.; Sarangi S.","Srinivasan, Karthik (37262070800); Singh, Dineshkumar (56046410400); Lonkar, Vaibhav (57202334530); Vutla, Pavan (57205670948); Alla, Divya (57205673859); Sarangi, Sanat (55633353700)","37262070800; 56046410400; 57202334530; 57205670948; 57205673859; 55633353700","Feedback system for improving capturing quality and quantity of livestock images using deep learning technology","2018","ACM International Conference Proceeding Series","","","","95","101","6","1","10.1145/3297121.3297138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061059538&doi=10.1145%2f3297121.3297138&partnerID=40&md5=fc059cf8fbaa9abb010a22cec51406eb","TATA Consultancy Services Ltd, Digital Farming Initiative, India","Srinivasan K., TATA Consultancy Services Ltd, Digital Farming Initiative, India; Singh D., TATA Consultancy Services Ltd, Digital Farming Initiative, India; Lonkar V., TATA Consultancy Services Ltd, Digital Farming Initiative, India; Vutla P., TATA Consultancy Services Ltd, Digital Farming Initiative, India; Alla D., TATA Consultancy Services Ltd, Digital Farming Initiative, India; Sarangi S., TATA Consultancy Services Ltd, Digital Farming Initiative, India","Livestock body parameters like shape, horn, teeth, muzzle, and udder provide useful information to determine livestock age and health. It is very difficult to continuously monitor and measure these parameters for 300 million bovine animals in India. We developed a Deep Learning (DL) based intelligent Livestock Health Monitoring System (LHMS) which derives these parameters from the livestock images. We developed a mobile application for Veterinarians and livestock Artificial Insemination Technicians (AIT) to collect and monitor livestock data and images throughout their pregnancy lifecycle. Though AIT captured 1.87 Lakh livestock data since 2016, it had only 1000 images. We conducted multiple iteration of the Design Thinking (DT) research to understand the challenges in the image capturing process. It was difficult for a human to see each image and provide feedback to the AITs about quality of images. DL models revealed the poor quality of the images, such as missing livestock as well as noisy and blurred images. Model accuracy decreased due to this. To address this challenge DL were methods to analyze the image, train system and generated an AIT Image Score (AIS) based on factors like quantity of images, accuracy of images, frequency of upload, geolocation etc. Based on AIS, we created a personalized feedback message and training instructions on how to click and collect images for each AIT. This paper captures our experiences on use of DT approach, which resulted in an 80% jump in image quantity over a three month study period and 78% improvement in the quality of the images. © 2018 Copyright is held by the owner/author(s).","Deep learning technology; Design thinking; ICT; Image quality analysis based feedback; Livestock image classification; Rural mobile app","Agriculture; Deep learning; Human computer interaction; Image analysis; Iterative methods; Mammals; Monitoring; Artificial insemination; Design thinking; Health monitoring system; Image quality analysis; Learning technology; Mobile app; Mobile applications; Personalized feedback; Image enhancement","","","","","Association for Computing Machinery","","978-145036214-6","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85061059538"
"Hridoy R.H.; Akter F.; Afroz M.","Hridoy, Rashidul Hasan (57226639517); Akter, Fatema (59073114200); Afroz, Maisha (57419535900)","57226639517; 59073114200; 57419535900","An Efficient Computer Vision Approach for Rapid Recognition of Poisonous Plants by Classifying Leaf Images using Transfer Learning","2021","2021 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021","","","","","","","4","10.1109/ICCCNT51525.2021.9580011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126208559&doi=10.1109%2fICCCNT51525.2021.9580011&partnerID=40&md5=79395d99257ca7ad798dc2156911e7c5","Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh","Hridoy R.H., Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh; Akter F., Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh; Afroz M., Department of Computer Science and Engineering, Daffodil International University, Dhaka, Bangladesh","Livestock poisoning by several kinds of poisonous plants causes grievous economic losses to the livestock industry. Poisonous plants are also a fatal threat to humans, ingesting these plants can cause several side effects in the body because of their toxicity. Hence, it is essential to develop a rapid approach to recognize poisonous plants efficiently. This paper addresses a recognition approach for eighteen poisonous plants using poisonous plants leaf (PPL) dataset which has been generated using image augmentation techniques that contains 54000 training, 27000 validation, and 9000 testing images. Six different state-of-the-art deep learning models have been used in this study such as Xception, ResNet152V2, InceptionResNetV2, MobileNetV2, DenseNet201, and NASNetLarge for classifying leaf images of poisonous plants. Xception has shown more significant performance than other models, achieved 99.71% training and 99.37% testing accuracy. NASNetLarge and InceptionResNetV2 have achieved 96.89% and 95.18% test accuracy, respectively, and MobileNetV2 achieved the lowest test accuracy. © 2021 IEEE.","Deep Learning; Depthwise Separable Convolutions; Poisonous Plants Recognition; Transfer Learning; Xception","Computer vision; Deep learning; Image classification; Losses; Statistical tests; Deep learning; Depthwise separable convolution; Economic loss; Leaf images; Plant recognition; Poisonoi plant recognition; Side effect; Test accuracy; Transfer learning; Xception; Agriculture","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-172818595-8","","","English","Int. Conf. Comput. Commun. Netw. Technol., ICCCNT","Conference paper","Final","","Scopus","2-s2.0-85126208559"
"Fu Y.; Xu J.; Tang Z.; Wang L.; Yin D.; Fan Y.; Zhang D.; Deng F.; Zhang Y.; Zhang H.; Wang H.; Xing W.; Yin L.; Zhu S.; Zhu M.; Yu M.; Li X.; Liu X.; Yuan X.; Zhao S.","Fu, Yuhua (56623651600); Xu, Jingya (57201874775); Tang, Zhenshuang (57211847818); Wang, Lu (57218931907); Yin, Dong (57209466749); Fan, Yu (55226872400); Zhang, Dongdong (57218844003); Deng, Fei (58278646300); Zhang, Yanping (57218932870); Zhang, Haohao (57189236322); Wang, Haiyan (57191848968); Xing, Wenhui (57201861661); Yin, Lilin (57191379782); Zhu, Shilin (57218931609); Zhu, Mengjin (8876648000); Yu, Mei (56963095700); Li, Xinyun (35179530900); Liu, Xiaolei (56540112900); Yuan, Xiaohui (55286920300); Zhao, Shuhong (55261459400)","56623651600; 57201874775; 57211847818; 57218931907; 57209466749; 55226872400; 57218844003; 58278646300; 57218932870; 57189236322; 57191848968; 57201861661; 57191379782; 57218931609; 8876648000; 56963095700; 35179530900; 56540112900; 55286920300; 55261459400","A gene prioritization method based on a swine multi-omics knowledgebase and a deep learning model","2020","Communications Biology","3","1","502","","","","45","10.1038/s42003-020-01233-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090820483&doi=10.1038%2fs42003-020-01233-4&partnerID=40&md5=d3412a8851f5bd6d0d5b550707ca64d2","Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China","Fu Y., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China, School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Xu J., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Tang Z., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Wang L., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Yin D., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Fan Y., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Zhang D., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Deng F., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Zhang Y., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Zhang H., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Wang H., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Xing W., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Yin L., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Zhu S., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Zhu M., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Yu M., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Li X., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Liu X., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China; Yuan X., School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, Hubei, China; Zhao S., Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, Hubei, China","The analyses of multi-omics data have revealed candidate genes for objective traits. However, they are integrated poorly, especially in non-model organisms, and they pose a great challenge for prioritizing candidate genes for follow-up experimental verification. Here, we present a general convolutional neural network model that integrates multi-omics information to prioritize the candidate genes of objective traits. By applying this model to Sus scrofa, which is a non-model organism, but one of the most important livestock animals, the model precision was 72.9%, recall 73.5%, and F1-Measure 73.4%, demonstrating a good prediction performance compared with previous studies in Arabidopsis thaliana and Oryza sativa. Additionally, to facilitate the use of the model, we present ISwine (http://iswine.iomics.pro/), which is an online comprehensive knowledgebase in which we incorporated almost all the published swine multi-omics data. Overall, the results suggest that the deep learning strategy will greatly facilitate analyses of multi-omics integration in the future. © 2020, The Author(s).","","animal experiment; animal model; Arabidopsis thaliana; article; convolutional neural network; deep learning; livestock; multiomics; nonhuman; pig; prediction; recall; rice","National Natural Science Foundation of China, NSFC; Natural Science Foundation of Hubei Province, (2019CFC855); Natural Science Foundation of Hubei Province; National Natural Science Foundation of China-Yunnan Joint Fund, NSFC-Yunnan Joint Fund, (31702087, 31730089, 31902156); National Natural Science Foundation of China-Yunnan Joint Fund, NSFC-Yunnan Joint Fund; National Key Research and Development Program of China, NKRDPC, (2016YFD0101900); National Key Research and Development Program of China, NKRDPC; Fundamental Research Funds for the Central Universities, (2020IVB025, 2662018JC033); Fundamental Research Funds for the Central Universities; National Dairy Industry and Technology System, (CARS-35); National Dairy Industry and Technology System","We thank Thomas A. Gavin, Professor Emeritus, Cornell University, for help with editing this paper. This work was supported by the National Natural Science Foundation of China [31902156, 31702087, 31730089], the National Key Research and Development Program [2016YFD0101900], the National Swine Industry Technology System [CARS-35], the Natural Science Foundation of Hubei Province of China [2019CFC855], and the Fundamental Research Funds for the Central Universities of China [2020IVB025, 2662018JC033].","X. Liu; Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, China; email: xiaoleiliu@mail.hzau.edu.cn; S. Zhao; Key Laboratory of Agricultural Animal Genetics, Breeding and Reproduction, Ministry of Education, Key Laboratory of Swine Genetics and Breeding, Ministry of Agriculture, College of Animal Science and Technology, Huazhong Agricultural University, Wuhan, 430070, China; email: shzhao@mail.hzau.edu.cn; X. Yuan; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, 430070, China; email: yuanxiaohui@whut.edu.cn","","Nature Research","23993642","","","32913254","English","Commun. Biolog.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090820483"
"Park H.; Park D.-H.; Kim S.-H.","Park, Hyeon (57192084164); Park, Dae-Heon (7403245696); Kim, Se-Han (54393433400)","57192084164; 7403245696; 54393433400","Deep learning-based method for detecting anomalies of operating equipment dynamically in livestock farms","2020","International Conference on ICT Convergence","2020-October","","9289351","1182","1185","3","4","10.1109/ICTC49870.2020.9289351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098947982&doi=10.1109%2fICTC49870.2020.9289351&partnerID=40&md5=ef2442d43e860a7d6f9d4790fea4dbfb","Etri, Sdf Convergence Research Department, Daejeon, South Korea","Park H., Etri, Sdf Convergence Research Department, Daejeon, South Korea; Park D.-H., Etri, Sdf Convergence Research Department, Daejeon, South Korea; Kim S.-H., Etri, Sdf Convergence Research Department, Daejeon, South Korea","The scale of livestock farms has grown significantly and the amount of livestock being reared is also increasing recently. As a result, interest in automated livestock smart farms is huge. To realize a smart farm, the environment for livestock (limited to pigs in this paper) in the barn is properly maintained for growth conditions, thereby increasing its productivity and animal welfare. To maintain such a suitable environment, many and various equipment are built and operated inside and outside the pig house. However, due to poor environments, the failures of lots of environment equipment are high. Furthermore, there are difficulties in detecting its malfunctions during equipment operation. In this paper, we provide a mechanism to simultaneously detect anomalies in such various and lots of equipment and to quickly detect them, which is adaptable to the environment of each pig house. Data from lots of equipment (environment sensors and controllers, etc.) installed in a pig house are collected. Through the data to predict malfunctions of each equipment, the learning model is built using RNN. When something goes wrong with the sensor, there is a difference between the predicted value and the measured value, which shows that the models can work well at the same time. It is possible to increase the productivity of pigs in various types of livestock farms where various and lots of equipment is built. © 2020 IEEE.","anomaly; Deep Learning; livestock; RNN","Anomaly detection; Controllers; Houses; Learning systems; Mammals; Productivity; Recurrent neural networks; Animal welfare; Growth conditions; Learning models; Learning-based methods; Measured values; Operating equipments; Pig house; Agriculture","Institute for Information and Communications Technology Promotion, IITP; Ministry of Science and ICT, South Korea, MSIT, (210 8 -0 00387)","A CKNOELW DGMENT This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.210 8 -0 00387, Development of ICT based Intelligent Smart elfW are Housing System for the Prevention and Control of ivL estock Disease)","","","IEEE Computer Society","21621233","978-172816758-9","","","English","Int. Conf. ICT Convergence","Conference paper","Final","","Scopus","2-s2.0-85098947982"
"Pook T.; Freudenthal J.; Korte A.; Simianer H.","Pook, Torsten (57214354391); Freudenthal, Jan (57215304929); Korte, Arthur (24335191200); Simianer, Henner (8894372500)","57214354391; 57215304929; 24335191200; 8894372500","Using Local Convolutional Neural Networks for Genomic Prediction","2020","Frontiers in Genetics","11","","561497","","","","33","10.3389/fgene.2020.561497","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096670163&doi=10.3389%2ffgene.2020.561497&partnerID=40&md5=10c8be9adc8bd462892be78c6f275c8a","Animal Breeding and Genetics Group, Department of Animal Sciences, Center for Integrated Breeding Research, University of Goettingen, Göttingen, Germany; Center for Computational and Theoretical Biology, University of Wuerzburg, Wuerzburg, Germany","Pook T., Animal Breeding and Genetics Group, Department of Animal Sciences, Center for Integrated Breeding Research, University of Goettingen, Göttingen, Germany; Freudenthal J., Center for Computational and Theoretical Biology, University of Wuerzburg, Wuerzburg, Germany; Korte A., Center for Computational and Theoretical Biology, University of Wuerzburg, Wuerzburg, Germany; Simianer H., Animal Breeding and Genetics Group, Department of Animal Sciences, Center for Integrated Breeding Research, University of Goettingen, Göttingen, Germany","The prediction of breeding values and phenotypes is of central importance for both livestock and crop breeding. In this study, we analyze the use of artificial neural networks (ANN) and, in particular, local convolutional neural networks (LCNN) for genomic prediction, as a region-specific filter corresponds much better with our prior genetic knowledge on the genetic architecture of traits than traditional convolutional neural networks. Model performances are evaluated on a simulated maize data panel (n = 10,000; p = 34,595) and real Arabidopsis data (n = 2,039; p = 180,000) for a variety of traits based on their predictive ability. The baseline LCNN, containing one local convolutional layer (kernel size: 10) and two fully connected layers with 64 nodes each, is outperforming commonly proposed ANNs (multi layer perceptrons and convolutional neural networks) for basically all considered traits. For traits with high heritability and large training population as present in the simulated data, LCNN are even outperforming state-of-the-art methods like genomic best linear unbiased prediction (GBLUP), Bayesian models and extended GBLUP, indicated by an increase in predictive ability of up to 24%. However, for small training populations, these state-of-the-art methods outperform all considered ANNs. Nevertheless, the LCNN still outperforms all other considered ANNs by around 10%. Minor improvements to the tested baseline network architecture of the LCNN were obtained by increasing the kernel size and of reducing the stride, whereas the number of subsequent fully connected layers and their node sizes had neglectable impact. Although gains in predictive ability were obtained for large scale data sets by using LCNNs, the practical use of ANNs comes with additional problems, such as the need of genotyping all considered individuals, the lack of estimation of heritability and reliability. Furthermore, breeding values are additive by design, whereas ANN-based estimates are not. However, ANNs also comes with new opportunities, as networks can easily be extended to account for additional inputs (omics, weather etc.) and outputs (multi-trait models), and computing time increases linearly with the number of individuals. With advances in high-throughput phenotyping and cheaper genotyping, ANNs can become a valid alternative for genomic prediction. © Copyright © 2020 Pook, Freudenthal, Korte and Simianer.","breeding; deep learning; genomic selection; Keras; machine learning; phenotype prediction; selection","Arabidopsis; Article; artificial neural network; convolutional neural network; gene frequency; genetic trait; genotype; maize; nonhuman; population size; quality control; simulation; single nucleotide polymorphism; whole genome sequencing","Bundesministerium für Bildung und Forschung, BMBF, (031B0195); Georg-August-Universität Göttingen, GAU","Funding text 1: This study was funded by the German Federal Ministry of Education and Research (BMBF) via the project MAZE (Accessing the genomic and functional diversity of maize to improve quantitative traits—Funding ID: 031B0195).; Funding text 2: We acknowledge support by the Open Access Publication Funds of the Göttingen University.","T. Pook; Animal Breeding and Genetics Group, Department of Animal Sciences, Center for Integrated Breeding Research, University of Goettingen, Göttingen, Germany; email: torsten.pook@uni-goettingen.de","","Frontiers Media S.A.","16648021","","","","English","Front. Genet.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096670163"
"Waldmann P.; Pfeiffer C.; Mészáros G.","Waldmann, Patrik (6603822225); Pfeiffer, Christina (55369979600); Mészáros, Gábor (23994075900)","6603822225; 55369979600; 23994075900","Sparse Convolutional Neural Networks for Genome-Wide Prediction","2020","Frontiers in Genetics","11","","25","","","","24","10.3389/fgene.2020.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081060406&doi=10.3389%2ffgene.2020.00025&partnerID=40&md5=4ee58fd14f1fd55345c533442ae4cb2b","Department of Animal Breeding and Genetics, The Swedish University of Agriculutural Sciences, Uppsala, Sweden; Division of Livestock Science, University of Natural Resources and Life Sciences Vienna (BOKU), Vienna, Austria","Waldmann P., Department of Animal Breeding and Genetics, The Swedish University of Agriculutural Sciences, Uppsala, Sweden; Pfeiffer C., Division of Livestock Science, University of Natural Resources and Life Sciences Vienna (BOKU), Vienna, Austria; Mészáros G., Division of Livestock Science, University of Natural Resources and Life Sciences Vienna (BOKU), Vienna, Austria","Genome-wide prediction (GWP) has become the state-of-the art method in artificial selection. Data sets often comprise number of genomic markers and individuals in ranges from a few thousands to millions. Hence, computational efficiency is important and various machine learning methods have successfully been used in GWP. Neural networks (NN) and deep learning (DL) are very flexible methods that usually show outstanding prediction properties on complex structured data, but their use in GWP is nevertheless rare and debated. This study describes a powerful NN method for genomic marker data that can easily be extended. It is shown that a one-dimensional convolutional neural network (CNN) can be used to incorporate the ordinal information between markers and, together with pooling and ℓ1-norm regularization, provides a sparse and computationally efficient approach for GWP. The method, denoted CNNGWP, is implemented in the deep learning software Keras, and hyper-parameters of the NN are tuned with Bayesian optimization. Model averaged ensemble predictions further reduce prediction error. Evaluations show that CNNGWP improves prediction error by more than 25% on simulated data and around 3% on real pig data compared with results obtained with GBLUP and the LASSO. In conclusion, the CNNGWP provides a promising approach for GWP, but the magnitude of improvement depends on the genetic architecture and the heritability. © Copyright © 2020 Waldmann, Pfeiffer and Mészáros.","deep learning; dominance; genomic selection; livestock breeding; machine learning; QTL","animal experiment; article; breeding; convolutional neural network; deep learning; genetic marker; heritability; livestock; nonhuman; pig; prediction; simulation; software","Sveriges Lantbruksuniversitet, SLU","Financial support was provided by the Beijer laboratory for animal science, SLU, Uppsala.","P. Waldmann; Department of Animal Breeding and Genetics, The Swedish University of Agriculutural Sciences, Uppsala, Sweden; email: Patrik.Waldmann@slu.se","","Frontiers Media S.A.","16648021","","","","English","Front. Genet.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85081060406"
"Nasirahmadi A.; Sturm B.; Edwards S.; Jeppsson K.-H.; Olsson A.-C.; Müller S.; Hensel O.","Nasirahmadi, Abozar (55842933900); Sturm, Barbara (54953383000); Edwards, Sandra (7401520133); Jeppsson, Knut-Håkan (55956159900); Olsson, Anne-Charlotte (36725679200); Müller, Simone (59272981900); Hensel, Oliver (26429285600)","55842933900; 54953383000; 7401520133; 55956159900; 36725679200; 59272981900; 26429285600","Deep learning and machine vision approaches for posture detection of individual pigs","2019","Sensors (Switzerland)","19","17","3738","","","","126","10.3390/s19173738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071735544&doi=10.3390%2fs19173738&partnerID=40&md5=a7fbe9192ae872d0773e5437106a0438","Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, United Kingdom; Department of Biosystems and Technology, Swedish University of Agricultural Sciences, Alnarp, 23053, Sweden; Department Animal Husbandry, Thuringian State Institute for Agriculture and Rural Development, Jena, 07743, Germany","Nasirahmadi A., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; Sturm B., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; Edwards S., School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, United Kingdom; Jeppsson K.-H., Department of Biosystems and Technology, Swedish University of Agricultural Sciences, Alnarp, 23053, Sweden; Olsson A.-C., Department of Biosystems and Technology, Swedish University of Agricultural Sciences, Alnarp, 23053, Sweden; Müller S., Department Animal Husbandry, Thuringian State Institute for Agriculture and Rural Development, Jena, 07743, Germany; Hensel O., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany","Posture detection targeted towards providing assessments for the monitoring of health and welfare of pigs has been of great interest to researchers from different disciplines. Existing studies applying machine vision techniques are mostly based on methods using three-dimensional imaging systems, or two-dimensional systems with the limitation of monitoring under controlled conditions. Thus, the main goal of this study was to determine whether a two-dimensional imaging system, along with deep learning approaches, could be utilized to detect the standing and lying (belly and side) postures of pigs under commercial farm conditions. Three deep learning-based detector methods, including faster regions with convolutional neural network features (Faster R-CNN), single shot multibox detector (SSD) and region-based fully convolutional network (R-FCN), combined with Inception V2, Residual Network (ResNet) and Inception ResNet V2 feature extractions of RGB images were proposed. Data from different commercial farms were used for training and validation of the proposed models. The experimental results demonstrated that the R-FCN ResNet101 method was able to detect lying and standing postures with higher average precision (AP) of 0.93, 0.95 and 0.92 for standing, lying on side and lying on belly postures, respectively and mean average precision (mAP) of more than 0.93. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Livestock; Lying posture; Standing posture","Algorithms; Animals; Deep Learning; Neural Networks, Computer; Posture; Swine; Agriculture; Computer vision; Convolution; Imaging systems; Mammals; Neural networks; Convolutional networks; Convolutional neural network; Livestock; Lying posture; Standing posture; Three dimensional imaging systems; Two-dimensional imaging system; Two-dimensional systems; algorithm; animal; body position; pig; Deep learning","Federal Office for Agriculture and Food, (2817ERA08D); German Federal Ministry of Food and Agriculture; Horizon 2020 Framework Programme, H2020, (696231); Horizon 2020 Framework Programme, H2020; Svenska Forskningsrådet Formas; Bundesministerium für Ernährung und Landwirtschaft, BMEL","Funding: This research was funded by the European Union’s Horizon 2020 research and innovation programme, grant number “No 696231”. The work was financially supported by the German Federal Ministry of Food and Agriculture (BMEL) through the Federal Office for Agriculture and Food (BLE), grant number “2817ERA08D” and The Swedish Research Council Formas, grant number “Dnr 2017-00152”.","A. Nasirahmadi; Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; email: abozar.nasirahmadi@uni-kassel.de","","MDPI AG","14248220","","","31470571","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85071735544"
"Bezen R.; Edan Y.; Halachmi I.","Bezen, Ran (57211393832); Edan, Yael (7004434501); Halachmi, Ilan (6701325703)","57211393832; 7004434501; 6701325703","Computer vision system for measuring individual cow feed intake using RGB-D camera and deep learning algorithms","2020","Computers and Electronics in Agriculture","172","","105345","","","","102","10.1016/j.compag.2020.105345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082381591&doi=10.1016%2fj.compag.2020.105345&partnerID=40&md5=9615611125edfca7ec86181eac0b1c9b","Institute of Agricultural Engineering, Agriculture Research Organization – The Volcani Centre, Israel; Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","Bezen R., Institute of Agricultural Engineering, Agriculture Research Organization – The Volcani Centre, Israel, Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; Edan Y., Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; Halachmi I., Institute of Agricultural Engineering, Agriculture Research Organization – The Volcani Centre, Israel, Dept. of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","This study entailed the design and implementation of a computer vision system for cow individual feed intake measurement, based on deep Convolutional Neural Networks (CNNs) models, and a low-cost RGB-D (Red, Green, Blue, Depth) camera. Individual feed intake of dairy cows is an important variable currently unavailable in commercial dairies. An RGB-D camera was positioned above the feeding area in an open cowshed. Feed intake was estimated by combining information from the RGB and depth images. Cow identification was conducted using the RGB image. Deep learning algorithms for identification and intake estimation were developed using CNN models. Data for CNN training were acquired by a specially developed automatic data acquisition system. A range of feed weights under varied configurations were collected over a period of seven days with the setup, which included an automatic scale, cameras, and a micro-controller. Test data for feed intake was acquired in an open cowshed research dairy farm, wherein the cows were fed Total Mix Ration (TMR). Images of cows eating over a period of 36 h provided the test data for cow identification. The system was able to accurately identify 93.65% of the cows. The amount of feed consumed, which ranged from 0 to 8 kg per meal, was measured with mean absolute and square errors (MAE and MSE) of 0.127 kg, and 0.034kg2 respectively. The analysis showed that the amount and diversity of data are important for model training. Better results were achieved for the model that was trained with high-diversity data than the model trained with homogeneous data (MAE of 1.025 kg, and MSE of 2.845 kg2 for a model trained on shadow conditions only). Additionally, the training analysis shows that the model based on RGB-D data shows better results than the model based on depth channel data without RGB (MAE of 0.241 kg, and MSE of 0.106 kg2). These results suggest the potential of low-cost cameras for individual feed intake measurements in advanced dairy farms. © 2020 Elsevier B.V.","Convolutional neural networks (CNN); Deep learning; Individual cow feed intake; precision livestock farming (PLF); RGB-D camera","Cameras; Computer vision; Convolution; Convolutional neural networks; Costs; Data acquisition; Deep neural networks; Farms; Green computing; Learning algorithms; Learning systems; Scales (weighing instruments); Automatic data acquisition; Computer vision system; Design and implementations; Feed intake; Model training; Model-based OPC; Precision livestock farming; Rgb-d cameras; artificial neural network; automation; cattle; computer vision; data acquisition; design; food intake; identification method; image analysis; learning; measurement method; training; Deep learning","Israeli Chief Scientist of Agriculture fund, (20-12-0029)","This study was supported by the Israeli Chief Scientist of Agriculture fund, “Kandel” PLF center of expertise, project number 20-12-0029 ; and was partially supported by the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering at Ben-Gurion University of the Negev. We gratefully thank all members of the Precision Livestock Farming (PLF) Lab. Special thanks to Dr. Victor Bloch and Harel Levit for designing and building the feed monitoring setup system; and the staff of the Volcani research cowshed for their assistance in the study.","I. Halachmi; PLF Lab. Agriculture Research Organization (ARO) – The Volcani Centre, Israel; email: halachmi@volcani.agri.gov.il","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85082381591"
"Kellenberger B.; Marcos D.; Tuia D.","Kellenberger, Benjamin (57194450044); Marcos, Diego (57188665642); Tuia, Devis (15766793800)","57194450044; 57188665642; 15766793800","Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning","2018","Remote Sensing of Environment","216","","","139","153","14","248","10.1016/j.rse.2018.06.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049529086&doi=10.1016%2fj.rse.2018.06.028&partnerID=40&md5=384f0ba56539b45054d5d38d18f245f3","Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands","Kellenberger B., Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands; Marcos D., Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands; Tuia D., Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands","Knowledge over the number of animals in large wildlife reserves is a vital necessity for park rangers in their efforts to protect endangered species. Manual animal censuses are dangerous and expensive, hence Unmanned Aerial Vehicles (UAVs) with consumer level digital cameras are becoming a popular alternative tool to estimate livestock. Several works have been proposed that semi-automatically process UAV images to detect animals, of which some employ Convolutional Neural Networks (CNNs), a recent family of deep learning algorithms that proved very effective in object detection in large datasets from computer vision. However, the majority of works related to wildlife focuses only on small datasets (typically subsets of UAV campaigns), which might be detrimental when presented with the sheer scale of real study areas for large mammal census. Methods may yield thousands of false alarms in such cases. In this paper, we study how to scale CNNs to large wildlife census tasks and present a number of recommendations to train a CNN on a large UAV dataset. We further introduce novel evaluation protocols that are tailored to censuses and model suitability for subsequent human verification of detections. Using our recommendations, we are able to train a CNN reducing the number of false positives by an order of magnitude compared to previous state-of-the-art. Setting the requirements at 90% recall, our CNN allows to reduce the amount of data required for manual verification by three times, thus making it possible for rangers to screen all the data acquired efficiently and to detect almost all animals in the reserve automatically. © 2018 Elsevier Inc.","Animal census; Convolutional Neural Networks; Deep learning; Object detection; Unmanned Aerial Vehicles; Wildlife monitoring","Animalia; Mammalia; Agriculture; Aircraft detection; Antennas; Conservation; Convolution; Convolutional neural networks; Deep neural networks; Large dataset; Learning algorithms; Mammals; Object recognition; Surveys; Unmanned aerial vehicles (UAV); Aerial vehicle; Animal census; Best practices; Convolutional neural network; Deep learning; Imbalanced dataset; Objects detection; Unmanned aerial vehicle; Vehicle images; Wildlife monitoring; algorithm; artificial neural network; census; data acquisition; data set; detection method; endangered species; genetic algorithm; instrumentation; livestock; mammal; satellite imagery; wild population; Object detection","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, (PZ00P2-136827)","This work has been supported by the Swiss National Science Foundation (grant PZ00P2-136827 (DT, http://p3.snf.ch/project-136827 ). The authors would like to acknowledge the SAVMAP consortium (in particular Dr. Friedrich Reinhard of Kuzikus Wildlife Reserve, Namibia) and the QCRI and Micromappers (in particular Dr. Ferda Ofli and Ji Kim Lucas) for the support in the collection of ground truth data.","B. Kellenberger; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands; email: benjamin.kellenberger@wur.nl","","Elsevier Inc.","00344257","","RSEEA","","English","Remote Sens. Environ.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85049529086"
"Tian M.; Guo H.; Chen H.; Wang Q.; Long C.; Ma Y.","Tian, Mengxiao (55554604500); Guo, Hao (55331600800); Chen, Hong (57050586300); Wang, Qing (57020582000); Long, Chengjiang (23976218300); Ma, Yuhao (57215733657)","55554604500; 55331600800; 57050586300; 57020582000; 23976218300; 57215733657","Automated pig counting using deep learning","2019","Computers and Electronics in Agriculture","163","","104840","","","","120","10.1016/j.compag.2019.05.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067302123&doi=10.1016%2fj.compag.2019.05.049&partnerID=40&md5=c42aa19c9a7110c63af4996c90651a84","College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Kitware Inc., Clifton Park, 12065, NY, United States","Tian M., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Guo H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China, College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; Chen H., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Wang Q., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Long C., Kitware Inc., Clifton Park, 12065, NY, United States; Ma Y., College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China","Pig counting is one of the most critical topics in farming management and asset estimation. Due to its complexity, traditional agriculture method relies on manual counting, which is obviously inefficient and a waste of manpower. The challenging aspects like partial occlusion, overlapping and different perspectives even limit the usage of traditional computer vision techniques. In recent years, deep learning has become more and more popular for computer vision applications, because of its superior performance comparing to traditional methods. In this paper, we propose a deep learning solution to address the pig counting problem. We present a modified Counting Convolutional Neural Network (Counting CNN) model according to the structure of ResNeXt, and tune a series of experimental parameters. Our CNN model learns the mapping from the image feature to the density map, and obtains the total number of pigs in the entire image by integrating the density map. In order to validate the efficacy of our proposed method, we conduct experiments on a real-world dataset collected from actual piggery farming with 15 pigs in an image averagely. We achieve 1.67 Mean Absolute Error (MAE) per image and outperforms the competing algorithms, which strongly demonstrates that our proposed method can accurately estimate the number of pigs even if they are partially occluded in different perspectives. The detection speed, 42 ms per image, meets the requirements of agricultural application. We share our code and the first pig dataset we collected for pig counting at https://github.com/xixiareone/counting-pigs for livestock husbandry and science research community. © 2019 Elsevier B.V.","Automatic; Deep learning; Pigs counting","Suidae; Agriculture; Computer vision; Mammals; Neural networks; Automatic; Competing algorithms; Computer vision applications; Convolutional neural network; Experimental parameters; Mean absolute error; Pigs counting; Traditional computers; algorithm; artificial neural network; automation; computer vision; detection method; experimental study; numerical method; parameter estimation; traditional agriculture; Deep learning","National Natural Science Foundation of China, NSFC, (41601491); National Natural Science Foundation of China, NSFC; Fundamental Research Funds for the Central Universities, (2019TC117); Fundamental Research Funds for the Central Universities","This work was supported by National Natural Science Foundation of China [Grant No. 41601491 ]; the Fundamental Research Funds for the Central Universities [Grant No. 2019TC117 ]. Thanks to ShangDong WeiHai swine-breeding center of DA BEI NONG GROUP they provide us materials for experiment. We also thank Daniel and Roberto for making their code available ( https://github.com/gramuah/ccnn ) for our research to build project.  ","H. Guo; College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; email: guohaolys@cau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85067302123"
"Khan P.W.; Byun Y.-C.; Park N.","Khan, Prince Waqas (57202838110); Byun, Yung-Cheol (8897891700); Park, Namje (8912582500)","57202838110; 8897891700; 8912582500","IoT-blockchain enabled optimized provenance system for food industry 4.0 using advanced deep learning","2020","Sensors (Switzerland)","20","10","2990","","","","199","10.3390/s20102990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085573437&doi=10.3390%2fs20102990&partnerID=40&md5=ac4cb463d76239e25fb6ef49c3e17bdd","Department of Computer Engineering, Jeju National University, Jeju City, 63243, South Korea; Department of Computer Education, Teachers College, Jeju National University, Jeju City, 63243, South Korea","Khan P.W., Department of Computer Engineering, Jeju National University, Jeju City, 63243, South Korea; Byun Y.-C., Department of Computer Engineering, Jeju National University, Jeju City, 63243, South Korea; Park N., Department of Computer Education, Teachers College, Jeju National University, Jeju City, 63243, South Korea","Agriculture and livestock play a vital role in social and economic stability. Food safety and transparency in the food supply chain are a significant concern for many people. Internet of Things (IoT) and blockchain are gaining attention due to their success in versatile applications. They generate a large amount of data that can be optimized and used efficiently by advanced deep learning (ADL) techniques. The importance of such innovations from the viewpoint of supply chain management is significant in different processes such as for broadened visibility, provenance, digitalization, disintermediation, and smart contracts. This article takes the secure IoT–blockchain data of Industry 4.0 in the food sector as a research object. Using ADL techniques, we propose a hybrid model based on recurrent neural networks (RNN). Therefore, we used long short-term memory (LSTM) and gated recurrent units (GRU) as a prediction model and genetic algorithm (GA) optimization jointly to optimize the parameters of the hybrid model. We select the optimal training parameters by GA and finally cascade LSTM with GRU. We evaluated the performance of the proposed system for a different number of users. This paper aims to help supply chain practitioners to take advantage of the state-of-the-art technologies; it will also help the industry to make policies according to the predictions of ADL. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Advanced deep learning; Blockchain; Industry 4.0; Internet of things; Livestock; Provenance","Blockchain; Deep Learning; Food Industry; Food Supply; Humans; Internet of Things; Agricultural robots; Agriculture; Blockchain; Food supply; Genetic algorithms; Industry 4.0; Internet of things; Long short-term memory; Supply chain management; Economic stability; Food industries; Genetic-algorithm optimizations; Internet of Things (IOT); Optimal training; Prediction model; Recurrent neural network (RNN); State-of-the-art technology; catering service; food industry; human; Deep learning","Ministry of Education, MOE; Ministry of Science, ICT and Future Planning, MSIP, (2019-0-00203); National Research Foundation of Korea, NRF, (NRF-2019S1A5C2A04083374); European Regional Development Fund, ERDF; Institute for Information and Communications Technology Promotion, IITP","Funding text 1: Funding: This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) [2019-0-00203, The Development of Predictive Visual Security Technology for Preemptive Threat Response]. And, This work was supported by the Ministry of Education of the Republic of Korea and the National Research Foundation of Korea (NRF-2019S1A5C2A04083374).; Funding text 2: In the existing supply chain of agriculture, the relationships between members are not very strong. In work done by Casado et al. [42], a model to strengthen this relationship and to ensure the right of information of customers was proposed. The European Regional Development Fund supported this work. This model involves BCT, smart contracts, and a multi-agent system to coordinate the tracking of food in the agriculture supply chain. Blockchain is currently used in many applications. This is a computerized system, which consists of multiple intelligent agents who interact with each other. The use of smart contracts in the blockchain to manage the entire supply chain makes the process more efficient.","Y.-C. Byun; Department of Computer Engineering, Jeju National University, Jeju City, 63243, South Korea; email: ycb@jejunu.ac.kr","","MDPI AG","14248220","","","32466209","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085573437"
"Qiao Y.; Su D.; Kong H.; Sukkarieh S.; Lomax S.; Clark C.","Qiao, Yongliang (56486770900); Su, Daobilige (56594211700); Kong, He (57203456679); Sukkarieh, Salah (6602844626); Lomax, Sabrina (56151402400); Clark, Cameron (7403546385)","56486770900; 56594211700; 57203456679; 6602844626; 56151402400; 7403546385","Data Augmentation for Deep Learning based Cattle Segmentation in Precision Livestock Farming","2020","IEEE International Conference on Automation Science and Engineering","2020-August","","9216758","979","984","5","22","10.1109/CASE48305.2020.9216758","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094123876&doi=10.1109%2fCASE48305.2020.9216758&partnerID=40&md5=57be9c5755d3cfe114a6be07a082cabb","University of Sydney, Australian Centre for Field Robotics, Australia; University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia","Qiao Y., University of Sydney, Australian Centre for Field Robotics, Australia; Su D., University of Sydney, Australian Centre for Field Robotics, Australia; Kong H., University of Sydney, Australian Centre for Field Robotics, Australia; Sukkarieh S., University of Sydney, Australian Centre for Field Robotics, Australia; Lomax S., University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia; Clark C., University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia","Accurate segmentation of cattle is a prerequisite for feature extraction and estimation. Convolutional neural networks (CNN) based approaches that train models on the largescale labeled datasets have achieved high levels of segmentation performance. However, pixel-wise manual labeling of a cattle image is challenging and time consuming due to the irregularity of the cattle contour. In this regard, data augmentation for deep learning based cattle segmentation is required. Our proposed data augmentation approach uses random image cropping and patching to expand the number of training images and their corresponding labels, then, a state-of-the-art deep neural net is trained to segment cattle images. Here we apply these techniques to images of cattle in a feedlot environment. Our data augmentation-based approach segmented cattle from a complex background with 99.5% mean Accuracy (mAcc) and 97.3% mean Intersection of Unions (mIoU), improving current techniques including a combination of random flipping, rotation and color jitter. © 2020 IEEE.","","Agriculture; Convolutional neural networks; Deep neural networks; Image segmentation; Complex background; Data augmentation; Labeled datasets; Manual labeling; Precision livestock farming; Segmentation performance; State of the art; Training image; Deep learning","Meat & Livestock Australia Donor Company, (P.PSH.0819)","The authors acknowledge the support of the Meat & Livestock Australia Donor Company through the project: Objective, robust, real-time animal welfare measures for the Australian red meat industry (P.PSH.0819). The authors also express their gratitude to Khalid Rafique, Javier Martinez, Amanda Doughty, Ashraful Islam, and Mike Reynolds for their help in experiment organization and data collection.","","","IEEE Computer Society","21618070","978-172816904-0","","","English","IEEE Int. Conf. Autom. Sci. Eng.","Conference paper","Final","","Scopus","2-s2.0-85094123876"
"Huang M.-H.; Lin E.-C.; Kuo Y.-F.","Huang, Mao-Hsiang (57211186757); Lin, En-Chung (7201721121); Kuo, Yan-Fu (55823045200)","57211186757; 7201721121; 55823045200","Determining the body condition scores of sows using convolutional neural networks","2019","2019 ASABE Annual International Meeting","","","","","","","3","10.13031/aim.201900915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084011502&doi=10.13031%2faim.201900915&partnerID=40&md5=8e35476b8da7ae59e5dcc40a8aa0f779","Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan; Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan","Huang M.-H., Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan; Lin E.-C., Department of Animal Science and Technology, National Taiwan University, Taipei, Taiwan; Kuo Y.-F., Department of Bio-Industrial Mechatronics Engineering, National Taiwan University, Taipei, Taiwan","The body condition of a sow indicates its degree of obesity and, hence, critically determines the health and productivity of the sow during the next pregnancy. Precisely scoring the body conditions of sows is essential in feeding management. Conventionally, the body conditions of sows are scored from rear views by breeders. Manual observation, however, largely relies on the experience of the breeders and can be subjective. This study aimed to score the body conditions of sows using image processing and deep learning. A convolutional neural network was developed to identify the bodies of sows in images. The aspect and conformation ratios of the sows were then determined. Body conditions were then scored based on these ratios. The study proved that image-based scoring of sow body conditions was achievable and could be applied to the livestock industry. © 2019 ASABE Annual International Meeting. All rights reserved.","Fully convolutional neural network; Image processing; Semantic segmentation; Sow body condition scores","Agriculture; Convolution; Deep learning; Image processing; Neural networks; Semantics; Body condition; Body condition score; Conformation ratio; Convolutional neural network; Image-based; Semantic segmentation; Image segmentation","","","","","American Society of Agricultural and Biological Engineers","","","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85084011502"
"Xu B.; Wang W.; Falzon G.; Kwan P.; Guo L.; Chen G.; Tait A.; Schneider D.","Xu, Beibei (57215186247); Wang, Wensheng (56937276700); Falzon, Greg (13407283800); Kwan, Paul (7004369297); Guo, Leifeng (56542160600); Chen, Guipeng (57115604400); Tait, Amy (57215188868); Schneider, Derek (34771975200)","57215186247; 56937276700; 13407283800; 7004369297; 56542160600; 57115604400; 57215188868; 34771975200","Automated cattle counting using Mask R-CNN in quadcopter vision system","2020","Computers and Electronics in Agriculture","171","","105300","","","","157","10.1016/j.compag.2020.105300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080029752&doi=10.1016%2fj.compag.2020.105300&partnerID=40&md5=3ac81821b1ce0d17accc6697e00baba4","Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; School of Science & Technology, University of New England, Armidale, 2351, NSW, Australia; Precision Agriculture Research Group, University of New England, Armidale, 2351, NSW, Australia; School of Information Technology & Engineering, Melbourne Institute of Technology, Melbourne, 3000, VIC, Australia; Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; School of Environmental and Rural Science, University of New England, Armidale, 2351, NSW, Australia","Xu B., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Wang W., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Falzon G., School of Science & Technology, University of New England, Armidale, 2351, NSW, Australia, Precision Agriculture Research Group, University of New England, Armidale, 2351, NSW, Australia; Kwan P., School of Information Technology & Engineering, Melbourne Institute of Technology, Melbourne, 3000, VIC, Australia; Guo L., Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; Chen G., Agricultural Economics and Information Institute, Jiangxi Academy of Agriculture Sciences, Nanchang, 330200, China; Tait A., School of Environmental and Rural Science, University of New England, Armidale, 2351, NSW, Australia; Schneider D., School of Science & Technology, University of New England, Armidale, 2351, NSW, Australia, Precision Agriculture Research Group, University of New England, Armidale, 2351, NSW, Australia","The accurate and reliable counting of animals in quadcopter acquired imagery is one of the most promising but challenging tasks in intelligent livestock management in the future. In this paper we demonstrate the application of the cutting-edge instance segmentation framework, Mask R-CNN, in the context of cattle counting in different situations such as extensive production pastures and also in intensive housing such as feedlots. The optimal IoU threshold (0.5) and the full-appearance detection for the algorithm in this study are verified through performance evaluation. Experimental results in this research show the framework's potential to perform reliably in offline quadcopter vision systems with an accuracy of 94% in counting cattle on pastures and 92% in feedlots. Compared with the existing typical competing algorithms, Mask R-CNN outperforms both in the counting accuracy and average precision especially on the datasets with occlusion and overlapping. Our research shows promising steps towards the incorporation of artificial intelligence using quadcopters for enhanced management of animals. © 2020 The Authors","Deep learning; Livestock management; Object detection; Quadcopter vision system; Remote monitoring","Animalia; Bos; Animals; Deep learning; Object detection; Competing algorithms; Cutting edges; Offline; Remote monitoring; Vision systems; algorithm; artificial intelligence; cattle; detection method; equipment; livestock farming; monitoring system; segmentation; Agriculture","Beijing Aokemei Technical Service Company Limited; Fundamental Research Funds of Agricultural Information Institute of Chinese Academy of Agriculture Sciences, (JBYW-AII-2019-19); General Project of Jiangxi Province Key Research and Development Plan, (20192BBF60053); Meat and Livestock Australia, MLA, (AEC18-308, AEC19-009); Meat and Livestock Australia, MLA; University of New England, UNE; Shanxi Province Science Foundation for Youths, (20192ACBL21023); Shanxi Province Science Foundation for Youths","This research was funded by Beijing Aokemei Technical Service Company Limited and also was supported by Fundamental Research Funds of Agricultural Information Institute of Chinese Academy of Agriculture Sciences, China ( JBYW-AII-2019-19 ), General Project of Jiangxi Province Key Research and Development Plan ( 20192BBF60053 ) and Jiangxi Province Science Foundation for Youths ( 20192ACBL21023 ). Imagery of the feedlot animals was provided by a University of New England project funded by Meat and Livestock Australia (MLA) (University of New England Animal Ethics Approval Number AEC18-308 ) and we are grateful to three private farmlands in New England in Australia for their kindly support with data collection (University of New England Standard Operating Procedure W14 Camera Traps and Animal Ethics Approval Number AEC19-009 ).  ","W. Wang; Agricultural Information Institute, Chinese Academy of Agriculture Sciences, Beijing, 100086, China; email: wangwensheng@caas.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85080029752"
"Ye C.-W.; Yu Z.-W.; Kang R.; Yousaf K.; Qi C.; Chen K.-J.; Huang Y.-P.","Ye, Chang-wen (56440789200); Yu, Zhen-wei (57193696428); Kang, Rui (55898264500); Yousaf, Khurram (56974101100); Qi, Chao (57204354922); Chen, Kun-jie (13103767000); Huang, Yu-ping (57196141992)","56440789200; 57193696428; 55898264500; 56974101100; 57204354922; 13103767000; 57196141992","An experimental study of stunned state detection for broiler chickens using an improved convolution neural network algorithm","2020","Computers and Electronics in Agriculture","170","","105284","","","","27","10.1016/j.compag.2020.105284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079902403&doi=10.1016%2fj.compag.2020.105284&partnerID=40&md5=8f2f45bbc0a6735fde263cd0b0fcf7bb","College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; National Innovation Pack for Modern Agri-technology (Nanjing), Nanjing, 211800, China; College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, Jiangsu, China","Ye C.-W., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China, National Innovation Pack for Modern Agri-technology (Nanjing), Nanjing, 211800, China; Yu Z.-W., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Kang R., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Yousaf K., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Qi C., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Chen K.-J., College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; Huang Y.-P., College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, Jiangsu, China","Effective recognition method of broiler stunned state has always been an important issue in real industries. In recent years, recognition methods such as neural networks have been receiving increasing attention due to their great merits of high diagnostic accuracy and easy implementation. To improve the accuracy and efficiency of broiler stunned state recognition, an improved fast region-based convolutional neural network (You Only Look Once + Multilayer Residual Module (YOLO + MRM)) algorithm was proposed and applied to the recognition of three broiler stunned states: insufficient, appropriate and excessive stuns. The images were collected from a broiler-slaughtering line using a complementary metal-oxide semiconductor (CMOS) camera. The area of the head and wings of a broiler in the original image was marked according to the PASCAL VOC data format and the dataset of each broiler stunned state was obtained. The results showed that the YOLO + MRM algorithm achieved good performance with an accuracy of 96.77%. To compare YOLO + MRM with other models, similar experiments were conducted using a conventional back propagation neural network (BP-NN) classifier, as well as YOLO, and the recognition accuracies were 90.11% and 94.74%, respectively. YOLO + MRM can complete the detection task of more than 180,000 broilers per hour. Compared with the traditional method, little prior expertise on image recognition is required, the recognition accuracy and speed are improved obviously. This study has provided a foundation and highlighted the potential for automatically detecting the stunned state of broiler chickens, which is crucial for the success of an automatic electric stunning process in the poultry industry. © 2020 Elsevier B.V.","Broiler; Convolutional neural network; Deep learning; Electrical stunning; Stunned state detection","Gallus gallus; Animals; Backpropagation; CMOS integrated circuits; Convolution; Deep learning; Deep neural networks; Image enhancement; Image recognition; Metals; MOS devices; Multilayer neural networks; Oxide semiconductors; Backpropagation neural networks; Broiler; Complementary metal oxide semiconductors; Convolution neural network; Diagnostic accuracy; Electrical stunning; Recognition accuracy; State Detection; accuracy assessment; artificial neural network; back propagation; experimental study; image analysis; livestock farming; poultry; volatile organic compound; Convolutional neural networks","China National Broiler Industry Technology System, (CARS-42-5); China National Science and Technology Support Program, (2015BAD19806)"," The China National Science and Technology Support Program (grant number “ 2015BAD19806 ”) and the China National Broiler Industry Technology System (grant number “CARS-42-5”) funded this research. The authors would also like to thank Professor Chen Kunjie of Nanjing Agricultural University for his technical support.  ","K.-J. Chen; College of Engineering, Nanjing Agricultural University, Nanjing, 210031, China; email: kunjiechen@njau.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85079902403"
"Samarin M.; Zweifel L.; Roth V.; Alewell C.","Samarin, Maxim (57212194839); Zweifel, Lauren (57211484276); Roth, Volker (7005561326); Alewell, Christine (57206454235)","57212194839; 57211484276; 7005561326; 57206454235","Identifying soil erosion processes in alpine grasslands on aerial imagery with a u-net convolutional neural network","2020","Remote Sensing","12","24","4149","1","21","20","15","10.3390/rs12244149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098257991&doi=10.3390%2frs12244149&partnerID=40&md5=be099faf73f5d27279ca8fbdeb91b312","Department of Mathematics and Computer Science, University of Basel, Spiegelgasse 1, Basel, 4051, Switzerland; Department of Environmental Sciences, University of Basel, Bernoullistrasse 30, Basel, 4056, Switzerland","Samarin M., Department of Mathematics and Computer Science, University of Basel, Spiegelgasse 1, Basel, 4051, Switzerland; Zweifel L., Department of Environmental Sciences, University of Basel, Bernoullistrasse 30, Basel, 4056, Switzerland; Roth V., Department of Mathematics and Computer Science, University of Basel, Spiegelgasse 1, Basel, 4051, Switzerland; Alewell C., Department of Environmental Sciences, University of Basel, Bernoullistrasse 30, Basel, 4056, Switzerland","Erosion in alpine grasslands is a major threat to ecosystem services of alpine soils. Natural causes for the occurrence of soil erosion are steep topography and prevailing climate conditions in combination with soil fragility. To increase our understanding of ongoing erosion processes and support sustainable land-use management, there is a need to acquire detailed information on spatial occurrence and temporal trends. Existing approaches to identify these trends are typically laborious, have lack of transferability to other regions, and are consequently only applicable to smaller regions. In order to overcome these limitations and create a sophisticated erosion monitoring tool capable of large-scale analysis, we developed a model based on U-Net, a fully convolutional neural network, to map different erosion processes on high-resolution aerial images (RGB, 0.25–0.5 m). U-Net was trained on a high-quality data set consisting of labeled erosion sites mapped with object-based image analysis (OBIA) for the Urseren Valley (Central Swiss Alps) for five aerial images (16 year period). We used the U-Net model to map the same study area and conduct quality assessments based on a held-out test region and a temporal transferability test on new images. Erosion classes are assigned according to their type (shallow landslide and sites with reduced vegetation affected by sheet erosion) or land-use impacts (livestock trails and larger management affected areas). We show that results obtained by OBIA and U-Net follow similar linear trends for the 16 year study period, exhibiting increases in total degraded area of 167% and 201%, respectively. Segmentations of eroded sites are generally in good agreement, but also display method-specific differences, which lead to an overall precision of 73%, a recall of 84%, and a F1-score of 78%. Our results show that U-Net is transferable to spatially (within our study area) and temporally unseen data (data from new years) and is therefore a method suitable to efficiently and successfully capture the temporal trends and spatial heterogeneity of degradation in alpine grasslands. Additionally, U-Net is a powerful and robust tool to map erosion sites in a predictive manner utilising large amounts of new aerial imagery. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Erosion mapping; Landslides; Livestock trails; Object-based image analysis; Remote sensing; Semantic segmentation; Sheet erosion","Aerial photography; Agriculture; Antennas; Convolution; Ecosystems; Erosion; Image analysis; Land use; Quality control; Soils; Topography; Ecosystem services; Erosion monitoring; High-resolution aerial images; Large-scale analysis; Object based image analysis (OBIA); Quality assessment; Spatial heterogeneity; Sustainable land use; Convolutional neural networks","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, (407540 167333, 167333)","Funding: This research was funded by Swiss National Science Foundation with the grant number 407540 167333 as part of the Swiss National Research Programme NRP 75 \u201CBig Data\u201D.","M. Samarin; Department of Mathematics and Computer Science, University of Basel, Basel, Spiegelgasse 1, 4051, Switzerland; email: maxim.samarin@unibas.ch","","MDPI AG","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85098257991"
"Huang L.; Guo H.; Rao Q.; Hou Z.; Li S.; Qiu S.; Fan X.; Wang H.","Huang, Lvwen (55497575600); Guo, Han (57211963155); Rao, Qinqin (57211969704); Hou, Zixia (57211969060); Li, Shuqin (55490641400); Qiu, Shicheng (57211961901); Fan, Xinyun (57200149289); Wang, Hongyan (57207240591)","55497575600; 57211963155; 57211969704; 57211969060; 55490641400; 57211961901; 57200149289; 57207240591","Body dimension measurements of qinchuan cattle with transfer learning from liDAR sensing","2019","Sensors (Switzerland)","19","22","5046","","","","32","10.3390/s19225046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075534801&doi=10.3390%2fs19225046&partnerID=40&md5=35a333d7b247e065edeaef68b46a8b52","College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, Xianyang, China; College of Computer Science, Wuhan University, Wuhan, 430072, China; Western E-commerce Co., Ltd, Yinchuan, 750004, China","Huang L., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, Xianyang, China; Guo H., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China; Rao Q., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China; Hou Z., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China; Li S., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, Xianyang, China; Qiu S., College of Information Engineering, Northwest A&F University, Yangling, 712100, Xianyang, China; Fan X., College of Computer Science, Wuhan University, Wuhan, 430072, China; Wang H., Western E-commerce Co., Ltd, Yinchuan, 750004, China","For the time-consuming and stressful body measuring task of Qinchuan cattle and farmers, the demand for the automatic measurement of body dimensions has become more and more urgent. It is necessary to explore automatic measurements with deep learning to improve breeding efficiency and promote the development of industry. In this paper, a novel approach to measuring the body dimensions of live Qinchuan cattle with on transfer learning is proposed. Deep learning of the Kd-network was trained with classical three-dimensional (3D) point cloud datasets (PCD) of the ShapeNet datasets. After a series of processes of PCD sensed by the light detection and ranging (LiDAR) sensor, the cattle silhouettes could be extracted, which after augmentation could be applied as an input layer to the Kd-network. With the output of a convolutional layer of the trained deep model, the output layer of the deep model could be applied to pre-train the full connection network. The TrAdaBoost algorithm was employed to transfer the pre-trained convolutional layer and full connection of the deep model. To classify and recognize the PCD of the cattle silhouette, the average accuracy rate after training with transfer learning could reach up to 93.6%. On the basis of silhouette extraction, the candidate region of the feature surface shape could be extracted with mean curvature and Gaussian curvature. After the computation of the FPFH (fast point feature histogram) of the surface shape, the center of the feature surface could be recognized and the body dimensions of the cattle could finally be calculated. The experimental results showed that the comprehensive error of body dimensions was close to 2%, which could provide a feasible approach to the non-contact observations of the bodies of large physique livestock without any human intervention. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Body dimensions; Deep learning; Feature recognition; FFPH; Kd-network; Non-contact measurement; Point cloud; Transfer learning","Agriculture; Convolution; Optical radar; Body dimensions; Feature recognition; FFPH; Noncontact measurements; Point cloud; Transfer learning; Deep learning","Chief Scientist of the National Beef Cattle Improvement Center; Key Research and Development Project in Ningxia Hui Nationality Autonomous Region, (2017BY067); Ministry of Agriculture and Rural Affairs of the People's Republic of China, MOA","Funding text 1: Funding: This research was funded by the Key Research and Development Project in Ningxia Hui Nationality Autonomous Region, grant number 2017BY067.; Funding text 2: Acknowledgments: All of the authors thank the research group of Prof. Linsen Zan (Chief Scientist of the National Beef Cattle Improvement Center, Ministry of Agriculture and Rural Affairs, China) for supporting the manual guidance and holding of Qinchuan cattle.","L. Huang; College of Information Engineering, Northwest A&F University, Yangling, 712100, China; email: huanglvwen@nwafu.edu.cn","","MDPI AG","14248220","","","31752400","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85075534801"
"Fernández-Carrión E.; Barasona J.A.; Sánchez A.; Jurado C.; Cadenas-Fernández E.; Sánchez-Vizcaíno J.M.","Fernández-Carrión, Eduardo (55600521200); Barasona, Jose Ángel (53063213000); Sánchez, Ángel (57210408461); Jurado, Cristina (57159598900); Cadenas-Fernández, Estefanía (57208565155); Sánchez-Vizcaíno, José Manuel (7003522540)","55600521200; 53063213000; 57210408461; 57159598900; 57208565155; 7003522540","Computer vision applied to detect lethargy through animal motion monitoring: A trial on african swine fever inwild boar","2020","Animals","10","12","2241","1","12","11","15","10.3390/ani10122241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098276131&doi=10.3390%2fani10122241&partnerID=40&md5=2b6e9309bc524a9214fbda0cacf73dc8","VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; Computer Science School, Universidad Rey Juan Carlos, Móstoles, 28933, Spain","Fernández-Carrión E., VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; Barasona J.A., VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; Sánchez A., Computer Science School, Universidad Rey Juan Carlos, Móstoles, 28933, Spain; Jurado C., VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; Cadenas-Fernández E., VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; Sánchez-Vizcaíno J.M., VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain","Early detection of infectious diseases is the most cost-effective strategy in disease surveillance for reducing the risk of outbreaks. Latest deep learning and computer vision improvements are powerful tools that potentially open up a new field of research in epidemiology and disease control. These techniques were used here to develop an algorithm aimed to track and compute animal motion in real time. This algorithm was used in experimental trials in order to assess African swine fever (ASF) infection course in Eurasian wild boar. Overall, the outcomes showed negative correlation between motion reduction and fever caused by ASF infection. In addition, infected animals computed significant lower movements compared to uninfected animals. The obtained results suggest that a motion monitoring system based on artificial vision may be used in indoors to trigger suspicions of fever. It would help farmers and animal health services to detect early clinical signs compatible with infectious diseases. This technology shows a promising non-intrusive, economic and real time solution in the livestock industry with especial interest in ASF, considering the current concern in the world pig industry. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","African swine fever; Artificial intelligence; Computer vision; Infectious disease","African swine fever; African swine fever virus; algorithm; animal experiment; animal health; Article; artificial intelligence; boar (male pig); China; computer vision; controlled study; disease control; disease surveillance; European wild boar; gait; genotype; health service; image segmentation; livestock; motion; nerve cell network; nonhuman; recognition; training","Instituto Nacional de Investigación y Tecnología Agraria y Alimentaria, INIA; Spanish Ministry of Economy.The; European Commission, EC, (H2020-SFS-2019-1, IJCI-2017-33539); Horizon 2020 Framework Programme, H2020, (862874); Ministerio de Economía y Competitividad, MINECO, (RTA2015-00033-C02-02); Ministerio de Educación, Cultura y Deporte, MECD, (RTI2018-098019-B-I00)","Funding text 1: This work was partially funded by the Spanish Ministry of Economy and Competitiveness, grant number RTA2015-00033-C02-02 (INIA) and by the European Commission, VACDIVA project, grant number H2020-SFS-2019-1. JAB is supported by a \u2018Juan de la Cierva\u2019 contract (IJCI-2017-33539)from MINECO and VACDIVA project (H2020-SFS-2019-1). CJ and EC are recipients of a Spanish Government-funded PhD fellowship for the Training of Future Scholars (FPU) given by the Spanish Ministry of Education, Culture and Sports. AS has been benefited from the financial support of the project RTI2018-098019-B-I00 from the Spanish Ministry of Economy.The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.; Funding text 2: Funding: This work was partially funded by the Spanish Ministry of Economy and Competitiveness, grant number RTA2015-00033-C02-02 (INIA) and by the European Commission, VACDIVA project, grant number H2020-SFS-2019-1. JAB is supported by a \u2019Juan de la Cierva\u2019 contract (IJCI-2017-33539)from MINECO and VACDIVA project (H2020-SFS-2019-1). CJ and EC are recipients of a Spanish Government-funded PhD fellowship for the Training of Future Scholars (FPU) given by the Spanish Ministry of Education, Culture and Sports. AS has been benefited from the financial support of the project RTI2018-098019-B-I00 from the Spanish Ministry of Economy.The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.","E. Cadenas-Fernández; VISAVET Center and Animal Health Department, Veterinary School, Universidad Complutense de Madrid, Madrid, 28040, Spain; email: estefaca@ucm.es","","MDPI AG","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85098276131"
"Qi Y.; Baiyang C.; Lan L.","Qi, Yan (57482725300); Baiyang, Cheng (57482725400); Lan, Luo (55196588300)","57482725300; 57482725400; 55196588300","Deep Learning Based Image Recognition in Animal Husbandry","2021","2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2021","","","","318","321","3","2","10.1109/ICCWAMTIP53232.2021.9674177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125999883&doi=10.1109%2fICCWAMTIP53232.2021.9674177&partnerID=40&md5=da1b37f8871e153353848bde6f9fbe13","Chengdu College of University of Electronic Science and Technology of China, Chengdu Sichuan, 611731, China","Qi Y., Chengdu College of University of Electronic Science and Technology of China, Chengdu Sichuan, 611731, China; Baiyang C., Chengdu College of University of Electronic Science and Technology of China, Chengdu Sichuan, 611731, China; Lan L., Chengdu College of University of Electronic Science and Technology of China, Chengdu Sichuan, 611731, China","Deep learning technology is an important new force in the emerging science and technology revolution and the revolution of the animal husbandry industry, and plays a crucial role in the process of being digitization, informatization and wisdom of the animal husbandry industry in China. The application of deep learning-based image recognition in the livestock industry provides a new solution to the problems of disease prevention, precise identification and biosafety prevention and control at the farming side, and will become a powerful booster to promote the livestock industry towards modernization. The use of convolutional neural network after extracting a feature to complete the link according to the type of feature classification, then complete the data pre-processing, and using super pixel-based image segmentation and SIFT algorithm to complete image segmentation and image feature extraction, and finally through the convolutional neural network and support vector machine to complete the classification and prediction of animal action, driving the overall management level of the livestock industry to improve, and become an effective way to promote the development of intelligent animal husbandry.  © 2021 IEEE.","Animal behavior analysis; Deep learning; Image segmentation; Smart animal husbandry","Agriculture; Animals; Convolution; Data handling; Deep learning; Disease control; Engineering education; Image enhancement; Image recognition; Image segmentation; Support vector machines; Animal behavior analyse; Animal behaviour; Animal husbandry; Behavior analysis; Convolutional neural network; Deep learning; Emerging science; Images segmentations; Learning technology; Smart animal husbandry; Convolutional neural networks","Chengdu Chengdian Network Technology Co.,Ltd., Chengdu Civil-military Integration Project Management Co.,Ltd.; Chengdu Haitian Digital Technology Co.,Ltd.; Chongqing Qinchengxing Technology Co.,Ltd.; Science and Technology Department of Chongqing Municipality; Sichuan YinTenGu Technology Co.,Ltd.; National Natural Science Foundation of China, NSFC; Department of Science and Technology of Sichuan Province, SPDST, (2021YFG0322); Chongqing Municipal Education Commission, CQMEC, (GrantNo.KJZD-K20211 4401); National High-tech Research and Development Program","This work was supported by the National Natural Science Foundation of China, the National High Technology Research and Development Program of China, the project of Science and Technology Department of Sichuan Province (Grant No.2021YFG0322), the project of Science and Technology Department of Chongqing Municipality, the Science and Technology Research Program of Chongqing Municipal Education Commission (GrantNo.KJZD-K20211 4401), Chongqing Qinchengxing Technology Co.,Ltd., Chengdu Haitian Digital Technology Co.,Ltd., Chengdu Chengdian Network Technology Co.,Ltd., Chengdu Civil-military Integration Project Management Co.,Ltd., and Sichuan YinTenGu Technology Co.,Ltd.","","","Institute of Electrical and Electronics Engineers Inc.","","978-166541364-0","","","English","Int. Comput. Conf. Wavelet Act. Media Technol. Inf. Process., ICCWAMTIP","Conference paper","Final","","Scopus","2-s2.0-85125999883"
"Zheng H.; Nakabayashi Y.; Masuda M.; Nishi H.; Yamanaka D.; Takahashi S.-I.; Hakuno F.; Miyauchi Y.; Okazaki T.; Yoshida K.; Shioya R.","Zheng, Hongjie (57199906148); Nakabayashi, Yasushi (7005773586); Masuda, Masato (55680989900); Nishi, Hiroki (56765334100); Yamanaka, Daisuke (57202674023); Takahashi, Shin-Ichiro (55481507500); Hakuno, Fumihiko (6602281004); Miyauchi, Yosuke (57952733400); Okazaki, Takashi (57952518600); Yoshida, Kazuhiro (57952733500); Shioya, Ryuji (6602958828)","57199906148; 7005773586; 55680989900; 56765334100; 57202674023; 55481507500; 6602281004; 57952733400; 57952518600; 57952733500; 6602958828","Effect Verification of Transfer Learning on Fatty Liver Classification with Chicken Liver Image based on Convolutional Neural Networks","2020","Transactions of the Japan Society for Computational Engineering and Science","2020","1","20201003","","","","1","10.11421/jsces.2020.20201003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124657059&doi=10.11421%2fjsces.2020.20201003&partnerID=40&md5=c898a82e3b218a1efae6aef3d853d4da","Faculty of Science and Engineering, Toyo University, Japan; Faculty of Information Sciences and Arts, Toyo University, Japan; Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Leave a Nest Co., Ltd., Japan","Zheng H., Faculty of Science and Engineering, Toyo University, Japan; Nakabayashi Y., Faculty of Information Sciences and Arts, Toyo University, Japan; Masuda M., Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Nishi H., Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Yamanaka D., Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Takahashi S.-I., Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Hakuno F., Graduate School of Agriculture and Life Sciences, The University of Tokyo, Japan; Miyauchi Y., Leave a Nest Co., Ltd., Japan; Okazaki T., Leave a Nest Co., Ltd., Japan; Yoshida K., Leave a Nest Co., Ltd., Japan; Shioya R., Faculty of Information Sciences and Arts, Toyo University, Japan","Fat content is an important index of the added value of meat and livestock by-products. In this study, CNN was used to classify the normal liver and fatty liver to identify the morphology of chicken liver. To recognize the appearance of chicken liver, the feature extraction method, and the trained deep learning model vgg16 were used for transfer learning. The validity of vgg16 is verified by comparing it with the baseline model without transfer learning. © 2020, Japan Society for Computational Engineering and Science. All rights reserved.","Chicken fatty liver; Convolution neural networks (CNN); Transfer learning","Agriculture; Animals; Convolution; Convolutional neural networks; Deep learning; Diseases; Learning systems; Transfer learning; Chicken fatty liver; Chicken liver; Convolution neural network; Convolutional neural network; Fat contents; Fatty liver; Image-based; Liver images; Transfer learning; Image classification","","","","","Japan Society for Computational Engineering and Science","13478826","","","","Japanese","Trans. Jpn. Soc. Comput. Eng. Sci.","Article","Final","","Scopus","2-s2.0-85124657059"
"Szymanski L.; Lee M.","Szymanski, Lech (55338133300); Lee, Michael (23489354700)","55338133300; 23489354700","Deep Sheep: Kinship assignment in livestock from facial images","2020","International Conference Image and Vision Computing New Zealand","2020-November","","9290558","","","","4","10.1109/IVCNZ51579.2020.9290558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099461044&doi=10.1109%2fIVCNZ51579.2020.9290558&partnerID=40&md5=293abd437a5cae5e16af9584573515b5","University of Otago, Department of Computer Science, Dunedin, New Zealand; University of Otago, Department of Mathematics and Statistics, Dunedin, New Zealand","Szymanski L., University of Otago, Department of Computer Science, Dunedin, New Zealand; Lee M., University of Otago, Department of Mathematics and Statistics, Dunedin, New Zealand","For the non-farmer folk all sheep might look the same, but they are in fact morphologically quite different; including when it comes to facial features. Image analysis has already demonstrated that computer-based facial recognition in livestock is very accurate. We investigate the viability of deep learning for assigning kinship in livestock for use in genetic evaluation-given two images of sheep faces, our proposed model predicts their genetic relationship. In this work we present two CNN models: one for face detection (reporting 80% accuracy) and one for kinship detection (reporting 68% balanced accuracy).  © 2020 IEEE.","","Agriculture; Computer aided analysis; Deep learning; CNN models; Facial feature; Facial images; Facial recognition; Genetic evaluation; Genetic relationships; Face recognition","Science for Technological Innovation National Science Challenge","ACKNOWLEDGMENT All figures are licensed by the authors for use under the Creative Commons Attribution-ShareAlike 3.0 Unported License (CC-BY-SA, https://creativecommons.org/licenses/by-sa/3.0/). If reusing these figures please make reference to this article. The authors acknowledge the Science for Technological Innovation National Science Challenge (SfTI) for funding this project and thank Beef + Lamb New Zealand Genetics for access to data and staff at AgRsearch limited, Invermay, Mosgiel for assisting with data collection.","","","IEEE Computer Society","21512191","978-172818579-8","","","English","Int. Conf. Image Vis. Comput. New Zealand","Conference paper","Final","","Scopus","2-s2.0-85099461044"
"Tödtmann H.; Vahl M.; von Lukas U.F.; Ullrich T.","Tödtmann, Helmut (57216438121); Vahl, Matthias (54685267100); von Lukas, Uwe Freiherr (6506546807); Ullrich, Torsten (11440413100)","57216438121; 54685267100; 6506546807; 11440413100","Time-unfolding object existence detection in low-quality underwater videos using convolutional neural networks","2020","VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5","","","370","377","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083493724&partnerID=40&md5=98f86ed9d75010c6a1ad578f6b049aee","Fraunhofer Institute for Computer Graphics Research IGD, Rostock, Germany; University of Rostock, Institute for Computer Science, Rostock, Germany; Fraunhofer Austria Research GmbH, Visual Computing, Graz, Austria; Institute of Computer Graphics and Knowledge Visualization, Graz University of Technology, Austria","Tödtmann H., Fraunhofer Institute for Computer Graphics Research IGD, Rostock, Germany, Institute of Computer Graphics and Knowledge Visualization, Graz University of Technology, Austria; Vahl M., Fraunhofer Institute for Computer Graphics Research IGD, Rostock, Germany; von Lukas U.F., Fraunhofer Institute for Computer Graphics Research IGD, Rostock, Germany, University of Rostock, Institute for Computer Science, Rostock, Germany; Ullrich T., Fraunhofer Austria Research GmbH, Visual Computing, Graz, Austria, Institute of Computer Graphics and Knowledge Visualization, Graz University of Technology, Austria","Monitoring the environment for early recognition of changes is necessary for assessing the success of renaturation measures on a facts basis. It is also used in fisheries and livestock production for monitoring and for quality assurance. The goal of the presented system is to count sea trouts annually over the course of several months. Sea trouts are detected with underwater camera systems triggered by motion sensors. Such a scenario generates many videos that have to be evaluated manually. This article describes the techniques used to automate the image evaluation process. An effective method has been developed to classify videos and determine the times of occurrence of sea trouts, while significantly reducing the annotation effort. A convolutional neural network has been trained via supervised learning. The underlying images are frame compositions automatically extracted from videos on which sea trouts are to be detected. The accuracy of the resulting detection system reaches values of up to 97.7 %. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Convolutional Neural Network; Deep Learning; Detection; Environmental Monitoring; Implicit Segmentation","Agriculture; Computer graphics; Computer vision; Convolution; Motion sensors; Object detection; Quality assurance; Detection system; Image evaluation; Livestock production; Low qualities; Renaturation; Convolutional neural networks","Carinthian Government; City of Klagenfurt within the innovation center KI4Life","Furthermore, the authors acknowledge the generous support of the Carinthian Government and the City of Klagenfurt within the innovation center KI4Life.","","Farinella G.M.; Radeva P.; Braz J.","SciTePress","","978-989758402-2","","","English","VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Comput. Graph. Theory Appl.","Conference paper","Final","","Scopus","2-s2.0-85083493724"
"Ruchay A.; Dorofeev K.; Kalschikov V.; Kolpakov V.; Dzhulamanov K.; Guo H.","Ruchay, Alexey (57192592568); Dorofeev, Konstantin (57203992985); Kalschikov, Vsevolod (57203985770); Kolpakov, Vladimir (58511094300); Dzhulamanov, Kinispay (57203805515); Guo, Hao (55331600800)","57192592568; 57203992985; 57203985770; 58511094300; 57203805515; 55331600800","Live weight prediction of cattle using deep image regression","2021","2021 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2021 - Proceedings","","","","32","36","4","4","10.1109/MetroAgriFor52389.2021.9628547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123418041&doi=10.1109%2fMetroAgriFor52389.2021.9628547&partnerID=40&md5=142819544a869e40b345c9cb9d0c516f","Federal Research Centre of Biological Systems and Agro-Technologies of RAS,Chelyabinsk State University,South Ural State University, Orenburg, Russian Federation; Federal Research Centre of Biological Systems and Agro-Technologies of RAS, Orenburg, Russian Federation; College of Land Science and Technology, China Agricultural Universityy, Beijing, China","Ruchay A., Federal Research Centre of Biological Systems and Agro-Technologies of RAS,Chelyabinsk State University,South Ural State University, Orenburg, Russian Federation; Dorofeev K., Federal Research Centre of Biological Systems and Agro-Technologies of RAS, Orenburg, Russian Federation; Kalschikov V., Federal Research Centre of Biological Systems and Agro-Technologies of RAS, Orenburg, Russian Federation; Kolpakov V., Federal Research Centre of Biological Systems and Agro-Technologies of RAS, Orenburg, Russian Federation; Dzhulamanov K., Federal Research Centre of Biological Systems and Agro-Technologies of RAS, Orenburg, Russian Federation; Guo H., College of Land Science and Technology, China Agricultural Universityy, Beijing, China","The traditional linear regression algorithm is used to predict the live weight of livestock. However, this traditional method is inadequate for accurate prediction. Recently, a few researchers have successfully applied various machine learning algorithms for predicting the live body weight using livestock morphological measures. We investigate deep learning methods for developing a live weight prediction model based on image regression in this study. We use only RGB images and depth maps for predicting the live cattle weight. The best model for our study is the proposed model with MAPE 9.1% using the RGB images and the depth maps. We have shown results on real-world datasets that demonstrate that the proposed model can reach levels of weight measurement accuracy comparable to those obtained by traditional weighting.  © 2021 IEEE.","deep learning; Hereford cattle; image regression; live weight; Prediction","Agriculture; Computer vision; Deep learning; Learning algorithms; Regression analysis; Accurate prediction; Body weight; Deep learning; Depthmap; Hereford cattle; Image regression; Linear regression algorithms; Live weight; Machine learning algorithms; RGB images; Forecasting","Russian Science Foundation, RSF, (21-76-20014); Russian Science Foundation, RSF","This work was supported by the Russian Science Foundation, grant no. 21-76-20014.","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540533-1","","","English","IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.","Conference paper","Final","","Scopus","2-s2.0-85123418041"
"Qiao Y.; Clark C.; Lomax S.; Kong H.; Su D.; Sukkarieh S.","Qiao, Yongliang (56486770900); Clark, Cameron (7403546385); Lomax, Sabrina (56151402400); Kong, He (57203456679); Su, Daobilige (56594211700); Sukkarieh, Salah (6602844626)","56486770900; 7403546385; 56151402400; 57203456679; 56594211700; 6602844626","Automated Individual Cattle Identification Using Video Data: A Unified Deep Learning Architecture Approach","2021","Frontiers in Animal Science","2","","759147","","","","14","10.3389/fanim.2021.759147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142495113&doi=10.3389%2ffanim.2021.759147&partnerID=40&md5=be3151f792f58dc42e4665c3a8cd49cf","Australian Centre for Field Robotics, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, NSW, Australia; College of Engineering, China Agricultural University, Beijing, China","Qiao Y., Australian Centre for Field Robotics, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; Clark C., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, NSW, Australia; Lomax S., Livestock Production and Welfare Group, School of Life and Environmental Sciences, Faculty of Science, The University of Sydney, Sydney, NSW, Australia; Kong H., Australian Centre for Field Robotics, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; Su D., College of Engineering, China Agricultural University, Beijing, China; Sukkarieh S., Australian Centre for Field Robotics, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia","Individual cattle identification is a prerequisite and foundation for precision livestock farming. Existing methods for cattle identification require radio frequency or visual ear tags, all of which are prone to loss or damage. Here, we propose and implement a new unified deep learning approach to cattle identification using video analysis. The proposed deep learning framework is composed of a Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) with a self-attention mechanism. More specifically, the Inception-V3 CNN was used to extract features from a cattle video dataset taken in a feedlot with rear-view. Extracted features were then fed to a BiLSTM layer to capture spatio-temporal information. Then, self-attention was employed to provide a different focus on the features captured by BiLSTM for the final step of cattle identification. We used a total of 363 rear-view videos from 50 cattle at three different times with an interval of 1 month between data collection periods. The proposed method achieved 93.3% identification accuracy using a 30-frame video length, which outperformed current state-of-the-art methods (Inception-V3, MLP, SimpleRNN, LSTM, and BiLSTM). Furthermore, two different attention schemes, namely, additive and multiplicative attention mechanisms were compared. Our results show that the additive attention mechanism achieved 93.3% accuracy and 91.0% recall, greater than multiplicative attention mechanism with 90.7% accuracy and 87.0% recall. Video length also impacted accuracy, with video sequence length up to 30-frames enhancing identification performance. Overall, our approach can capture key spatio-temporal features to improve cattle identification accuracy, enabling automated cattle identification for precision livestock farming. Copyright © 2021 Qiao, Clark, Lomax, Kong, Su and Sukkarieh.","BiLSTM; cattle identification; deep learning; precision livestock farming; self-attention","","Meat & Livestock Australia Donor Company, (P.PSH.0819)","The authors declare that this study received funding from Meat & Livestock Australia Donor Company (grant number P.PSH.0819). The funder was not involved in the study design, collection, analysis, interpretation of data, the writing of this article or the decision to submit it for publication. ","Y. Qiao; Australian Centre for Field Robotics, Faculty of Engineering, The University of Sydney, Sydney, Australia; email: yongliang.qiao@sydney.edu.au","","Frontiers Media SA","26736225","","","","English","Front. Anim. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142495113"
"Suparwito H.; Wong K.W.; Xie H.; Rai S.; Thomas D.","Suparwito, Hari (57212196155); Wong, Kok Wai (7404759276); Xie, Hong (35231294900); Rai, Shri (8250275300); Thomas, Dean (16242777700)","57212196155; 7404759276; 35231294900; 8250275300; 16242777700","A Hierarchical Classification Method Used to Classify Livestock Behaviour from Sensor Data","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11909 LNAI","","","204","215","11","3","10.1007/978-3-030-33709-4_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076182247&doi=10.1007%2f978-3-030-33709-4_18&partnerID=40&md5=50288525b9a2a10e1b2b125d2f892ab8","Murdoch University, Perth, WA, Australia; CSIRO Floreat, Perth, WA, Australia","Suparwito H., Murdoch University, Perth, WA, Australia; Wong K.W., Murdoch University, Perth, WA, Australia; Xie H., Murdoch University, Perth, WA, Australia; Rai S., Murdoch University, Perth, WA, Australia; Thomas D., CSIRO Floreat, Perth, WA, Australia","One of the fundamental tasks in the management of livestock is to understand their behaviour and use this information to increase livestock productivity and welfare. Developing new and improved methods to classify livestock behaviour based on their daily activities can greatly improve livestock management. In this paper, we propose the use of a hierarchical machine learning method to classify livestock behaviours. We first classify the livestock behaviours into two main behavioural categories. Each of the two categories is then broken down at the next level into more specific behavioural categories. We have tested the proposed methodology using two commonly used classifiers, Random Forest, Support Vector Machine and a newer approach involving Deep Belief Networks. Our results show that the proposed hierarchical classification technique works better than the conventional approach. The experimental studies also show that Deep Belief Networks perform better than the Random Forest and Support Vector Machine for most cases. © Springer Nature Switzerland AG 2019.","Hierarchical classification; Livestock behaviour; Machine learning; Sensor data","Classification (of information); Decision trees; Deep learning; Learning systems; Machine learning; Support vector machines; Conventional approach; Daily activity; Deep belief networks; Hierarchical classification; Livestock behaviour; Machine learning methods; Random forests; Sensor data; Agriculture","Commonwealth Scientific and Industrial Research Organisation, CSIRO","Acknowledgement. This research was supported by CSIRO Floreat, Western Australia. We are grateful for their cooperation and permission to use their data.","H. Suparwito; Murdoch University, Perth, Australia; email: shirsj@jesuits.net","Chamchong R.; Wong K.W.","Springer","03029743","978-303033708-7","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85076182247"
"Wagner N.; Antoine V.; Koko J.; Mialon M.-M.; Lardy R.; Veissier I.","Wagner, Nicolas (57214364781); Antoine, Violaine (36560949300); Koko, Jonas (56023908700); Mialon, Marie-Madeleine (6602306970); Lardy, Romain (36918617100); Veissier, Isabelle (7003853930)","57214364781; 36560949300; 56023908700; 6602306970; 36918617100; 7003853930","Comparison of Machine Learning Methods to Detect Anomalies in the Activity of Dairy Cows","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12117 LNAI","","","342","351","9","7","10.1007/978-3-030-59491-6_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092077761&doi=10.1007%2f978-3-030-59491-6_32&partnerID=40&md5=9228dff80dc6f3a10f29cae72f9cf85a","UCA, LIMOS, UMR 6158, CNRS, Clermont-Ferrand, France; UCA, INRAE, UMR Herbivores, Saint-Genès-Champanelle, 63122, France","Wagner N., UCA, LIMOS, UMR 6158, CNRS, Clermont-Ferrand, France, UCA, INRAE, UMR Herbivores, Saint-Genès-Champanelle, 63122, France; Antoine V., UCA, LIMOS, UMR 6158, CNRS, Clermont-Ferrand, France; Koko J., UCA, LIMOS, UMR 6158, CNRS, Clermont-Ferrand, France; Mialon M.-M., UCA, INRAE, UMR Herbivores, Saint-Genès-Champanelle, 63122, France; Lardy R., UCA, INRAE, UMR Herbivores, Saint-Genès-Champanelle, 63122, France; Veissier I., UCA, INRAE, UMR Herbivores, Saint-Genès-Champanelle, 63122, France","Farmers need to detect any anomaly in animals as soon as possible for production efficiency (e.g. detection of estrus) and animal welfare (e.g. detection of diseases). The number of animals per farm is however increasing, making it difficult to detect anomalies. To help solving this problem, we undertook a study on dairy cows, in which their activity was captured by an indoor tracking system and considered as time series. The state of cows (diseases, estrus, no problem) was manually labelled by animal caretakers or by a sensor for ruminal pH (acidosis). In the present study, we propose a new Fourier based method (FBAT) to detect anomalies in time series. We compare FBAT with the best machine learning methods for time series classification in the current literature (BOSS, Hive-Cote, DTW, FCN and ResNet). It follows that BOSS, FBAT and deep learning methods yield the best performance but with different characteristics. © 2020, Springer Nature Switzerland AG.","Deep learning; Detection of anomalies; Machine learning; Precision livestock farming; Time series classification","Animals; Deep learning; Fourier series; Intelligent systems; Time series; Animal welfare; Dairy cow; Fourier; Indoor tracking; Learning methods; Machine learning methods; Production efficiency; Time series classifications; Learning systems","Institut National de la Recherche Agronomique, INRA; Université Clermont-Auvergne, UCA","Acknowledgment. This collaborative work was made possible thanks to the French Government IDEX-ISITE initiative 16-IDEX-0001 (CAP 20–25). The PhD grant for N. Wagner was provided by INRA and Université Clermont Auvergne. We thank the HERBIPOLE staff, B. Meunier, Y. Gaudron and M. Silberberg for data.","N. Wagner; UCA, LIMOS, UMR 6158, CNRS, Clermont-Ferrand, France; email: nicolas.wagner@uca.fr","Helic D.; Stettinger M.; Felfernig A.; Leitner G.; Ras Z.W.","Springer Science and Business Media Deutschland GmbH","03029743","978-303059490-9","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85092077761"
"Heinrich F.; Wutke M.; Das P.P.; Kamp M.; Gültas M.; Link W.; Schmitt A.O.","Heinrich, Felix (57208835958); Wutke, Martin (57217124907); Das, Pronaya Prosun (57217126023); Kamp, Miriam (57217127255); Gültas, Mehmet (25653919300); Link, Wolfgang (7005377558); Schmitt, Armin Otto (7201405093)","57208835958; 57217124907; 57217126023; 57217127255; 25653919300; 7005377558; 7201405093","Identification of regulatory SNPs associated with vicine and convicine content of vicia faba based on genotyping by sequencing data using deep learning","2020","Genes","11","6","614","","","","17","10.3390/genes11060614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086355634&doi=10.3390%2fgenes11060614&partnerID=40&md5=fc6cb6ab2e4b82362944c9ea9f455a5e","Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Department of Crop Sciences, Georg-August University, Von-Siebold-Str. 8, Göttingen, 37075, Germany","Heinrich F., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Wutke M., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Das P.P., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Kamp M., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany; Gültas M., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany, Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Link W., Department of Crop Sciences, Georg-August University, Von-Siebold-Str. 8, Göttingen, 37075, Germany; Schmitt A.O., Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Margarethe von Wrangell-Weg 7, Göttingen, 37075, Germany, Center for Integrated Breeding Research (CiBreed), Georg-August University, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany","Faba bean (Vicia faba) is a grain legume, which is globally grown for both human consumption as well as feed for livestock. Despite its agro-ecological importance the usage of Vicia faba is severely hampered by its anti-nutritive seed-compounds vicine and convicine (V+C). The genes responsible for a low V+C content have not yet been identified. In this study, we aim to computationally identify regulatory SNPs (rSNPs), i.e., SNPs in promoter regions of genes that are deemed to govern the V+C content of Vicia faba. For this purpose we first trained a deep learning model with the gene annotations of seven related species of the Leguminosae family. Applying our model, we predicted putative promoters in a partial genome of Vicia faba that we assembled from genotyping-by-sequencing (GBS) data. Exploiting the synteny between Medicago truncatula and Vicia faba, we identified two rSNPs which are statistically significantly associated with V+C content. In particular, the allele substitutions regarding these rSNPs result in dramatic changes of the binding sites of the transcription factors (TFs) MYB4, MYB61, and SQUA. The knowledge about TFs and their rSNPs may enhance our understanding of the regulatory programs controlling V+C content of Vicia faba and could provide new hypotheses for future breeding programs. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; GBS; Promoter; RSNP; Vicia faba; Vicin/convicin","Deep Learning; Genotype; Glucosides; Polymorphism, Single Nucleotide; Pyrimidinones; Regulatory Sequences, Nucleic Acid; Seeds; Synteny; Transcription Factors; Uridine; Vicia faba; protein MYB4; protein MYB61; protein SQUA; transcription factor; unclassified drug; convicine; glucoside; pyrimidinone derivative; transcription factor; uridine; vicine; allele; Article; barrel medic; binding site; deep learning; Fabaceae; gene identification; gene sequence; genetic association; molecular genetics; nonhuman; plant breeding; plant genetics; plant seed; prediction; promoter region; single nucleotide polymorphism; Vicia faba; genetics; genotype; plant seed; regulatory sequence; single nucleotide polymorphism; synteny","Niedersächsische Ministerium für Wissenschaft und Kultur, (MWK 11-76251-99-30/16); Deutsche Forschungsgemeinschaft, DFG; Georg-August-Universität Göttingen, GAU","Funding text 1: Acknowledgments: We acknowledge support by the German Research Foundation and the Open Access Publication Funds of the University of Göttingen. We are grateful to Rebecca Tacke, Thomas Lange, and Wolfgang Ecke for providing valuable advice on some biological aspects. We would like to thank the reviewers for their thoughtful comments and efforts towards improving our manuscript.; Funding text 2: Funding: This research was partially funded by the Lower Saxony Ministry of Science and Culture, grant number MWK 11-76251-99-30/16.","A.O. Schmitt; Breeding Informatics Group, Department of Animal Sciences, Georg-August University, Göttingen, Margarethe von Wrangell-Weg 7, 37075, Germany; email: armin.schmitt@uni-goettingen.de","","MDPI AG","20734425","","","32516876","English","Genes","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85086355634"
"Campos De Vasconcellos B.; Pereira Trindade J.P.; Bochi Da Silva Volk L.; Bidese De Pinho L.","Campos De Vasconcellos, Bruno (57217106151); Pereira Trindade, Jose Pedro (24475881100); Bochi Da Silva Volk, Leandro (57217099701); Bidese De Pinho, Leonardo (9239472600)","57217106151; 24475881100; 57217099701; 9239472600","Method Applied to Animal MonitoringThrough VANT Images","2020","IEEE Latin America Transactions","18","7","9099770","1280","1287","7","1","10.1109/TLA.2020.9099770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086263579&doi=10.1109%2fTLA.2020.9099770&partnerID=40&md5=594a665eb735473aa983a6e340d126f6","Unipampa - Universidade Federal Do Pampa, Bagé, RS, 96413-170, Brazil; Embrapa - Empresa Brasileira de Pesquisa Agropecuária, Bagé, RS, 96401-970, Brazil","Campos De Vasconcellos B., Unipampa - Universidade Federal Do Pampa, Bagé, RS, 96413-170, Brazil; Pereira Trindade J.P., Embrapa - Empresa Brasileira de Pesquisa Agropecuária, Bagé, RS, 96401-970, Brazil; Bochi Da Silva Volk L., Embrapa - Empresa Brasileira de Pesquisa Agropecuária, Bagé, RS, 96401-970, Brazil; Bidese De Pinho L., Unipampa - Universidade Federal Do Pampa, Bagé, RS, 96413-170, Brazil","One of the necessary demands in extensive livestock systems is the counting of animals in areas of tens of hectares, costly when carried out manually and locally. In this context, this work proposes and discusses the efficacy of a semi-autonomous, non-invasive method for remote identification of animals in the field, applicable to precision livestock systems. The method was conceived from an exploratory research methodology based on remote sensing techniques that include image collection processes by aerial surveying with RGB camera embedded in unmanned aerial vehicle, persistence of images obtained by means of storage in space-time databases and processing of stored images for the construction of a rural property orthomosaic succeeded by the application of patterns discovery processes, making use of deep learning, especially convolutional neural networks. According to the experiments carried out, the method was effective, being able to identify and count animals from the collection of images made at 100 m height, with an accuracy of up to 95%, including the approximate geographical position of the animals to field. © 2003-2012 IEEE.","cattle herding; convolutional neural network; deep learning","Agriculture; Antennas; Convolutional neural networks; Deep learning; Noninvasive medical procedures; Remote sensing; Exploratory research; Geographical positions; Image collections; Livestock systems; Noninvasive methods; Remote identification; Remote sensing techniques; RGB cameras; Animals","","","","","IEEE Computer Society","15480992","","","","Portuguese","IEEE. Lat. Am. Trans.","Article","Final","","Scopus","2-s2.0-85086263579"
"Szymanski L.; Lee M.","Szymanski, Lech (55338133300); Lee, Michael (23489354700)","55338133300; 23489354700","Coarse facial feature detection in sheep","2021","International Conference Image and Vision Computing New Zealand","2021-December","","","","","","1","10.1109/IVCNZ54163.2021.9653248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124427373&doi=10.1109%2fIVCNZ54163.2021.9653248&partnerID=40&md5=9909f4bf266909ae2e5d1f0577c36ea0","University of Otago, Department of Computer Science, Dunedin, New Zealand; University of Otago, Department of Mathematics and Statistics, Dunedin, New Zealand","Szymanski L., University of Otago, Department of Computer Science, Dunedin, New Zealand; Lee M., University of Otago, Department of Mathematics and Statistics, Dunedin, New Zealand","We present a deep learning model for facial feature detection in sheep, which is part of a project on machine learning for kinship detection in livestock. Using YOLO-like training, we obtained a model capable of identifying the eye and nose line of the animal in the image, with a mean test error of ∼5 pixels (in a 256×384 input image). We further evaluated the effectiveness of the resulting pose estimation and alignment for kinship prediction and report its balanced accuracy of 73% - an improvement of 5% over our previous effort that had no benefit of the image pre-processing provided by the proposed feature detection network.  © 2021 IEEE.","Classification; Deep learning; Facial recognition","Agriculture; Biometrics; Deep learning; Feature extraction; Image enhancement; Deep learning; Facial feature detection; Facial recognition; Features detections; Image preprocessing; Input image; Learning models; Pose alignments; Pose-estimation; Test errors; Face recognition","NeSI; NeSI’s; Science for Technological Innovation National Science Challenge; Nvidia","ACKNOWLEDGMENT The authors acknowledge: the Science for Technological Innovation National Science Challenge (SfTI) for funding this project and thank Beef + Lamb New Zealand Genetics for access to data and staff at AgRsearch limited, Invermay, Mosgiel for assisting with data collection; the support of NVIDIA Corporation with the donation of the TITAN X GPU used for this research; the use of New Zealand eScience Infrastructure (NeSI) high performance computing facilities, consulting support and/or training services as part of this research (New Zealand’s national facilities are provided by NeSI and funded jointly by NeSI’s collaborator institutions and through the Ministry of Business, Innovation & Employment’s Research Infrastructure programme; https://www.nesi.org.nz).","","Cree M.J.","IEEE Computer Society","21512191","978-166540645-1","","","English","Int. Conf. Image Vis. Comput. New Zealand","Conference paper","Final","","Scopus","2-s2.0-85124427373"
"Xu B.; Wang W.; Falzon G.; Kwan P.; Guo L.; Sun Z.; Li C.","Xu, Beibei (57215186247); Wang, Wensheng (56937276700); Falzon, Greg (13407283800); Kwan, Paul (7004369297); Guo, Leifeng (56542160600); Sun, Zhiguo (55560141900); Li, Chunlei (58392305700)","57215186247; 56937276700; 13407283800; 7004369297; 56542160600; 55560141900; 58392305700","Livestock classification and counting in quadcopter aerial images using Mask R-CNN","2020","International Journal of Remote Sensing","41","21","","8121","8142","21","77","10.1080/01431161.2020.1734245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083694864&doi=10.1080%2f01431161.2020.1734245&partnerID=40&md5=6ef62e37b9d6396c0da8f7879b362247","Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China; Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, China; School of Science Technology, University of New England, Armidale, Australia; Precision Agriculture Research Group, University of New England, Armidale, Australia; School of Information Technology Engineering, Melbourne Institute of Technology, Melbourne, Australia","Xu B., Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China; Wang W., Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China, Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, China; Falzon G., School of Science Technology, University of New England, Armidale, Australia, Precision Agriculture Research Group, University of New England, Armidale, Australia; Kwan P., School of Science Technology, University of New England, Armidale, Australia, School of Information Technology Engineering, Melbourne Institute of Technology, Melbourne, Australia; Guo L., Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China, Key Laboratory of Agricultural Big Data, Ministry of Agriculture and Rural Affairs, Beijing, China; Sun Z., Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China; Li C., Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, China","Quadcopters equipped with machine learning vision systems are bound to become an essential technique for precision agriculture applications in pastures in the near future. This paper presents a low-cost approach for livestock counting jointly with classification and semantic segmentation which provide the potential of biometrics and welfare monitoring in animals in real time. The method used in the paper adopts the state-of-the-art deep-learning technique known as Mask R-CNN for feature extraction and training in the images captured by quadcopters. Key parameters such as IoU (Intersection over Union) threshold, the quantity of the training data and the effect the proposed system performs on various densities have been evaluated to optimize the model. A real pasture surveillance dataset is used to evaluate the proposed method and experimental results show that our proposed system can accurately classify the livestock with an accuracy of 96% and estimate the number of cattle and sheep to within 92% of the visual ground truth, presenting competitive advantages of the approach feasible for monitoring the livestock. © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","","Animalia; Bos; Ovis aries; Agricultural robots; Antennas; Classification (of information); Competition; Deep learning; Image classification; Learning systems; Semantics; Agriculture applications; Competitive advantage; Learning techniques; Low cost approach; Semantic segmentation; State of the art; Various densities; Vision systems; accuracy assessment; aerial survey; artificial neural network; classification; data set; experimental study; livestock farming; satellite imagery; Agriculture","Beijing Aokemei Technical Service Company Limited; Inner Mongolia Autonomous Region Science and Technology Major Project, (AEC19-009, ZD20190039); Youth Science Foundation of Jiangxi Province, (20192ACBL21023); Central Public-interest Scientific Institution Basal Research Fund, Chinese Academy of Fishery Sciences, (JBYW-AII-2019-19); Jiangsu Provincial Key Research and Development Program, (20192BBF60053)","This research was funded by Beijing Aokemei Technical Service Company Limited and also was supported by Central Public-interest Scientific Institution Basal Research Fund under Grant (JBYW-AII-2019-19), Key Research and Development Projects of Jiangxi Province (20192BBF60053), Youth Science Foundation Project of Jiangxi Province (20192ACBL21023) and Inner Mongolia Autonomous Region Science and Technology Major Project （ZD20190039). We would like to extend our sincere gratitude to the reviewers and editors for their time and efforts. We are also grateful to the private farmland in New England in Australia for their kind support with data collection (University of New England Standard Operating Procedure W14 Camera Traps and Animal Ethics Approval Number AEC19-009).","W. Wang; Agricultural Information Institute, Chinese Academy of Agriculture Science, Beijing, No.12 South Street, Zhongguancun, Haidian District, China; email: wangwensheng@caas.cn","","Taylor and Francis Ltd.","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85083694864"
"Jiang M.; Rao Y.; Zhang J.; Shen Y.","Jiang, Min (57218159888); Rao, Yuan (56343988100); Zhang, Jingyao (57209694294); Shen, Yiming (57218477413)","57218159888; 56343988100; 57209694294; 57218477413","Automatic behavior recognition of group-housed goats using deep learning","2020","Computers and Electronics in Agriculture","177","","105706","","","","67","10.1016/j.compag.2020.105706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089286487&doi=10.1016%2fj.compag.2020.105706&partnerID=40&md5=524ef4983462e6cdd7277e5b9fb56771","School of Information and Computer Sciences, Anhui Agricultural University, Hefei, 230036, China","Jiang M., School of Information and Computer Sciences, Anhui Agricultural University, Hefei, 230036, China; Rao Y., School of Information and Computer Sciences, Anhui Agricultural University, Hefei, 230036, China; Zhang J., School of Information and Computer Sciences, Anhui Agricultural University, Hefei, 230036, China; Shen Y., School of Information and Computer Sciences, Anhui Agricultural University, Hefei, 230036, China","Daily behavior is one important manifestation for health and welfare status of livestock. In traditional behavior recognition methods, it was often mandatory to detect animal heads or depend on extra tools. To overcome such shortcomings, this paper proposed one efficient behavior recognition approach using deep learning to recognize eating, drinking, active and inactive behaviors of group-housed goats from video sequences of top upper-side view. Firstly, the approach of detecting individual goat was designed by means of investigating the characteristics and suitability of several popular deep learning methods. Secondly, we proposed a general behavior recognition framework of group-housed goats for videos acquired from top upper-side view. Four types of goat behaviors were recognized by analyzing the spatial location relationship between goat bounding boxes and feeding/drinking zones, as well as the temporal movement amount of bounding box centroids of the same goat among consecutive frames. One inferential strategy was presented for estimating the missing behaviors caused by goat detection failure in frames. The experimental results showed that YOLOv4 was superior to other models in terms of both goat detection speed and accuracy, and the average recognition accuracies of 97.87%, 98.27%, 96.86% and 96.92%, respectively, for eating, drinking, active and inactive behaviors were achieved on the experimental videos, in real-time manner with the average analysis speed of 17 frames per second on a conventional hardware configuration. Hence, it was demonstrated that the proposed approach could offer one effective way for automatically conducting comprehensive behavior recognition of group-housed livestock. © 2020","Behavior recognition; Deep learning; Group-housed goats; Video sequences; YOLOv4","Animalia; Capra hircus; Agriculture; Behavioral research; Learning systems; Behavior recognition; Detection speed; Frames per seconds; Hardware configurations; Learning methods; Recognition accuracy; Spatial location; Video sequences; algorithm; detection method; experimental study; goat; hardware; videography; Deep learning","Anhui Province Key Laboratory of Smart Agricultural Technology and Equipment, (APKLSATE2019X004); Key Research and Development Plan of Anhui Province, (1804a07020108, 201904a06020056); Natural Science Major Project for Anhui Provincial University, (KJ2019ZD20)","Funding text 1: The subject is sponsored by the Anhui Province Key Laboratory of Smart Agricultural Technology and Equipment (No. APKLSATE2019X004), the Key Research and Development Plan of Anhui Province (No. 1804a07020108, 201904a06020056), the Natural Science Major Project for Anhui Provincial University (No. KJ2019ZD20). The authors would like to thank the research farms that participated in this study for providing the study environment. Many thanks to Junxiang Ma and Qinqiang Lv for their invaluable assistance with experiments.; Funding text 2: The subject is sponsored by the Anhui Province Key Laboratory of Smart Agricultural Technology and Equipment (No. APKLSATE2019X004 ), the Key Research and Development Plan of Anhui Province (No. 1804a07020108 , 201904a06020056 ), the Natural Science Major Project for Anhui Provincial University (No. KJ2019ZD20 ). The authors would like to thank the research farms that participated in this study for providing the study environment. Many thanks to Junxiang Ma and Qinqiang Lv for their invaluable assistance with experiments.  ","Y. Rao; Hefei, No. 130, Changjiang West Road, 230036, China; email: ry9925@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85089286487"
"Qiao Y.; Truman M.; Sukkarieh S.","Qiao, Yongliang (56486770900); Truman, Matthew (57220258908); Sukkarieh, Salah (6602844626)","56486770900; 57220258908; 6602844626","Cattle segmentation and contour extraction based on Mask R-CNN for precision livestock farming","2019","Computers and Electronics in Agriculture","165","","104958","","","","160","10.1016/j.compag.2019.104958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070915998&doi=10.1016%2fj.compag.2019.104958&partnerID=40&md5=cf933eb68d6d4b8fb108ce62a314048b","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Truman M., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, NSW, Australia","In precision livestock farming, computer vision based approaches have been widely used to obtain individual cattle health and welfare information such as body condition score, live weight, activity behaviours. For this, precisely segmenting each cattle image from its background is a prerequisite, which is an important step towards obtaining real-time individual cattle information. In this paper, an instance segmentation approach based on a Mask R-CNN deep learning framework is proposed to solve cattle instance segmentation and contour extraction problems in a real feedlot environment. The proposed approach consists of the following steps: key frame extraction (detect the huge cattle motion frames), image enhancement (reduce the illumination and shadow influence), cattle segmentation and body contour extraction. We trained and tested the proposed approach on a challenging cattle image dataset. According to the experimental results, the proposed approach can render fairly desirable cattle segmentation performance with 0.92 Mean Pixel Accuracy (MPA) and achieve contour extraction with an Average Distance Error (ADE) of 33.56 pixel, which is better than that of the state-of-the-art SharpMask and DeepMask instance segmentation methods. © 2019 Elsevier B.V.","Cattle contour; Deep learning; Instance segmentation; Mask R-CNN; Precision livestock farming","Bos; Agriculture; Deep learning; Extraction; Image enhancement; Pixels; Body condition score; Cattle contour; Key-frame extraction; Learning frameworks; Precision livestock farming; Segmentation methods; Segmentation performance; Vision-based approaches; accuracy assessment; cattle; computer vision; data set; livestock farming; machine learning; performance assessment; pixel; precision; Image segmentation","Australian Country Choice; Meat and Livestock Australia, MLA","Funding text 1: The authors express their gratitude to Australian Country Choice and the Brisbane Valley farm staff for their help with data collection. Also particular thanks to Sabrina Lomax, Cameron Clark, Amanda Doughty, Ashraful Islam and Mike Reynolds for their involvement and efforts in the whole experiment organization and cattle information collection. The authors also acknowledge the support of the Meat & Livestock Australia Donor Company through the project: Objective, robust, real-time animal welfare measures for the Australian red meat industry. In addition, thanks to He Kong for his support in the manuscript revision.  ; Funding text 2: The authors express their gratitude to Australian Country Choice and the Brisbane Valley farm staff for their help with data collection. Also particular thanks to Sabrina Lomax, Cameron Clark, Amanda Doughty, Ashraful Islam and Mike Reynolds for their involvement and efforts in the whole experiment organization and cattle information collection. The authors also acknowledge the support of the Meat & Livestock Australia Donor Company through the project: Objective, robust, real-time animal welfare measures for the Australian red meat industry. In addition, thanks to He Kong for his support in the manuscript revision.","Y. Qiao; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, 2006, Australia; email: yongliang.qiao@sydney.edu.au","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85070915998"
"Qiao Y.; Su D.; Kong H.; Sukkarieh S.; Lomax S.; Clark C.","Qiao, Yongliang (56486770900); Su, Daobilige (56594211700); Kong, He (57203456679); Sukkarieh, Salah (6602844626); Lomax, Sabrina (56151402400); Clark, Cameron (7403546385)","56486770900; 56594211700; 57203456679; 6602844626; 56151402400; 7403546385","Individual Cattle Identification Using a Deep Learning Based Framework","2019","IFAC-PapersOnLine","52","30","","318","323","5","72","10.1016/j.ifacol.2019.12.558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081068402&doi=10.1016%2fj.ifacol.2019.12.558&partnerID=40&md5=819444d0313a5a65972e1ab16250c59a","Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, 2006, NSW, Australia; Livestock Production and Welfare Group, School of Life and Environmental Sciences, University of Sydney, 2006, NSW, Australia","Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, 2006, NSW, Australia; Su D., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, 2006, NSW, Australia; Kong H., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, 2006, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, 2006, NSW, Australia; Lomax S., Livestock Production and Welfare Group, School of Life and Environmental Sciences, University of Sydney, 2006, NSW, Australia; Clark C., Livestock Production and Welfare Group, School of Life and Environmental Sciences, University of Sydney, 2006, NSW, Australia","Individual cattle identification is required for precision livestock farming. Current methods for individual cattle identification requires either visual, or unique radio frequency, ear tags. We propose a deep learning based framework to identify beef cattle using image sequences unifying the advantages of both CNN (Convolutional Neural Network) and LSTM (Long Short-Term Memory) network methods. A CNN network was used (Inception-V3) to extract features from a rear-view cattle video dataset and these extracted features were then used to train an LSTM model to capture temporal information and identify each individual animal. A total of 516 rear-view videos of 41 cattle at three time points separated by one month were collected. Our method achieved an accuracy of 88% and 91% for 15-frame and 20-frame video length, respectively. Our approach outperformed the framework that only uses CNN (identification accuracy 57%). Our framework will now be further improved using additional data before integrating the system into on-farm management processes. © 2019 Elsevier B.V. All rights reserved.","Cattle identification; CNN; deep learning; LSTM; precision livestock farming","Agriculture; Convolutional neural networks; Information management; Long short-term memory; Radio frequency identification (RFID); Additional datum; Farm management; Identification accuracy; LSTM; Network methods; Precision livestock farming; Radio frequencies; Temporal information; Deep learning","","","","Fitch R.; Katupitiya J.; Whitty M.","Elsevier B.V.","24058963","","","","English","IFAC-PapersOnLine","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081068402"
"Wang Z.; Shadpour S.; Chan E.; Rotondo V.; Wood K.M.; Tulpan D.","Wang, Zhuoyi (57222267096); Shadpour, Saeed (57222266800); Chan, Esther (57222267755); Rotondo, Vanessa (57190351585); Wood, Katharine M. (56315319100); Tulpan, Dan (8985607700)","57222267096; 57222266800; 57222267755; 57190351585; 56315319100; 8985607700","ASAS-NANP SYMPOSIUM: Applications of machine learning for livestock body weight prediction from digital images","2021","Journal of Animal Science","99","2","","","","","53","10.1093/jas/skab022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102096068&doi=10.1093%2fjas%2fskab022&partnerID=40&md5=6937cf3f95da5fe14caa26255c76265a","Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, ON, Canada; Department of Animal Biosciences, University of Guelph, Guelph, N1G 2W1, ON, Canada","Wang Z., Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, ON, Canada; Shadpour S., Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, ON, Canada; Chan E., Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, ON, Canada; Rotondo V., Department of Animal Biosciences, University of Guelph, Guelph, N1G 2W1, ON, Canada; Wood K.M., Department of Animal Biosciences, University of Guelph, Guelph, N1G 2W1, ON, Canada; Tulpan D., Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, ON, Canada","Monitoring, recording, and predicting livestock body weight (BW) allows for timely intervention in diets and health, greater efficiency in genetic selection, and identification of optimal times to market animals because animals that have already reached the point of slaughter represent a burden for the feedlot. There are currently two main approaches (direct and indirect) to measure the BW in livestock. Direct approaches include partial-weight or full-weight industrial scales placed in designated locations on large farms that measure passively or dynamically the weight of livestock. While these devices are very accurate, their acquisition, intended purpose and operation size, repeated calibration and maintenance costs associated with their placement in high-temperature variability, and corrosive environments are significant and beyond the affordability and sustainability limits of small and medium size farms and even of commercial operators. As a more affordable alternative to direct weighing approaches, indirect approaches have been developed based on observed or inferred relationships between biometric and morphometric measurements of livestock and their BW. Initial indirect approaches involved manual measurements of animals using measuring tapes and tubes and the use of regression equations able to correlate such measurements with BW. While such approaches have good BW prediction accuracies, they are time consuming, require trained and skilled farm laborers, and can be stressful for both animals and handlers especially when repeated daily. With the concomitant advancement of contactless electro-optical sensors (e.g., 2D, 3D, infrared cameras), computer vision (CV) technologies, and artificial intelligence fields such as machine learning (ML) and deep learning (DL), 2D and 3D images have started to be used as biometric and morphometric proxies for BW estimations. This manuscript provides a review of CV-based and ML/DL-based BW prediction methods and discusses their strengths, weaknesses, and industry applicability potential. © The Author(s) 2021. Published by Oxford University Press on behalf of the American Society of Animal Science. This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com","Biometrics; Body weight; Computer vision; Digital images; Machine learning; Morphometrics","Animals; Artificial Intelligence; Body Weight; Livestock; Machine Learning; Selection, Genetic; animal; artificial intelligence; body weight; genetic selection; livestock; machine learning","American Society of Animal Science; Canadian Society of Animal Science, (9); National Animal Nutrition Program; University of Guelph, (499063); Canada First Research Excellence Fund, CFREF, (499100)","The authors would like to thank Dr Renee Bergeron and the anonymous reviewers for valuable discussions, suggestions, and corrections that lead to manuscript improvements. Funded from a University of Guelph Food from Thought research program grant (499063) and a Canada First Research Excellence Fund (499100). Presented at the ASAS-NANP Symposium: Mathematical Modeling in Animal Nutrition: Training the Future Generation in Data and Predictive Analytics for Sustainable Development at the 2020 Virtual Annual Meeting & Trade Show of the American Society of Animal Science, Canadian Society of Animal Science, and Western Section of the American Society of Animal Science on July 19-23, with publications sponsored by the Journal of Animal Science and the American Society of Animal Science. This symposium was sponsored by the National Research Support Project #9 from the National Animal Nutrition Program (https://animalnutrition.org/).","D. Tulpan; Department of Animal Biosciences, Centre for Genetic Improvement of Livestock, University of Guelph, Guelph, N1G 2W1, Canada; email: dtulpan@uoguelph.ca","","Oxford University Press","00218812","","","33626149","English","J. Anim. Sci.","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85102096068"
"Pan X.; Zhu J.; Tai W.; Fu Y.","Pan, Xiang (57447913600); Zhu, Jing (57219179178); Tai, Weipeng (22954999200); Fu, Yan (57222180652)","57447913600; 57219179178; 22954999200; 57222180652","An automated method to quantify the composition of live pigs based on computed tomography segmentation using deep neural networks","2021","Computers and Electronics in Agriculture","183","","105987","","","","14","10.1016/j.compag.2021.105987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101844589&doi=10.1016%2fj.compag.2021.105987&partnerID=40&md5=6395b49aecaa2c8d33cbc4e0a9dc0ac2","School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, 214122, China; College of Computer Science and Technology, Anhui University of Technology, Maanshan, 243002, China; Han Swine Food Group Co., Ltd., Maanshan, 238200, China","Pan X., School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, 214122, China; Zhu J., School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, 214122, China; Tai W., College of Computer Science and Technology, Anhui University of Technology, Maanshan, 243002, China; Fu Y., Han Swine Food Group Co., Ltd., Maanshan, 238200, China","Knowledge of the body composition of growing pigs is of interest to breeding companies and producers, as it can be used to optimize production. As an emerging noninvasive technology computed tomography (CT) has been extensively applied in animal production studies. Recently, deep learning has generated new insights in medical image segmentation. In this paper, we describe a noninvasive method to automatically divide and quantify the composition of live pigs based on CT imaging and the application of deep neural network. The method consists of torso segmentation, visceral tissue removal, and identification and quantification of bone, lean meat, and fat using CT scans. The challenge addressed by this method is to identify the internal organs with complex structures and variable densities, the heart, lung, liver, stomach, spleen, kidney, colon, cecum, esophagus, jejunum, uterine horn, bladder, ureter, and rectum. We developed a bidirectional convolutional residual (BCR) framework to automatically and precisely segment the internal organs in a set of CT scans, and all internal organs share one label to remove these organs at once. To facilitate comparison with previous models, we tested BCR framework and found that it outperforms the classical approaches U-Net, residual U-Net (Res-U-Net) and dense U-Net (Dense-U-Net). We experimented on 40 pigs and compared our method with manual dissection. The results correlated well, and demonstrate that our method can accurately estimate the body composition proportion of fat, lean meat and bone—of live pigs, so the method will be valuable for livestock production. © 2021","Body composition; Computed tomography; Deep neural networks; Internal organs segmentation; Tissue content","Agriculture; Biochemistry; Deep learning; Deep neural networks; Image segmentation; Mammals; Medical imaging; Neural networks; Noninvasive medical procedures; Animal production; Automated methods; Classical approach; Complex structure; Livestock production; Non-invasive technology; Noninvasive methods; Variable density; artificial neural network; automation; cell component; computer system; fat; livestock farming; meat; pig; tomography; Computerized tomography","National Natural Science Foundation of China, NSFC, (61602007); Natural Science Foundation of Zhejiang Province, ZJNSF, (LZ15F010001); National Key Research and Development Program of China, NKRDPC, (2017YFC0109402); Fundamental Research Funds for the Central Universities, (61731008, JUSRP11851)","This work is supported by National Natural Science Foundation of China grant 61602007 , the Fundamental Research Funds for the Central Universities grant JUSRP11851 , and is supported in part by the grants from National Natural Science Foundation of China ( 61731008 ), National Key Research & Development Program ( 2017YFC0109402 ), and Zhejiang Provincial Natural Science Foundation of China ( LZ15F010001 ). ","W. Tai; College of Computer Science and Technology, Anhui University of Technology, Maanshan, 243002, China; email: taiweipeng@ahut.edu.cn","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85101844589"
"Xue W.; Hu X.-J.; Wei Z.; Mei X.-L.; Chen X.-J.; Xu Y.-C.","Xue, Wei (56747160700); Hu, Xue-jiao (57206261095); Wei, Zhong (39062426400); Mei, Xin-lan (36573907800); Chen, Xing-jian (57206277175); Xu, Yang-chun (8584885000)","56747160700; 57206261095; 39062426400; 36573907800; 57206277175; 8584885000","Prediction of compost maturity based on convolutional neural network; [基于卷积神经网络的堆肥腐熟度预测]","2019","Journal of Plant Nutrition and Fertilizers","25","11","","1977","1988","11","2","10.11674/zwyf.18477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079688565&doi=10.11674%2fzwyf.18477&partnerID=40&md5=9d568550ab53a385d52aa744d910ab9f","College of Information Science and Technology, Nanjing Agricultural University, Nanjing, 210095, China; College of Resources and Environmental Sciences, Nanjing Agricultural University, Jiangsu Key Laboratory of Solid Organic Waste Utilization, Nanjing, 210095, China","Xue W., College of Information Science and Technology, Nanjing Agricultural University, Nanjing, 210095, China; Hu X.-J., College of Information Science and Technology, Nanjing Agricultural University, Nanjing, 210095, China; Wei Z., College of Resources and Environmental Sciences, Nanjing Agricultural University, Jiangsu Key Laboratory of Solid Organic Waste Utilization, Nanjing, 210095, China; Mei X.-L., College of Resources and Environmental Sciences, Nanjing Agricultural University, Jiangsu Key Laboratory of Solid Organic Waste Utilization, Nanjing, 210095, China; Chen X.-J., College of Information Science and Technology, Nanjing Agricultural University, Nanjing, 210095, China; Xu Y.-C., College of Resources and Environmental Sciences, Nanjing Agricultural University, Jiangsu Key Laboratory of Solid Organic Waste Utilization, Nanjing, 210095, China","【Objectives】The compost maturity is mainly judged by complex chemical and biological experiments，which is difficult to operate and inefficient. Convolutional neural networks simulate human vision, which can retain the color information of compost images, and extract representative features such as contour, lines and granularity at the same time, thus avoiding the influence of different illumination conditions on the prediction effect of compost maturity. This paper proposed and verified a prediction model combining images of compost and convolution neural network.【Methods】Composting samples were collected from Jiangsu, Shandong and Zhejiang provinces, and the composting materials in three provinces were straw, cauliflower residues and livestock manure, respectively, and the composting cycle was 50 days, 45 days and 60 days in turn. In the factory shed, HIKVISION video camera (model C3W) was used to take composting images of different maturing stages in JPEG format under automatic light and compensation at night. The used focal length was 2.8 mm, resolution was 1080 p, and the distance was 1 m from the compost surface. Except for the image data sets from the three composts, the fourth image data set was taken in the mixture of them three in ratio of 1∶1∶1. 80% of the images from each data set was used to train CNN model and to establish prediction model parameters, and the remaining 20% to testify the prediction accuracy of the model.【Results】The compost maturity prediction model was composed of one input layer, three convolution layers, three pool layers, two full connection layers and one output layer. The accuracy of the predicted maturity from the compost images was averaged 98.7%, 98.7%, 98.8% and 98.2% for the vegetables residues, straws, livestock manure and the mix of above，respectively. Comparing with the most optimal result of classical algorithm on each data set, the average accuracy of this method in image feature extraction and classification were improved by 3 to 14 percentage points. Texture feature was more effective than color feature in judging compost maturity by CNN method.【Conclusions】As the priority of convolutional neural network in extracting the appearance features of compost images, the compost maturity prediction model based on it could accurately and rapidly identify the compost maturity directly through compost images under natural light conditions. © 2019 Chinese Academy of Agriculture Sciences,Editorial Department of Journal of Plant Nutrition and Fertilizer. All rights reserved.","Compost; Convolutional neural network; Deep learning; Machine vision; Maturity","","","","","","Chinese Academy of Agriculture Sciences,Editorial Department of Journal of Plant Nutrition and Fertilizer","1008505X","","","","Chinese","J. plant Nutr. Fertil.","Article","Final","","Scopus","2-s2.0-85079688565"
"Nunes L.; Ampatzidis Y.; Costa L.; Wallau M.","Nunes, Leon (57208342023); Ampatzidis, Yiannis (25648879400); Costa, Lucas (57215859659); Wallau, Marcelo (56395279700)","57208342023; 25648879400; 57215859659; 56395279700","Horse foraging behavior detection using sound recognition techniques and artificial intelligence","2021","Computers and Electronics in Agriculture","183","","106080","","","","16","10.1016/j.compag.2021.106080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101897476&doi=10.1016%2fj.compag.2021.106080&partnerID=40&md5=7a1ae556dd7172e0545483b80bdb25bb","Agricultural and Biological Engineering Department, Southwest Florida Research and Education Center, University of Florida, IFAS, 2685 SR 29 North, Immokalee, 34142, FL, United States; Agronomy Department, University of Florida, IFAS, Gainesville, 32611, FL, United States","Nunes L., Agricultural and Biological Engineering Department, Southwest Florida Research and Education Center, University of Florida, IFAS, 2685 SR 29 North, Immokalee, 34142, FL, United States; Ampatzidis Y., Agricultural and Biological Engineering Department, Southwest Florida Research and Education Center, University of Florida, IFAS, 2685 SR 29 North, Immokalee, 34142, FL, United States; Costa L., Agricultural and Biological Engineering Department, Southwest Florida Research and Education Center, University of Florida, IFAS, 2685 SR 29 North, Immokalee, 34142, FL, United States; Wallau M., Agronomy Department, University of Florida, IFAS, Gainesville, 32611, FL, United States","Wearable sensing technologies can be used for precision livestock production and to study foraging strategies to better understand the relationships between herbivores, vegetation, and landscape. In this context, monitoring grazing behavior (i.e. chew and bite events) can provide critical information for livestock management. This study presents a computational tool that utilizes wearable sensing and deep learning to distinguish chew and bite events in horses. A micro camera equipped with a microphone (0–18 kHz) was used to obtain video/audio data from horses during grazing. The collected audio data were treated in a pre-processing filtering step, then used to train a recurrent neural network (RNN) with a long short-term memory (LSTM) layer to detect and distinguish chews, bites, and noise events. A post-processing sliding window technique was used to filter events with low confidence levels and lengths. Initial evaluation of this system showed an accuracy of 88.64% for bite identification and an accuracy of 94.13% for chew identification. The distinction between events and evaluation of responses to different pasture species and structure can provide useful information on the plant-animal interface. That is aligned to information such as bite rate, bite mass, and grazing time, and can help determine management strategies that optimize intake and provide data for modeling foraging behavior to predict pasture use and animal performance. © 2021 Elsevier B.V.","Animal behavior; Deep learning; Livestock; LSTM; RNN; Wearable devices","Equidae; Agriculture; Animals; Audio acoustics; Deep learning; Multilayer neural networks; Plants (botany); Wearable technology; Animal performance; Computational tools; Foraging behaviors; Livestock production; Management strategies; Pre-processing filtering; Recurrent neural network (RNN); Sliding window techniques; accuracy assessment; artificial intelligence; detection method; economic instrument; electronic equipment; equipment; foraging behavior; instrumentation; livestock farming; Long short-term memory","","","Y. Ampatzidis; Agricultural and Biological Engineering Department, Southwest Florida Research and Education Center, University of Florida, IFAS, Immokalee, 2685 SR 29 North, 34142, United States; email: i.ampatzidis@ufl.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85101897476"
"Chuluunsaikhan T.; Ryu G.-A.; Yoo K.-H.; Rah H.; Nasridinov A.","Chuluunsaikhan, Tserenpurev (57219242254); Ryu, Ga-Ae (57203790082); Yoo, Kwan-Hee (8859846800); Rah, Hyungchul (58691486500); Nasridinov, Aziz (55516461700)","57219242254; 57203790082; 8859846800; 58691486500; 55516461700","Incorporating deep learning and news topic modeling for forecasting pork prices: The case of South Korea","2020","Agriculture (Switzerland)","10","11","513","1","22","21","18","10.3390/agriculture10110513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094867344&doi=10.3390%2fagriculture10110513&partnerID=40&md5=a725d4584d6a175511e397ab3854fbab","Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Department of Management Information System, Chungbuk National University, Cheongju, 28644, South Korea","Chuluunsaikhan T., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Ryu G.-A., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Yoo K.-H., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; Rah H., Department of Management Information System, Chungbuk National University, Cheongju, 28644, South Korea; Nasridinov A., Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea","Knowing the prices of agricultural commodities in advance can provide governments, farmers, and consumers with various advantages, including a clearer understanding of the market, planning business strategies, and adjusting personal finances. Thus, there have been many efforts to predict the future prices of agricultural commodities in the past. For example, researchers have attempted to predict prices by extracting price quotes, using sentiment analysis algorithms, through statistical information from news stories, and by other means. In this paper, we propose a methodology that predicts the daily retail price of pork in the South Korean domestic market based on news articles by incorporating deep learning and topic modeling techniques. To do this, we utilized news articles and retail price data from 2010 to 2019. We initially applied a topic modeling technique to obtain relevant keywords that can express price fluctuations. Based on these keywords, we constructed prediction models using statistical, machine learning, and deep learning methods. The experimental results show that there is a strong relationship between the meaning of news articles and the price of pork. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Agri‐food; Livestock price; LSTM forecast; Pork price; Price forecast; Topic modeling","","Institute for Information and Communications Technology Planning and Evaluation; Ministry of Science and ICT, (01462); Rural Development Administration, RDA; Institute for Information and Communications Technology Promotion, IITP","Funding text 1: Acknowledgments: This work was carried out with the support of the “Cooperative Research Program for Agriculture Science and Technology Development” (Project No. PJ015341012020), Rural Development Administration, Republic of Korea. This research was also supported by the MSIT (Ministry of Science and ICT),; Funding text 2: This work was carried out with the support of the ?Cooperative Research Program for Agriculture Science and Technology Development? (Project No. PJ015341012020), Rural Development Administration, Republic of Korea. This work was carried out with the support of the ?Cooperative Research Program for Agriculture Science and Technology Development? (Project No. PJ015341012020), Rural Development Administration, Republic of Korea. This research was also supported by the MSIT (Ministry of Science and ICT), Korea, under the Grand Information Technology Research Center support program (IITP?2020?0?01462) supervised by the IITP (Institute for Information and Communications Technology Planning and Evaluation).; Funding text 3: Funding: This work was carried out with the support of the “Cooperative Research Program for Agriculture Science and Technology Development” (Project No. PJ015341012020), Rural Development Administration, Republic of Korea.","A. Nasridinov; Department of Computer Science, Chungbuk National University, Cheongju, 28644, South Korea; email: aziz@chungbuk.ac.kr","","MDPI AG","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85094867344"
"Unal Z.","Unal, Zeynep (57444928100)","57444928100","Smart Farming Becomes even Smarter with Deep Learning - A Bibliographical Analysis","2020","IEEE Access","8","","9108212","105587","105609","22","106","10.1109/ACCESS.2020.3000175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086712805&doi=10.1109%2fACCESS.2020.3000175&partnerID=40&md5=d75f1a2310ade71c24e980bfa651ec1c","Niǧtaş Company, Niǧde, Turkey","Unal Z., Niǧtaş Company, Niǧde, Turkey","Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection. © 2013 IEEE.","artificial neural networks; internet of things; Machine learning; precision agriculture","Agricultural robots; Agriculture; Bibliographies; Learning systems; Neural networks; Object recognition; Weed control; Agricultural industries; Automatic feature extraction; Information resource; Land cover identifications; Machine learning methods; Plant classification; Precision livestock farming; Science citation index; Deep learning","","","Z. Unal; Niǧtaş Company, Niǧde, Turkey; email: zeynepunal1010@hotmail.com","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086712805"
"","","","15th International Conference on Soft Computing Models in Industrial and Environmental Applications, SOCO 2020","2021","Advances in Intelligent Systems and Computing","1268 AISC","","","","","871","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091292024&partnerID=40&md5=e193967517c86d3009366e110bb9b47b","","","The proceedings contain 83 papers. The special focus in this conference is on Soft Computing Models in Industrial and Environmental Applications. The topics include: A Smart Crutch Tip for Monitoring the Activities of Daily Living Based on a Novel Neural-Network Intelligent Classifier; Hourly Air Quality Index (AQI) Forecasting Using Machine Learning Methods; interpretable Deep Learning with Hybrid Autoencoders to Predict Electric Energy Consumption; on the Performance of Deep Learning Models for Time Series Classification in Streaming; An Approach to Forecasting and Filtering Noise in Dynamic Systems Using LSTM Architectures; novel Approach for Person Detection Based on Image Segmentation Neural Network; an Adaptive Cognitive Model to Integrate Machine Learning and Visual Streaming Data; smart Song Equalization Based on the Classification of Musical Genres; machine Learning in Classification of the Wax Structure of Breathing Openings on Leaves Affected by Air Pollution; A Preliminary Study for Automatic Activity Labelling on an Elder People ADL Dataset; software Sensors for the Monitoring of Bioprocesses; RGB Images Driven Recognition of Grapevine Varieties; discovering Spatio-Temporal Patterns in Precision Agriculture Based on Triclustering; counting Livestock with Image Segmentation Neural Network; smart, Precision or Digital Agriculture and Farming - Current State of Technology; an Automated Platform for Microrobot Manipulation; growth Models of Female Dairy Cattle; a Preliminary Study on Crop Classification with Unsupervised Algorithms for Time Series on Images with Olive Trees and Cereal Crops; blocks of Jobs for Solving Two-Machine Flow Shop Problem with Normal Distributed Processing Times; soft Computing Analysis of Pressure Decay Leak Test Detection.","","","","","","Herrero A.; Cambra C.; Urda D.; Sedano J.; Quintián H.; Corchado E.","Springer Science and Business Media Deutschland GmbH","21945357","978-303057801-5","","","English","Adv. Intell. Sys. Comput.","Conference review","Final","","Scopus","2-s2.0-85091292024"
"Nayak H.M.; Naresh E.; Murthy S.V.N.","Nayak, Harshitha M (57221915019); Naresh, E. (57192084190); Murthy, S.V.N. (57205440288)","57221915019; 57192084190; 57205440288","Predicting the Cattle Production Parameters Through Deep Learning Approach: A Review","2021","Proceedings of CONECCT 2021: 7th IEEE International Conference on Electronics, Computing and Communication Technologies","","","","","","","0","10.1109/CONECCT52877.2021.9622550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123353209&doi=10.1109%2fCONECCT52877.2021.9622550&partnerID=40&md5=8b41f6137d087f67cdbc5ec8048b044f","M.S.Ramaiah Institute of Technology, Department of ISE, Bangalore, India","Nayak H.M., M.S.Ramaiah Institute of Technology, Department of ISE, Bangalore, India; Naresh E., M.S.Ramaiah Institute of Technology, Department of ISE, Bangalore, India; Murthy S.V.N., M.S.Ramaiah Institute of Technology, Department of ISE, Bangalore, India","The requirement of image processing is very much required on the present world. Almost in every field it has a huge application. Detecting and recognizing the face is one of the most trending image processing systems as it is mostly used for the authentication process to identify specific person etc. Along with face detection now-a-days understanding emotions also plays a vital role. These all processing's are not only required in humans but also in animals in order to understand the emotions of them. In this paper, we have endured through various technical papers which have different methods or algorithms used for detecting and recognizing the face and also for detecting the emotions. These reviews helped us in finding the best algorithm in deep learning perception for cattle face detection and recognition and also understanding the emotions so that we can improve the production parameters of cattle which include production of milk, good yield in agricultural land etc. © 2021 IEEE.","Cattle; Deep Learning; Emotion Recognition; Face Detection; Face recognition; Livestock","Behavioral research; Deep learning; Face recognition; Cattle; Cattle production; Deep learning; Emotion recognition; Face detection and recognition; Faces detection; Image processing system; Images processing; Learning approach; Production parameters; Agriculture","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166542849-1","","","English","Proc. CONECCT: IEEE Int. Conf. Electron., Comput. Commun. Technol.","Conference paper","Final","","Scopus","2-s2.0-85123353209"
"Tassinari P.; Bovo M.; Benni S.; Franzoni S.; Poggi M.; Mammi L.M.E.; Mattoccia S.; Di Stefano L.; Bonora F.; Barbaresi A.; Santolini E.; Torreggiani D.","Tassinari, Patrizia (13609849700); Bovo, Marco (55504159800); Benni, Stefano (24334072400); Franzoni, Simone (57221929517); Poggi, Matteo (56912008100); Mammi, Ludovica Maria Eugenia (57192013700); Mattoccia, Stefano (6505990778); Di Stefano, Luigi (7004306730); Bonora, Filippo (57189442488); Barbaresi, Alberto (55980314200); Santolini, Enrica (57193406635); Torreggiani, Daniele (24336908000)","13609849700; 55504159800; 24334072400; 57221929517; 56912008100; 57192013700; 6505990778; 7004306730; 57189442488; 55980314200; 57193406635; 24336908000","A computer vision approach based on deep learning for the detection of dairy cows in free stall barn","2021","Computers and Electronics in Agriculture","182","","106030","","","","88","10.1016/j.compag.2021.106030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100664602&doi=10.1016%2fj.compag.2021.106030&partnerID=40&md5=a9cb83b5066ee3c3b675d5deee6f0fb7","Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Veterinary Medical Sciences, University of Bologna, Bologna, Italy","Tassinari P., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Bovo M., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Benni S., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Franzoni S., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Poggi M., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Mammi L.M.E., Department of Veterinary Medical Sciences, University of Bologna, Bologna, Italy; Mattoccia S., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Di Stefano L., Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Bonora F., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Barbaresi A., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Santolini E., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; Torreggiani D., Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy","Precision Livestock Farming relies on several technological approaches to acquire in the most efficient way precise and up-to-date data concerning individual animals. In dairy farming, particular attention is paid to the automatic cow detection and tracking, as such information is closely related to animal welfare and thus to possible health issues. Computer vision represents a suitable and promising method for this purpose. This paper describes the first step for the development of a computer vision system, based on deep learning, aiming to recognize in real-time the individual cows, detect their positions, actions and movements and record the time history outputs for each animal. Specifically, a neural network based on deep learning techniques has been trained and validated on a case study farm, for the automatic recognition of individual cows in videos recorded in the barn. Four cows were selected to train and validate a YOLO neural network able to recognize a cow starting from the coat pattern. Then, precision-recall curves of the identification of individual cows were elaborated for both the specific target classes and the whole dataset in order to assess the performances of the network. By means of data augmentation techniques, an enlarged dataset has been created and considered in order to improve the performance of the network and to provide indications to increase detection efficiency in those cases where data acquisition is not easy to be carried out for long periods. The mean average precision of the detection, ranging from 0.64 to 0.66, showed that it is possible to properly identify individual cows based on their morphological appearance and that the piebald spotting pattern of a cow's coat represents a clearly distinguishable object for a computer vision network. The results also led to obtain indications about the quantity and the characteristics of the images to be used for the network training in order to achieve efficient detections when facing with applications involving animals. © 2021 Elsevier B.V.","Computer vision; Dairy cow; Deep learning; Herd management; Precision livestock farming","Animals; Computer vision; Data acquisition; Farm buildings; Learning systems; Neural networks; Automatic recognition; Computer vision system; Detection and tracking; Detection efficiency; Efficient detection; Identification of individuals; Learning techniques; Precision livestock farming; algorithm; animal welfare; computer vision; data acquisition; detection method; image analysis; precision; Deep learning","Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR, (20178AN8NC); Università di Bologna, UNIBO","Funding text 1: The authors wish to thank Prof. Andrea Formigoni, Scientific Supervisor of the Experimental and Didactic Dairy Cows Unit of the University of Bologna, where all the video recordings, images and data necessary for carrying out the research have been acquired. Funding, The activity presented in the paper is part of the research project PRIN 2017 “Smart dairy farming: innovative solutions to improve herd productivity” funded by the Italian Ministry of Education, University and Research [20178AN8NC].; Funding text 2: The activity presented in the paper is part of the research project PRIN 2017 “Smart dairy farming: innovative solutions to improve herd productivity” funded by the Italian Ministry of Education, University and Research [20178AN8NC].","M. Bovo; Department of Agricultural and Food Sciences, University of Bologna, Bologna, Italy; email: marco.bovo@unibo.it","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100664602"
"Liu C.; Wen H.; Liu X.; Sun S.; Yang L.","Liu, Chao (57211583592); Wen, Hongyuan (16314549200); Liu, Xiaojun (57212031681); Sun, Songli (57188849954); Yang, Lei (57562058300)","57211583592; 16314549200; 57212031681; 57188849954; 57562058300","Development of Portable Device for Non-Destructive Testing of Livestock Meat Quality based on Machine Vision","2021","Proceedings - 2021 International Conference on Networking, Communications and Information Technology, NetCIT 2021","","","","336","342","6","0","10.1109/NetCIT54147.2021.00074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127565917&doi=10.1109%2fNetCIT54147.2021.00074&partnerID=40&md5=f52b326742fb03ac5edb9bfbbdae8e54","Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China; School of Engineering, Nanjing Agricultural University, Nanjing, China; Nanjing University of Science and Technology, Business School, Taizhou Institute of Science and Technology, Taizhou, 225300, China","Liu C., Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China, School of Engineering, Nanjing Agricultural University, Nanjing, China; Wen H., Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China; Liu X., Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China; Sun S., Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China; Yang L., Nanjing University of Science and Technology, Business School, Taizhou Institute of Science and Technology, Taizhou, 225300, China","A portable non-destructive testing device for meat quality is designed, including the design of the lower computer and the design of the upper computer. The lower computer design involves a single chip control unit, a light source unit (LED light source), an image acquisition unit, data processing and communication (CCD binocular camera). The upper computer uses Anaconda Prompt software and TensorFlow framework to establish a visualization program, which is connected to the lower computer through serial communication to display the gas sensor data collection results, the meat quality judgment results, store the collected images and other data. A training data set is established, and the meat quality identification model is trained based on the ResNet network framework, and the model identification accuracy rate reaches 96%.  © 2021 IEEE.","deep learning; meat quality; non-destructive testing; ResNet network","Agriculture; Computer vision; Data acquisition; Data handling; Data visualization; Deep learning; Display devices; Image acquisition; Light sources; Chip control; Computer designs; Deep learning; Machine-vision; Meat quality; On-machines; Portable device; Resnet network; Single-chip; Upper computer; Nondestructive examination","2020 Taizhou Science and Technology Support Plan, (TN202011, TS201919); Design of Intelligent Detection Device for Pork Freshness Based on Machine Vision, (TS201918); Taizhou Science and Technology Support Plan","ACKNOWLEDGMENT Fund Project: 2019 Taizhou Science and Technology Support Plan (Social Development) Project: Design of Intelligent Detection Device for Pork Freshness Based on Machine Vision (TS201918); 2020 Taizhou Science and Technology Support Plan (Agriculture) Project: Key technologies for sorting Pleurotus eryngiibased on deep learning (TN202011); 2019 Taizhou Science and Technology Support Plan (Social Development) Project: Key technologies for integrated water quality monitoring and sampling based on drones (TS201919).","C. Liu; Nanjing University of Science and Technology, College of Intelligent Manufacturing, Taizhou Institute of Science and Technology, Taizhou, 225300, China; email: lucholiu@qq.com","","Institute of Electrical and Electronics Engineers Inc.","","978-166540070-1","","","English","Proc. - Int. Conf. Netw., Commun. Inf. Technol., NetCIT","Conference paper","Final","","Scopus","2-s2.0-85127565917"
"Ayadi S.; Ben Said A.; Jabbar R.; Aloulou C.; Chabbouh A.; Achballah A.B.","Ayadi, Safa (57222096590); Ben Said, Ahmed (56495176100); Jabbar, Rateb (57203315917); Aloulou, Chafik (6507209973); Chabbouh, Achraf (57209400574); Achballah, Ahmed Ben (55324472300)","57222096590; 56495176100; 57203315917; 6507209973; 57209400574; 55324472300","Dairy Cow Rumination Detection: A Deep Learning Approach","2020","Communications in Computer and Information Science","1348","","","123","139","16","23","10.1007/978-3-030-65810-6_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101386207&doi=10.1007%2f978-3-030-65810-6_7&partnerID=40&md5=f019d6a71e4b21e698aa9c2a390cd977","LifeEye LLC, Tunis, Tunisia; Faculty of Economics and Management of Sfax, University of Sfax, Sfax, 3018, Tunisia; Department of Computer Science and Engineering, Qatar University, Doha, Qatar","Ayadi S., LifeEye LLC, Tunis, Tunisia, Faculty of Economics and Management of Sfax, University of Sfax, Sfax, 3018, Tunisia; Ben Said A., LifeEye LLC, Tunis, Tunisia, Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Jabbar R., LifeEye LLC, Tunis, Tunisia, Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Aloulou C., Faculty of Economics and Management of Sfax, University of Sfax, Sfax, 3018, Tunisia; Chabbouh A., LifeEye LLC, Tunis, Tunisia; Achballah A.B., LifeEye LLC, Tunis, Tunisia","Cattle activity is an essential index for monitoring health and welfare of the ruminants. Thus, changes in the livestock behavior are a critical indicator for early detection and prevention of several diseases. Rumination behavior is a significant variable for tracking the development and yield of animal husbandry. Therefore, various monitoring methods and measurement equipment have been used to assess cattle behavior. However, these modern attached devices are invasive, stressful and uncomfortable for the cattle and can influence negatively the welfare and diurnal behavior of the animal. Multiple research efforts addressed the problem of rumination detection by adopting new methods by relying on visual features. However, they only use few postures of the dairy cow to recognize the rumination or feeding behavior. In this study, we introduce an innovative monitoring method using Convolution Neural Network (CNN)-based deep learning models. The classification process is conducted under two main labels: ruminating and other, using all cow postures captured by the monitoring camera. Our proposed system is simple and easy-to-use which is able to capture long-term dynamics using a compacted representation of a video in a single 2D image. This method proved efficiency in recognizing the rumination behavior with 95%, 98% and 98% of average accuracy, recall and precision, respectively. © 2020, Springer Nature Switzerland AG.","Action recognition; Computer vision; Dairy cows; Deep learning; Machine learning; Rumination behavior","Agriculture; Mammals; Monitoring; Classification process; Convolution neural network; Learning approach; Long term dynamics; Measurement equipment; Monitoring methods; Recall and precision; Significant variables; Deep learning","LifeEye LLC","Acknowledgment. This research work is supported by LifeEye LLC. The statements made herein are solely the responsibility of the authors.","S. Ayadi; LifeEye LLC, Tunis, Tunisia; email: safa.ayadi@lifeye.io","Jemili I.; Mosbah M.","Springer Science and Business Media Deutschland GmbH","18650929","978-303065809-0","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85101386207"
"Atkinson G.A.; Smith L.N.; Smith M.L.; Reynolds C.K.; Humphries D.J.; Moorby J.M.; Leemans D.K.; Kingston-Smith A.H.","Atkinson, Gary A. (13104569100); Smith, Lyndon N. (9237709400); Smith, Melvyn L. (55495905800); Reynolds, Christopher K. (7402331705); Humphries, David J. (7005771893); Moorby, Jon M. (7004913966); Leemans, David K. (7003629493); Kingston-Smith, Alison H. (6603916455)","13104569100; 9237709400; 55495905800; 7402331705; 7005771893; 7004913966; 7003629493; 6603916455","A computer vision approach to improving cattle digestive health by the monitoring of faecal samples","2020","Scientific Reports","10","1","17557","","","","12","10.1038/s41598-020-74511-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092652090&doi=10.1038%2fs41598-020-74511-0&partnerID=40&md5=8a8888b66e0db044f21bd0c86af58dc9","Centre for Machine Vision, Bristol Robotics Laboratory, University of the West of England, Bristol, BS16 1QY, United Kingdom; Centre for Dairy Research, School of Agriculture, Policy and Development, Earley Gate, University of Reading, Reading, RG6 6AR, United Kingdom; Institute of Biological, Environmental and Rural Sciences, Aberystwyth University, Gogerddan, Aberystwyth, SY23 3EE, United Kingdom","Atkinson G.A., Centre for Machine Vision, Bristol Robotics Laboratory, University of the West of England, Bristol, BS16 1QY, United Kingdom; Smith L.N., Centre for Machine Vision, Bristol Robotics Laboratory, University of the West of England, Bristol, BS16 1QY, United Kingdom; Smith M.L., Centre for Machine Vision, Bristol Robotics Laboratory, University of the West of England, Bristol, BS16 1QY, United Kingdom; Reynolds C.K., Centre for Dairy Research, School of Agriculture, Policy and Development, Earley Gate, University of Reading, Reading, RG6 6AR, United Kingdom; Humphries D.J., Centre for Dairy Research, School of Agriculture, Policy and Development, Earley Gate, University of Reading, Reading, RG6 6AR, United Kingdom; Moorby J.M., Institute of Biological, Environmental and Rural Sciences, Aberystwyth University, Gogerddan, Aberystwyth, SY23 3EE, United Kingdom; Leemans D.K., Institute of Biological, Environmental and Rural Sciences, Aberystwyth University, Gogerddan, Aberystwyth, SY23 3EE, United Kingdom; Kingston-Smith A.H., Institute of Biological, Environmental and Rural Sciences, Aberystwyth University, Gogerddan, Aberystwyth, SY23 3EE, United Kingdom","The digestive health of cows is one of the primary factors that determine their well-being and productivity. Under- and over-feeding are both commonplace in the beef and dairy industry; leading to welfare issues, negative environmental impacts, and economic losses. Unfortunately, digestive health is difficult for farmers to routinely monitor in large farms due to many factors including the need to transport faecal samples to a laboratory for compositional analysis. This paper describes a novel means for monitoring digestive health via a low-cost and easy to use imaging device based on computer vision. The method involves the rapid capture of multiple visible and near-infrared images of faecal samples. A novel three-dimensional analysis algorithm is then applied to objectively score the condition of the sample based on its geometrical features. While there is no universal ground truth for comparison of results, the order of scores matched a qualitative human prediction very closely. The algorithm is also able to detect the presence of undigested fibres and corn kernels using a deep learning approach. Detection rates for corn and fibre in image regions were of the order 90%. These results indicate the potential to develop this system for on-farm, real time monitoring of the digestive health of individual animals, allowing early intervention to effectively adjust feeding strategy. © 2020, The Author(s).","","Algorithms; Animal Feed; Animal Husbandry; Animal Welfare; Animals; Behavior, Animal; Calibration; Cattle; Dairying; Deep Learning; Farms; Feces; Image Processing, Computer-Assisted; Livestock; Software; Spectroscopy, Near-Infrared; agricultural land; algorithm; animal; animal behavior; animal food; animal husbandry; animal welfare; bovine; calibration; dairying; devices; feces; image processing; livestock; near infrared spectroscopy; procedures; software","Sustainable Agriculture Research and Innovation Club; Natural Environment Research Council, NERC, (NE/P007996/1); National Eye Research Centre, NERC","The authors with to thank members of the Sustainable Agriculture Research and Innovation Club (SARIC) for supporting this research (Grant NERC NE/P007996/1). In addition, gratitude is shown to Mark Hansen and Gytis Bernotas for their work on the design of the PS rig.","G.A. Atkinson; Centre for Machine Vision, Bristol Robotics Laboratory, University of the West of England, Bristol, BS16 1QY, United Kingdom; email: gary.atkinson@uwe.ac.uk","","Nature Research","20452322","","","33067502","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092652090"
"Huang C.; Mao P.; Li P.; Geng Q.; Fang Q.; Zhang J.","Huang, Chuanpeng (58298955000); Mao, Pengjun (36706156700); Li, Pengju (58298823500); Geng, Qian (58298575500); Fang, Qian (57963625700); Zhang, Jiarui (58298328000)","58298955000; 36706156700; 58298823500; 58298575500; 57963625700; 58298328000","Research and trend of autonomous flight technology of agricultural UAV; [农用无人机自主飞行技术研究与趋势]","2020","Journal of Chinese Agricultural Mechanization","41","11","","162","170","8","2","10.13733/j.jcam.issn.2095-5553.2020.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142700067&doi=10.13733%2fj.jcam.issn.2095-5553.2020.11.025&partnerID=40&md5=effa343f66ed67347fa6fed6404fadb4","Henan University of Science and Technology, Luoyang, 471003, China","Huang C., Henan University of Science and Technology, Luoyang, 471003, China; Mao P., Henan University of Science and Technology, Luoyang, 471003, China; Li P., Henan University of Science and Technology, Luoyang, 471003, China; Geng Q., Henan University of Science and Technology, Luoyang, 471003, China; Fang Q., Henan University of Science and Technology, Luoyang, 471003, China; Zhang J., Henan University of Science and Technology, Luoyang, 471003, China","With the rapid development of artificial intelligence and deep learning, autonomous flight technology of UAV has become one of the intelligent evaluation of UAV. Under the guidance of precision agriculture and intelligent agriculture, UAV is developing rapidly in the field of agriculture. The application scenarios of agricultural UAV include: crop pollination, spraying operation, agricultural situation monitoring, toppling and pruning, flower and fruit thinning and livestock tracking, and the importance of their autonomy is self-evident. It was summarized the research status of UAV flight technology at home and abroad, and introduced the research progress of agricultural UAV: method of autonomous control, method of obstacle avoidance, trajectory planning algorithm and the research progress of precision spraying method. It was pointed out that agricultural UAV system autonomous environmental awareness was poor, such as poor sense of autonomous environment, slow speed of information processing, slow convergence of path planning algorithm, low rate of crop recognition. The improved methods such as multi-sensor combination, dual-redundancy control, multi-algorithm fusion and crop feature recognition based on depth learning were proposed. It provides a theoretical basis for autonomous flight technology of Agricultural UAV to meet the requirements of intelligent operation. © 2020 Journal of Chinese Agricultural Mechanization Editorial Office. All rights reserved.","agricultural UAV; deep learning; obstacle avoidance; precision agriculture; trajectory planning","","","","","","Journal of Chinese Agricultural Mechanization Editorial Office","20955553","","","","Chinese","J. Chin. Agric. Mech.","Article","Final","","Scopus","2-s2.0-85142700067"
"Lima L.M.; Cavalcante V.C.; De Sousa M.G.; Fleury C.A.; Oliveira D.; De Andrade Freitas E.N.","Lima, Lucas Mendes (57795115200); Cavalcante, Victor Calebe (57795383700); De Sousa, Mariana Guimaraes (57795115300); Fleury, Claudio Afonso (56879968400); Oliveira, Diogo (57194333825); De Andrade Freitas, Eduardo Noronha (57205342850)","57795115200; 57795383700; 57795115300; 56879968400; 57194333825; 57205342850","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review","2021","Proceedings - 2021 International Conference on Computational Science and Computational Intelligence, CSCI 2021","","","","1708","1715","7","2","10.1109/CSCI54926.2021.00324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133929385&doi=10.1109%2fCSCI54926.2021.00324&partnerID=40&md5=cbecba50cadfdbf5d2297c88d8f166bf","Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil; Florida State University, Fsu, Florida, United States","Lima L.M., Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil; Cavalcante V.C., Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil; De Sousa M.G., Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil; Fleury C.A., Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil; Oliveira D., Florida State University, Fsu, Florida, United States; De Andrade Freitas E.N., Federal Institute of Goiás, Ifg, GO, Goiânia, Brazil","Context: Although agribusiness corresponded to more than 20% of Brazil's Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow's body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow's back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques. © 2021 IEEE.","behavioral tracking; computer vision; dairy cows; welfare monitoring","Agriculture; Cost effectiveness; Deep learning; Image recognition; Paper; Alternative technologies; Behavioral tracking; Control monitoring; Dairy cattles; Dairy cow; Gross domestic products; Manual monitoring; Systematic literature review; Systematic Review; Welfare monitoring; Computer vision","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166545841-2","","","English","Proc. - Int. Conf. Comput. Sci. Comput. Intell., CSCI","Conference paper","Final","","Scopus","2-s2.0-85133929385"
"Meckbach C.; Tiesmeyer V.; Traulsen I.","Meckbach, Cornelia (56090981600); Tiesmeyer, Verena (57222365947); Traulsen, Imke (35410826800)","56090981600; 57222365947; 35410826800","A promising approach towards precise animal weight monitoring using convolutional neural networks","2021","Computers and Electronics in Agriculture","183","","106056","","","","23","10.1016/j.compag.2021.106056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102480477&doi=10.1016%2fj.compag.2021.106056&partnerID=40&md5=85c35a8464495c08f6d509138031c5ec","Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Campus Institute Data Science, Goldschmidtstraße 1, Göttingen, 37077, Germany","Meckbach C., Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany, Campus Institute Data Science, Goldschmidtstraße 1, Göttingen, 37077, Germany; Tiesmeyer V., Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany; Traulsen I., Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Albrecht-Thaer-Weg 3, Göttingen, 37075, Germany","Accurate monitoring of the live weight of pigs provides important information about the health state, the daily gain, and the time point for marketing. However, manual weight determination is time-consuming and stressful for both stockman and pig. In order to overcome these problems, non-invasive weighing mechanisms have to be established. In this study, we present an approach for live weight determination based on convolutional neuronal networks applied solely on the depth images of pigs, without further feature extraction. Our data basis consists of >400 pigs, recorded at four weighing time points, ending up with a weight range between 20 and 133 kg. Training and testing on this data, we achieved a coefficient of determination R2>0.97. Our results reveal that providing solely the images and the related weight to the ConvNets is sufficient to reach an accurate weight prediction. Therefore, our study can be viewed as a preliminary work that confirms the ability of using a ConvNets for accurate weight determination at different life stages. With the aim of using them under usual housing conditions for pigs, we increase animal welfare by precise animal monitoring in the sense of precision livestock farming. © 2021 Elsevier B.V.","Convolutional neural network; Deep learning; Depth image; Pig weight estimation; Precision livestock farming","Convolution; Deep neural networks; Mammals; Neurons; Weighing; Animal weights; Convnet; Convolutional neural network; Deep learning; Depth image; Health state; Pig weight estimation; Precision livestock farming; Time points; Weight determination; accuracy assessment; artificial island; artificial nest; artificial neural network; detection method; livestock farming; Agriculture","","","C. Meckbach; Department of Animal Sciences, Livestock Systems, Georg-August-University Göttingen, Göttingen, Albrecht-Thaer-Weg 3, 37075, Germany; email: cornelia.meckbach@uni-goettingen.de","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85102480477"
"Qiao Y.; Su D.; Kong H.; Sukkarieh S.; Lomax S.; Clark C.","Qiao, Yongliang (56486770900); Su, Daobilige (56594211700); Kong, He (57203456679); Sukkarieh, Salah (6602844626); Lomax, Sabrina (56151402400); Clark, Cameron (7403546385)","56486770900; 56594211700; 57203456679; 6602844626; 56151402400; 7403546385","BiLSTM-based Individual Cattle Identification for Automated Precision Livestock Farming","2020","IEEE International Conference on Automation Science and Engineering","2020-August","","9217026","967","972","5","30","10.1109/CASE48305.2020.9217026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094108275&doi=10.1109%2fCASE48305.2020.9217026&partnerID=40&md5=3145e157a4ea41ebea60f48e9367f0f5","University of Sydney, Australian Centre for Field Robotics, Australia; University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia","Qiao Y., University of Sydney, Australian Centre for Field Robotics, Australia; Su D., University of Sydney, Australian Centre for Field Robotics, Australia; Kong H., University of Sydney, Australian Centre for Field Robotics, Australia; Sukkarieh S., University of Sydney, Australian Centre for Field Robotics, Australia; Lomax S., University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia; Clark C., University of Sydney, Livestock Production and Welfare Group, School of Life and Environmental Sciences, Australia","Individual cattle identification plays an important role for automation in precision livestock management. Existing methods for cattle identification require radio frequency and visual ear tags, all of which are prone to loss or damage. In this work, we propose a deep learning-based framework to identify beef cattle using image sequences, unifying merits of both Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) network methods. A CNN (Inception-V3) was used to extract features from a video dataset taken of the rear-view of cattle, after which extracted features were fed to a BiLSTM layer to capture spatial-temporal information enabling the identification of each individual animal. A total of 363 rear-view videos of 50 cattle were collected for our dataset. The proposed method achieved 91% identification accuracy using a 30-frame video length, improving that of Inception-V3 use or LSTM. Additionally, increasing video sequence length to 30-frames enhanced identification performance. Our approach can use spatial-temporal features to identify cattle, and enables automated identification for precision livestock farming. © 2020 IEEE.","","Agriculture; Automation; Bismuth compounds; Convolutional neural networks; Deep learning; Long short-term memory; Radio frequency identification (RFID); Automated identification; Identification accuracy; Network methods; Precision livestock farming; Radio frequencies; Spatial temporals; Spatial-temporal features; Video sequences; Damage detection","Meat and Livestock Australia, MLA, (P.PSH.0819); Meat and Livestock Australia, MLA","ACKNOWLEDGMENTS The authors acknowledge the support of the Meat & Livestock Australia Donor Company through the project: Objective, robust, real-time animal welfare measures for the Australian red meat industry (P.PSH.0819). The authors also express their gratitude to Khalid Rafique, Javier Martinez, Amanda Doughty, Ashraful Islam, and Mike Reynolds for their help in experiment organization and data collection.","","","IEEE Computer Society","21618070","978-172816904-0","","","English","IEEE Int. Conf. Autom. Sci. Eng.","Conference paper","Final","","Scopus","2-s2.0-85094108275"
"Cevik K.K.; Boga M.","Cevik, Kerim Kursat (56816848700); Boga, Mustafa (24279437800)","56816848700; 24279437800","Body Condition Score (BCS) Classification with Deep Learning; [Derin Öǧrenme ile Vöcut Kondisyon Skoru (VKS) Siniflandirilmasi]","2019","Proceedings - 2019 Innovations in Intelligent Systems and Applications Conference, ASYU 2019","","","8946405","","","","7","10.1109/ASYU48272.2019.8946405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078328389&doi=10.1109%2fASYU48272.2019.8946405&partnerID=40&md5=675fe0acde9445d87893e2842b2fac60","Akdeniz Üniversitesi, Işletme Enformatiǧi Bölümü, Antalya, Turkey; Gida Işleme Bölümü, Niǧde Ömer Halisdemir Üniversitesi, Niǧde, Turkey","Cevik K.K., Akdeniz Üniversitesi, Işletme Enformatiǧi Bölümü, Antalya, Turkey; Boga M., Gida Işleme Bölümü, Niǧde Ömer Halisdemir Üniversitesi, Niǧde, Turkey","The most important indicator of whether animals 'needs are met in livestock enterprises is the animals' body condition score (BCS) score. In dairy cattle BCS is based on scoring from 1 to 5 according to the external appearance of the animals. BCS is a subjective method based on visual or palpation method to determine the relationship between subcutaneous fat thickness and bone protrusions in pelvic region in back, waist and coccyx regions in cattle. Generally, BCS values in the enterprises are determined by a method based on expert knowledge and determined by observation. If the animal is above or below the desired BCS, at this stage, diseases resulting from metabolic problems, low yield or animal losses may be observed. With the regular control of this situation, the profitability of the enterprise may increase with the production of more health animals. For this purpose, it was aimed to determine the BCS score with a computer-Aided software. Images from cattle were arranged in specific forms and classified by Convolutional Neural Networks (CNN). Of the 180 images, 75% were used for training and 25% for testing. In this study, system performance was increased by using pre-Trained CNN architectures and the responses of different architectures to BCS classification problem were tested. As a result, it was seen that BCS scoring can be done more than 60% successfully by using CNN methods. © 2019 IEEE.","Body Condition Score; Convolutional Neural Networks; Deep Learning; Feeding Animal","Agriculture; Convolution; Deep learning; Deep neural networks; Intelligent systems; Network architecture; Neural networks; Body condition score; Computer aided; Convolutional neural network; Dairy cattles; Expert knowledge; Low-yield; Subcutaneous fat thickness; Subjective methods; Animals","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-172812868-9","","","Turkish","Proc. - Innov. Intell. Syst. Appl. Conf., ASYU","Conference paper","Final","","Scopus","2-s2.0-85078328389"
"Alameer A.; Kyriazakis I.; Bacardit J.","Alameer, Ali (57190429643); Kyriazakis, Ilias (7006474425); Bacardit, Jaume (12242294300)","57190429643; 7006474425; 12242294300","Automated recognition of postures and drinking behaviour for the detection of compromised health in pigs","2020","Scientific Reports","10","1","13665","","","","76","10.1038/s41598-020-70688-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089371652&doi=10.1038%2fs41598-020-70688-6&partnerID=40&md5=885a633f993b3205f5603a564e02b215","School of Natural and Environmental Sciences, Newcastle University, Newcastle Upon Tyne, NE1 7RU, United Kingdom; School of Computing, Newcastle University, Newcastle Upon Tyne, NE4 5TG, United Kingdom; Institute for Global Food Security, Queen’s University, Belfast, BT9 5DL, United Kingdom","Alameer A., School of Natural and Environmental Sciences, Newcastle University, Newcastle Upon Tyne, NE1 7RU, United Kingdom, School of Computing, Newcastle University, Newcastle Upon Tyne, NE4 5TG, United Kingdom; Kyriazakis I., Institute for Global Food Security, Queen’s University, Belfast, BT9 5DL, United Kingdom; Bacardit J., School of Computing, Newcastle University, Newcastle Upon Tyne, NE4 5TG, United Kingdom","Changes in pig behaviours are a useful aid in detecting early signs of compromised health and welfare. In commercial settings, automatic detection of pig behaviours through visual imaging remains a challenge due to farm demanding conditions, e.g., occlusion of one pig from another. Here, two deep learning-based detector methods were developed to identify pig postures and drinking behaviours of group-housed pigs. We first tested the system ability to detect changes in these measures at group-level during routine management. We then demonstrated the ability of our automated methods to identify behaviours of individual animals with a mean average precision of 0.989 ± 0.009 , under a variety of settings. When the pig feeding regime was disrupted, we automatically detected the expected deviations from the daily feeding routine in standing, lateral lying and drinking behaviours. These experiments demonstrate that the method is capable of robustly and accurately monitoring individual pig behaviours under commercial conditions, without the need for additional sensors or individual pig identification, hence providing a scalable technology to improve the health and well-being of farm animals. The method has the potential to transform how livestock are monitored and address issues in livestock farming, such as targeted treatment of individuals with medication. © 2020, The Author(s).","","Animals; Behavior, Animal; Drinking Behavior; Health Behavior; Immunocompromised Host; Posture; Swine; animal; animal behavior; body position; drinking behavior; health behavior; immunocompromised patient; physiology; pig","Harbro Nutrition Ltd.; European Commission, EC; Innovent UK Ltd.; RAFT solutions Ltd; Zoetis Inc.; Horizon 2020; UK Research and Innovation, UKRI; Biotechnology and Biological Sciences Research Council, BBSRC, (BB/M011364/1); Horizon 2020 Framework Programme, H2020, (773436)","The animal trial was supported by the Biotechnology and Biological Sciences Research Council, UK, through an Agri-tech Research grant (BB/M011364/1) in conjunction with Zoetis Inc., Harbro Nutrition Ltd., Innovent UK Ltd., and RAFT solutions Ltd. This work was conducted under the Healthylivestock project. Healthylive-stock received funding from the European Commission under the European Union Framework Programme for Research and Innovation Horizon 2020 under Grant Agreement No: 773436. We are grateful to Amy Miller for advice on the interpretation of animal behaviours.","A. Alameer; School of Computing, Newcastle University, Newcastle Upon Tyne, NE4 5TG, United Kingdom; email: Ali.Alameer@Newcastle.ac.uk","","Nature Research","20452322","","","32788633","English","Sci. Rep.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089371652"
"Wang X.; Cheng X.; Chen Z.; Xu F.","Wang, Xi (57208310051); Cheng, XiaoDong (36454703300); Chen, ZheFu (57221283762); Xu, Fei (57221286903)","57208310051; 36454703300; 57221283762; 57221286903","A Method for Individual Identification of Dairy Cows Based on Deep Learning","2020","ACM International Conference Proceeding Series","","","3439079","186","191","5","5","10.1145/3438872.3439079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098735818&doi=10.1145%2f3438872.3439079&partnerID=40&md5=16743fe4ea06785ed0ae92fe53a0eb4e","College of Electronic, Information Engineering, Inner Mongolia University, Hohhot, China","Wang X., College of Electronic, Information Engineering, Inner Mongolia University, Hohhot, China; Cheng X., College of Electronic, Information Engineering, Inner Mongolia University, Hohhot, China; Chen Z., College of Electronic, Information Engineering, Inner Mongolia University, Hohhot, China; Xu F., College of Electronic, Information Engineering, Inner Mongolia University, Hohhot, China","Individual animal identification is an essential part of the large-scale, informatized, and refined development of animal husbandry, because it can trace the milk source and meat products of each cow, and it can also promote the smooth development of beef cattle insurance policies. Deep learning technologies are used to help identify individual livestock. These technologies can help staff avoid too intensive labor and time-consuming, worry-free and accurate identification of animals. This research proposes a robust deep learning method to classify 5 Holstein cows in the same cowshed. In order to prove this concept, the 48-hour activity data of 5 cows collected by a collar equipped with an activity collector were combined to create a data set. After feature extraction of the data, a deep neural network model based on the Keras framework was selected. The best result of the deep neural network reached 93.81% precision, 93.49% recall and 93.45% F1-score. The results show that the method has high precision, recall and F1 score, and can classify and identify different cows. © 2020 ACM.","Activity data; Animal husbandry; Cattle identification; Deep learning","Agriculture; Animals; Deep neural networks; Insurance; Learning systems; Neural networks; Robotics; Animal husbandry; Animal identification; High-precision; Individual identification; Insurance policies; Learning methods; Learning technology; Neural network model; Deep learning","Hohhot City Applied Technology Research and Development Project, (2019-HT01); Natural Science Foundation of Inner Mongolia, (2019 MS06011)","This research was supported by the Natural Science Foundation of Inner Mongolia Autonomous Region(2019 MS06011) and Hohhot City Applied Technology Research and Development Project(2019-HT01).","","","Association for Computing Machinery","","978-145038830-6","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85098735818"
"Psota E.T.; Mittek M.; Pérez L.C.; Schmidt T.; Mote B.","Psota, Eric T. (24315220900); Mittek, Mateusz (55960148500); Pérez, Lance C. (55343909000); Schmidt, Ty (7402839670); Mote, Benny (13807961600)","24315220900; 55960148500; 55343909000; 7402839670; 13807961600","Multi-pig part detection and association with a fully-convolutional network","2019","Sensors (Switzerland)","19","4","852","","","","91","10.3390/s19040852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061992275&doi=10.3390%2fs19040852&partnerID=40&md5=319f182a5b5a205914fc1c38c304957b","Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Lincoln, 68505, NE, United States; Department of Animal Science, University of Nebraska–Lincoln, Lincoln, 68588, NE, United States","Psota E.T., Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Lincoln, 68505, NE, United States; Mittek M., Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Lincoln, 68505, NE, United States; Pérez L.C., Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Lincoln, 68505, NE, United States; Schmidt T., Department of Animal Science, University of Nebraska–Lincoln, Lincoln, 68588, NE, United States; Mote B., Department of Animal Science, University of Nebraska–Lincoln, Lincoln, 68588, NE, United States","Computer vision systems have the potential to provide automated, non-invasive monitoring of livestock animals, however, the lack of public datasets with well-defined targets and evaluation metrics presents a significant challenge for researchers. Consequently, existing solutions often focus on achieving task-specific objectives using relatively small, private datasets. This work introduces a new dataset and method for instance-level detection of multiple pigs in group-housed environments. The method uses a single fully-convolutional neural network to detect the location and orientation of each animal, where both body part locations and pairwise associations are represented in the image space. Accompanying this method is a new dataset containing 2000 annotated images with 24,842 individually annotated pigs from 17 different locations. The proposed method achieves over 99% precision and over 96% recall when detecting pigs in environments previously seen by the network during training. To evaluate the robustness of the trained network, it is also tested on environments and lighting conditions unseen in the training set, where it achieves 91% precision and 67% recall. The dataset is publicly available for download. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Animal detection; Computer vision; Deep learning; Image processing; Pose estimation; Precision livestock","Agriculture; Computer vision; Convolution; Deep learning; Image processing; Location; Neural networks; Petroleum reservoir evaluation; Computer vision system; Convolutional networks; Convolutional neural network; Evaluation metrics; Lighting conditions; Non-invasive monitoring; Pose estimation; Precision livestock; Mammals","NPB, (16-122); National Pork Board, NPB, (NPB #16-122); National Pork Board, NPB","Funding text 1: This research was supported by the National Pork Board (NPB #16-122).; Funding text 2: Funding: This research was supported by the National Pork Board (NPB #16-122).","E.T. Psota; Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Lincoln, 68505, United States; email: epsota@unl.edu","","MDPI AG","14248220","","","30791377","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85061992275"
"Måløy H.; Aamodt A.; Misimi E.","Måløy, Håkon (57211923495); Aamodt, Agnar (55391786000); Misimi, Ekrem (15846457300)","57211923495; 55391786000; 15846457300","A spatio-temporal recurrent network for salmon feeding action recognition from underwater videos in aquaculture","2019","Computers and Electronics in Agriculture","167","","105087","","","","107","10.1016/j.compag.2019.105087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075391331&doi=10.1016%2fj.compag.2019.105087&partnerID=40&md5=edf3ee65854c2d7b0b9835e67a67edb3","NTNU, Department of Computer Science, Sem Sælandsvei 9, Trondheim, Norway; SINTEF Ocean, Brattørkaia 17C, Trondheim, 7010, Norway","Måløy H., NTNU, Department of Computer Science, Sem Sælandsvei 9, Trondheim, Norway; Aamodt A., NTNU, Department of Computer Science, Sem Sælandsvei 9, Trondheim, Norway; Misimi E., SINTEF Ocean, Brattørkaia 17C, Trondheim, 7010, Norway","Recent developments have shown that Deep Learning approaches are well suited for Human Action Recognition. On the other hand, the application of deep learning for action or behaviour recognition in other domains such as animal or livestock is comparatively limited. Action recognition in fish is a particularly challenging task due to specific research challenges such as the lack of distinct poses in fish behavior and the capture of spatio-temporal changes. Action recognition of salmon is valuable in relation to managing and optimizing many aquaculture operations today such as feeding, as one of the most costly operations in aquaculture. Inspired by these application domains and research challenges we introduce a deep video classification network for action recognition of salmon from underwater videos. We propose a Dual-Stream Recurrent Network (DSRN) to automatically capture the spatio-temporal behavior of salmon during swimming. The DSRN combines the spatial and motion-temporal information through the use of a spatial network, a 3D-convolutional motion network and a LSTM recurrent classification network. The DSRN shows an accuracy that is suitable for industrial use in prediction of salmon behavior with a prediction accuracy of 80%, validated on the task of predicting Feeding and NonFeeding behavior in salmon at a real fish farm during production. Our results show that the DSRN architecture has high potential in feeding action recognition for salmon in aquaculture and for applications domains lacking distinct poses and with dynamic spatio-temporal changes. © 2019 The Authors","Action recognition; Aquaculture; Convolutional neural network; Fish action/behaviour recognition; Fish feeding; Optical flow; Recurrent neural network; Video analysis","Animalia; Agriculture; Aquaculture; Behavioral research; Classification (of information); Convolution; Deep learning; Feeding; Fish; Forecasting; Optical flows; Recurrent neural networks; Action recognition; Aquaculture operations; Classification networks; Convolutional neural network; Human-action recognition; Spatio-temporal changes; Spatiotemporal behaviors; Video analysis; aquaculture industry; aquaculture production; aquaculture system; artificial neural network; feeding behavior; fish; fish culture; livestock; optical property; salmonid; salmonid culture; spatiotemporal analysis; swimming behavior; videography; Long short-term memory","","","H. Måløy; NTNU, Department of Computer Science, Trondheim, Sem Sælandsvei 9, Norway; email: hakon.maloy@ntnu.no","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85075391331"
"Ghosh P.; Mustafi S.; Mukherjee K.; Dan S.; Roy K.; Mandal S.N.; Banik S.","Ghosh, Pritam (57218283838); Mustafi, Subhranil (57218283970); Mukherjee, Kaushik (57218282685); Dan, Sanket (57218279239); Roy, Kunal (57218283233); Mandal, Satyendra Nath (26423827600); Banik, Santanu (14819215200)","57218283838; 57218283970; 57218282685; 57218279239; 57218283233; 26423827600; 14819215200","Image-Based Identification of Animal Breeds Using Deep Learning","2021","Studies in Computational Intelligence","984","","","415","445","30","3","10.1007/978-3-030-77939-9_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116742907&doi=10.1007%2f978-3-030-77939-9_12&partnerID=40&md5=c8b4383fec87da66e302a615bd1edf46","Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; ICAR-National Research Centre on Pig, Rani, Guwahati, Assam, 781131, India","Ghosh P., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Mustafi S., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Mukherjee K., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Dan S., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Roy K., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Mandal S.N., Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; Banik S., ICAR-National Research Centre on Pig, Rani, Guwahati, Assam, 781131, India","Accurate and reliable breed identification of domestic animals from images is one of the most promising but challenging tasks in intelligent livestock management. Traditional methods for animal breed identification are very costly and time consuming. Therefore, there is a need for a faster and cheaper technique for animal breed identification, which can be used by anyone without much technical knowhow. Deep Learning based animal breed classification from images can be used to solve this problem. Recent developments in deep Convolutional Neural Network (CNN) has drastically improved the accuracy of image recognition systems, but choosing the optimal model for the required task is very important for best performance. In this study, the performance of nine different deep CNN-based models have been analyzed to find the optimal model which can precisely determine the breed identity of individual animals from its image. All nine CNN models have been separately trained end-to-end on Pig Breed Dataset and Goat Breed Dataset using a set of identical hyperparameters. From the results obtained it has been established that MobileNetV2 is the best deep-CNN model for Goat Breed Classification with 95.00% prediction accuracy and InceptionV3 is the best model for pig breed classification with 100.00% prediction accuracy. Breed classification performance of goat and pig obtained in this study have been compared with other techniques used for animal breed classification. Comparison results show that our CNN-based technique has performed on par with all other methods. With these encouraging results, it can be confidently stated that deep CNN-based models can be used for solving the animal breed classification problem with high accuracy and can be used as ready to use technology for intelligent livestock management. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Animal breed classification; Convolutional neural network; Deep learning; Intelligent livestock management; Performance evaluation; Supervised learning","","Digital India Corporation; ITRA; ITRA Ag & Food; Uttar Banga Krishi Viswa Vidyalaya; Indian Council of Agricultural Research, ICAR; Ministry of Electronics and Information technology, Meity; ICAR-Indian Veterinary Research Institute, ICAR-IVRI","Funding text 1: The authors would like to thank ITRA (Digital India Corporation, formerly Medialab Asia), MeitY, Govt. of India, for funding. The authors are grateful to Dr. Amitabha Bandyopadhyay (Senior Consultant, ITRA Ag & Food) and Dr. Dilip Kumar Hazra (Assistant Professor, Department of Agronomy, Uttar Banga Krishi Viswa Vidyalaya) for constant encouragement, constructive criticism, and scientific input. The authors gratefully acknowledge the unconditional support from the Director, ICAR-CIRG Makhdoom, UP; the Joint Director of ICAR-IVRI Eastern Regional Station, Kolkata; ICAR National Research Centre on Pig, Rani, Assam; ICAR Research Complex for NEH Region, Umiam, Meghalaya and ICAR Research Complex for NEH Region, Tripura Centre, Tripura for permitting us to access their organized pig farms. The authors are also thankful to Dr. Sourabh Kumar Das (Principal, Kalyani Government Engineering College) for his continuous support.; Funding text 2: Acknowledgements The authors would like to thank ITRA (Digital India Corporation, formerly Medialab Asia), MeitY, Govt. of India, for funding. The authors are grateful to Dr. Amitabha Bandyopadhyay (Senior Consultant, ITRA Ag & Food) and Dr. Dilip Kumar Hazra (Assistant Professor, Department of Agronomy, Uttar Banga Krishi Viswa Vidyalaya) for constant encouragement, constructive criticism, and scientific input. The authors gratefully acknowledge the unconditional support from the Director, ICAR-CIRG Makhdoom, UP; the Joint Director of ICAR-IVRI Eastern Regional Station, Kolkata; ICAR National Research Centre on Pig, Rani, Assam; ICAR Research Complex for NEH Region, Umiam, Meghalaya and ICAR Research Complex for NEH Region, Tripura Centre, Tripura for permitting us to access their organized pig farms. The authors are also thankful to Dr. Sourabh Kumar Das (Principal, Kalyani Government Engineering College) for his continuous support.","P. Ghosh; Department of Information Technology, Kalyani Government Engineering College, Kalyani, Nadia, 741235, India; email: ghoshpritam25@gmail.com","","Springer Science and Business Media Deutschland GmbH","1860949X","","","","English","Stud. Comput. Intell.","Book chapter","Final","","Scopus","2-s2.0-85116742907"
"Park J.-S.; Kim S.-C.","Park, Jin-Seong (57377244000); Kim, Sang-Cheol (55561975800)","57377244000; 55561975800","Deep learning-based estimation of observation error area and recognition of objects hidden by obstacles for growth information collection in smart farms","2021","Journal of Institute of Control, Robotics and Systems","27","12","","984","990","6","1","10.5302/J.ICROS.2021.21.0147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121437749&doi=10.5302%2fJ.ICROS.2021.21.0147&partnerID=40&md5=9b9d3a672703a33bbdbcf75b56552b02","Division of Electronics and Information Engineering, Chonbuk National University, South Korea; Intelligent Robots Research Center, Chonbuk National University, South Korea","Park J.-S., Division of Electronics and Information Engineering, Chonbuk National University, South Korea, Intelligent Robots Research Center, Chonbuk National University, South Korea; Kim S.-C., Intelligent Robots Research Center, Chonbuk National University, South Korea","Growth information in smart farms is essential to optimize the farm environment for crops and livestock. In this study, we verified the performance of a system for the collection of paprika growth information using semantic segmentation. Although most current deep learning methods recognize objects along the outline, this study investigated the recognition of objects hidden by obstacles during the growth investigation process. In addition, the ability of the artificial intelligence method to recognize only the stems necessary for growth investigation among several stems of paprika was investigated. Furthermore, the application of the deep learning method to areas where errors may occur depending on the observer was examined. The experimental results revealed that the deep learning models recognized and estimated objects covered by obstacles, and only objects necessary for growth investigation were recognized within a certain error. For practical use, an optimized model was proposed, and the model exhibited an mIoU of 0.7369. © ICROS 2021.","Observation error; Occlusion inference; Paprika; Semantic segmentation; Smart farm","Agriculture; Deep learning; Errors; Semantic Segmentation; Farm environments; Growth information; Information collections; Learning methods; Observation errors; Occlusion inference; Paprika; Recognition of objects; Semantic segmentation; Smart farm; Semantics","","","S.-C. Kim; Intelligent Robots Research Center, Chonbuk National University, South Korea; email: sckim7777@jbnu.ac.kr","","Institute of Control, Robotics and Systems","19765622","","","","Korean","J. Inst. Control Rob. Syst.","Article","Final","","Scopus","2-s2.0-85121437749"
"Yukun S.; Pengju H.; Yujie W.; Ziqi C.; Yang L.; Baisheng D.; Runze L.; Yonggen Z.","Yukun, Sun (8833976300); Pengju, Huo (57210949262); Yujie, Wang (59158432100); Ziqi, Cui (57210947876); Yang, Li (57872816500); Baisheng, Dai (57225754325); Runze, Li (57211262144); Yonggen, Zhang (7601315132)","8833976300; 57210949262; 59158432100; 57210947876; 57872816500; 57225754325; 57211262144; 7601315132","Automatic monitoring system for individual dairy cows based on a deep learning framework that provides identification via body parts and estimation of body condition score","2019","Journal of Dairy Science","102","11","","10140","10151","11","68","10.3168/jds.2018-16164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071967459&doi=10.3168%2fjds.2018-16164&partnerID=40&md5=2a7532adb4c0de033bb1b70c2af49237","College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China","Yukun S., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; Pengju H., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; Yujie W., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; Ziqi C., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; Yang L., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; Baisheng D., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China, Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs, Yangling, 712100, China; Runze L., College of Electrical Engineering and Information, Northeast Agricultural University, Harbin, 150030, China; Yonggen Z., College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China","Body condition score (BCS) is a common tool for indirectly estimating the mobilization of energy reserves in the fat and muscle of cattle that meets the requirements of animal welfare and precision livestock farming for the effective monitoring of individual animals. However, previous studies on automatic BCS systems have used manual scoring for data collection, and traditional image extraction methods have limited model performance accuracy. In addition, the radio frequency identification device system commonly used in ranching has the disadvantages of misreadings and damage to bovine bodies. Therefore, the aim of this research was to develop and validate an automatic system for identifying individuals and assessing BCS using a deep learning framework. This work developed a linear regression model of BCS using ultrasound backfat thickness to determine BCS for training sets and tested a system based on convolutional neural networks with 3 channels, including depth, gray, and phase congruency, to analyze the back images of 686 cows. After we performed an analysis of image model performance, online verification was used to evaluate the accuracy and precision of the system. The results showed that the selected linear regression model had a high coefficient of determination value (0.976), and the correlation coefficient between manual BCS and ultrasonic BCS was 0.94. Although the overall accuracy of the BCS estimations was high (0.45, 0.77, and 0.98 within 0, 0.25, and 0.5 unit, respectively), the validation for actual BCS ranging from 3.25 to 3.5 was weak (the F1 scores were only 0.6 and 0.57, respectively, within the 0.25-unit range). Overall, individual identification and BCS assessment performed well in the online measurement, with accuracies of 0.937 and 0.409, respectively. A system for individual identification and BCS assessment was developed, and a convolutional neural network using depth, gray, and phase congruency channels to interpret image features exhibited advantages for monitoring thin cows. © 2019 American Dairy Science Association","backfat thickness; body condition score; convolutional neural network; individual identification","Animal Welfare; Animals; Body Composition; Cattle; Dairying; Deep Learning; Female; Lactation; Linear Models; Ultrasonography; animal; animal welfare; body composition; bovine; dairying; echography; female; lactation; physiology; statistical model; veterinary medicine","Key Laboratory of Agricultural Internet of Things; Nestle dairy cow training center; Postdoctoral Foundation in Heilongjiang Province, (LBH-Z17035); Young Talents Project of the Northeast Agricultural University; Heilongjiang Postdoctoral Science Foundation; China Postdoctoral Science Foundation, (2019M651252); China Postdoctoral Science Foundation; Ministry of Agriculture, Food and Rural Affairs, MAFRA, (2018AIOT-02); Ministry of Agriculture, Food and Rural Affairs, MAFRA; Agriculture Research System of China, (CARS-36); Agriculture Research System of China; Ministry of Agriculture and Rural Affairs of the People's Republic of China, MOA; “Young Talents” Project of Northeast Agricultural University, (18QC35); “Young Talents” Project of Northeast Agricultural University","This study was financially supported by the China Agriculture Research System (Beijing, China, CARS-36), the Young Talents Project of the Northeast Agricultural University (Harbin, China, 18QC35), the China Postdoctoral Science Foundation Grant for the 65nd batch (Beijing, China, 2019M651252), and the Postdoctoral Foundation in Heilongjiang Province (Harbin, China, LBH-Z17035), the Key Laboratory of Agricultural Internet of Things, Ministry of Agriculture and Rural Affairs under a grant (Beijing, China, 2018AIOT-02). The authors also thank the Nestle dairy cow training center (Harbin, China) for its support.","Z. Yonggen; College of Animal Sciences and Technology, Northeast Agriculture University, Harbin, 150030, China; email: zhangyonggen@sina.com","","Elsevier Inc.","00220302","","","31521348","English","J. Dairy Sci.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85071967459"
"Achour B.; Belkadi M.; Filali I.; Laghrouche M.; Lahdir M.","Achour, Brahim (57211492688); Belkadi, Malika (56039932100); Filali, Idir (55601298700); Laghrouche, Mourad (8629285500); Lahdir, Mourad (54393294300)","57211492688; 56039932100; 55601298700; 8629285500; 54393294300","Image analysis for individual identification and feeding behaviour monitoring of dairy cows based on Convolutional Neural Networks (CNN)","2020","Biosystems Engineering","198","","","31","49","18","105","10.1016/j.biosystemseng.2020.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089190213&doi=10.1016%2fj.biosystemseng.2020.07.019&partnerID=40&md5=e6d6aa8a5c4691a42f4119db76bab60d","LARI Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; LAMPA Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria","Achour B., LARI Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; Belkadi M., LARI Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; Filali I., LARI Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; Laghrouche M., LAMPA Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; Lahdir M., LAMPA Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria","In precision livestock farming, individual identification and analysis of feeding behaviour have a great impact on optimising the productivity and improving health monitoring. The sensors usually used to measure several parameters from an individual dairy cow (RFID, Accelerometer, etc.) are invasive, uncomfortable and stressful for animals. To overcome these limits, we have developed a non-invasive system based entirely on image analysis. The top of the dairy cow's head image, captured in a dairy cow farm, was used as a Region of Interest (ROI) and different classifiers based on Convolutional Neural Network (CNN) model are used to monitor the feeding behaviour and perform individual identification of seventeen Holstein dairy cows. We use one CNN to detect the dairy cow presence in the feeder zone. A second CNN is used to determine the dairy cow position in front of the feeder, standing or feeding. A third CNN is used to check the availability of food in the feeder and if so recognise the food category. The last CNN is devoted to individual identification of the dairy cow. Furthermore, we also explore the contribution of a CNN coupled to Support Vector Machine (SVM) and the combination of multiple CNNs in the individual identification process. For the evaluation step, we have used a dataset composed of 7265 images of 17 Holstein dairy cows during feeding periods on a commercial farm. Results show that our method yields high scores in each step of our algorithm. © 2020 IAgrE","Convolutional neural network; Dairy cow; Dairy cow identification; Deep learning; Feeding behaviour; Precision livestock","Agriculture; Convolution; Feeding; Image analysis; Image segmentation; Support vector machines; Behaviour monitoring; Commercial farms; Dairy cow; Feeding period; Health monitoring; Individual identification; Precision livestock farming; Region of interest; Convolutional neural networks","","","M. Belkadi; LARI Laboratory, Mouloud Mammeri University of Tizi-Ouzou, Algeria; email: belkadi_dz@yahoo.fr","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85089190213"
"Jung D.-H.; Kim N.Y.; Moon S.H.; Jhin C.; Kim H.-J.; Yang J.-S.; Kim H.S.; Lee T.S.; Lee J.Y.; Park S.H.","Jung, Dae-Hyun (55823060200); Kim, Na Yeon (56315305100); Moon, Sang Ho (8333204200); Jhin, Changho (55360507500); Kim, Hak-Jin (57191721745); Yang, Jung-Seok (9333318900); Kim, Hyoung Seok (57193389559); Lee, Taek Sung (7501441271); Lee, Ju Young (57206731846); Park, Soo Hyun (57205063566)","55823060200; 56315305100; 8333204200; 55360507500; 57191721745; 9333318900; 57193389559; 7501441271; 57206731846; 57205063566","Deep learning-based cattle vocal classification model and real-time livestock monitoring system with noise filtering","2021","Animals","11","2","357","1","16","15","54","10.3390/ani11020357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100108233&doi=10.3390%2fani11020357&partnerID=40&md5=e807fbb23b9261b2035be70c1705917a","Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea; Asia Pacific Ruminant Institute, Icheon, 17385, South Korea; Department of Smartfarm Research, 1778 Living Tech, Sejong, 30033, South Korea; Department of Biosystems and Biomaterial Engineering, College of Agriculture and Life Sciences, Seoul National University, Seoul, 08826, South Korea","Jung D.-H., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Kim N.Y., Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea, Asia Pacific Ruminant Institute, Icheon, 17385, South Korea; Moon S.H., Department of Bio-Convergence Science, College of Biomedical and Health Science, Konkuk University, Chungju, 27478, South Korea; Jhin C., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea, Department of Smartfarm Research, 1778 Living Tech, Sejong, 30033, South Korea; Kim H.-J., Department of Biosystems and Biomaterial Engineering, College of Agriculture and Life Sciences, Seoul National University, Seoul, 08826, South Korea; Yang J.-S., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Kim H.S., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Lee T.S., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Lee J.Y., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; Park S.H., Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea","The priority placed on animal welfare in the meat industry is increasing the importance of understanding livestock behavior. In this study, we developed a web-based monitoring and recording system based on artificial intelligence analysis for the classification of cattle sounds. The deep learning classification model of the system is a convolutional neural network (CNN) model that takes voice information converted to Mel-frequency cepstral coefficients (MFCCs) as input. The CNN model first achieved an accuracy of 91.38% in recognizing cattle sounds. Further, short-time Fourier transform-based noise filtering was applied to remove background noise, improving the classification model accuracy to 94.18%. Categorized cattle voices were then classified into four classes, and a total of 897 classification records were acquired for the classification model development. A final accuracy of 81.96% was obtained for the model. Our proposed web-based platform that provides information obtained from a total of 12 sound sensors provides cattle vocalization monitoring in real time, enabling farm owners to determine the status of their cattle. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cattle vocalization; Convolutional neural network; MFCC; Sound classification","animal experiment; animal welfare; Article; artificial intelligence; bovine; convolutional neural network; dairy cattle; data base; deep learning; electroencephalography; filtration; food industry; Fourier transform; intensive care unit; livestock; lung cancer; mathematical analysis; monitoring; noise; nonhuman; short time Fourier transform; vocalization; voice","National Institute of Agricultural Sciences; Research Program for Agricultural Science and Technology Development, (PJ01389103); Rural Development Administration, RDA","Funding: This study was supported by the Research Program for Agricultural Science and Technology Development (Project No. PJ01389103), National Institute of Agricultural Sciences, Rural Development Administration, Korea.","S.H. Park; Smart Farm Research Center, Korea Institute of Science and Technology (KIST), Gangneung, 25451, South Korea; email: ecoloves@kist.re.kr","","MDPI","20762615","","","","English","Animals","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85100108233"
"Fang P.; Hao H.; Li T.; Wang H.","Fang, Peng (57190016010); Hao, Hongyun (57223212527); Li, Tengfei (55541375000); Wang, Hongying (55688840900)","57190016010; 57223212527; 55541375000; 55688840900","Instance Segmentation of Broiler Image Based on Attention Mechanism and Deformable Convolution; [基于注意力机制和可变形卷积的鸡只图像实例分割提取]","2021","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","52","4","","257","265","8","14","10.6041/j.issn.1000-1298.2021.04.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105356467&doi=10.6041%2fj.issn.1000-1298.2021.04.027&partnerID=40&md5=c1b82d766e9cb8145c0b6310b8b63279","College of Engineering, China Agricultural University, Beijing, 100083, China; Beijing 3D Printing Research Institute, Beijing City University, Beijing, 100083, China","Fang P., College of Engineering, China Agricultural University, Beijing, 100083, China; Hao H., College of Engineering, China Agricultural University, Beijing, 100083, China; Li T., Beijing 3D Printing Research Institute, Beijing City University, Beijing, 100083, China; Wang H., College of Engineering, China Agricultural University, Beijing, 100083, China","Segmentation and extraction of birds contour is the premise of precision livestock farming management, such as behavior, health, welfare status monitoring based on machine vision technology. The precision and accuracy of image segmentation directly affect the reliability of relevant monitoring technology and decision-making. An instance segmentation approach based on Mask R-CNN deep learning framework was proposed to solve broiler instance segmentation and contour extraction problems in stacked-cage henhouse. Furthermore, a broiler image segmentation and contour extraction network was constructed to segment broiler images and realize birds individual contour extraction. In this network, totally 41 layers deep residual network (ResNet) based on attention mechanism and deformable convolution was integrated with feature pyramid networks (FPN) as the backbone network to extract the image features, and regions of interest were extracted by region proposal networks. Finally, target classification, segmentation and box regression were realized through network heads. Broiler image segmentation experiment showed that compared with Mask R-CNN network, the average precision and mean accuracy of the optimized network were improved from 78.23% and 84.48% to 88.60% and 90.37%, respectively, and the recall rate of the model was 77.48%, which can realize the pixel level segmentation of chicken contour. The research result can provide technical support for the real-time monitoring of birds welfare and health status. © 2021, Chinese Society of Agricultural Machinery. All right reserved.","Attention mechanism; Broiler; Contour extraction; Deformable convolution neural network; Instance segmentation","Agriculture; Birds; Convolution; Convolutional neural networks; Decision making; Deep learning; Deformation; Extraction; Image enhancement; Network layers; Attention mechanisms; Contour Extraction; Learning frameworks; Monitoring technologies; Precision livestock farming; Real time monitoring; Regions of interest; Target Classification; Image segmentation","","","H. Wang; College of Engineering, China Agricultural University, Beijing, 100083, China; email: hongyingw@cau.edu.cn","","Chinese Society of Agricultural Machinery","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85105356467"
"Witte J.-H.; Gerberding J.; Melching C.; Gómez J.M.","Witte, Jan-Hendrik (57667657200); Gerberding, Johann (57667319900); Melching, Christian (57200447409); Gómez, Jorge Marx (7402100609)","57667657200; 57667319900; 57200447409; 7402100609","Evaluation of Deep Learning Instance Segmentation models for Pig Precision Livestock Farming","2021","Business Information Systems","1","","","209","220","11","9","10.52825/bis.v1i.59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129557774&doi=10.52825%2fbis.v1i.59&partnerID=40&md5=2c0cb0971bb11001b13c5f7152f6a4d0","Universität Oldenburg, Germany","Witte J.-H., Universität Oldenburg, Germany; Gerberding J., Universität Oldenburg, Germany; Melching C., Universität Oldenburg, Germany; Gómez J.M., Universität Oldenburg, Germany","In this paper, the deep learning instance segmentation architectures DetectoRS, SOLOv2, DETR and Mask R-CNN were applied to data from the field of Pig Precision Livestock Farming to investigate whether these models can address the specific challenges of this domain. For this purpose, we created a custom dataset consisting of 731 images with high heterogeneity and high-quality segmentation masks. For evaluation, the standard metric for benchmarking instance segmentation models in computer vision, the mean average precision, was used. The results show that all tested models can be applied to the considered domain in terms of prediction accuracy. With a mAP of 0.848, DetectoRS achieves the best results on the test set, but is also the largest model with the greatest hardware requirements. It turns out that increasing model complexity and size does not have a large impact on prediction accuracy for instance segmentation of pigs. DETR, SOLOv2, and Mask R-CNN achieve similar results to DetectoRS with a parameter count almost three times smaller. Visual evaluation of predictions shows quality differences in terms of accuracy of segmentation masks. DetectoRS generates the best masks overall, while DETR has advantages in correctly segmenting the tail region. However, it can be observed that each of the tested models has problems in assigning segmentation masks correctly once a pig is overlapped. The results demonstrate the potential of deep learning instance segmentation models in Pig Precision Livestock Farming and lay the foundation for future research in this area. © Authors.","Computer Vision; Deep Learning; Instance Segmentation; Pig; Precision Livestock Farming","","","","","Abramowicz W.; Lewanska E.; Auer S.","Technische Informationsbibliothek (TIB)","27479986","","","","English","Bus. Inf. Sys.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85129557774"
"Guo Y.; Qiao Y.; Sukkarieh S.; Chai L.; He D.","Guo, Yangyang (57200132879); Qiao, Yongliang (56486770900); Sukkarieh, Salah (6602844626); Chai, Lilong (57222280822); He, Dongjian (19933691800)","57200132879; 56486770900; 6602844626; 57222280822; 19933691800","BIGRU-ATTENTION BASED COW BEHAVIOR CLASSIFICATION USING VIDEO DATA FOR PRECISION LIVESTOCK FARMING","2021","Transactions of the ASABE","64","6","","1823","1833","10","18","10.13031/trans.14658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117474887&doi=10.13031%2ftrans.14658&partnerID=40&md5=2d215dd94329af2f94a2b08180595a91","College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, China; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, NSW, Australia; Department of Poultry Science, University of Georgia, Athens, GA, United States","Guo Y., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, China, Department of Poultry Science, University of Georgia, Athens, GA, United States; Qiao Y., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, NSW, Australia; Sukkarieh S., Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, NSW, Australia; Chai L., Department of Poultry Science, University of Georgia, Athens, GA, United States; He D., College of Mechanical and Electronic Engineering, Northwest A&F University, Shaanxi, Yangling, China","Animal behavior consists of time series activities, which can reflect animals' health and welfare status. Monitoring and classifying animal behavior facilitates management decisions to optimize animal performance, welfare, and environmental outcomes. In recent years, deep learning methods have been applied to monitor animal behavior worldwide. To achieve high behavior classification accuracy, a BiGRU-attention based method is proposed in this article to classify some common behaviors, such as exploring, feeding, grooming, standing, and walking. In our work, (1) Inception-V3 was first applied to extract convolutional neural network (CNN) features for each image frame in videos, (2) bidirectional gated recurrent unit (BiGRU) was used to further extract spatial-temporal features, (3) an attention mechanism was deployed to allocate weights to each of the extracted spatial-temporal features according to feature similarity, and (4) the weighted spatial-temporal features were fed to a Softmax layer for behavior classification. Experiments were conducted on two datasets (i.e., calf and adult cow), and the proposed method achieved 82.35% and 82.26% classification accuracy on the calf and adult cow datasets, respectively. In addition, in comparison with other methods, the proposed BiGRU-attention method outperformed long short-term memory (LSTM), bidirectional LSTM (BiLSTM), and BiGRU. Overall, the proposed BiGRU-attention method can capture key spatial-temporal features to significantly improve animal behavior classification, which is favorable for automatic behavior classification in precision livestock farming. © 2021 American Society of Agricultural and Biological Engineers","BiGRU; Cow behavior; Deep learning; LSTM; Precision livestock farming","Agriculture; Animals; Classification (of information); Convolutional neural networks; Animal behaviour; Behaviour classification; Bidirectional gated recurrent unit; Classification accuracy; Cow behavior; Deep learning; Precision livestock farming; Spatial-temporal features; Times series; Video data; behavior; livestock farming; precision; videography; Long short-term memory","National Natural Science Foundation of China, NSFC, (61473235); National Natural Science Foundation of China, NSFC; National Key Research and Development Program of China, NKRDPC, (2017YFD0701603); National Key Research and Development Program of China, NKRDPC","This work was supported by the general program from the National Natural Science Foundation of China (Grant No. 61473235) and the National Key Technology R&D Program of China (Grant No. 2017YFD0701603). The first two authors contributed equally to this work and share first authorship.","Y. Qiao; Australian Centre for Field Robotics (ACFR), Faculty of Engineering, University of Sydney, Australia; email: yongliang.qiao@sydney.edu.au; D. He; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, Shaanxi, China; email: hdj168@nwsuaf.edu.cn","","American Society of Agricultural and Biological Engineers","21510032","","","","English","Trans. ASABE","Article","Final","","Scopus","2-s2.0-85117474887"
"Chae J.-W.; Cho H.-C.","Chae, Jung-woo (57215821871); Cho, Hyun-chong (22233514800)","57215821871; 22233514800","Identifying the Mating Posture of Cattle Using Deep Learning-Based Object Detection with Networks of Various Settings","2021","Journal of Electrical Engineering and Technology","16","3","","1685","1692","7","11","10.1007/s42835-021-00701-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102335542&doi=10.1007%2fs42835-021-00701-z&partnerID=40&md5=b4b9f697432497a2a5a4b0e620829fcd","Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, 1 Kangwondaehak-gil, Chuncheon-si, 24341, Gangwon-do, South Korea; Department of Electronics Engineering, Kangwon National University, 1 Kangwondaehak-gil, Chuncheon-si, 24341, Gangwon-do, South Korea","Chae J.-W., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, 1 Kangwondaehak-gil, Chuncheon-si, 24341, Gangwon-do, South Korea; Cho H.-C., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, 1 Kangwondaehak-gil, Chuncheon-si, 24341, Gangwon-do, South Korea, Department of Electronics Engineering, Kangwon National University, 1 Kangwondaehak-gil, Chuncheon-si, 24341, Gangwon-do, South Korea","Estrus detection in cattle is an important factor in livestock farming. With timely estrus detection, cattle are artificially fertilized and isolated for safety, which directly affects the productivity of livestock farms. Estrus can be successfully detected by identifying the mating posture of cattle. Therefore, in this paper, we propose the identification of cattle mating posture based on video inputs for prompt estrus detection. A deep learning-based object detection network that focuses on real-time processing with high processing speeds is applied. The use of deep learning-based object detection shows high accuracy, even with noise robustness. The performance of the network is improved through the inclusion of an additional layer and a new activation function. The composition of the additional layer enables training by extracting more features required for object detection. The application of the new activation function, Mish, which has a smoother curve, allows for better generalization and improves the accuracy of the results. The data needed for training were gathered by installing cameras at a livestock farm, and various datasets were used depending on camera placement. The results of this study were verified by the evaluation of four networks using test datasets containing image and video data from different environments. The identification of the mating posture of cattle attained 98.5% precision, 97.2% recall, and 97.8% accuracy. © 2021, The Korean Institute of Electrical Engineers.","Cattle; Deep learning network; Estrus; Mating posture; Mish activation function; Object detection","Agriculture; Cameras; Chemical activation; Object detection; Object recognition; Camera placement; Detection networks; High-accuracy; Livestock farming; New activation functions; Noise robustness; Processing speed; Realtime processing; Deep learning","Ministry of Science, ICT and Future Planning, MSIP, (IITP-2020-2018-0-01433); National Research Foundation of Korea, NRF, (2017R1E1A1A03070297); Institute for Information and Communications Technology Promotion, IITP","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2017R1E1A1A03070297). This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2020-2018-0-01433) supervised by the IITP (Institute for Information & communications Technology Promotion). ","H.-C. Cho; Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, Chuncheon-si, 1 Kangwondaehak-gil, 24341, South Korea; email: hyuncho@kangwon.ac.kr","","Korean Institute of Electrical Engineers","19750102","","","","English","J. Electr. Eng. Technol.","Article","Final","","Scopus","2-s2.0-85102335542"
"Nasirahmadi A.; Gonzalez J.; Sturm B.; Hensel O.; Knierim U.","Nasirahmadi, Abozar (55842933900); Gonzalez, Jennifer (57216183450); Sturm, Barbara (54953383000); Hensel, Oliver (26429285600); Knierim, Ute (55959591600)","55842933900; 57216183450; 54953383000; 26429285600; 55959591600","Pecking activity detection in group-housed turkeys using acoustic data and a deep learning technique","2020","Biosystems Engineering","194","","","40","48","8","23","10.1016/j.biosystemseng.2020.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082777987&doi=10.1016%2fj.biosystemseng.2020.03.015&partnerID=40&md5=c7bfe1654156b904e92536637e3e9878","Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; Department of Farm Animal Behaviour and Husbandry, University of Kassel, Witzenhausen, 37213, Germany; School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, United Kingdom","Nasirahmadi A., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; Gonzalez J., Department of Farm Animal Behaviour and Husbandry, University of Kassel, Witzenhausen, 37213, Germany; Sturm B., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany, School of Natural and Environmental Sciences, Newcastle University, Newcastle upon Tyne, NE1 7RU, United Kingdom; Hensel O., Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; Knierim U., Department of Farm Animal Behaviour and Husbandry, University of Kassel, Witzenhausen, 37213, Germany","Visual monitoring of behaviour on-farm is mostly challenging due to the number of animals to be observed and the time required. However, behavioural problems such as cannibalism in turkeys may be preceded by subtle changes in behaviour. Machine learning techniques allow automatic behavioural monitoring of livestock to be carried out under different farming conditions. The aim of this study was to develop and test a novel pecking activity detection tool for potential use on turkey farms by means of acoustic data and a convolutional neural networks (CNN) model. Under near to commercial conditions, two metallic balls were used as pecking objects and suspended from the ceiling. Each pecking object was equipped with a microphone connected via a cable to a top view camera positioned on the ceiling. The recorded sound data were sampled in slots of 1 s and high pass filtering was performed to eliminate background noises. A total of 9200 filtered sound files were used for training and validation, and 3900 for testing set. They were labelled manually as peck or non-peck, using 7360 (80%) for training and 1840 (20%) for validation files, and fed into the CNN model. An additional 3900 new filtered sound clips were used to test the detection phase of the trained model. The experimental results illustrate that the deep learning-based detection method achieved high overall accuracy, precision, recall and F1-score of 96.8, 89.6, 92.0 and 90.8% in the detection phase. This indicates that the proposed technique could be used as a precise method for the detection of pecking activity levels in turkeys. © 2020 IAgrE","Machine learning; Monitoring; Poultry; Sound","Acoustic waves; Agriculture; Cameras; Convolutional neural networks; High pass filters; Learning algorithms; Learning systems; Monitoring; Poultry; Activity detection; Background noise; Detection methods; High-pass filtering; Learning techniques; Machine learning techniques; Overall accuracies; Visual monitoring; Deep learning","Federal Office for Agriculture and Food; German Federal Ministry of Food and Agriculture; Ministry of Science and Arts of Lower Saxony; Stiftung TierÃ¤rztliche Hochschule Hannover, TIHO; Bundesministerium für Ernährung und Landwirtschaft, BMEL; Bundesanstalt für Landwirtschaft und Ernährung, BLE, (2817903615)","We gratefully acknowledge financial support by the German Federal Ministry of Food and Agriculture (BMEL) through the Federal Office for Agriculture and Food (BLE), grant number 2817903615 due to a decision of the German Federal Parliament (www.ble.de/ptble/innovationsfoerderung-bmel/). Jennifer Gonzalez was funded by the Ministry of Science and Arts of Lower Saxony, Germany within the PhD Program Animal Welfare in Intensive Livestock Production Systems. We also thank the team of the farm for education and research in Ruthe of the University of Veterinary Medicine Hannover, Foundation, for the good cooperation.","A. Nasirahmadi; Department of Agricultural and Biosystems Engineering, University of Kassel, Witzenhausen, 37213, Germany; email: abozar.nasirahmadi@uni-kassel.de","","Academic Press","15375110","","BEINB","","English","Biosyst. Eng.","Article","Final","","Scopus","2-s2.0-85082777987"
"Barbedo J.G.A.; Koenigkan L.V.; Santos P.M.; Ribeiro A.R.B.","Barbedo, Jayme Garcia Arnal (9279436800); Koenigkan, Luciano Vieira (56175232700); Santos, Patrícia Menezes (7102995675); Ribeiro, Andrea Roberto Bueno (59290737600)","9279436800; 56175232700; 7102995675; 59290737600","Counting cattle in UAV images-dealing with clustered animals and animal/background contrast changes","2020","Sensors (Switzerland)","20","7","2126","","","","51","10.3390/s20072126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083415912&doi=10.3390%2fs20072126&partnerID=40&md5=2fc880ed4c825a2862f246405d309dbe","Embrapa Agricultural Informatics, Campinas, 13083-886, Brazil; Embrapa Southeast Livestock, São Carlos, 13560-970, Brazil; Universidade Santo Amaro, UNISA, UNIP, São Paulo, 04743-030, Brazil","Barbedo J.G.A., Embrapa Agricultural Informatics, Campinas, 13083-886, Brazil; Koenigkan L.V., Embrapa Agricultural Informatics, Campinas, 13083-886, Brazil; Santos P.M., Embrapa Southeast Livestock, São Carlos, 13560-970, Brazil; Ribeiro A.R.B., Universidade Santo Amaro, UNISA, UNIP, São Paulo, 04743-030, Brazil","The management of livestock in extensive production systems may be challenging, especially in large areas. Using Unmanned Aerial Vehicles (UAVs) to collect images from the area of interest is quickly becoming a viable alternative, but suitable algorithms for extraction of relevant information from the images are still rare. This article proposes a method for counting cattle which combines a deep learning model for rough animal location, color space manipulation to increase contrast between animals and background, mathematical morphology to isolate the animals and infer the number of individuals in clustered groups, and image matching to take into account image overlap. Using Nelore and Canchim breeds as a case study, the proposed approach yields accuracies over 90% under a wide variety of conditions and backgrounds. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Canchim breed; Convolutional neural networks; Mathematical morphology; Nelore breed; Unmanned aerial vehicles","Aircraft; Animals; Cattle; Image Processing, Computer-Assisted; Neural Networks, Computer; Agriculture; Antennas; Color matching; Deep learning; Mathematical morphology; Unmanned aerial vehicles (UAV); Area of interest; Color space; Contrast changes; Learning models; Production system; aircraft; animal; bovine; image processing; Animals","Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2018/12845-9); Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP","Funding: This work was funded by Fapesp under grant number 2018/12845-9.","J.G.A. Barbedo; Embrapa Agricultural Informatics, Campinas, 13083-886, Brazil; email: jayme.barbedo@embrapa.br","","MDPI AG","14248220","","","32290316","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85083415912"
"Yoon H.; Jang A.-R.; Jung C.; Ko H.; Lee K.-N.; Lee E.","Yoon, Hachung (55498241400); Jang, Ah-Reum (58708761200); Jung, Chungsik (57216433701); Ko, Hunseok (57218993783); Lee, Kwang-Nyeong (35285884300); Lee, Eunesub (55723743900)","55498241400; 58708761200; 57216433701; 57218993783; 35285884300; 55723743900","Risk assessment program of highly pathogenic avian influenza with deep learning algorithm","2020","Osong Public Health and Research Perspectives","11","4","","239","244","5","3","10.24171/j.phrp.2020.11.4.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090962943&doi=10.24171%2fj.phrp.2020.11.4.13&partnerID=40&md5=7fbaea73944042596c9647548d6d96c4","Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, South Korea; Korea Telecom, Seoul, South Korea; Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, 177 Hyeoksin 8-ro, Gimcheon, 39660, South Korea","Yoon H., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, 177 Hyeoksin 8-ro, Gimcheon, 39660, South Korea; Jang A.-R., Korea Telecom, Seoul, South Korea; Jung C., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, South Korea; Ko H., Korea Telecom, Seoul, South Korea; Lee K.-N., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, South Korea; Lee E., Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, South Korea","Objectives: This study presents the development and validation of a risk assessment program of highly pathogenic avian influenza (HPAI). This program was developed by the Korean government (Animal and Plant Quarantine Agency) and a private corporation (Korea Telecom, KT), using a national database (Korean animal health integrated system, KAHIS). Methods: Our risk assessment program was developed using the multilayer perceptron method using R Language. HPAI outbreaks on 544 poultry farms (307 with H5N6, and 237 with H5N8) that had available visit records of livestock-related vehicles amongst the 812 HPAI outbreaks that were confirmed between January 2014 and June 2017 were involved in this study. Results: After 140,000 iterations without drop-out, a model with 3 hidden layers and 10 nodes per layer, were selected. The activation function of the model was hyperbolic tangent. Precision and recall of the test gave F1 measures of 0.41, 0.68 and 0.51, respectively, at validation. The predicted risk values were higher for the “outbreak” (average ± SD, 0.20 ± 0.31) than “non-outbreak” (0.18 ± 0.30) farms (p < 0.001). Conclusion: The risk assessment model developed was employed during the epidemics of 2016/2017 (pilot version) and 2017/2018 (complementary version). This risk assessment model enhanced risk management activities by enabling preemptive control measures to prevent the spread of diseases. © 2020 Korea Centers for Disease Control and Prevention. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).","Avian influenza; Deep learning; Risk assessment","Article; deep learning; deep neural network; epidemic; government; highly pathogenic avian influenza; learning algorithm; multilayer perceptron; nonhuman; poultry; priority journal; risk assessment; South Korea","Animal and Plant Quarantine Agency, QIA, (I-1543068-2019-20-01)","This study was funded by the Animal and Plant Quarantine Agency (R&D Project no.: I-1543068-2019-20-01).","H. Yoon; Veterinary Epidemiology Division, Animal and Plant Quarantine Agency, Gimcheon, 177 Hyeoksin 8-ro, 39660, South Korea; email: heleney@korea.kr","","Korean Disease Control and Prevention Agency","22109099","","","","English","Osong Public Health Res. Perspect.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090962943"
"Kumar S.; Kumar S.; Mishra P.; Chaube M.K.","Kumar, Santosh (59511763600); Kumar, Sunil (59355783200); Mishra, Prerna (57215925779); Chaube, Mithilesh K. (36243062700)","59511763600; 59355783200; 57215925779; 36243062700","Internet of animal health things (IoAT): A new frontier in animal biometrics and data analytics research","2020","IoT-Based Data Analytics for the Healthcare Industry: Techniques and Applications","","","","261","275","14","2","10.1016/B978-0-12-821472-5.00003-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128703904&doi=10.1016%2fB978-0-12-821472-5.00003-X&partnerID=40&md5=2b8e583bd9adee0c165a95dfed12c6be","Department of Computer Science and Engineering, Dr. SPM IIIT-Naya Raipur, Atal Nagar, Chhattisgarh, Raipur, India; Mathematical Sciences, Dr. SPM IIIT-Naya Raipur, Atal Nagar, Chhattisgarh, Raipur, India; Marine Engineering Research Institute (MERI), West Bengal, Kolkata, India","Kumar S., Department of Computer Science and Engineering, Dr. SPM IIIT-Naya Raipur, Atal Nagar, Chhattisgarh, Raipur, India; Kumar S., Marine Engineering Research Institute (MERI), West Bengal, Kolkata, India; Mishra P., Department of Computer Science and Engineering, Dr. SPM IIIT-Naya Raipur, Atal Nagar, Chhattisgarh, Raipur, India; Chaube M.K., Mathematical Sciences, Dr. SPM IIIT-Naya Raipur, Atal Nagar, Chhattisgarh, Raipur, India","Animal biometrics is one of the upcoming emerging fields in computer vision. It provides a better learning paradigm monitoring, identification, and tracking of endangered species and individual animals. IoT and data analytics are gaining more proliferated due to the wide range of applications in the last decade. It has made technology integral to everyday activities and even our culture. The number of intelligent sensors and devices shipped has improved more than seven times, from 7.25 billion in 2012 to 59.90 billion in 2020. As a result, data management practices of smart farming are also changing with the advent of precision farming. The outpouring in the global population is compelling a shift toward enabling the IoT-based smart livestock management framework for livestock animals’ health monitoring. The health monitoring system-enabled IoT applications to allocate sensors that can be mounted on the animal body. However, at present, no such IoT-enabled learning frameworks are using the data science paradigm for providing on the current health status and real-time monitoring capability of the individual animal in smart livestock. At real, to detect animals’ health status, we needed to wait for veterinary expertise, which takes a long time for its arrival. In traditional livestock frameworks, health monitoring and production techniques are regularly labor-intensive and driven by very slim margins, in this chapter, a comprehensive review of the internet of things and processing of sensed data for animal health management. © 2021 Elsevier Inc. All rights reserved.","Animal biometrics; Computer vision; Data analytics; Deep learning; IoAT; IoT","","","","","","Elsevier","","978-012821472-5","","","English","IoT-Based Data Analytics for the Healthc. Industry: Techniques and Applications","Book chapter","Final","","Scopus","2-s2.0-85128703904"
"Liu C.; Jian Z.; Xie M.; Cheng I.","Liu, Chuyang (57425437600); Jian, Zihao (57222981908); Xie, Minshan (58712125700); Cheng, Irene (54790535600)","57425437600; 57222981908; 58712125700; 54790535600","A Real-Time Mobile Application for Cattle Tracking using Video Captured from a Drone","2021","2021 International Symposium on Networks, Computers and Communications, ISNCC 2021","","","","","","","11","10.1109/ISNCC52172.2021.9615648","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123425738&doi=10.1109%2fISNCC52172.2021.9615648&partnerID=40&md5=da9922cf084debae79928cd9b380e6cd","University Of Alberta Edmonton, Multimedia Research Centre, Canada","Liu C., University Of Alberta Edmonton, Multimedia Research Centre, Canada; Jian Z., University Of Alberta Edmonton, Multimedia Research Centre, Canada; Xie M., University Of Alberta Edmonton, Multimedia Research Centre, Canada; Cheng I., University Of Alberta Edmonton, Multimedia Research Centre, Canada","In many countries, governments have issued laws on cattle traceability, which require farmers to keep track of their livestock. While there are many algorithms to monitor cattle in confined indoor environments, monitoring cattle of a large number in outdoor pastures is still an open research problem. In recent years, with the advancement of unmanned aerial vehicles, e.g., drones, cattle monitoring can be based on images captured by drones. However, the challenges include image analysis in real-time, and keeping track of a dynamic scene (cattle movements) based on a moving sensor device. In this work, we develop an iOS application that can send captured herd images to our deep-learning-based server for cow segmentation and counting. Furthermore, the app can guide the operator to control the drone flight route and viewing perspectives. © 2021 IEEE.","Cattle Detection; Drone Tracking; Mobile Application; Smart Agriculture; YOLOv4","Agriculture; Antennas; Deep learning; Drones; Image segmentation; Mobile computing; Cattle detection; Cattle tracking; Environment monitoring; Indoor environment; Keep track of; Mobile applications; Real- time; Research problems; Smart agricultures; YOLOv4; Aircraft detection","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-073811316-6","","","English","Int. Symp. Networks, Comput. Commun., ISNCC","Conference paper","Final","","Scopus","2-s2.0-85123425738"
"Bezen R.; Edan Y.; Halachmi I.","Bezen, R. (57211393832); Edan, Y. (7004434501); Halachmi, I. (6701325703)","57211393832; 7004434501; 6701325703","An automatic data acquisition system for acquiring training data for a deep learning algorithm for individual cow intake prediction","2019","Precision Livestock Farming 2019 - Papers Presented at the 9th European Conference on Precision Livestock Farming, ECPLF 2019","","","","284","291","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073762083&partnerID=40&md5=13685c07deb0c464fd0a55a55a7e083b","Institute of Agricultural Engineering, Agriculture Research Organization, Rishon LeZion, Israel; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel","Bezen R., Institute of Agricultural Engineering, Agriculture Research Organization, Rishon LeZion, Israel, Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Edan Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Halachmi I., Institute of Agricultural Engineering, Agriculture Research Organization, Rishon LeZion, Israel, Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel","Individual feed intake of dairy cows is an important, currently unavailable, variable in commercial dairies. Earlier systems developed were either costly or unreliable enough for commercial farms. This research developed a low-cost individual feed intake system using RGB-D cameras and deep learning algorithm. Depth and colour images are produced from an RGB-D camera, and are used to build a CNNs (Convolutional Neural Networks) regression model for weight intake prediction. To provide training data, an automatic data acquisition system was designed to collect a wide range of food weights, in different configurations and conditions (indoor, outdoor, direct-sun). The system included a scale and a micro-controller set in the Volcani research dairy facility, an open cowshed with Holstein cows, eating Total Mix Ration. With this setup, 28,761 data were collected over seven days. Additional data were created by data augmentation methods. The model was evaluated on a test-dataset acquired in the same dairy farm. The model was tested for different combinations of training data (direct-sun/outdoor) to evaluate the importance of the data diversity. Per meal, mean absolute and square errors were 0.127 kg, and 0.034 kg2, respectively, the consumed amount of feed measured in range of 0-8 kg. The sensitivity analysis shows that the amount and diversity of data is important for model training. Better results were achieved for the model that was trained with high diversity data. The results suggest that cameras and CNNs are feasible for individual feed intake measurement on the dairy farm. © Precision Livestock Farming 2019 - Papers Presented at the 9th European Conference on Precision Livestock Farming, ECPLF 2019. All rights reserved.","3D camera; Deep learning; Individual cow feed intake; Machine vision; Precision livestock farming (PLF)","Cameras; Computer vision; Data acquisition; Farms; Learning algorithms; Light polarization; Neural networks; Regression analysis; Sensitivity analysis; Statistical tests; 3-D cameras; Additional datum; Automatic data acquisition; Convolutional neural network; Dairy facilities; Data augmentation; Feed intake; Precision livestock farming; Deep learning","Israeli Chief Scientist of Agriculture","This study was supported by grants of the Israeli Chief Scientist of Agriculture, project “Kendel”, and we gratefully thank all members of the Precision Livestock Farming (PLF) Lab. It was also partially supported by the Rabbi W. Gunther Plaut Chair in Manufacturing Engineering at Ben-Gurion University of the Negev. The authors thank the staff of Volcani research cowshed for assistance in the study.","R. Bezen; Institute of Agricultural Engineering, Agriculture Research Organization, Rishon LeZion, Israel; email: ranbez@post.bgu.ac.il","O'Brien B.; Hennessy D.; Shalloo L.","Organising Committee of the 9th European Conference on Precision Livestock Farming (ECPLF), Teagasc, Animal and Grassland Research and Innovation Centre","","978-184170654-2","","","English","Precis. Livest. Farming - Pap. Present. Eur. Conf. Precis. Livest. Farming, ECPLF","Conference paper","Final","","Scopus","2-s2.0-85073762083"
"Bergamini L.; Trachtman A.R.; Palazzi A.; Negro E.D.; Capobianco Dondona A.; Marruchella G.; Calderara S.","Bergamini, Luca (15924875500); Trachtman, Abigail Rose (57209712272); Palazzi, Andrea (57191537487); Negro, Ercole Del (57208837362); Capobianco Dondona, Andrea (55871878300); Marruchella, Giuseppe (8638634100); Calderara, Simone (23099524400)","15924875500; 57209712272; 57191537487; 57208837362; 55871878300; 8638634100; 23099524400","Segmentation Guided Scoring of Pathological Lesions in Swine Through CNNs","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11808 LNCS","","","352","360","8","2","10.1007/978-3-030-30754-7_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072871473&doi=10.1007%2f978-3-030-30754-7_35&partnerID=40&md5=8f8f60b73ee4e47899aecbbbfc5a4766","AImageLab, University of Modena and Reggio Emilia, Modena, Italy; Farm4Trade, Chieti, Italy; University of Teramo, Teramo, Italy","Bergamini L., AImageLab, University of Modena and Reggio Emilia, Modena, Italy; Trachtman A.R., University of Teramo, Teramo, Italy; Palazzi A., AImageLab, University of Modena and Reggio Emilia, Modena, Italy; Negro E.D., Farm4Trade, Chieti, Italy; Capobianco Dondona A., Farm4Trade, Chieti, Italy; Marruchella G., University of Teramo, Teramo, Italy; Calderara S., University of Teramo, Teramo, Italy","The slaughterhouse is widely recognised as a useful checkpoint for assessing the health status of livestock. At the moment, this is implemented through the application of scoring systems by human experts. The automation of this process would be extremely helpful for veterinarians to enable a systematic examination of all slaughtered livestock, positively influencing herd management. However, such systems are not yet available, mainly because of a critical lack of annotated data. In this work we: (i) introduce a large scale dataset to enable the development and benchmarking of these systems, featuring more than 4000 high-resolution swine carcass images annotated by domain experts with pixel-level segmentation; (ii) exploit part of this annotation to train a deep learning model in the task of pleural lesion scoring. In this setting, we propose a segmentation-guided framework which stacks together a fully convolutional neural network performing semantic segmentation with a rule-based classifier integrating a-priori veterinary knowledge in the process. Thorough experimental analysis against state-of-the-art baselines proves our method to be superior both in terms of accuracy and in terms of model interpretability. Code and dataset are publicly available here: https://github.com/lucabergamini/swine-lesion-scoring. © 2019, Springer Nature Switzerland AG.","Computer vision; Deep learning; Slaughterhouse scoring system; Swine lesions recognition","Agriculture; Computer vision; Deep learning; Image analysis; Large dataset; Neural networks; Pattern recognition; Semantics; Convolutional neural network; Experimental analysis; Large-scale dataset; Pathological lesions; Rule-based classifier; Scoring systems; Semantic segmentation; Swine lesions recognition; Image segmentation","","","L. Bergamini; AImageLab, University of Modena and Reggio Emilia, Modena, Italy; email: luca.bergamini24@unimore.it","Cristani M.; Prati A.; Lanz O.; Messelodi S.; Sebe N.","Springer Verlag","03029743","978-303030753-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85072871473"
"Fernandes A.F.A.; DÃ³rea J.R.R.; Valente B.D.; Fitzgerald R.; Herring W.; Rosa G.J.M.","Fernandes, Arthur F.A. (18934431100); DÃ³rea, JoÃ£o R.R. (57219482409); Valente, Bruno Dourado (6602725113); Fitzgerald, Robert (23466670000); Herring, William (7003943007); Rosa, Guilherme J.M. (35581971400)","18934431100; 57219482409; 6602725113; 23466670000; 7003943007; 35581971400","Comparison of data analytics strategies in computer vision systems to predict pig body composition traits from 3D images","2020","Journal of Animal Science","98","8","","","","","33","10.1093/jas/skaa250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090074509&doi=10.1093%2fjas%2fskaa250&partnerID=40&md5=fcd832fd9d3cd0540a977d312d91b527","Department of Animal and Dairy Sciences, University of Wisconsin-Madison, Madison, 53706, WI, United States; Genus PLC, Hendersonville, 37075, TN, United States","Fernandes A.F.A., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, Madison, 53706, WI, United States; DÃ³rea J.R.R., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, Madison, 53706, WI, United States; Valente B.D., Genus PLC, Hendersonville, 37075, TN, United States; Fitzgerald R., Genus PLC, Hendersonville, 37075, TN, United States; Herring W., Genus PLC, Hendersonville, 37075, TN, United States; Rosa G.J.M., Department of Animal and Dairy Sciences, University of Wisconsin-Madison, Madison, 53706, WI, United States","Computer vision systems (CVS) have been shown to be a powerful tool for the measurement of live pig body weight (BW) with no animal stress. With advances in precision farming, it is now possible to evaluate the growth performance of individual pigs more accurately. However, important traits such as muscle and fat deposition can still be evaluated only via ultrasound, computed tomography, or dual-energy x-ray absorptiometry. Therefore, the objectives of this study were: 1) to develop a CVS for prediction of live BW, muscle depth (MD), and back fat (BF) from top view 3D images of finishing pigs and 2) to compare the predictive ability of different approaches, such as traditional multiple linear regression, partial least squares, and machine learning techniques, including elastic networks, artificial neural networks, and deep learning (DL). A dataset containing over 12,000 images from 557 finishing pigs (average BW of 120 Â± 12 kg) was split into training and testing sets using a 5-fold cross-validation (CV) technique so that 80% and 20% of the dataset were used for training and testing in each fold. Several image features, such as volume, area, length, widths, heights, polar image descriptors, and polar Fourier transforms, were extracted from the images and used as predictor variables in the different approaches evaluated. In addition, DL image encoders that take raw 3D images as input were also tested. This latter method achieved the best overall performance, with the lowest mean absolute scaled error (MASE) and root mean square error for all traits, and the highest predictive squared correlation (R2). The median predicted MASE achieved by this method was 2.69, 5.02, and 13.56, and R2 of 0.86, 0.50, and 0.45, for BW, MD, and BF, respectively. In conclusion, it was demonstrated that it is possible to successfully predict BW, MD, and BF via CVS on a fully automated setting using 3D images collected in farm conditions. Moreover, DL algorithms simplified and optimized the data analytics workflow, with raw 3D images used as direct inputs, without requiring prior image processing. © The Author(s) 2020.","Body composition; Image analysis; Machine learning; Precision livestock farming; Ultrasound","Algorithms; Animals; Body Composition; Body Weight; Data Science; Humans; Image Processing, Computer-Assisted; Linear Models; Machine Learning; Muscles; Neural Networks, Computer; Phenotype; Swine; Tomography, X-Ray Computed; Ultrasonography; algorithm; anatomy and histology; animal; body composition; body weight; comparative study; echography; human; image processing; machine learning; muscle; phenotype; physiology; pig; procedures; statistical model; veterinary medicine; x-ray computed tomography","Advanced Computing Initiative; Coordination for the Improvement of High Education Personnel; Department of Computer Sciences; UW-Madison; Wisconsin Institutes for Discovery; National Science Foundation, NSF; U.S. Department of Energy, USDOE; Wisconsin Alumni Research Foundation, WARF; Office of Science, SC; College of Engineering, University of Wisconsin-Madison; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES","Funding text 1: This research was performed using the compute resources and assistance of the UW-Madison Center For High Throughput Computing (CHTC) in the Department of Computer Sciences. The CHTC is supported by UW-Madison, the Advanced Computing Initiative, the Wisconsin Alumni Research Foundation, the Wisconsin Institutes for Discovery, and the National Science Foundation and is an active member of the Open Science Grid, which is supported by the National Science Foundation and the U.S. Department of Energy’s Office of Science. The first author acknowledges financial support from the Coordination for the Improvement of High Education Personnel (CAPES), Brazil.; Funding text 2: This research was performed using the compute resources and assistance of the UW-Madison Center For High Throughput Computing (CHTC) in the Department of Computer Sciences. The CHTC is supported by UW-Madison, the Advanced Computing Initiative, the Wisconsin Alumni Research Foundation, the Wisconsin Institutes for Discovery, and the National Science Foundation and is an active member of the Open Science Grid, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science. The first author acknowledges financial support from the Coordination for the Improvement of High Education Personnel (CAPES), Brazil.","G.J.M. Rosa; Department of Animal and Dairy Sciences, University of Wisconsin-Madison, Madison, 53706, United States; email: grosa@wisc.edu","","Oxford University Press","00218812","","","32770242","English","J. Anim. Sci.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090074509"
"Menad H.; Ben-naoum F.; Amine A.","Menad, Hanane (57208494954); Ben-naoum, Farah (56968194600); Amine, Abdelmalek (24721247800)","57208494954; 56968194600; 24721247800","Deep convolutional neural network for pollen grains classification","2019","CEUR Workshop Proceedings","2351","","","","","","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064941698&partnerID=40&md5=243c715e4fc7f59a4e7d757f13e06b61","EEDIS Laboratory, University of Djillali Liabes, Sidi Bel Abbes, Algeria; GeCoDe Laboratory, University of Dr Moulay Tahar, Saida, Algeria","Menad H., EEDIS Laboratory, University of Djillali Liabes, Sidi Bel Abbes, Algeria; Ben-naoum F., EEDIS Laboratory, University of Djillali Liabes, Sidi Bel Abbes, Algeria; Amine A., GeCoDe Laboratory, University of Dr Moulay Tahar, Saida, Algeria","The beekeeping is the art of cultivating the bees in the aim to remove from this industry the maximum performance with the minimum expenditure. The apiculture products marketed are the honey, wax, pollen, propolis and royal jelly. This activity of topping up contributes to the development of the livestock and to the protection of the Environment. This paper presents the application of deep convolutional neural network for pollen grains recognition based on their images classification. The neural network contains 8 hidden layers where first 5 are convolutionnal neurones responsible for image representations and next 3 are fully connected layers for image classification. The obtained results proved the efficiency of the proposed approach for pollen grains recognition. © 2019 CEUR-WS. All rights reserved.","Convolutional neural network; Deep learning; Honey pollen classification; Melissopalynology","Agriculture; Convolution; Deep learning; Food products; Image classification; Neural networks; Convolutional neural network; Hidden layers; Image representations; Images classification; Melissopalynology; Pollen classification; Pollen grains; Protection of the environments; Deep neural networks","","","","Boudia M.A.; Hamou R.M.; Amine A.; Yahlali M.","CEUR-WS","16130073","","","","English","CEUR Workshop Proc.","Conference paper","Final","","Scopus","2-s2.0-85064941698"
"Du A.; Guo H.; Lu J.; Su Y.; Ruchay A.; Pezzuolo A.","Du, Ao (57393576900); Guo, Hao (55331600800); Lu, Jie (57393576800); Su, Yang (57204974615); Ruchay, Alexey (57192592568); Pezzuolo, Andrea (56023708900)","57393576900; 55331600800; 57393576800; 57204974615; 57192592568; 56023708900","Automatic heart girth measurement for cattle based on deep learning","2021","2021 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2021 - Proceedings","","","","27","31","4","2","10.1109/MetroAgriFor52389.2021.9628696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123437434&doi=10.1109%2fMetroAgriFor52389.2021.9628696&partnerID=40&md5=fd84ef212d3d28e3fc7ee75709545d7b","China Agricultural University, College of Land Science and Technology, Beijing, 100083, China; China Agricultural University, College of Land Science and Technology, College of Information and Electrical Engineering, Beijing, 100083, China; Fed. Research Centre of Biological Systems and Agro-technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; University of Padova, Department of Land, Environment, Agriculture, and Forestry, Legnaro, 35020, Italy","Du A., China Agricultural University, College of Land Science and Technology, Beijing, 100083, China; Guo H., China Agricultural University, College of Land Science and Technology, College of Information and Electrical Engineering, Beijing, 100083, China; Lu J., China Agricultural University, College of Land Science and Technology, Beijing, 100083, China; Su Y., China Agricultural University, College of Land Science and Technology, Beijing, 100083, China; Ruchay A., Fed. Research Centre of Biological Systems and Agro-technologies of the Russian Academy of Sciences, Orenburg, 460000, Russian Federation; Pezzuolo A., University of Padova, Department of Land, Environment, Agriculture, and Forestry, Legnaro, 35020, Italy","Body measurement is an important way to assess the health status of livestock. As an indicator of body condition, heart girth is relevant to the weight and health of livestock. The emerging of consumer level sensors provides an efficient and low cost way to monitor the status of livestock. Meanwhile, many studies proposed non-contact body measurement methods utilizing sensors. However, constraints still exist in the pipeline of many data acquisition and measurement methods. To improve flexibility and efficiency, this paper presents an automatic heart girth measurement method based on deep learning. Unlike methods that solely compute on point clouds, we combine the RGB image and 3D point cloud captured by depth sensors to implement the detection and measurement. We train a detector to find the girth of cattle RGB images and project the girth points onto the surface of the livestock. Then we use ellipse to fit the girth curve, and compute its circumstance as the girth length. We test our method on 103 cattle data, the mean error of our results is 6.47%. In comparison with the previous method, our method is more robust and accurate.  © 2021 IEEE.","body measurement; cattle; deep learning; heart girth; point cloud","Agriculture; Data acquisition; Deep learning; Body condition; Body measurements; Cattle; Deep learning; Girth measurement; Health status; Heart girth; Measurement methods; Point-clouds; RGB images; Heart","National Natural Science Foundation of China, NSFC, (41601491, 42071449); National Natural Science Foundation of China, NSFC","This research is supported by the National Natural Science Foundation of China [grant numbers 42071449, 41601491].","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540533-1","","","English","IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.","Conference paper","Final","","Scopus","2-s2.0-85123437434"
"Cang Y.; He H.; Qiao Y.","Cang, Yan (35755532700); He, Hengxiang (57202399815); Qiao, Yulong (14021900400)","35755532700; 57202399815; 14021900400","An Intelligent Pig Weights Estimate Method Based on Deep Learning in Sow Stall Environments","2019","IEEE Access","7","","8896840","164867","164875","8","58","10.1109/ACCESS.2019.2953099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077529847&doi=10.1109%2fACCESS.2019.2953099&partnerID=40&md5=77a38304d0b4cb73e052c2fe78f44d4b","College of Information and Communication Engineering, Harbin Engineering University, Harbin, 150001, China; Focused Loong Technology Company, Ltd, Beijing, 100086, China","Cang Y., College of Information and Communication Engineering, Harbin Engineering University, Harbin, 150001, China; He H., College of Information and Communication Engineering, Harbin Engineering University, Harbin, 150001, China, Focused Loong Technology Company, Ltd, Beijing, 100086, China; Qiao Y., College of Information and Communication Engineering, Harbin Engineering University, Harbin, 150001, China","To further the application of artificial intelligence techniques in agriculture, this study proposes an approach based on deep neural network to estimate the live weights of pigs in saw stalls. We design a neural network that uses the back of pigs in top-view depth images as the input and outputs the pig weights estimates. The proposed network, which is based on a Faster-RCNN network with an added regressive branch, integrates the pig detection and live weights regressive network into an end-to-end network. It simultaneously performs pig recognition, location and pig weights estimate. Alternating the training method optimises the proposed network. Image simulation using circles with various overlapping areas and radii is used to prove the efficacy of the proposed network. When the overlap area is greater than 30% of the total area, the proposed network is invalid. Real farm experiments were conducted for three months to construct the back of pigs in top-view depth image data set to train the proposed network. The test results not only prove the relationship between size of back area and pig weights, but also verify that the proposed neural network can accurately estimate pig weights. The study will promote the application of intelligent technique in the livestock farming, and provides some references for intelligent weighing researchers. © 2013 IEEE.","deep learning; Digital image; live weights estimation; object recognition","Agriculture; Deep learning; Deep neural networks; Object recognition; Artificial intelligence techniques; Digital image; End-to-end network; Image simulations; Input and outputs; Intelligent techniques; Livestock farming; Weights estimation; Mammals","National Nature Science Foundation of China; National Aerospace Science Foundation of China, (61871142)","This work was supported in part by the National Nature Science Foundation of China under Grant 61871142, and in part by the Name was Research on Signal Processing on Graph-Based Dynamic Texture Analysis and Applications.","Y. Cang; College of Information and Communication Engineering, Harbin Engineering University, Harbin, 150001, China; email: cangyan@hrbeu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85077529847"
"Katayama T.; Kawada H.; Nishiyama M.; Iwai Y.","Katayama, Toshiki (57226715113); Kawada, Hirohumi (57226716721); Nishiyama, Masashi (57191888628); Iwai, Yoshio (57201788506)","57226715113; 57226716721; 57191888628; 57201788506","Estimation of beef marbling standard for live cattle using multi-input convolutional neural network with ultrasound images","2021","Proceedings of SPIE - The International Society for Optical Engineering","11794","","1179417","","","","0","10.1117/12.2588263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112464286&doi=10.1117%2f12.2588263&partnerID=40&md5=7ca23223f511d308ea250d0a044df48e","Tottori University, Tottori, Japan","Katayama T.; Kawada H.; Nishiyama M.; Iwai Y., Tottori University, Tottori, Japan","There is substantial interest among livestock farmers in estimating the value of live beef cattle from ultrasound images. In this study, we aimed to clarify the knowledge employed for the estimation of beef marbling standard (BMS) by experts in visual inspection. To do this, we conducted a field survey, which revealed that the experts observe ultrasound images containing various body parts of live cattle: in particular, the loin part, iliocostalis part, and shoulder clod part. To automatically estimate the BMS value, we designed a convolutional neural network architecture that incorporates the experts' knowledge. We demonstrated that our multi-input neural network, with ultrasound images of various body parts, obtained a high accuracy in BMS estimation. © 2021 SPIE.","Beef cattle; Beef marbling standard; Deep learning; Ultrasound images","Agriculture; Beef; Convolution; Network architecture; Quality control; Ultrasonic applications; Vision; Beef cattle; Field surveys; High-accuracy; Livestock farmers; Multiinput; Shoulder clod; Ultrasound images; Visual inspection; Convolutional neural networks","","","M. Nishiyama; email: nishiyama@tottori-u.ac.jp; M. Nishiyama; email: nishiyama@tottori-u.ac.jp","Terada K.; Nakamura A.; Komuro T.; Shimizu T.","SPIE","0277786X","978-151064426-7","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85112464286"
"","","","1st International Symposium on Geometry and Vision, ISGV 2021","2021","Communications in Computer and Information Science","1386 CCIS","","","","","392","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104869502&partnerID=40&md5=b4389a1043f289f768d0eacb94ba0719","","","The proceedings contain 29 papers. The special focus in this conference is on Geometry and Vision. The topics include: Coverless Video Steganography Based on Inter Frame Combination; character Photo Selection for Mobile Platform; close Euclidean Shortest Path Crossing an Ordered 3D Skew Segment Sequence; a Lane Line Detection Algorithm Based on Convolutional Neural Network; segment- and Arc-Based Vectorizations by Multi-scale/Irregular Tangential Covering; algorithms for Computing Topological Invariants in Digital Spaces; discrete Linear Geometry on Non-square Grid; electric Scooter and Its Rider Detection Framework Based on Deep Learning for Supporting Scooter-Related Injury Emergency Services; tracking Livestock Using a Fully Connected Network and Kalman Filter; traffic-Sign Recognition Using Deep Learning; a Comparison of Approaches for Synchronizing Events in Video Streams Using Audio; union-Retire: A New Paradigm for Single-Pass Connected Component Analysis; improving Object Detection in Real-World Traffic Scenes; comparison of Red versus Blue Laser Light for Accurate 3D Measurement of Highly Specular Surfaces in Ambient Lighting Conditions; fruit Detection from Digital Images Using CenterNet; a Graph-Regularized Non-local Hyperspectral Image Denoising Method; random Convolutional Network for Hyperspectral Image Classification; mamboNet: Adversarial Semantic Segmentation for Autonomous Driving; effective Pavement Crack Delineation Using a Cascaded Dilation Module and Fully Convolutional Networks; d-GaussianNet: Adaptive Distorted Gaussian Matched Filter with Convolutional Neural Network for Retinal Vessel Segmentation; tree Leaves Detection Based on Deep Learning; deep Learning in Medical Applications: Lesion Segmentation in Skin Cancer Images Using Modified and Improved Encoder-Decoder Architecture; apple Ripeness Identification Using Deep Learning; a Hand-Held Sensor System for Exploration and Thermal Mapping of Volcanic Fumarole Fields.","","","","","","Nguyen M.; Yan W.Q.; Ho H.","Springer Science and Business Media Deutschland GmbH","18650929","978-303072072-8","","","English","Commun. Comput. Info. Sci.","Conference review","Final","","Scopus","2-s2.0-85104869502"
"Inoue H.","Inoue, Hidehiko (58445505100)","58445505100","Development of a Raspberry Pi based pest detection device for use in livestock feed storage systems","2019","2019 ASABE Annual International Meeting","","","","","","","1","10.13031/aim.201900337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084016630&doi=10.13031%2faim.201900337&partnerID=40&md5=572d659bee25690b30f90de25fcb665e","Institute of Agricultural Machinery, NARO, Tochigi, Japan","Inoue H., Institute of Agricultural Machinery, NARO, Tochigi, Japan","Stored feed is frequently damaged by rats. In this study, we made use of the compact “Raspberry Pi” computer to develop an animal damage countermeasure. We also attempted to make the automatic photographing apparatus for dark environments. This device included a Raspberry Pi, infrared camera module, MOSFET, infrared LED floodlight, and pyroelectric infrared ray sensor. The operation of the apparatus is as follows: When the pyroelectric infrared sensor is triggered by the presence of an animal and the signal becomes HIGH, a signal of HIGH is output from GPIO of the Raspberry Pi and a voltage of 12 V is applied to the MOSFET to make it function as a relay circuit. The infrared LED projector then lights up. At the same time the device starts taking a still image or video using the infrared camera that is connected to the Raspberry Pi. The apparatus succeeded in shooting a movie of a rat, and it successfully took still pictures. We installed the machine-learning library “TensorFlow” on the Raspberry Pi and performed image recognition of murine images using a deep learning program. When the target animal occupies a small part of the image, the image recognition results tend to be erroneous; however, enlarging the area of interest results in more accurate detection. In addition, erroneous judgment was often made in infrared camera images. The time required for the determination was 15 to 45 seconds for the Pi 3 Model B. These results show that it is possible to create a low-cost pest countermeasure device or robot controlled by a Raspberry Pi, and capable of various responses and input processing. © 2019 ASABE Annual International Meeting. All rights reserved.","Animal damage; Feeding damage; Machine-learning; Raspberry Pi","Agriculture; Animals; Deep learning; Electric lighting; Image recognition; Infrared detectors; Infrared devices; Infrared radiation; Learning systems; Light emitting diodes; Machine learning; Static random access storage; Temperature indicating cameras; Area of interest; Detection device; Infra-red cameras; Infrared leds; Infrared rays; Learning programs; Livestock feed; Pyroelectric Infrared (PIR) sensor; MOSFET devices","","","","","American Society of Agricultural and Biological Engineers","","","","","English","ASABE Annu. Int. Meet.","Conference paper","Final","","Scopus","2-s2.0-85084016630"
"Chae J.-W.; Cho H.-C.","Chae, Jung-Woo (57215821871); Cho, Hyun-Chong (22233514800)","57215821871; 22233514800","Detecting abnormal behavior of cattle based on object detection algorithm","2020","Transactions of the Korean Institute of Electrical Engineers","69","3","","468","473","5","1","10.5370/KIEE.2020.69.3.468","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081956116&doi=10.5370%2fKIEE.2020.69.3.468&partnerID=40&md5=89c664899de14cd8cd7752ba714c1ab7","Dept. of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea; Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea","Chae J.-W., Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea; Cho H.-C., Dept. of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea","Detection of abnormal behavior and unusual signs of cattle has been an important factor for managers in the livestock farming. Among them, it is of great interest for livestock farmers to detect estrus of cattle, which are unusual behaviors related to breeding management. Most studies on detecting estrus combined with IT technologies have been based on sensors attached to the neck or leg of cattle. But sensor-type devices have the potential to cause stress on the cattle by attaching them, and there are additional cost issues for maintenance. In this study, to solve the above problems, we present a estrus detection system which used regular CCTV and applied the object detection algorithm based on the deep learning model. To run real-time detection in video stream, the activity detection is performed within a single image frame and makes prediction with a single network evaluation. The results of the proposed system showed 97% precision and 96% accuracy. Copyright © The Korean Institute of Electrical Engineers.","Abnormal behavior; Activity recognition; Cattle; Deep learning model; Estrus; Object detection","Agriculture; Behavioral research; Deep learning; Learning systems; Object recognition; Signal detection; Abnormal behavior; Activity recognition; Cattle; Estrus; Learning models; Object detection","Ministry of Science and ICT, (IITP-2020-2018-0-01433); National Research Foundation of Korea, NRF, (2017R1E1A1A03070297); Institute for Information and Communications Technology Promotion, IITP","This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2017R1E1A1A03070297). This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2020-2018-0-01433) supervised by the IITP (Institute for Information & communications Technology Promotion).","H.-C. Cho; Dept. of Electronics Engineering, Interdisciplinary Graduate Program for BIT Medical Convergence, Kangwon National University, South Korea; email: hyuncho@kangwon.ac.kr","","Korean Institute of Electrical Engineers","19758359","","","","Korean","Trans. Korean Inst. Electr. Eng.","Article","Final","","Scopus","2-s2.0-85081956116"
"Psota E.T.; Luc E.K.; Pighetti G.M.; Schneider L.G.; Trout Fryxell R.T.; Keele J.W.; Kuehn L.A.","Psota, E.T. (24315220900); Luc, E.K. (57220766613); Pighetti, G.M. (6602198047); Schneider, L.G. (57189499846); Trout Fryxell, R.T. (57195117139); Keele, J.W. (7003553881); Kuehn, L.A. (7005675647)","24315220900; 57220766613; 6602198047; 57189499846; 57195117139; 7003553881; 7005675647","Development and validation of a neural network for the automated detection of horn flies on cattle","2021","Computers and Electronics in Agriculture","180","","105927","","","","9","10.1016/j.compag.2020.105927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097578872&doi=10.1016%2fj.compag.2020.105927&partnerID=40&md5=83009cd29794a4403489d824bd1f36b3","University of Nebraska, Lincoln, NE, United States; University of Tennessee, Knoxville, TN, United States; USDA, ARS, U.S. Meat Animal Research Center, Clay Center, NE, United States","Psota E.T., University of Nebraska, Lincoln, NE, United States; Luc E.K., University of Tennessee, Knoxville, TN, United States; Pighetti G.M., University of Tennessee, Knoxville, TN, United States; Schneider L.G., University of Tennessee, Knoxville, TN, United States; Trout Fryxell R.T., University of Tennessee, Knoxville, TN, United States; Keele J.W., USDA, ARS, U.S. Meat Animal Research Center, Clay Center, NE, United States; Kuehn L.A., USDA, ARS, U.S. Meat Animal Research Center, Clay Center, NE, United States","When the number of horn flies that blood feed on cattle exceeds the economic threshold, they can adversely affect the health and wellbeing of their hosts. Excessive horn fly burdens also lead to reduced weight gain and, consequently, diminished profits for livestock producers. Effective management and treatment require reliable surveillance methods for estimating the degree of horn fly burden (i.e., counting the number of flies on cattle). Traditionally, these estimates are obtained through human visual estimation, either in-person or by counting images on a photo; however, these methods are costly both in terms of time and labor and are prone to subjectivity and bias. In contrast, automated methods can expedite the counting process and remove subjectivity and bias. To this end, a 2-stage method is presented here that uses computer vision and deep learning to identify the location of flies in digital images. The first stage segments the salient cow from all other parts of the image to remove flies on neighboring cattle from consideration. The second stage processes full-resolution patches of the original image and produces a heat map at the location of flies in the images. The method was trained on a set of 375 human-annotated images and tested on 120 images, where significant variation was observed amongst the human scorers. Counting results are compared to four separate human scorers and demonstrate that the neural network produces consistent results and that the method is reliable. Thus, the developed method can be used for monitoring changes in horn fly populations over time by anyone and allows for increased rigor and repeatability. An examination of individual images where the method was closest to and farthest from the human counts provides valuable insights regarding photographic processes that lead to success and failure. © 2020 Elsevier B.V.","Deep learning; Fly counting; Horn flies; Precision livestock","Haematobia irritans; Muscidae; Agriculture; Deep learning; Neural networks; Automated detection; Automated methods; Counting process; Economic threshold; Effective management; Full resolutions; Monitoring change; Surveillance methods; artificial neural network; automation; blood; cattle; computer vision; fly; model validation; Image processing","OREI, (2015-51300-24140); National Institute of Food and Agriculture, NIFA","This study was partly supported by OREI grant no. 2015-51300-24140 from the USDA National Institute of Food and Agriculture. Additional support came from the S1076 Regional Hatch Project (Fly management in animal agriculture systems and impacts on animal health and food safety). We appreciate the dairy producers for agreeing to participate in the collection of pictures. Thank you to all the undergraduate students (Shanon Park, Adam Allen, Diondria Richards, Brett Anderson, Amanda Young, Caitlin Zaring, Pari Baker, Sarah Fiedler, Calyn Fulton, Emma Graf, Jalyn Westmoreland, Kaitlin Cracchiola, Malone Young, Tanner Thornton), graduate students (Hannah Malcomson), and research specialists (Dave Paulsen, Wyatt Simmerman, Collin McCorkel, Natasha Bales) that assisted with the data collection and annotations.","E.T. Psota; University of Nebraska, Lincoln, United States; email: epsota@unl.edu","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85097578872"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.; Olubummo D.A.; Enuma O.C.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900); Olubummo, Daniel A. (57216345013); Enuma, O. Charles (57221132880)","57209469141; 57190968285; 35570816900; 57216345013; 57221132880","Enhanced Deep Learning Framework for Cow Image Segmentation","2021","IAENG International Journal of Computer Science","48","4","IJCS_48_4_38","","","","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122454556&partnerID=40&md5=23be8863e48b73c8e1d8c21d5afa4861","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Research and Statistics, Ministry of Health, Bayelsa, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Olubummo D.A., Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Enuma O.C., Research and Statistics, Ministry of Health, Bayelsa, Nigeria","The applications of deep learning to livestock farming have in recent years gained wide acceptance from the computer vision community due to the continuous achievement of its applications to agricultural tasks. Moreover, the essentiality of deep learning is its practicality in detecting, segmenting, and classifying video and image objects without which precision livestock farming would have been impossible. However, the applications of most of the state-of-the-art models of deep learning to multiple cow objects image segmentation are not accurate and cannot generate colorimetric information due to poor pre-processing mechanism inherent in the associated methods and unequal training of their backbone layers. To overcome the above-mentioned limitations, an enhanced deep learning framework of Mask Region-based Convolutional Neural Network (Mask R-CNN) based on Generalized Color Fourier Descriptors (GCFD) is proposed. The enhanced model produced 0.93 mean Average Precision (mAP). The result shows the performance capability of the proposed framework over the state-of-the-art models for cow image segmentation © 2021. IAENG International Journal of Computer Science. All Rights Reserved.","Deep learning; Gcfd; Image segmentation; Mask r-cnn","Agriculture; Convolutional neural networks; Deep learning; Image enhancement; ART model; Deep learning; Gcfd; Images segmentations; ITS applications; Learning frameworks; Livestock farming; Mask r-cnn; State of the art; Vision communities; Image segmentation","","","R.-W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sirbrw@yahoo.com","","International Association of Engineers","1819656X","","","","English","IAENG Int. J. Comput. Sci.","Article","Final","","Scopus","2-s2.0-85122454556"
"Riekert M.; Klein A.; Adrion F.; Hoffmann C.; Gallmann E.","Riekert, Martin (57191991815); Klein, Achim (55429027200); Adrion, Felix (55798495500); Hoffmann, Christa (7103302650); Gallmann, Eva (6602626300)","57191991815; 55429027200; 55798495500; 7103302650; 6602626300","Automatically detecting pig position and posture by 2D camera imaging and deep learning","2020","Computers and Electronics in Agriculture","174","","105391","","","","124","10.1016/j.compag.2020.105391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084439306&doi=10.1016%2fj.compag.2020.105391&partnerID=40&md5=bfca86395eff4aa3c64478835797520e","Chair of Information Systems 2, University of Hohenheim, Schwerzstraße 35, Stuttgart, 70599, Germany; Research Division on Competitiveness and System Evaluation, Agroscope, Tänikon 1, Ettenhausen, 8356, Switzerland; Bildungs- und Wissenszentrum Boxberg, Seehöfer Straße 50, Boxberg-Windischbuch, 97944, Germany; Institute of Agricultural Engineering, University of Hohenheim, Garbenstraße 9, Stuttgart, 70599, Germany","Riekert M., Chair of Information Systems 2, University of Hohenheim, Schwerzstraße 35, Stuttgart, 70599, Germany; Klein A., Chair of Information Systems 2, University of Hohenheim, Schwerzstraße 35, Stuttgart, 70599, Germany; Adrion F., Research Division on Competitiveness and System Evaluation, Agroscope, Tänikon 1, Ettenhausen, 8356, Switzerland; Hoffmann C., Bildungs- und Wissenszentrum Boxberg, Seehöfer Straße 50, Boxberg-Windischbuch, 97944, Germany; Gallmann E., Institute of Agricultural Engineering, University of Hohenheim, Garbenstraße 9, Stuttgart, 70599, Germany","Prior livestock research provides evidence for the importance of accurate detection of pig positions and postures for better understanding animal welfare. Position and posture detection can be accomplished by machine vision systems. However, current machine vision systems require rigid setups of fixed vertical lighting, vertical top-view camera perspectives or complex camera systems, which hinder their adoption in practice. Moreover, existing detection systems focus on specific pen contexts and may be difficult to apply in other livestock facilities. Our main contribution is twofold: First, we design a deep learning system for position and posture detection that only requires standard 2D camera imaging with no adaptations to the application setting. This deep learning system applies the state-of-the-art Faster R-CNN object detection pipeline and the state-of-the-art Neural Architecture Search (NAS) base network for feature extraction. Second, we provide a labelled open access dataset with 7277 human-made annotations from 21 standard 2D cameras, covering 31 different one-hour long video recordings and 18 different pens to train and test the approach under realistic conditions. On unseen pens under similar experimental conditions with sufficient similar training images of pig fattening, the deep learning system detects pig position with an Average Precision (AP) of 87.4%, and pig position and posture with a mean Average Precision (mAP) of 80.2%. Given different and more difficult experimental conditions of pig rearing with no or little similar images in the training set, an AP of over 67.7% was achieved for position detection. However, detecting the position and posture achieved a mAP between 44.8% and 58.8% only. Furthermore, we demonstrate exemplary applications that can aid pen design by visualizing where pigs are lying and how their lying behavior changes through the day. Finally, we contribute open data that can be used for further studies, replication, and pig position detection applications. © 2020 The Authors","2D vision cameras; Deep learning; Object detection; Pig behavior; Pig position and posture detection","Animalia; Suidae; Agriculture; Cameras; Computer vision; Facsimile; Feature extraction; Learning systems; Machinery; Object detection; Open Data; Statistical tests; Video recording; Camera perspectives; Detection system; Experimental conditions; Machine vision systems; Neural architectures; Position detection; Posture detection; Realistic conditions; detection method; imaging method; machine learning; pig; posture; Deep learning","LabelFit project team; Ministry of Rural Development and Consumer Protection of Baden-Württemberg; Bundesministerium für Ernährung und Landwirtschaft, BMEL; Bundesanstalt für Landwirtschaft und Ernährung, BLE, (2819200415)","Funding text 1: The authors would like to thank the staff at the Boxberg Teaching and Research Centre – Centre for pig rearing and pig breeding (LSZ Boxberg) for their support and advise during the experiments, especially Hansjörg Schrade, Barbara Keßler, Andrea Wild, Max Staiger, Karen Kauselmann and William Gordillo. We also thank the student assistants Tobias Zimpel and Endrit Tejeci for annotating the test set used in our studies. We thank the LabelFit project team, especially Svenja Opderbeck and Benedikt Glitz, who allowed us to use their video dataset. We thank Prof. Dr. Stefan Kirn and Dr. Marcus Müller (University of Hohenheim) for discussions and ideas. We thank Dr. Jörg Leukel (University of Hohenheim) for his comments on an earlier version of the manuscript. Finally, we thank the machine learning community for providing many free resources for our research. This work was supported by the project “Landwirtschaft 4.0: Info-System”, funded by the Ministry of Rural Development and Consumer Protection of Baden-Württemberg, Germany. This work was also supported by the project LabelFit by funds of the Federal Ministry of Food and Agriculture (BMEL) based on a decision of the Parliament of the Federal Republic of Germany via the Federal Office for Agriculture and Food (BLE) under the innovation support programme (2819200415).; Funding text 2: This work was supported by the project “Landwirtschaft 4.0: Info-System”, funded by the Ministry of Rural Development and Consumer Protection of Baden-Württemberg, Germany . This work was also supported by the project LabelFit by funds of the Federal Ministry of Food and Agriculture (BMEL) based on a decision of the Parliament of the Federal Republic of Germany via the Federal Office for Agriculture and Food (BLE) under the innovation support programme ( 2819200415 ).","M. Riekert; Chair of Information Systems 2, University of Hohenheim, Stuttgart, Schwerzstraße 35, 70599, Germany; email: martin.riekert@uni-hohenheim.de","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85084439306"
"Almeida H.S.L.; Dos Reis A.A.; Werner J.P.S.; Antunes J.F.G.; Zhong L.; Figueiredo G.K.D.A.; Esquerdo J.C.D.M.; Coutinho A.C.; Lamparelli R.A.C.; Magalhães P.S.G.","Almeida, Henrique S.L. (57221452220); Dos Reis, Aliny A. (55320336300); Werner, João P.S. (57212420665); Antunes, João F.G. (15020172200); Zhong, Liheng (54782083000); Figueiredo, Gleyce K.D.A. (57185295400); Esquerdo, Júlio C.D.M. (15020462500); Coutinho, Alexandre C. (55513012600); Lamparelli, Rubens A.C. (6602713722); Magalhães, Paulo S.G. (7003731076)","57221452220; 55320336300; 57212420665; 15020172200; 54782083000; 57185295400; 15020462500; 55513012600; 6602713722; 7003731076","DEEP NEURAL NETWORKS FOR MAPPING INTEGRATED CROP-LIVESTOCK SYSTEMS USING PLANETSCOPE TIME SERIES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","4224","4227","3","4","10.1109/IGARSS47720.2021.9554500","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129808429&doi=10.1109%2fIGARSS47720.2021.9554500&partnerID=40&md5=b0549ea1097f75d87a918d83b3186263","Interdisciplinary Center of Energy Planning, University of Campinas, UNICAMP, SP, Campinas, Brazil; School of Agricultural Engineering, University of Campinas, UNICAMP, SP, Campinas, Brazil; Embrapa Agricultural Informatics, Brazilian Agricultural Research Corporation, Campinas, SP Embrapa, Brazil; Ant Group, World Financial Center, Beijing, China","Almeida H.S.L., Interdisciplinary Center of Energy Planning, University of Campinas, UNICAMP, SP, Campinas, Brazil; Dos Reis A.A., Interdisciplinary Center of Energy Planning, University of Campinas, UNICAMP, SP, Campinas, Brazil, School of Agricultural Engineering, University of Campinas, UNICAMP, SP, Campinas, Brazil; Werner J.P.S., School of Agricultural Engineering, University of Campinas, UNICAMP, SP, Campinas, Brazil; Antunes J.F.G., Embrapa Agricultural Informatics, Brazilian Agricultural Research Corporation, Campinas, SP Embrapa, Brazil; Zhong L., Ant Group, World Financial Center, Beijing, China; Figueiredo G.K.D.A., School of Agricultural Engineering, University of Campinas, UNICAMP, SP, Campinas, Brazil; Esquerdo J.C.D.M., Embrapa Agricultural Informatics, Brazilian Agricultural Research Corporation, Campinas, SP Embrapa, Brazil; Coutinho A.C., Embrapa Agricultural Informatics, Brazilian Agricultural Research Corporation, Campinas, SP Embrapa, Brazil; Lamparelli R.A.C., Interdisciplinary Center of Energy Planning, University of Campinas, UNICAMP, SP, Campinas, Brazil; Magalhães P.S.G., Interdisciplinary Center of Energy Planning, University of Campinas, UNICAMP, SP, Campinas, Brazil","Mapping highly dynamic cropping systems using satellite image time series is still challenging even when robust approaches are used. We assessed the potential of using high spatial and temporal resolution PlanetScope time series and deep neural networks (Convolutional Neural Networks (CNN) in one dimension - Conv1D, Long Short-Term Memory (LSTM), and Multi-Layer Perceptron (MLP)) for mapping integrated crop-livestock systems (ICLS) and different land covers in the western region of São Paulo State, Brazil. We used 10-day and 15-day composite EVI and NDVI time series (both individually and combined) as input data in the neural network classifiers. Conv1D using both EVI and NDVI 10 day-composite time series outperformed the other classifiers evaluated in this study (LSTM and MLP), allowing improved discrimination of land parcels with ICLS in our study area. ©2021 IEEE","Convolutional Neural Networks; Deep learning; EVI; Nano-Satellites; NDVI","Convolution; Convolutional neural networks; Deep neural networks; Long short-term memory; Mapping; Multilayer neural networks; Time series; Convolutional neural network; Cropping systems; Deep learning; EVI; Livestock systems; Memory layers; Multilayers perceptrons; Nano-satellites; NDVI; Times series; Crops","Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, (2017/50205-9, 2018/24985-0, 2019/16870-0)","The authors would like to thank FAPESP for the financial support of this study (Grants: 2019/16870-0, 2018/24985-0, and 2017/50205-9) and Jansle V. Rocha, Ricardo Torres, and Allan Pinto for their contribution to this research.","","","Institute of Electrical and Electronics Engineers Inc.","","","IGRSE","","English","Dig Int Geosci Remote Sens Symp (IGARSS)","Conference paper","Final","","Scopus","2-s2.0-85129808429"
"Wang R.; Shi Z.; Li Q.; Gao R.; Zhao C.; Feng L.","Wang, Rong (57221059245); Shi, Zaifeng (57322569700); Li, Qifeng (57205964175); Gao, Ronghua (24824729600); Zhao, Chunjiang (57303287700); Feng, Lu (57223331687)","57221059245; 57322569700; 57205964175; 24824729600; 57303287700; 57223331687","PIG FACE RECOGNITION MODEL BASED on A CASCADED NETWORK","2021","Applied Engineering in Agriculture","37","5","","879","890","11","10","10.13031/aea.14482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118507980&doi=10.13031%2faea.14482&partnerID=40&md5=ad72c3a5844129170aa3712a5af85560","College of Information Engineering, Northwest Agriculture and Forestry University, Shaanxi, Yangling, China; Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China; Tianjin University, Tianjin, Tianjin, China; National Engineering Research Center for Information Technology in Agriculture, Beijing, Beijing, China; Key Laboratory for Information Technologies in Agriculture, Beijing, Beijing, China","Wang R., College of Information Engineering, Northwest Agriculture and Forestry University, Shaanxi, Yangling, China, Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China; Shi Z., Tianjin University, Tianjin, Tianjin, China; Li Q., Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, Beijing, China, Key Laboratory for Information Technologies in Agriculture, Beijing, Beijing, China; Gao R., Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, Beijing, China, Key Laboratory for Information Technologies in Agriculture, Beijing, Beijing, China; Zhao C., College of Information Engineering, Northwest Agriculture and Forestry University, Shaanxi, Yangling, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, Beijing, China; Feng L., Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China, National Engineering Research Center for Information Technology in Agriculture, Beijing, Beijing, China, Key Laboratory for Information Technologies in Agriculture, Beijing, Beijing, China","The identification and tracking of livestock using artificial intelligence technology have been a research hotspot in recent years. Automatic individual recognition is the key to realizing intelligent feeding. Although RFID can achieve identification tasks, it is expensive and easily fails. In this article, a pig face recognition model that cascades a pig face detection network and a pig face recognition network is proposed. First, the pig face detection network is utilized to crop the pig face images from videos and eliminate the complex background of the pig shed. Second, batch normalization, dropout, skip connection, and residual modules are exploited to design a pig face recognition network for individual identification. Finally, the cascaded network model based on the pig face detection and recognition network is deployed on a GPU server, and an application is developed to automatically recognize individual pigs. Additionally, class activation maps generated by grad-CAM are used to analyze the performance of features of pig faces learned by the model. Under free and unconstrained conditions, 46 pigs are selected to make a positive pig face dataset, original multiangle pig face dataset and enhanced multiangle pig face dataset to verify the pig face recognition cascaded model. The proposed cascaded model reaches accuracies of 99.38%, 98.96%, and 97.66% on the three datasets, which are higher than those of other pig face recognition models. The results of this study improved the recognition performance of pig faces under multiangle and multienvironment conditions. © 2021 American Society of Agricultural and Biological Engineers","CNN; Deep learning; Pig face detection; Pig face recognition","Agriculture; Deep learning; Mammals; CNN; Deep learning; Detection networks; Faces detection; Model-based OPC; Multi angle; Performance; Pig face detection; Pig face recognition; Recognition models; Face recognition","Tianjin Research and Development Program Key Projects, (20YFZCSN00220); Youths Foundation Project of Beijing Academy of Agricultural and Forestry Sciences, (QNJJ201913); Natural Science Foundation of Beijing Municipality, (4202029); Natural Science Foundation of Beijing Municipality","This article was supported by the Natural Science Foundation of Beijing, China (No. 4202029), Youths Foundation Project of Beijing Academy of Agricultural and Forestry Sciences (No. QNJJ201913), and Tianjin Research and Development Program Key Projects (No. 20YFZCSN00220).","R. Gao; Beijing Research Center for Information Technology in Agriculture, Beijing, Beijing, China; email: gaorh@nercita.org.cn; C. Zhao; College of Information Engineering, Northwest Agriculture and Forestry University, Yangling, Shaanxi, China; email: zhaocj@nercita.org.cn","","American Society of Agricultural and Biological Engineers","08838542","","AEAGE","","English","Appl Eng Agric","Article","Final","","Scopus","2-s2.0-85118507980"
"Chen P.; Tang W.; Yin D.; Yang B.","Chen, Peng (57226026376); Tang, Wengsheng (23390830100); Yin, Dan (57226593055); Yang, Bo (58833716500)","57226026376; 23390830100; 57226593055; 58833716500","Sow Estrus Diagnosis from Sound Samples Based on Improved Deep Learning","2021","Communications in Computer and Information Science","1422","","","132","143","11","5","10.1007/978-3-030-78615-1_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112041131&doi=10.1007%2f978-3-030-78615-1_12&partnerID=40&md5=dcced2edb17d52620b8a5ebaee7daa68","College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China","Chen P., College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China; Tang W., College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China; Yin D., College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China; Yang B., College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China","The inability to diagnose sow estrus at scale has become the burning issues in the livestock farming. An intelligent screening tool would be a game changer. Changing the dependence on the traditional method of artificial detection, this paper proposes, develops and uses an Artificial Intelligence-based screening solution for sow estrus detection. Sow estrus call is one of the significant calls which can reflect the physiological conditions. This makes the recognition of sow estrus call an exceedingly challenging problem. This problem is addressed by investigating the distinctness of Mel spectrum in the sound system generated in estrus when compared to other pig sounds. To overcome the lack of sow estrus sounds training data, transfer learning is exploited. To reduce the risk of misdiagnosis we raise a two-pronged AI architecture, one is collecting and transmitting sound to the detector, the other is identifying calls in sow estrus diagnosis system. Results show the AI architecture can distinguish among sow estrus calls and other non-sow-estrus-calls. The performance is good enough to encourage a large-scale collection of labeled sow estrus call data to improve the generalization capability of the AI architecture. © 2021, Springer Nature Switzerland AG.","Deep learning; Speech recognition; Transfer learning","Agriculture; Architecture; Mammals; Transfer learning; Diagnosis systems; Generalization capability; Intelligent screenings; Livestock farming; Physiological condition; Sound sample; Sound systems; Training data; Deep learning","","","W. Tang; College of Information Science and Engineering, Hunan Normal University, Changsha, 410000, China; email: tangws@hunnu.edu.cn","Sun X.; Zhang X.; Xia Z.; Bertino E.","Springer Science and Business Media Deutschland GmbH","18650929","978-303078614-4","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85112041131"
"Sarwar F.; Griffin A.; Chong P.H.J.; Pasang T.","Sarwar, Farah (57224466004); Griffin, Anthony (55708552200); Chong, Peter Han Joo (7102970085); Pasang, Timotius (56962784100)","57224466004; 55708552200; 7102970085; 56962784100","Pasture Fence Line Detection in UAV Videos","2021","International Conference Image and Vision Computing New Zealand","2021-December","","","","","","1","10.1109/IVCNZ54163.2021.9653285","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124380423&doi=10.1109%2fIVCNZ54163.2021.9653285&partnerID=40&md5=a92b452a99064ce96fd911f8905655ab","Auckland University of Technology, Department of Electrical and Electronic Engineering, Auckland, New Zealand; Auckland University of Technology, Department of Computer Science, Auckland, New Zealand; Oregon Institute of Technology, Department of Manufacturing and Mechanical Engineering and Technology, Oregon, United States","Sarwar F., Auckland University of Technology, Department of Electrical and Electronic Engineering, Auckland, New Zealand; Griffin A., Auckland University of Technology, Department of Computer Science, Auckland, New Zealand; Chong P.H.J., Auckland University of Technology, Department of Electrical and Electronic Engineering, Auckland, New Zealand; Pasang T., Oregon Institute of Technology, Department of Manufacturing and Mechanical Engineering and Technology, Oregon, United States","Farms in many countries use fenced pastures - known as paddocks - for keeping livestock. These farms usually spread over many hectares with paddocks of various shapes and sizes. Farm managers face the difficulty of counting their animals on regular basis and recently the use of an unmanned aerial vehicle (UAV) has been proposed for counting livestock. To get the data from only a respective paddock, the fence lines must also be detected correctly in the aerial images and videos. This will help to keep the data from each paddock separate. A unique system based on a combination of deep learning and machine learning is proposed to accurately detect the fence lines in the videos recorded by a UAV at an altitude of 80 m.  © 2021 IEEE.","Computer vision; DBSCAN; Deep learning; Fence detection; UAV","Agriculture; Aircraft detection; Antennas; Computer vision; Deep learning; Fences; Aerial images; Aerial video; DBSCAN; Deep learning; Fence detection; Line detection; Shape and size; Unmanned aerial vehicles (UAV)","","","","Cree M.J.","IEEE Computer Society","21512191","978-166540645-1","","","English","Int. Conf. Image Vis. Comput. New Zealand","Conference paper","Final","","Scopus","2-s2.0-85124380423"
"Karegowda A.G.; Devika G.; Geetha M.","Karegowda, Asha Gowda (26639459000); Devika, G. (56880333600); Geetha, M. (58375989700)","26639459000; 56880333600; 58375989700","Deep learning solutions for agricultural and farming activities","2020","Deep Learning Applications and Intelligent Decision Making in Engineering","","","","256","287","31","4","10.4018/978-1-7998-2108-3.ch011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123161998&doi=10.4018%2f978-1-7998-2108-3.ch011&partnerID=40&md5=74c4558908de5be8e430a38828d87ba9","Siddaganga Institute of Technology, India; Government Engineering College, India; Gulbarga University, Bapuji Institute of Engineering and Technology (BIET), India","Karegowda A.G., Siddaganga Institute of Technology, India; Devika G., Government Engineering College, India; Geetha M., Gulbarga University, Bapuji Institute of Engineering and Technology (BIET), India","The continuously growing population throughout globe demands an ample food supply, which is one of foremost challenge of smart agriculture. Timely and precise identification of weeds, insects, and diseases in plants are necessary for increased crop yield to satisfy demand for sufficient food supply. With fewer experts in this field, there is a need to develop an automated system for predicting yield, detection of weeds, insects, and diseases in plants. In addition to plants, livestock such as cattle, pigs, and chickens also contribute as major food. Hence, livestock demands precision methods for reducing the mortality rate of livestock by identifying diseases in livestock. Deep learning is one of the upcoming technologies that when combined with image processing promises smart agriculture to be a reality. Various applications of DL for smart agriculture are covered. © 2021 by IGI Global. All rights reserved.","","","","","","","IGI Global","","978-179982110-6; 978-179982108-3","","","English","Deep Learning Applications and intell. decis. mak. in eng.","Book chapter","Final","","Scopus","2-s2.0-85123161998"
"Lin R.; Hu H.; Wen Z.; Yin L.","Lin, Runheng (57425114400); Hu, Hao (57425400400); Wen, Zhikun (57425400500); Yin, Ling (36024801300)","57425114400; 57425400400; 57425400500; 36024801300","Research on denoising and segmentation algorithm application of pigs' point cloud based on DBSCAN and PointNet","2021","2021 IEEE International Workshop on Metrology for Agriculture and Forestry, MetroAgriFor 2021 - Proceedings","","","","42","47","5","8","10.1109/MetroAgriFor52389.2021.9628501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123397271&doi=10.1109%2fMetroAgriFor52389.2021.9628501&partnerID=40&md5=a992d841fdf33bb70953d9eb06ecdc3a","South China Agricultural University, College of Mathematics and Informatics, Guangzhou, China","Lin R., South China Agricultural University, College of Mathematics and Informatics, Guangzhou, China; Hu H., South China Agricultural University, College of Mathematics and Informatics, Guangzhou, China; Wen Z., South China Agricultural University, College of Mathematics and Informatics, Guangzhou, China; Yin L., South China Agricultural University, College of Mathematics and Informatics, Guangzhou, China","At present, 3D vision technology has been more and more applied in the field of livestock breading industry. Three depth cameras are installed in the data acquisition walkway from different views, as the animals pass through the walkway, the depth cameras will get the local point cloud of livestock at the same time. By this method the livestock point cloud exists inevitable noise, as well as the background point cloud including railing and ground. Therefore, the segmentation of livestock point cloud from the original point cloud mixed with noise and background is the important first step of the application of three-dimensional vision in livestock breading industry. This paper proposed using the PointNet++ model to separate the target point cloud from the background point cloud. The improved DBSCAN clustering algorithm is proposed to denoise and segment the data preliminarily, which is helpful to label the data manually. The labeled data are used to train the PointNet++ neural network model to extract target point cloud efficiently and accurately. The experimental results show that the mean Intersection over Union (MIOU) of pig target point cloud segmentation based on single-scale grouping and multi-scale grouping Pointnet++ are 0.932 and 0.940, respectively, which both obtain good segmentation results.  © 2021 IEEE.","Cluster; Deep Learning; Point Cloud Segmentation; PointNet++","Cameras; Clustering algorithms; Data acquisition; Deep learning; Image segmentation; Mammals; Breadings; Cluster; De-noising algorithm; Deep learning; Depth camera; Point cloud segmentation; Point-clouds; Pointnet++; Segmentation algorithms; Target point; Agriculture","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540533-1","","","English","IEEE Int. Workshop Metrol. Agric. For., MetroAgriFor - Proc.","Conference paper","Final","","Scopus","2-s2.0-85123397271"
"Sarwar F.; Griffin A.; Pasang T.","Sarwar, Farah (57224466004); Griffin, Anthony (55708552200); Pasang, Timotius (56962784100)","57224466004; 55708552200; 56962784100","Tracking Livestock Using a Fully Connected Network and Kalman Filter","2021","Communications in Computer and Information Science","1386 CCIS","","","247","261","14","1","10.1007/978-3-030-72073-5_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104856208&doi=10.1007%2f978-3-030-72073-5_19&partnerID=40&md5=e79d5539fa9a7bc1b3f26ca63bba9768","Electrical and Electronic Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand; Mechanical Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand","Sarwar F., Electrical and Electronic Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand; Griffin A., Electrical and Electronic Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand; Pasang T., Mechanical Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand","Multiple object tracking (MOT) consists of following the trajectories of different objects in a video with either fixed or moving background. In recent years, the use of deep learning for MOT in the videos recorded by unmanned aerial vehicles (UAVs) has introduced more challenges and hence has a lot of room for extensive research. For the tracking-by-detection method, the three main components, object detector, tracker and data associator, play an equally important role and each part should be tuned to the highest efficiency to increase the overall performance. In this paper, the parameter selection of the Kalman filter and Hungarian algorithm for sheep tracking in paddock videos is discussed. An experimental comparison is presented to show that if the detector is already providing good results, a small change in the system can degrade or improve the tracking capabilities of remaining components. The encouraging results provide an important step in an automated UAV-based sheep tracking system. © 2021, Springer Nature Switzerland AG.","CNN; Kalman filter; Livestock; Tracking; UAV","Agriculture; Aircraft detection; Antennas; Deep learning; Kalman filters; Object detection; Unmanned aerial vehicles (UAV); Experimental comparison; Fully connected networks; Hungarian algorithm; Multiple object tracking; Parameter selection; Remaining component; Tracking by detections; Tracking capability; Object tracking","","","A. Griffin; Electrical and Electronic Engineering Department, Auckland University of Technology, Auckland, 1142, New Zealand; email: anthony.griffin@aut.ac.nz","Nguyen M.; Yan W.Q.; Ho H.","Springer Science and Business Media Deutschland GmbH","18650929","978-303072072-8","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85104856208"
"Nosratabadi S.; Ardabili S.; Lakner Z.; Mako C.; Mosavi A.","Nosratabadi, Saeed (57192426323); Ardabili, Sina (57189099323); Lakner, Zoltan (6506033503); Mako, Csaba (6508289335); Mosavi, Amir (57191408081)","57192426323; 57189099323; 6506033503; 6508289335; 57191408081","Prediction of food production using machine learning algorithms of multilayer perceptron and anfis","2021","Agriculture (Switzerland)","11","5","408","","","","78","10.3390/agriculture11050408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106575281&doi=10.3390%2fagriculture11050408&partnerID=40&md5=0a240b58c6c4eea275f85f0077c74a4c","Doctoral School of Economic and Regional Sciences, Hungarian University of Agriculture and Life Sciences, Godollo, 2100, Hungary; Department of Biosystem Engineering, University of Mohaghegh Ardabili, Ardabil, 5619911367, Iran; Institute of Economic Sciences, Hungarian University of Agriculture and Life Sciences, Godollo, 2100, Hungary; nstitute of Information Society, University of Public Service, Budapest, 1083, Hungary; Faculty of Civil Engineering, Technische Universitat Dresden, Dresden, 01069, Germany; John von Neumann Faculty of Informatics, Obuda University, Budapest, 1034, Hungary; Information Systems, University of Siegen, Siegen, 57072, Germany","Nosratabadi S., Doctoral School of Economic and Regional Sciences, Hungarian University of Agriculture and Life Sciences, Godollo, 2100, Hungary; Ardabili S., Department of Biosystem Engineering, University of Mohaghegh Ardabili, Ardabil, 5619911367, Iran; Lakner Z., Institute of Economic Sciences, Hungarian University of Agriculture and Life Sciences, Godollo, 2100, Hungary; Mako C., nstitute of Information Society, University of Public Service, Budapest, 1083, Hungary; Mosavi A., Faculty of Civil Engineering, Technische Universitat Dresden, Dresden, 01069, Germany, John von Neumann Faculty of Informatics, Obuda University, Budapest, 1034, Hungary, Information Systems, University of Siegen, Siegen, 57072, Germany","Advancing models for accurate estimation of food production is essential for policymaking and managing national plans of action for food security. This research proposes two machine learning models for the prediction of food production. The adaptive network-based fuzzy inference system (ANFIS) and multilayer perceptron (MLP) methods are used to advance the prediction models. In the present study, two variables of livestock production and agricultural production were considered as the source of food production. Three variables were used to evaluate livestock production, namely livestock yield, live animals, and animal slaughtered, and two variables were used to assess agricultural production, namely agricultural production yields and losses. Iran was selected as the case study of the current study. Therefore, time-series data related to livestock and agricultural productions in Iran from 1961 to 2017 have been collected from the FAOSTAT database. First, 70% of this data was used to train ANFIS and MLP, and the remaining 30% of the data was used to test the models. The results disclosed that the ANFIS model with generalized bell-shaped (Gbell) built-in membership functions has the lowest error level in predicting food production. The findings of this study provide a suitable tool for policymakers who can use this model and predict the future of food production to provide a proper plan for the future of food security and food supply for the next generations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Agricultural production; Artificial intelligence; Big data; Data science; Data-driven decision making; Deep learning; Food demand; Food production; Forecasting; Machine learning; Prediction","","","","C. Mako; nstitute of Information Society, University of Public Service, Budapest, 1083, Hungary; email: mako.csaba@tk.mta.hu; A. Mosavi; Faculty of Civil Engineering, Technische Universitat Dresden, Dresden, 01069, Germany; email: amir.mosavi@mailbox.tu-dresden.de","","MDPI AG","20770472","","","","English","Agric.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85106575281"
"","","","11th International Symposium on Ambient Intelligence, ISAmI 2020","2021","Advances in Intelligent Systems and Computing","1239 AISC","","","","","312","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091486536&partnerID=40&md5=50b5f5774d838257e08905eb2b7c0386","","","The proceedings contain 37 papers. The special focus in this conference is on Ambient Intelligence. The topics include: Detection violent behaviors: A survey; system for recommending financial products adapted to the user’s profile; a cots (uhf) rfid floor for device-free ambient assisted living monitoring; using jason framework to develop a multi-agent system to manage users and spaces in an adaptive environment system; towards the development of iot protocols; livestock welfare by means of an edge computing and iot platform; sleep performance and physical activity estimation from multisensor time series sleep environment data; face detection and recognition, face emotion recognition through nvidia jetson nano; video analysis system using deep learning algorithms; society of “citizen science through dancing”; towards learning travelers’ preferences in a context-aware fashion; reputation algorithm for users and activities in a public transport oriented application; extraction of travellers’ preferences using their tweets; adaptivity as a service (aaas): Personalised assistive robotics for ambient assisted living; time in multi-agent systems; public tendering processes based on blockchain technologies; low-power distributed ai and iot for measuring lamb’s milk ingestion and predicting meat yield and malnutrition diseases; clifford algebras: A proposal towards improved image recognition in machine learning; new approach to recommend banking products through a hybrid recommender system; an iot-based rouv for environmental monitoring; the activage marketplace: Hybrid logic- and text-based discovery of active and healthy ageing iot applications; deep symbolic learning and semantics for an explainable and ethical artificial intelligence; development of a multiagent simulator to genetic regulatory networks; explainable intelligent environments.","","","","","","Novais P.; Vercelli G.; Larriba-Pey J.L.; Herrera F.; Chamoso P.","Springer Science and Business Media Deutschland GmbH","21945357","978-303058355-2","","","English","Adv. Intell. Sys. Comput.","Conference review","Final","","Scopus","2-s2.0-85091486536"
"Tiwari A.; Sachdeva K.; Jain N.","Tiwari, Anurag (57212912237); Sachdeva, Kavita (57463371500); Jain, Neha (57642909600)","57212912237; 57463371500; 57642909600","Computer Vision and Deep Learningbased Framework for Cattle Monitoring","2021","2021 IEEE 8th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering, UPCON 2021","","","","","","","8","10.1109/UPCON52273.2021.9667617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125043303&doi=10.1109%2fUPCON52273.2021.9667617&partnerID=40&md5=9ec7332eb3c99fa0261836a5d5a148c5","Babu Banarasi das Institute of Technology Management, Lucknow, India","Tiwari A., Babu Banarasi das Institute of Technology Management, Lucknow, India; Sachdeva K., Babu Banarasi das Institute of Technology Management, Lucknow, India; Jain N., Babu Banarasi das Institute of Technology Management, Lucknow, India","Precision livestock farming techniques enable us to obtain an accurate count of individual animals in a timely manner. Cattle counting and tracking are closely related to animal welfare in dairy farming. To accomplish this task, a computer vision framework is proposed in which ResNetV2 extracts features with the help of the optimizer YOLOv4, which significantly improves detection speed and accuracy.,The output will be updated in the centralised repository via IoT sensors and inconsistencies detected will be immediately communicated to the respective farmer. The proposed framework will assist government officials in taking corrective measures through livestock tracking in order to provide farmers with an efficient, quick, and accurate cattle count.  © 2021 IEEE.","Cattle Monitoring; Granular Computing; IoT; ResNet","Animals; Computer vision; Deep learning; Internet of things; Animal welfare; Cattle monitoring; Centralised; Dairy farming; Detection accuracy; Detection speed; Optimizers; Precision livestock farming; Resnet; Vision frameworks; Agriculture","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-166540962-9","","","English","IEEE Uttar Pradesh Sect. Int. Conf. Electr., Electron. Comput. Eng., UPCON","Conference paper","Final","","Scopus","2-s2.0-85125043303"
"Castro W.; Junior J.M.; Polidoro C.; Osco L.P.; Gonçalves W.; Rodrigues L.; Santos M.; Jank L.; Barrios S.; Valle C.; Simeão R.; Carromeu C.; Silveira E.; Jorge L.A.C.; Matsubara E.","Castro, Wellington (57218627975); Junior, José Marcato (55640064500); Polidoro, Caio (57211181271); Osco, Lucas Prado (57196329154); Gonçalves, Wesley (23396539500); Rodrigues, Lucas (57201520796); Santos, Mateus (57199791589); Jank, Liana (55953785100); Barrios, Sanzio (57078381600); Valle, Cacilda (6601992769); Simeão, Rosangela (6701415911); Carromeu, Camilo (36100083100); Silveira, Eloise (57218625594); Jorge, Lúcio André de Castro (56406211700); Matsubara, Edson (23393072700)","57218627975; 55640064500; 57211181271; 57196329154; 23396539500; 57201520796; 57199791589; 55953785100; 57078381600; 6601992769; 6701415911; 36100083100; 57218625594; 56406211700; 23393072700","Deep learning applied to phenotyping of biomass in forages with uav-based rgb imagery","2020","Sensors (Switzerland)","20","17","4802","1","18","17","59","10.3390/s20174802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089832181&doi=10.3390%2fs20174802&partnerID=40&md5=8cfa81bf6ac431ecb83f4015ad894e11","Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Faculty of Engineering, Architecture and Urbanism, University of Western São Paulo, Presidente Prudente, 19067175, SP, Brazil; Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Embrapa Instrumentation, São Carlos, 13560970, SP, Brazil","Castro W., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Junior J.M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Polidoro C., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Osco L.P., Faculty of Engineering, Architecture and Urbanism, University of Western São Paulo, Presidente Prudente, 19067175, SP, Brazil; Gonçalves W., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Rodrigues L., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Santos M., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Jank L., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Barrios S., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Valle C., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Simeão R., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Carromeu C., Embrapa Beef Cattle, Brazilian Agricultural Research Corporation, Campo Grande, 79106550, MS, Brazil; Silveira E., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil; Jorge L.A.C., Embrapa Instrumentation, São Carlos, 13560970, SP, Brazil; Matsubara E., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, MS, Brazil","Monitoring biomass of forages in experimental plots and livestock farms is a time-consuming, expensive, and biased task. Thus, non-destructive, accurate, precise, and quick phenotyping strategies for biomass yield are needed. To promote high-throughput phenotyping in forages, we propose and evaluate the use of deep learning-based methods and UAV (Unmanned Aerial Vehicle)-based RGB images to estimate the value of biomass yield by different genotypes of the forage grass species Panicum maximum Jacq. Experiments were conducted in the Brazilian Cerrado with 110 genotypes with three replications, totaling 330 plots. Two regression models based on Convolutional Neural Networks (CNNs) named AlexNet and ResNet18 were evaluated, and compared to VGGNet—adopted in previous work in the same thematic for other grass species. The predictions returned by the models reached a correlation of 0.88 and a mean absolute error of 12.98% using AlexNet considering pre-training and data augmentation. This proposal may contribute to forage biomass estimation in breeding populations and livestock areas, as well as to reduce the labor in the field. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Biomass yield; Convolutional Neural Network; Data augmentation; Phenotyping","Animal Feed; Animals; Biomass; Brazil; Deep Learning; Livestock; Phenotype; Plants; Remote Sensing Technology; Agriculture; Antennas; Biomass; Convolutional neural networks; Regression analysis; Unmanned aerial vehicles (UAV); Biomass estimation; Brazilian cerrado; Breeding populations; Experimental plots; High-throughput phenotyping; Learning-based methods; Mean absolute error; UAV (unmanned aerial vehicle); animal; animal food; biomass; Brazil; classification; livestock; phenotype; plant; remote sensing; Deep learning","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Empresa Brasileira de Pesquisa Agropecuária, EMBRAPA; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303559/2019-5, 433783/2018-4); Fundação de Apoio ao Desenvolvimento do Ensino, Ciência e Tecnologia do Estado de Mato Grosso do Sul, FUNDECT, (59/300.075/2015); Ministério da Educação, MEC; Associação para o Fomento à Pesquisa de Melhoramento de Forrageiras, UNIPASTO; Universidade Federal de Mato Grosso do Sul, UFMS","Funding: This research was funded by Fundação de Apoio ao Desenvolvimento do Ensino, Ciência e Tecnologia do Estado do Mato Grasso do Sul (FUNDECT-MS) grant number 59/300.075/2015, Empresa Brasileira de Pesquisa Agropecuária (EMBRAPA), and Associaçao para o Fomento à Pesquisa de Melhormento de Forrageiras (UNIPASTO), CNPq (433783/2018-4, and 303559/2019-5). The authors acknowledge the support of the Universidade Federal de Mato Grosso do Sul (Federal University of Mato Grosso do Sul)—UFMS/MEC—Brasil. Finally, this study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior—Brasil (CAPES)—Finance Code 001.","J.M. Junior; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070900, Brazil; email: jose.marcato@ufms.br","","MDPI AG","14248220","","","32858803","English","Sensors","Letter","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089832181"
"Fuentes A.; Yoon S.; Park J.; Park D.S.","Fuentes, Alvaro (57194569998); Yoon, Sook (35779575000); Park, Jongbin (56095724400); Park, Dong Sun (7403245797)","57194569998; 35779575000; 56095724400; 7403245797","Deep learning-based hierarchical cattle behavior recognition with spatio-temporal information","2020","Computers and Electronics in Agriculture","177","","105627","","","","101","10.1016/j.compag.2020.105627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089937288&doi=10.1016%2fj.compag.2020.105627&partnerID=40&md5=fafbf6e3e2fcc8ef6a28b0506f316571","Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea; Department of Computer Engineering, Mokpo National University, Jeonnam, South Korea; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China; Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju, South Korea","Fuentes A., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea; Yoon S., Department of Computer Engineering, Mokpo National University, Jeonnam, South Korea; Park J., Department of Electronics Engineering, Jeonbuk National University, Jeonju, South Korea; Park D.S., College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China, Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju, South Korea","Behavior is an important indicator for understanding the well-being of animals. This process has been frequently carried out by observing video records to detect changes with statistical analysis, or by using portable devices to monitor animal movements. However, regarding animal welfare, the use of such devices could affect the normal behavior of the animal, and present limitations in its applicability. This paper introduces an approach for hierarchical cattle behavior recognition with spatio-temporal information based on deep learning. Our research extends the idea of activity recognition in video and focuses specifically on cattle behavior. Our framework involves appearance features at frame-level and spatio-temporal information that incorporates more context-temporal features. The system can detect (class) and localize (bounding box) regions containing multiple cattle behaviors in the video frames. Additionally, we introduce our cattle behavior dataset that includes videos recorded with RGB cameras on different livestock farms during day and night environments. Experimental results show that our system can effectively recognize 15 different types of hierarchical activities divided into individual and group activities, and also part actions. Qualitative and quantitative evaluation evidence the performance of our framework as an effective method to monitor cattle behavior. © 2020","Behavior; Cattle; Deep learning; Spatio-temporal; Video","Animalia; Bos; Agriculture; Animals; Behavioral research; Activity recognition; Animal movement; Behavior recognition; Group activities; Portable device; Quantitative evaluation; Spatiotemporal information; Temporal features; algorithm; animal welfare; cattle; equipment; experimental study; livestock; spatiotemporal analysis; Deep learning","Ministry of Education, MOE, (2019R1A6A1A09031717); Ministry of Science, ICT and Future Planning, MSIP, (2020R1A2C2013060); National Research Foundation of Korea, NRF","This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2019R1A6A1A09031717 ). This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (2020R1A2C2013060).","D.S. Park; Division of Electronics and Information Engineering, Jeonbuk National University, Jeonju, South Korea; email: dspark@jbnu.ac.kr","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85089937288"
"Sarwar F.; Griffin A.; Periasamy P.; Portas K.; Law J.","Sarwar, Farah (57224466004); Griffin, Anthony (55708552200); Periasamy, Priyadharsini (57207941024); Portas, Kurt (57207937032); Law, Jim (57195127235)","57224466004; 55708552200; 57207941024; 57207937032; 57195127235","Detecting and Counting Sheep with a Convolutional Neural Network","2018","Proceedings of AVSS 2018 - 2018 15th IEEE International Conference on Advanced Video and Signal-Based Surveillance","","","8639306","","","","46","10.1109/AVSS.2018.8639306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063265445&doi=10.1109%2fAVSS.2018.8639306&partnerID=40&md5=aecdca64126dcdfbc0a69365815d6066","High Performance Computing Research Lab, New Zealand; Electrical and Electronic Engineering Department, Auckland University of Technology, New Zealand, New Zealand; Palliser Ridge Limited, New Zealand","Sarwar F., High Performance Computing Research Lab, New Zealand, Electrical and Electronic Engineering Department, Auckland University of Technology, New Zealand, New Zealand; Griffin A., High Performance Computing Research Lab, New Zealand, Electrical and Electronic Engineering Department, Auckland University of Technology, New Zealand, New Zealand; Periasamy P., Electrical and Electronic Engineering Department, Auckland University of Technology, New Zealand, New Zealand; Portas K., Palliser Ridge Limited, New Zealand; Law J., Palliser Ridge Limited, New Zealand","Counting livestock is generally done only during major events, such as drenching, shearing or loading, and thus farmers get stock numbers sporadically throughout the year. More accurate and timely stock information would enable farmers to manage their herds better. Additionally, prompt response to any stock in distress is extremely valuable, both in terms of animal welfare and the avoidance of financial loss. In this regard, the evolution of deep learning algorithms and Unmanned Aerial Vehicles (UAVs) is forging a new research area for remote monitoring and counting of different animal species under various climatic conditions. In this paper, we focus on detecting and counting sheep in a paddock from UAV video. Sheep are counted using a model based on Region-based Convolutional Neural Networks and the results are then compared with other techniques to evaluate their performance. © 2018 IEEE.","","Agriculture; Aircraft detection; Animals; Convolution; Deep learning; Losses; Neural networks; Unmanned aerial vehicles (UAV); Aerial vehicle; Animal species; Animal welfare; Climatic conditions; Convolutional neural network; Financial loss; Major events; Model-based OPC; Remote monitoring; Research areas; Antennas","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-153869294-3","","","English","Proc. AVSS - IEEE Int. Conf. Adv. Video Signal-Based Surveill.","Conference paper","Final","","Scopus","2-s2.0-85063265445"
"Hansen M.F.; Smith M.L.; Smith L.N.; Salter M.G.; Baxter E.M.; Farish M.; Grieve B.","Hansen, Mark F. (57192115769); Smith, Melvyn L. (55495905800); Smith, Lyndon N. (9237709400); Salter, Michael G. (57201273282); Baxter, Emma M. (23481382800); Farish, Marianne (23481626100); Grieve, Bruce (6602328968)","57192115769; 55495905800; 9237709400; 57201273282; 23481382800; 23481626100; 6602328968","Towards on-farm pig face recognition using convolutional neural networks","2018","Computers in Industry","98","","","145","152","7","254","10.1016/j.compind.2018.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044155039&doi=10.1016%2fj.compind.2018.02.016&partnerID=40&md5=a71cb23e87cb0ae6e5405f73f5410c68","Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; AB Agri Ltd., Innovation Way, Peterborough, United Kingdom; Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; School of Electrical and Electronic Engineering, University of Manchester, M13 9PL, United Kingdom","Hansen M.F., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Smith M.L., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Smith L.N., Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; Salter M.G., AB Agri Ltd., Innovation Way, Peterborough, United Kingdom; Baxter E.M., Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; Farish M., Animal Behaviour and Welfare, Animal and Veterinary Sciences Research Group, SRUC, West Mains Road, Edinburgh, EH9 3JG, United Kingdom; Grieve B., School of Electrical and Electronic Engineering, University of Manchester, M13 9PL, United Kingdom","Identification of individual livestock such as pigs and cows has become a pressing issue in recent years as intensification practices continue to be adopted and precise objective measurements are required (e.g. weight). Current best practice involves the use of RFID tags which are time-consuming for the farmer and distressing for the animal to fit. To overcome this, non-invasive biometrics are proposed by using the face of the animal. We test this in a farm environment, on 10 individual pigs using three techniques adopted from the human face recognition literature: Fisherfaces, the VGG-Face pre-trained face convolutional neural network (CNN) model and our own CNN model that we train using an artificially augmented data set. Our results show that accurate individual pig recognition is possible with accuracy rates of 96.7% on 1553 images. Class Activated Mapping using Grad-CAM is used to show the regions that our network uses to discriminate between pigs. © 2018 Elsevier B.V.","Biometrics; Convolutional neural network; Deep learning; Pig face recognition","Agriculture; Animals; Biometrics; Convolution; Deep learning; Mammals; Neural networks; Statistical tests; Augmented data set; Best practices; Convolutional neural network; Convolutional Neural Networks (CNN); Farm environments; Human face recognition; Identification of individuals; Objective measurement; Face recognition","Sustainable Agriculture Research & Innovation Club; Nvidia; Biotechnology and Biological Sciences Research Council, BBSRC, (BB/L017407/1); Natural Environment Research Council, NERC, (NE/P007945/1)","This research has been funded by Natural Environment Research Council, UK (Grant Reference: NE/P007945/1 ) as part of the Sustainable Agriculture Research & Innovation Club in collaboration with AB Agri. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Maxwell Titan X GPU used for this research.","M.F. Hansen; Centre for Machine Vision, BRL, UWE Bristol, Bristol, BS16 1QY, United Kingdom; email: mark.hansen@uwe.ac.uk","","Elsevier B.V.","01663615","","CINUD","","English","Comput Ind","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85044155039"
"Katamreddy S.; Riordan D.; Doody P.","Katamreddy, Sukumar (57195405395); Riordan, Daniel (55662731000); Doody, Pat (36711526700)","57195405395; 55662731000; 36711526700","Artificial calf weaning strategies and the role of machine learning: A review","2017","2017 28th Irish Signals and Systems Conference, ISSC 2017","","","7983634","","","","5","10.1109/ISSC.2017.7983634","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027875218&doi=10.1109%2fISSC.2017.7983634&partnerID=40&md5=515116686c2593633ea6aca7b1266d74","IMaR Technology Gateway, Institute of Technology Tralee Co, Kerry, Ireland","Katamreddy S., IMaR Technology Gateway, Institute of Technology Tralee Co, Kerry, Ireland; Riordan D., IMaR Technology Gateway, Institute of Technology Tralee Co, Kerry, Ireland; Doody P., IMaR Technology Gateway, Institute of Technology Tralee Co, Kerry, Ireland","Research in Machine Learning has increased dramatically over the past couple of decades, with applications greatly benefitting industry. These applications vary widely in the fields of space exploration technologies, transportation, robotics, agriculture, animal husbandry, medicine and finance. However, the agricultural and animal husbandry related applications are relatively few when compared to other fields. This paper reviews the past and present calf weaning strategies mainly focussing on the incorporation of Machine Learning techniques in classifying the behavioural responses in cattle and makes an attempt to extend the scope of machine learning classification applications related to calf weaning. This review further discusses the integration of relevant sensors (sensor fusion), deep learning and the improvement of prediction accuracy. © 2017 IEEE.","calf weaning; cattle behavioural classification; deep learning; livestock management; Machine Learning; sensor fusion; sensors","Agricultural machinery; Agriculture; Animals; Artificial intelligence; Deep learning; Sensors; Space research; Animal husbandry; calf weaning; Machine learning classification; Machine learning techniques; Past and present; Prediction accuracy; Sensor fusion; Space Exploration Technologies; Learning systems","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-153861046-6","","","English","Ir. Signals Syst. Conf., ISSC","Conference paper","Final","","Scopus","2-s2.0-85027875218"
"","","","2016 International Conference on Information Science and Communications Technologies, ICISCT 2016","2016","2016 International Conference on Information Science and Communications Technologies, ICISCT 2016","","","","","","155","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010589377&partnerID=40&md5=aa0d053ef1671cd28af13a933c245ed9","","","The proceedings contain 37 papers. The topics discussed include: block structure of data mining algorithms; features measurement parameters and control functioning of integrated chips; optimization detection of smiling and opening eyes in faces with algorithm LBP; provision of dispersed resources and services of the company based on service-oriented corporate information system; software defined networking: management of network resources and data flow; virtual fences for controlling livestock using satellite-tracking and warning signals; linearization spectral characteristics through passage by means of akustooptical reconstructed filters; method of calculating the distribution of light in a plane parallel to the light guide plate control devices; original educational devices and benches on basis of solar elements; and spark based distributed deep learning framework for big data applications.","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-150903546-5","","","English","Int. Conf. Inf. Sci. Commun. Technol., ICISCT","Conference review","Final","","Scopus","2-s2.0-85010589377"
"","","","Proceedings - 2018 IEEE International Conference on Internet of Things and Intelligence System, IOTAIS 2018","2018","Proceedings - 2018 IEEE International Conference on Internet of Things and Intelligence System, IOTAIS 2018","","","","","","249","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172551097&partnerID=40&md5=11aa0c6b64846a7f911ac3db08195afd","","","The proceedings contain 38 papers. The topics discussed include: IoT device fingerprint using deep learning; personal authentication and hand motion recognition based on wrist EMG analysis by a convolutional neural network; IoT-based water quality monitoring system for soft-shell crab farming; automatic vehicle license plate recognition system for smart transportation; digital twin for energy optimization in an SMT-PCB assembly line; IoT based poultry environment monitoring system; mobile LoRa gateway for smart livestock monitoring system; authentication of aerial input numerals by leap motion and CNN; and analog ultra low-power acoustic wake-up system based on frequency detection.","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","978-153867358-4","","","English","Proc. - IEEE Int. Conf. Internet Things Intell. Syst., IOTAIS","Conference review","Final","","Scopus","2-s2.0-85172551097"
"Kumar S.; Pandey A.; Sai Ram Satwik K.; Kumar S.; Singh S.K.; Singh A.K.; Mohan A.","Kumar, Santosh (59511763600); Pandey, Amit (57212284906); Sai Ram Satwik, K. (57196413504); Kumar, Sunil (59355783200); Singh, Sanjay Kumar (57413852300); Singh, Amit Kumar (58345761200); Mohan, Anand (57203177243)","59511763600; 57212284906; 57196413504; 59355783200; 57413852300; 58345761200; 57203177243","Deep learning framework for recognition of cattle using muzzle point image pattern","2018","Measurement: Journal of the International Measurement Confederation","116","","","1","17","16","153","10.1016/j.measurement.2017.10.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032981373&doi=10.1016%2fj.measurement.2017.10.064&partnerID=40&md5=c5c9b0daddbbc6846a322aea761fc8d9","Computer Science and Engineering, Dr. Shyama Prasad Mukhurjee, IIIT Naya Raipur, 493661, Chhattisgarh, India; Department of Electrical Engineering, Indian Institute of Technology (B. H. U.), Varanasi, 221005, Uttar Pradesh, India; Indian Maritime University – Kolkata Campus, 700088, India; Department of Computer Science and Engineering, Indian Institute of Technology (BHU), Varanasi, 221005, Uttar Pradesh, India; Department of Computer Science and Engineering, Jaypee University of Information Technology, Solan, 173234, Himachal Pradesh, India; Department of Electronics Engineering, Indian Institute of Technology (BHU), Varanasi, 221005, Uttar Pradesh, India","Kumar S., Computer Science and Engineering, Dr. Shyama Prasad Mukhurjee, IIIT Naya Raipur, 493661, Chhattisgarh, India; Pandey A., Department of Electrical Engineering, Indian Institute of Technology (B. H. U.), Varanasi, 221005, Uttar Pradesh, India; Sai Ram Satwik K., Department of Electrical Engineering, Indian Institute of Technology (B. H. U.), Varanasi, 221005, Uttar Pradesh, India; Kumar S., Indian Maritime University – Kolkata Campus, 700088, India; Singh S.K., Department of Computer Science and Engineering, Indian Institute of Technology (BHU), Varanasi, 221005, Uttar Pradesh, India; Singh A.K., Department of Computer Science and Engineering, Jaypee University of Information Technology, Solan, 173234, Himachal Pradesh, India; Mohan A., Department of Electronics Engineering, Indian Institute of Technology (BHU), Varanasi, 221005, Uttar Pradesh, India","Animal biometrics is a frontier area of computer vision, pattern recognition and cognitive science to plays the vital role for the registration, unique identification, and verification of livestock (cattle). The existing handcrafted texture feature extraction and appearance based feature representation techniques are unable to perform the animal recognition in the unconstrained environment. Recently deep learning approaches have achieved more attention for recognition of species or individual animal using visual features. In this research, we propose the deep learning based approach for identification of individual cattle based on their primary muzzle point (nose pattern) image pattern characteristics to addressing the problem of missed or swapped animals and false insurance claims. The major contributions of the work as follows: (1) preparation of muzzle point image database, which are not publically available, (2) extraction of the salient set of texture features and representation of muzzle point image of cattle using the deep learning based convolution neural network, deep belief neural network proposed approaches. The stacked denoising auto-encoder technique is applied to encode the extracted feature of muzzle point images and (3) experimental results and analysis of proposed approach. Extensive experimental results illustrate that the proposed deep learning approach outperforms state-of-the-art methods for recognition of cattle on muzzle point image database. The efficacy of the proposed deep learning approach is computed under different identification settings. With multiple test galleries, rank-1 identification accuracy of 98.99% is achieved. © 2017 Elsevier Ltd","Cattle recognition; Computer vision; Convolution Neural Network; DBN; Deep learning; LBP; LDA; Muzzle point image; PCA; SDAE; SURF; Verification; VLAD","Agriculture; Animals; Computer vision; Convolution; Deep learning; Deep neural networks; Extraction; Insurance; Pattern recognition; Verification; Cattle recognition; Convolution neural network; Muzzle point image; SDAE; SURF; VLAD; Image processing","","","S. Kumar; Computer Science and Engineering, Dr. Shyama Prasad Mukhurjee, IIIT Naya Raipur, 493661, India; email: santosh@iiitnr.edu.in","","Elsevier B.V.","02632241","","MSRMD","","English","Meas J Int Meas Confed","Article","Final","","Scopus","2-s2.0-85032981373"
"Santoni M.M.; Sensuse D.I.; Arymurthy A.M.; Fanany M.I.","Santoni, Mayanda Mega (55625597700); Sensuse, Dana Indra (53264793000); Arymurthy, Aniati Murni (36815724000); Fanany, Mohamad Ivan (6505757323)","55625597700; 53264793000; 36815724000; 6505757323","Cattle Race Classification Using Gray Level Co-occurrence Matrix Convolutional Neural Networks","2015","Procedia Computer Science","59","","","493","502","9","46","10.1016/j.procs.2015.07.525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948409676&doi=10.1016%2fj.procs.2015.07.525&partnerID=40&md5=cab0e08e5c2e3acbb83e38a11bb84306","Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia","Santoni M.M., Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia; Sensuse D.I., Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia; Arymurthy A.M., Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia; Fanany M.I., Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia","In e-Livestock management system, practical and accurate cattle race identification is paramount. This paper presents a cattle race identification system from their images. We propose a deep learning architecture, which is called as Gray Level Co-occurrence Matrix Convolutional Neural Networks (GLCM-CNN), to semi-unsupervisedly identify a cattles race given thousands of its images with complex background settings. We introduce and evaluate GLCM features, i.e., contrast, energy, and homogeneity into CNN learning for GLCMs capability to recognize pattern with diverse variations, robustness to geometric distortion, and simple transformation. Our experiments show that GLCM-CNN is gives higher classification accuracy and requires less number of learning iterations than the original CNN. In our approach, the data input layer has better distinguishing features than the original image. In addition, our method does not require any prior segmentation process. In this paper, we also address the reduction of computation overhead using saliency maps. © 2015 Published by Elsevier B.V.","Cattle; Convolutional Neural Networks (CNN); e-Livestock Indonesia; Gray Level Co-occurrence Matrix (GLCM)","Agriculture; Artificial intelligence; Cobalt compounds; Complex networks; Convolution; Mathematical transformations; Neural networks; Cattle; Classification accuracy; Computation overheads; Convolutional neural network; Geometric distortion; Gray level co occurrence matrix(GLCM); Gray level co-occurrence matrix; Indonesia; Matrix algebra","","","M.M. Santoni; Faculty of Computer Science, University of Indonesia, Depok, West-Java, 16424, Indonesia; email: mayandamega@ui.ac.id","Budiharto W.","Elsevier","18770509","","","","English","Procedia Comput. Sci.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84948409676"
"Zheng C.; Zhu X.; Yang X.; Wang L.; Tu S.; Xue Y.","Zheng, Chan (36629646600); Zhu, Xunmu (57200724080); Yang, Xiaofan (57200723915); Wang, Lina (57209578944); Tu, Shuqin (56472292000); Xue, Yueju (12241464400)","36629646600; 57200724080; 57200723915; 57209578944; 56472292000; 12241464400","Automatic recognition of lactating sow postures from depth images by deep learning detector","2018","Computers and Electronics in Agriculture","147","","","51","63","12","132","10.1016/j.compag.2018.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042304605&doi=10.1016%2fj.compag.2018.01.023&partnerID=40&md5=91e2286baeeba32d7063739fa0db578f","College of Mathematics and Informatics, South China Agricultural University, China; College of Electronic Engineering, South China Agricultural University, China; College of Animal Science, South China Agricultural University, China; Guangdong Engineering Research Center for Datamation of Modern Pig Production, Guangzhou, China; Guangdong Engineering Research Center for Information Monitoring in Agriculture, Guangzhou, China","Zheng C., College of Mathematics and Informatics, South China Agricultural University, China, College of Electronic Engineering, South China Agricultural University, China; Zhu X., College of Electronic Engineering, South China Agricultural University, China; Yang X., College of Electronic Engineering, South China Agricultural University, China; Wang L., College of Animal Science, South China Agricultural University, China; Tu S., College of Mathematics and Informatics, South China Agricultural University, China; Xue Y., College of Electronic Engineering, South China Agricultural University, China, Guangdong Engineering Research Center for Datamation of Modern Pig Production, Guangzhou, China, Guangdong Engineering Research Center for Information Monitoring in Agriculture, Guangzhou, China","The behaviors of livestock on farms are the primary representatives of animal welfare, health conditions and social interactions. Measuring behavior quantitatively in an automatic detection system on computer vision provides valuable behavioral information in an efficient and noninvasive way compared with manual observations or sensing techniques. Lactating sow postures, which are the crucial indicator of maternal evaluation, provide fundamental information for studying the maternal behavioral characteristics and regularities. We introduce a detector, Faster R-CNN, on deep learning framework to identify five postures (standing, sitting, sternal recumbency, ventral recumbency and lateral recumbency) and obtain sows accurate location in loose pens. The detection system consists of a Kinect v2 sensor that acquires depth images and a program that identifies sow postures and locates its bounding-boxes. The depth images of testing dataset of a sow were acquired at 5 frames per second in 24 h on the 15th day of postpartum, and training dataset were collected by some different sows. Since the identification performance from RGB images are impacted by the color and illumination variations caused by in-situ heat lamp and day-night cycle, we show that the automatic detection from depth images could avoid disturbances of the light. We find that the sow spent greater amount of time in recumbency (92.9% at night and 84.1% during the daytime) as compared with standing (0.4% at night and 10.5% during the daytime) and sitting (0.55% at night and 3.4% during the daytime). Statistically, the sow's activity level is non-uniform in 24-h of a day, and her preferred lying positions is accordant with the pen's floor design. The posture's change frequency and average duration are presented. From the estimated general manner of posture change, we find that the sow takes more time in descending body than ascending, which could be a favorable indication of maternal ability with a slow-motion falling to avoid crushing piglets. © 2018 Elsevier B.V.","Behavior identification; Deep learning; Depth image; Faster R-CNN; Loose pen; Posture detection","Animalia; Agriculture; Image processing; Statistical tests; Behavior identifications; Depth image; Faster R-CNN; Loose pen; Posture detection; Deep learning","Ministry of Science and Technology of China, (2015BAD06B03-3); National Key Technology Research and Development Program; Guangzhou Municipal Science and Technology Project, (201604016122, 201605030013); Guangzhou Municipal Science and Technology Project; Science and Technology Planning Project of Guangdong Province, (2014A020208108, 2015A020209148, 2015A020224038); Science and Technology Planning Project of Guangdong Province","We acknowledge Lejiazhuang Pig Farm for using their animals and facilities. We also thank Ren et al.’s code ( https://github.com/ShaoqingRen/faster_rcnn ) from which the code was extended for this project, and the Caffe framework (Jia et al., 2014) adopted to train the deep learning models. This study is sponsored by Science and Technology Planning Project of Guangdong Province, China (grants 2015A020209148, 2014A020208108 and 2015A020224038 ), the National Key Technology Research and Development Program of the Ministry of Science and Technology of China (grants 2015BAD06B03-3 ), and the Guangzhou Municipal Science and Technology Planning Project (grants 201605030013 , 201604016122 ).  ","Y. Xue; College of Electronic Engineering, South China Agricultural University, China; email: xueyueju@163.com","","Elsevier B.V.","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85042304605"
